@incollection{BHATE2024165,
title = {Chapter 7 - Tissue schematics: Representing tissues as assemblies of neighborhoods},
editor = {Wendy J. Fantl},
booktitle = {Revealing Uncharted Biology With Single Cell Multiplex Proteomic Technologies},
publisher = {Academic Press},
pages = {165-189},
year = {2024},
isbn = {978-0-12-822209-6},
doi = {https://doi.org/10.1016/B978-0-12-822209-6.00005-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128222096000059},
author = {Salil S. Bhate and Graham L. Barlow and Garry P. Nolan},
keywords = {Cellular neighborhoods, Spatial biology, Tissue schematics, Tumor microenvironment},
abstract = {The technologies discussed in previous chapters provide an unprecedented lens on single cells, phenotypes, and their interactions. In this chapter, we discuss tissue schematics (as introduced in Bhate et al., 2021): a computational and conceptual framework for using highly multiplexed imaging data to infer and visually represent how complex, tissue-level behaviors are generated through the composition of simpler, interpretable biological processes. We focus on the practical application of tissue schematics for generating biological understanding of tissues from imaging data.}
}
@article{KRISHNANUNNI2025117938,
title = {An adaptive and stability-promoting layerwise training approach for sparse deep neural network architecture},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {441},
pages = {117938},
year = {2025},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2025.117938},
url = {https://www.sciencedirect.com/science/article/pii/S0045782525002105},
author = {C.G. Krishnanunni and Tan Bui-Thanh},
keywords = {Architecture adaptation, Manifold regularization, Stability-promoting algorithm, Sequential learning},
abstract = {This work presents a two-stage adaptive framework for progressively developing deep neural network (DNN) architectures that generalize well for a given training data set. In the first stage, a layerwise training approach is adopted where a new layer is added each time and trained independently by freezing parameters in the previous layers. We impose desirable structures on the DNN by employing manifold regularization, sparsity regularization, and physics-informed terms. We introduce a ɛ−δ− stability-promoting concept as a desirable property for a learning algorithm and show that employing manifold regularization yields a ɛ−δ stability-promoting algorithm. Further, we also derive the necessary conditions for the trainability of a newly added layer and investigate the training saturation problem. In the second stage of the algorithm (post-processing), a sequence of shallow networks is employed to extract information from the residual produced in the first stage, thereby improving the prediction accuracy. Numerical investigations on prototype regression and classification problems demonstrate that the proposed approach can outperform fully connected DNNs of the same size. Moreover, by equipping the physics-informed neural network (PINN) with the proposed adaptive architecture strategy to solve partial differential equations, we numerically show that adaptive PINNs not only are superior to standard PINNs but also produce interpretable hidden layers with provable stability. We also apply our architecture design strategy to solve inverse problems governed by elliptic partial differential equations.}
}
@article{GAESSER2020104325,
title = {Episodic mindreading: Mentalizing guided by scene construction of imagined and remembered events},
journal = {Cognition},
volume = {203},
pages = {104325},
year = {2020},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2020.104325},
url = {https://www.sciencedirect.com/science/article/pii/S001002772030144X},
author = {Brendan Gaesser},
keywords = {Episodic simulation, Memory, Scene construction, Mentalizing, Theory of mind, Perspective taking, Social cognition, Morality},
abstract = {Attributing mental states to other people fundamentally shapes how we bond, coordinate, and predict the actions of others. Perceiving a person's facial expressions and body language in the present contribute to our ability to understand what they are thinking and feeling. Yet, people do not exist in a vacuum and individuals often think about people who are not directly in front of them. People inhabit remembered and imagined episodes, where the surrounding location and objects can guide attributions of their mental states. In this article, I propose the episodic mindreading hypothesis, arguing that the episodic representation of past and future events in which a target person is embedded will affect whether and how the target's mind is read. The content and phenomenological quality of imagined and remembered episodes can alter what mental states are attributed to a target and the accessibility of those mental states. This hypothesis encourages researchers to think about mentalizing as neither dependent on nor completely exclusive from the episodic memory system. Instead, the episodic memory system can modulate and inform mindreading, and likely vice versa. The article reviews extant knowledge and highlights open questions for future research to explore with implications for healthy and impaired social cognition.}
}
@article{GOUTAUDIER2021113755,
title = {Proper Generalized Decomposition with time adaptive space separation for transient wave propagation problems in separable domains},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {380},
pages = {113755},
year = {2021},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2021.113755},
url = {https://www.sciencedirect.com/science/article/pii/S0045782521000918},
author = {Dimitri Goutaudier and Laurent Berthe and Francisco Chinesta},
keywords = {Proper Generalized Decomposition (PGD), Transient wave propagation, Time adaptive space separation, Separable domain, Scalar wave equation, Elastodynamics},
abstract = {Transient wave propagation problems may involve rich discretizations, both in space and in time, leading to computationally expensive simulations, even for simple spatial domains. The Proper Generalized Decomposition (PGD) is an attractive model order reduction technique to address this issue, especially when the spatial domain is separable. In this work, we propose a space separation with a time adaptive number of modes to efficiently capture transient wave propagation in separable domains. We combine standard time integration schemes with this original space separated representation for empowering standard procedures. The numerical behavior of the proposed method is explored through several 2D wave propagation problems involving radial waves, propagation on long time analyses, and wave conversions. We show that the PGD solution approximates its standard finite element solution counterpart with acceptable accuracy, while reducing the storage needs and the computation time (CPU time). Numerical results show that the CPU time per time step linearly increases when refining the mesh, even with implicit time integration schemes, which is not the case with standard procedures.}
}
@article{MURAMATSU2005201,
title = {Emotions as a mechanism for boundedly rational agents: The fast and frugal way},
journal = {Journal of Economic Psychology},
volume = {26},
number = {2},
pages = {201-221},
year = {2005},
issn = {0167-4870},
doi = {https://doi.org/10.1016/j.joep.2004.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167487004000285},
author = {Roberta Muramatsu and Yaniv Hanoch},
keywords = {Bounded rationality, Emotion, Evolution, Mechanism},
abstract = {Herbert Simon has warned us that an explanatory account of human rationality must identify the significance of emotions for choice behavior. Customarily emphasizing the cognitive dimensions of decision making, relatively few researchers have paid close attention to specifying the complex ways in which emotion may shape human thinking and decisions. Accordingly, this paper is an attempt to follow Simon's suggestion and specify how emotions can enter into the theory of bounded rationality. To accomplish our task, we capitalize on Rom Harré's work on causal powers, from which we propose a strategy to study the significance of emotion in decision-making processes. In an attempt to elaborate on an explanation of behavior by mechanism, we discuss a version of bounded rationality recently put forward by Gigerenzer, Todd, and the ABC Research Group [Simple Heuristics that Make us Smart, Oxford University Press, New York, 1999] and Gigerenzer and Selten [Bounded Rationality: The Adaptive Toolbox, MIT Press, Cambridge, MA, 2001, pp. 1–12], the so-called adaptive toolbox of fast and frugal heuristics. Coupled with insights from evolutionary psychology and neuroscience, this version of bounded rationality gives us a better grasp of the functional role of emotions within the human decision machinery.}
}
@article{ARS2021102805,
title = {Underground ancient mine work ventilation modeling},
journal = {Journal of Archaeological Science: Reports},
volume = {37},
pages = {102805},
year = {2021},
issn = {2352-409X},
doi = {https://doi.org/10.1016/j.jasrep.2021.102805},
url = {https://www.sciencedirect.com/science/article/pii/S2352409X21000171},
author = {Christophe Ars and Joseph Gauthier and Nicolas Florsch},
keywords = {Mining archaeology, Ventilation system, Numerical modeling, Computational fluid dynamics, Air quality, Sainte-Marie-aux-Mines},
abstract = {Excavations at the Giro mine located in the commune of Sainte-Marie-aux-Mines (France) have revealed a metallic and cylindrical artifact that resembles a connecting element for wooden duct sections. Early modern literature, and especially De Re Metallica, mention such technologies intended in particular to force the ventilation of underground mines in which air quality was harmful for miners. The connecting element was found in a gallery leading to a stope of which ventilation seems problematic. The numerical simulation of the air flow in the tunnel makes it possible to test the ventilation hypotheses formulated from archaeological data. These simulations are performed with OpenFOAM, a free and open source software for computational fluid dynamics (CFD). Simulating not only the air flow, but also the heat and CO2 production of five miners at work highlights the need to force ventilation in the underground with a ventilation system. It also appears that the construction of scaffolding in the stope can fulfill the double function of facilitating circulation and improving ventilation. This first quantitative approach to one of the main obstacles to mining offers a new method for testing the solutions implemented by miners of the past.}
}
@article{FAVELA2019156,
title = {Editor’s introduction: Innovative dynamical approaches to cognitive systems},
journal = {Cognitive Systems Research},
volume = {58},
pages = {156-159},
year = {2019},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2019.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041719303389},
author = {Luis H. Favela},
keywords = {Cognition, Cognitive systems, Dynamical systems theory},
abstract = {This Editor’s Introduction to the Cognitive Systems Research special issue, “Innovative Dynamical Approaches to Cognitive Systems,” has three aims: First, the background and motivation for the topic are stated. Second, overviews of the contributing papers are presented. Third, based on the papers, speculations on future directions in dynamical approaches to the investigation of cognitive systems are presented. Here, the focus is on concepts, data analysis methods, and computational modeling.}
}
@article{JAHANIYEKTA2024100078,
title = {The general intelligence of GPT–4, its knowledge diffusive and societal influences, and its governance},
journal = {Meta-Radiology},
volume = {2},
number = {2},
pages = {100078},
year = {2024},
issn = {2950-1628},
doi = {https://doi.org/10.1016/j.metrad.2024.100078},
url = {https://www.sciencedirect.com/science/article/pii/S2950162824000316},
author = {Mohammad Mahdi {Jahani Yekta}},
keywords = {GPT–4, Artificial general intelligence, Knowledge diffusion, Interpretability and explainability, Societal influences, Governance},
abstract = {Recent breakthroughs in artificial intelligence (AI) research include advancements in natural language processing (NLP) achieved by large language models (LLMs), and; in particular, generative pre–trained transformer (GPT) architectures. The latest GPT developed by OpenAI, GPT–4, has shown remarkable intelligence across various domains and tasks. It exhibits capabilities in abstraction, comprehension, vision, computer coding, mathematics, and more, suggesting it to be a significant step towards artificial general intelligence (AGI), a level of AI that possesses capabilities similar to human intelligence. This paper explores this AGI, its knowledge diffusive and societal influences, and its governance. In addition to coverage of the major associated topics studied in the literature, and making up for their loopholes, we scrutinize how GPT-4 can facilitate the diffusion of knowledge across different areas of science by promoting their interpretability and explainability (IE) to inexperts. Where applicable, the topics are also accompanied by their specific potential implications on medical imaging.}
}
@article{GROOS2024105420,
title = {Combining user-centered design and behavioral theory to enhance health technologies: A personas-based approach for a primary-care based multifactorial falls risk assessment tool},
journal = {International Journal of Medical Informatics},
volume = {186},
pages = {105420},
year = {2024},
issn = {1386-5056},
doi = {https://doi.org/10.1016/j.ijmedinf.2024.105420},
url = {https://www.sciencedirect.com/science/article/pii/S1386505624000832},
author = {Sara S. Groos and Annemiek J. Linn and Judith I. Kuiper and Natasja M. {van Schoor} and Nathalie {van der Velde} and Julia C.M. {van Weert}},
keywords = {Personas, User-centered design, Behavioral theory, Multifactorial falls risk assessment tools, Implementation},
abstract = {Introduction
Multifactorial falls risk assessment tools (FRATs) can be an effective falls prevention method for older adults, but are often underutilized by health care professionals (HCPs). This study aims to enhance the use and implementation of multifactorial FRATs by combining behavioral theory with the user-centered design (UCD) method of personas construction. Specifically, the study aimed to (1) construct personas that are based on external (i.e., needs, preferences) and intrinsic user characteristics (i.e., behavioral determinants); and (2) use these insights to inform requirements for optimizing an existing Dutch multifactorial FRAT (i.e., the ‘Valanalyse’).
Methods
Survey data from HCPs (n = 31) was used to construct personas of the ‘Valanalyse.’ To examine differences between clusters on 68 clustering variables, a multivariate cluster analysis technique with non-parametric analyses and computational methods was used. The aggregated external and intrinsic user characteristics of personas were used to inform key design and implementation requirements for the ‘Valanalyse,’ respectively, whereby intrinsic user characteristics were matched with appropriate behavior change techniques to guide implementation.
Results
Significant differences between clusters were observed in 20 clustering variables (e.g., behavioral beliefs, situations for use). These variables were used to construct six personas representing users of each cluster. Together, the six personas helped operationalize four key design requirements (e.g., guide treatment-related decision making) and 14 implementation strategies (e.g., planning coping responses) for optimizing the ‘Valanalyse’ in Dutch geriatric, primary care settings.
Conclusion
The findings suggest that theory- and evidence-based personas that encompass both external and intrinsic user characteristics are a useful method for understanding how the use and implementation of multifactorial FRATs can be optimized with and for HCPs, providing important implications for developers and eHealth interventions with regards to encouraging technology adoption.}
}
@article{BOYCE2004565,
title = {Critical accounting education: teaching and learning outside the circle},
journal = {Critical Perspectives on Accounting},
volume = {15},
number = {4},
pages = {565-586},
year = {2004},
note = {A Critical Response to Managerialism in the Academy},
issn = {1045-2354},
doi = {https://doi.org/10.1016/S1045-2354(03)00047-9},
url = {https://www.sciencedirect.com/science/article/pii/S1045235403000479},
author = {Gordon Boyce},
keywords = {Critical accounting, Corporate university, Accounting research, Accounting education, Intellectuals},
abstract = {The development of the corporate university is an element in the suite of “economically rational” public policy changes promulgated in recent decades. Working from a position that the practice of accounting is centrally implicated in these changes, it is contended in this paper that accounting, and accounting education, can in fact play a part in challenging these positions. Extant accounting research is sufficiently well-developed such that we are aware of the conflicts and contradictions both within accounting and flowing from the practice of the discipline, yet the effect of this body of knowledge on the content of teaching and learning within the accounting classroom remains limited. By and large, accounting education continues to be constrained within narrowly defined, but mis-conceived, disciplinary boundaries, focusing on the techniques and “skills” of accounting practice. In outlining a case for broadening the accounting education curriculum, this paper adopts the heuristic of “tangential thinking” as a means of transcending narrowly constructed disciplinary boundaries. In doing so, it is suggested that accounting education reform needs to go well beyond the putative reform agenda of the organised professional accounting bodies. The exploration of tangents lead to areas of knowledge that initially seem to be outside of accounting, but which nevertheless have an integral connection to the realities of the practice of the discipline. The paper outlines a case for tangential thinking in teaching and learning activities in the accounting classroom, within extant accreditation and curricular arrangements. Teaching and learning “outside the circle” in this manner is suggested as a way to make accounting education relevant in its socio-historical context and, particularly, relevant to the lived experience of students.}
}
@incollection{YUCESOY2024,
title = {Systems Biology in Immunotoxicology},
booktitle = {Reference Module in Biomedical Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-12-801238-3},
doi = {https://doi.org/10.1016/B978-0-323-95488-4.00046-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323954884000462},
author = {Berran Yucesoy and Randle Gallucci},
keywords = {Biological networks, Biomarkers, Epigenetics, Genetics, Immune system, Immunotoxicology, Proteomics, Risk assessment, System biology, Transcriptomics},
abstract = {Systems biology is an emerging field that focuses on the interactions between the components of biological systems. In the past two decades, high-throughput, large-scale molecular biology approaches (omics technologies) and advances in computational approaches have significantly grown and provided valuable insight into disease mechanisms, underlying toxicities, and gene–environment interactions. Systems biology approaches have also been widely used in immunotoxicology and helped understand the structure and function of immune system at multiple levels. Integrative models of the human immune response allowed for the assessment of complex immune effects, multilevel networks of interactions, identification of biomarkers and extrapolation of early molecular/cellular events to long-term outcomes at the organism level. Such efforts also created a potential for more predictive and accurate risk-assessment strategies. This chapter focuses on systems biology approaches and computational tools used in immunotoxicology and discuss their application in risk assessment.}
}
@article{SCHROEDER2015530,
title = {Situated phenomenology and biological systems: Eastern and Western synthesis},
journal = {Progress in Biophysics and Molecular Biology},
volume = {119},
number = {3},
pages = {530-537},
year = {2015},
note = {Integral Biomathics: Life Sciences, Mathematics, and Phenomenological Philosophy},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2015.06.019},
url = {https://www.sciencedirect.com/science/article/pii/S0079610715000942},
author = {Marcin J. Schroeder and Jordi Vallverdú},
keywords = {Phenomenology, Experience, Situated cognition, Eastern, Western, Cybernetics, Robotics, System, Biology},
abstract = {Phenomenology was born with the mission to give foundations for science of experience and to open consciousness to scientific study. The influence of phenomenology initiated in the works of Husserl and continued in a wide range of works of others was immense, but mainly within the confines of philosophy and the humanities. The actual attempts to develop a scientific discipline of the study of consciousness and to carry out research on cognition and consciousness were always based on the methods of traditional science in which elimination of the subjective has been always a primary tenet. Thus, focus was mainly on neurological correlates of conscious phenomena. The present paper is an attempt to initiate an extension and revision of phenomenological methodology with the use of philosophical and scientific experience and knowledge accumulated in a century of inquiry and research in relevant disciplines. The question which disciplines are relevant is crucial and our answer is innovative. The range of disciplines involved here is from information science and studies of computation, up to cultural psychology and the studies of philosophical traditions of the East. Concepts related to information and computation studies provide a general conceptual framework free from the limitations of particular languages and of linguistic analysis. This conceptual framework is extending the original perspective of phenomenology to issues of modern technology and science. Cultural psychology gives us tools to root out what in phenomenology was considered universal for humanity, but was a result of European ethnocentrism. Most important here is the contrast between individualistic and collectivistic cultural determinants of consciousness. Finally, philosophical tradition of the East gives alternatives in seeking solutions for fundamental problems. This general outline of the research methodology is illustrated by an example of its use when phenomenology is studied within the conceptual framework of information.}
}
@article{HAN20212821,
title = {Artificial protein assemblies with well-defined supramolecular protein nanostructures},
journal = {Biochemical Society Transactions},
volume = {49},
number = {6},
pages = {2821-2830},
year = {2021},
issn = {1470-8752},
doi = {https://doi.org/10.1042/BST20210808},
url = {https://www.sciencedirect.com/science/article/pii/S1470875221001033},
author = {Suyeong Han and Yongwon Jung},
keywords = {protein assembly, protein engineering, protein nanostructures},
abstract = {Nature uses a wide range of well-defined biomolecular assemblies in diverse cellular processes, where proteins are major building blocks for these supramolecular assemblies. Inspired by their natural counterparts, artificial protein-based assemblies have attracted strong interest as new bio-nanostructures, and strategies to construct ordered protein assemblies have been rapidly expanding. In this review, we provide an overview of very recent studies in the field of artificial protein assemblies, with the particular aim of introducing major assembly methods and unique features of these assemblies. Computational de novo designs were used to build various assemblies with artificial protein building blocks, which are unrelated to natural proteins. Small chemical ligands and metal ions have also been extensively used for strong and bio-orthogonal protein linking. Here, in addition to protein assemblies with well-defined sizes, protein oligomeric and array structures with rather undefined sizes (but with definite repeat protein assembly units) also will be discussed in the context of well-defined protein nanostructures. Lastly, we will introduce multiple examples showing how protein assemblies can be effectively used in various fields such as therapeutics and vaccine development. We believe that structures and functions of artificial protein assemblies will be continuously evolved, particularly according to specific application goals.}
}
@article{SCHWARZ2014283,
title = {On computing time-to-collision for automation scenarios},
journal = {Transportation Research Part F: Traffic Psychology and Behaviour},
volume = {27},
pages = {283-294},
year = {2014},
note = {Vehicle Automation and Driver Behaviour},
issn = {1369-8478},
doi = {https://doi.org/10.1016/j.trf.2014.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S1369847814000898},
author = {Chris Schwarz},
keywords = {TTC, Automation, Computational methods, Algorithms, Computational geometry},
abstract = {Time to collision (TTC) has been a key vehicle safety metric for decades. With the increasing prevalence of advanced driver assistance systems and vehicle automation, TTC and many related metrics are being applied to the analysis of more complicated scenarios, as well as being integrated into automation algorithms. While the TTC metric was originally conceived to be inclusive of generic two-dimensional situations, its applications has been mostly limited to one-dimensional scenarios. This paper derives general equations and algorithms using two-dimensional information. Additionally, methods from computational geometry, a field that didn’t exist when TTC was first used, are employed for the general case of computing TTC between bounding boxes. Parametric equations for lines play a prominent role and offer an elegant way to express the geometry of the scenarios described in this paper. Throughout, the approach is not to derive specific algebraic conditions as in previous efforts. Rather, the focus in on developing general algorithms for computation. The techniques presented are not necessary for traditional car following scenarios; but offer options for more complex situations that trade off analytic solutions for computational flexibility.}
}
@article{PANDEY2018141,
title = {Understanding the mechanics of creep deformation to develop a surrogate model for contact assessment in CANDU® fuel channels},
journal = {Nuclear Engineering and Design},
volume = {330},
pages = {141-156},
year = {2018},
issn = {0029-5493},
doi = {https://doi.org/10.1016/j.nucengdes.2018.01.032},
url = {https://www.sciencedirect.com/science/article/pii/S0029549318300323},
author = {M.D. Pandey and F.J. Tallavo and N.C. Christodoulou and B. Leitch and G.A. Bickel},
keywords = {Fuel channel, Creep deformation, Finite element method, Surrogate model, Pressure tube, Calandria tube, Contact assessment, Zirconium alloy, Probabilistic assessments},
abstract = {A key element of the fuel channel life cycle management in CANDU® reactors is to prevent contact between the pressure tube (PT) and the calandria tube (CT) and to avoid the development of hydride blisters that lead to delayed hydride cracking of the PT. The PT-CT contact is the result of in-reactor deformation due to irradiation induced creep of the fuel channel assembly, which in turn is affected by uncertainties associated with various parameters like material properties, dimensional changes in the channel and boundary conditions (e.g., end slopes) of the channel. To account for these uncertainties, probabilistic assessment methods are developed to evaluate the risk of PT-CT contact and demonstrate compliance with provisions of the CSA Standard N285.8. Currently, a simulation is based on probabilistic assessments in which input parameters to a finite element model (FEM) of creep deformation are randomly sampled from their respective distributions. A simulation model involves numerous repetitive solutions of the FEM model to determine the probability distribution of PT-CT gap and the time to contact. Since the creep deformation analysis using FEM is computationally involved, this brute force Monte Carlo simulation method is not an efficient way to carry out the probabilistic assessment of the reactor core. This paper proposes a new line of thinking for probabilistic assessments of PT-CT contact in fuel channels, which is based on replacing a full FEM model by a surrogate model of a much simpler analytical form. The surrogate model not only simplifies the creep deformation analysis, but also provides a more logical basis for probabilistic assessments. This paper presents an insightful analysis of creep deformation and shows that a simple surrogate model can be developed to predict the PT-CT gap as a linear function of two primary random variables, namely, a creep factor and end slopes. This simplified representation has a far reaching effect on the probabilistic assessment of fuel channels.}
}
@article{LAVIGNE2007630,
title = {Statistical reasoning of middle school children engaged in survey inquiry},
journal = {Contemporary Educational Psychology},
volume = {32},
number = {4},
pages = {630-666},
year = {2007},
issn = {0361-476X},
doi = {https://doi.org/10.1016/j.cedpsych.2006.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0361476X06000488},
author = {Nancy C. Lavigne and Susanne P. Lajoie},
keywords = {Statistical reasoning, Inquiry, Mathematics education, Middle school, Thinking, Cognition},
abstract = {The case study examined two groups of grade 7 students as they engaged in four inquiry phases: posing a question and collecting, analyzing, and representing data. Previous studies reported analyses of statistical reasoning on a single inquiry phase. Our goal was to identify the modes of statistical reasoning displayed during group discussions in all phases as children designed and conducted their own inquiry. A content analysis of audio and video recorded discussions yielded 10 statistical reasoning modes: six relate to Garfield and Gal’s [Garfield, J., Gal, I. (1999). Teaching and assessing statistical reasoning. In L. V. Stiff, & F. R. Curcio (Eds.), Developing mathematical reasoning in grades K-12. 1999 Yearbook (pp. 207–219). Reston, VA: National Council of Teachers of Mathematics] statistical reasoning types involved in the collection, analysis, and representation of data and four modes deal with an aspect of inquiry not exclusively focused upon in the literature on statistical reasoning—i.e., the problem-posing phase. Although students’ reasoning reflected an incomplete understanding of statistics they serve as building blocks for instruction.}
}
@article{CORREA2008140,
title = {Connected and culturally embedded beliefs: Chinese and US teachers talk about how their students best learn mathematics},
journal = {Teaching and Teacher Education},
volume = {24},
number = {1},
pages = {140-153},
year = {2008},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2006.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X06001715},
author = {Christopher A. Correa and Michelle Perry and Linda M. Sims and Kevin F. Miller and Ge Fang},
keywords = {Teacher beliefs, Mathematics education, China, United States, Cross-cultural, Culture},
abstract = {This study compares US and Chinese elementary mathematics teachers' beliefs about how students learn mathematics. Interviews with teachers in each country revealed that Chinese and US teachers have distinct ways of thinking about how mathematics should be taught and how students learn. Many Chinese teachers talked about developing students’ interest in mathematics and relating the content of mathematics lessons to real-life situations. The US teachers talked about students' learning styles and using hands-on approaches to learning mathematics. Furthermore, these beliefs may be widespread and persistent within each country because the set of ideas among teachers appear to be internally consistent. Implications for teacher change and the study of teachers' beliefs are discussed.}
}
@article{WEERASINGHE2025270,
title = {ABC-based forecasting in misspecified state space models},
journal = {International Journal of Forecasting},
volume = {41},
number = {1},
pages = {270-289},
year = {2025},
issn = {0169-2070},
doi = {https://doi.org/10.1016/j.ijforecast.2024.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S016920702400044X},
author = {Chaya Weerasinghe and Rubén Loaiza-Maya and Gael M. Martin and David T. Frazier},
keywords = {Approximate Bayesian computation, Auxiliary model, Loss-based prediction, Focused Bayesian prediction, Proper scoring rules, Stochastic volatility model},
abstract = {Approximate Bayesian Computation (ABC) has gained popularity as a method for conducting inference and forecasting in complex models, most notably those which are intractable in some sense. In this paper, we use ABC to produce probabilistic forecasts in state space models (SSMs). Whilst ABC-based forecasting in correctly-specified SSMs has been studied, the misspecified case has not been investigated. It is this case that we emphasize. We invoke recent principles of ‘focused’ Bayesian prediction, whereby Bayesian updates are driven by a scoring rule that rewards predictive accuracy; the aim being to produce predictives that perform well in that rule, despite misspecification. Two methods are investigated for producing the focused predictions. In a simulation setting, ‘coherent’ predictions are in evidence for both methods. That is, the predictive constructed using a particular scoring rule often predicts best according to that rule. Importantly, both focused methods typically produce more accurate forecasts than an exact but misspecified predictive, in particular when the degree of misspecification is marked. An empirical application to a truly intractable SSM completes the paper.}
}
@incollection{WANG202427,
title = {2 - Neuromorphic computing},
editor = {Min Gu and Elena Goi and Yangyundou Wang and Zhengfen Wan and Yibo Dong and Yuchao Zhang and Haoyi Yu},
booktitle = {Neuromorphic Photonic Devices and Applications},
publisher = {Elsevier},
pages = {27-45},
year = {2024},
series = {Photonic Materials and Applications Series},
isbn = {978-0-323-98829-2},
doi = {https://doi.org/10.1016/B978-0-323-98829-2.00006-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323988292000062},
author = {Wenju Wang and Haoran Zhou and Wei Li and Elena Goi},
keywords = {Artificial intelligence, machine learning, deep learning, machine learning models, neuromorphic computing},
abstract = {One of the aims of neuromorphic engineering is to implement elements of artificial intelligence (AI) algorithms, in particular artificial neural networks, with hardware that reflects the massively distributed nature of these bioinspired architectures. In this chapter, we introduce history, basic concepts, and applications of AI, in order to establish a working framework for the development of neuromorphic systems. We present the fundamentals of machine learning (ML) and deep learning (DL) and expound on the relationship among these algorithms based on neural networks, to generate a broader understanding of the methodical underpinning of current AI systems. Basic concepts and working principles, such as neurons, activation functions, feedforward networks, etc., are presented, and the performances of these algorithms in terms of functionality, computational complexity, and energy consumption are reviewed. Moreover, strengths and limits of different AI architectures are discussed, giving an overview of the development and applications of neuromorphic computing architectures.}
}
@article{DECHARMS2007473,
title = {Reading and controlling human brain activation using real-time functional magnetic resonance imaging},
journal = {Trends in Cognitive Sciences},
volume = {11},
number = {11},
pages = {473-481},
year = {2007},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2007.08.014},
url = {https://www.sciencedirect.com/science/article/pii/S1364661307002471},
author = {R. Christopher deCharms},
abstract = {Understanding how to control how the brain's functioning mediates mental experience and the brain's processing to alter cognition or disease are central projects of cognitive and neural science. The advent of real-time functional magnetic resonance imaging (rtfMRI) now makes it possible to observe the biology of one's own brain while thinking, feeling and acting. Recent evidence suggests that people can learn to control brain activation in localized regions, with corresponding changes in their mental operations, by observing information from their brain while inside an MRI scanner. For example, subjects can learn to deliberately control activation in brain regions involved in pain processing with corresponding changes in experienced pain. This may provide a novel, non-invasive means of observing and controlling brain function, potentially altering cognitive processes or disease.}
}
@article{RUFFO2023100531,
title = {Studying fake news spreading, polarisation dynamics, and manipulation by bots: A tale of networks and language},
journal = {Computer Science Review},
volume = {47},
pages = {100531},
year = {2023},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2022.100531},
url = {https://www.sciencedirect.com/science/article/pii/S157401372200065X},
author = {Giancarlo Ruffo and Alfonso Semeraro and Anastasia Giachanou and Paolo Rosso},
keywords = {Disinformation, Network analysis, Natural language processing, Opinion dynamics, Fake news spreading, Social bots},
abstract = {With the explosive growth of online social media, the ancient problem of information disorders interfering with news diffusion has surfaced with a renewed intensity threatening our democracies, public health, and news outlets’ credibility. Therefore, thousands of scientific papers have been published in a relatively short period, making researchers of different disciplines struggle with an information overload problem. The aim of this survey is threefold: (1) we present the results of a network-based analysis of the existing multidisciplinary literature to support the search for relevant trends and central publications; (2) we describe the main results and necessary background to attack the problem under a computational perspective; (3) we review selected contributions using network science as a unifying framework and computational linguistics as the tool to make sense of the shared content. Despite scholars working on computational linguistics and networks traditionally belong to different scientific communities, we expect that those interested in the area of fake news should be aware of crucial aspects of both disciplines.}
}
@article{CHEN20247,
title = {QoE oriented intelligent online learning evaluation technology in B5G scenario},
journal = {Digital Communications and Networks},
volume = {10},
number = {1},
pages = {7-15},
year = {2024},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2022.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S2352864822001110},
author = {Mingzi Chen and Xin Wei and Peizhong Xie and Zhe Zhang},
keywords = {B5G, Online learning, Quality of experience},
abstract = {Students' demand for online learning has exploded during the post-COVID-19 pandemic era. However, due to their poor learning experience, students' dropout rate and learning performance of online learning are not always satisfactory. The technical advantages of Beyond Fifth Generation (B5G) can guarantee a good multimedia Quality of Experience (QoE). As a special case of multimedia services, online learning takes into account both the usability of the service and the cognitive development of the users. Factors that affect the Quality of Online Learning Experience (OL-QoE) become more complicated. To get over this dilemma, we propose a systematic scheme by integrating big data, Machine Learning (ML) technologies, and educational psychology theory. Specifically, we first formulate a general definition of OL-QoE by data analysis and experimental verification. This formula considers both the subjective and objective factors (i.e., video watching ratio and test scores) that most affect OL-QoE. Then, we induce an extended layer to the classic Broad Learning System (BLS) to construct an Extended Broad Learning System (EBLS) for the students' OL-QoE prediction. Since the extended layer can increase the width of the BLS model and reduce the redundant nodes of BLS, the proposed EBLS can achieve a trade-off between the prediction accuracy and computation complexity. Finally, we provide a series of early intervention suggestions for different types of students according to their predicted OL-QoE values. Through timely interventions, their OL-QoE and learning performance can be improved. Experimental results verify the effectiveness of the proposed scheme.}
}
@article{ABONDIO202237,
title = {Single Cell Multiomic Approaches to Disentangle T Cell Heterogeneity},
journal = {Immunology Letters},
volume = {246},
pages = {37-51},
year = {2022},
issn = {0165-2478},
doi = {https://doi.org/10.1016/j.imlet.2022.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0165247822000669},
author = {Paolo Abondio and Carlo {De Intinis} and João Lídio {da Silva Gonçalves Vianez Júnior} and Luigia Pace},
keywords = {T cells, scRNA-seq, Analysis pipeline, Pseudotime, TCR},
abstract = {Single-cell multi-omics is a rapidly evolving field, thanks to a fast technological improvement and the growing accuracy of dedicated computational tools for data analysis. Its importance is highlighted by the possibility to distinguish apparently identical cells based on their pattern of gene expression. In this review, the mostly used methodological pipelines for single-cell analysis, as well as the advantages and potential limitations of several analytical steps, are presented and discussed, with specific sections focusing on crucial parts of this procedure, their bioinformatic tools, as well as their advantages and potential drawbacks. The current bioinformatic approaches for T-cell receptor (TCR) reconstruction are also introduced, as well as a comparison of single-cell sequencing technologies. Critical points that may introduce analytical biases and potential inaccuracies in data interpretation are also highlighted.}
}
@article{SAMSONOVICH2015235,
title = {Cognitive Processes in Preparation for Problem Solving},
journal = {Procedia Computer Science},
volume = {71},
pages = {235-247},
year = {2015},
note = {6th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2015, 6-8 November Lyon, France},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.12.218},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915036790},
author = {Alexei V. Samsonovich and Anastasia Kitsantas and Ellen O’Brien and Kenneth A. {De Jong}},
keywords = {self-regulation, planning, metacognition, intelligent tutoring systems},
abstract = {The aim of this study was to examine the role of a software tool in diagnosing student's thinking during problem solving in mathematics with 41 college students. Students were asked to select relevant steps, facts and strategies represented on the screen and connect them by arrows, indicating their plan of solution. Only after the diagram was completed, students were allowed to solve the problem. The findings are: (i) forward chaining is significantly more predominant, and backward chaining is significantly less frequent, compared to other possibilities or arrow entering. This result is unexpected, because classical planning methods produce backward chaining in this task. (ii) Students scoring in the middle are more likely to enter convergent pairs of arrows compared to students who scored low or high. This finding enables diagnosing student problem solving. Both findings imply constraints on selection of cognitive architectures used for modeling student problem solving.}
}
@article{LEE2024100211,
title = {A systematic review of AI education in K-12 classrooms from 2018 to 2023: Topics, strategies, and learning outcomes},
journal = {Computers and Education: Artificial Intelligence},
volume = {6},
pages = {100211},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100211},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000122},
author = {Sang Joon Lee and Kyungbin Kwon},
keywords = {Artificial intelligence, AI education, Systematic review, K-12},
abstract = {AI education aims to teach AI concepts, essential knowledge, and skills related to the fundamental ideas in AI. As AI becomes increasingly prevalent in our daily lives, schools and educators have started to recognize the importance of AI education in K-12 schools. However, there have been a limited number of studies reporting on the implementation of AI education in classrooms. This systematic review aimed to provide an overview of the current state of AI education in K-12 schools, exploring topics, instructional approaches, and learning outcomes. Twenty-five peer-reviewed journal articles published between 2018 and 2023 were selected for this systematic review. The findings highlighted that various topics were covered in K-12 AI education, including fundamental AI concepts, different types of AI, AI applications, and ethical considerations related to AI. To facilitate meaningful learning experiences, educators frequently integrated hands-on activities and project-based learning. The findings supported the benefits of AI education in enhancing students' AI literacy, problem-solving skills, and ethical reflections on AI's societal impact. Furthermore, it fostered motivation, positive attitudes toward AI, and an interest in technology while inspiring career aspirations. It is recommended to develop tailored AI curricula, instructional strategies, and appropriate tools and resources that seamlessly integrate into various subjects within the standard school curriculum.}
}
@article{KENETT2019271,
title = {A Semantic Network Cartography of the Creative Mind},
journal = {Trends in Cognitive Sciences},
volume = {23},
number = {4},
pages = {271-274},
year = {2019},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2019.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364661319300245},
author = {Yoed N. Kenett and Miriam Faust},
keywords = {creativity, semantic networks, network science},
abstract = {The role of semantic memory in creativity is theoretically assumed, but far from understood. In recent years, computational network science tools have been applied to investigate this role. These studies shed unique quantitative insights on the role of semantic memory structure in creativity, via measures of connectivity, distance, and structure.}
}
@incollection{QIN2025775,
title = {Representation of others' beliefs},
editor = {Jordan Henry Grafman},
booktitle = {Encyclopedia of the Human Brain (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {775-792},
year = {2025},
isbn = {978-0-12-820481-8},
doi = {https://doi.org/10.1016/B978-0-12-820480-1.00159-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204801001595},
author = {Jingmin Qin and Haiyan Wu},
keywords = {Theory of mind, Mentalizing, Perspective-taking, Metacognition, Social inference, Social cognition, Developmental changes, Neural mechanisms, Psychopathology, Computational models},
abstract = {This article delves into the diverse aspects in which individuals make inferences about the beliefs and values held by others. By reviewing the psychological factors underlying representing the beliefs of others, as well as the individual differences for both the individuals being represented and those undertaking the representation, this article sheds light on the intricate nature of representation of other's belief. Furthermore, it discusses the ways and considerations involved in updating these beliefs (such as observation, active interaction, and Bayesian inference) and offers suggestions for future research.}
}
@article{HAGHGOO2024108332,
title = {The percolation inception of the CNT-polymer nanocomposites with the magneto-electric field effects on the CNT subbands},
journal = {Composites Part A: Applied Science and Manufacturing},
volume = {185},
pages = {108332},
year = {2024},
issn = {1359-835X},
doi = {https://doi.org/10.1016/j.compositesa.2024.108332},
url = {https://www.sciencedirect.com/science/article/pii/S1359835X24003294},
author = {Mojtaba Haghgoo and Reza Ansari and Mohammad {Kazem Hassanzadeh-Aghdam} and Jaehwan Kim},
keywords = {A. Carbon nanotubes and nanofibers, A. Multifunctional composites, B. Electrical properties, C. Analytical modelling},
abstract = {The percolation inception of CNT-polymer nanocomposites is studied considering the magneto-electric field effects on CNT subbands. The analytical model predicts the electrical conductivity where CNTs are modeled as slender rods with their geometric orientations as randomly distributed or aligned to transfer electrons at tunneling distance range. The tunneling effect takes into account the electron transmission between every linked pair of CNTs when evaluating electrical resistance. The subsequent CNT displacement computation and the resistance change comprise the other phase of the modeling approach. Piezoresistivity results of the analyses agree well with the experimental data when considering tunneling behavior in the percolation transition zone. The magnetic field enhances the field affected subbands and increases the electrical conductivity by enhancing the mobility of the charges. The results reveal that the efficiency of CNT network in transmitting charges is increased with higher aspect ratio CNTs that scaled the sensitivity to lower values.}
}
@article{PUTTEGOWDA2025109910,
title = {Artificial intelligence and machine learning in mechanical engineering: Current trends and future prospects},
journal = {Engineering Applications of Artificial Intelligence},
volume = {142},
pages = {109910},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109910},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624020694},
author = {Madhu Puttegowda and Sharath {Ballupete Nagaraju}},
keywords = {Artificial intelligence, Machine learning, Structural health monitoring, Predictive maintenance, Renewable energy systems},
abstract = {This review examines the transformative influence of artificial intelligence (AI) and machine learning (ML) on mechanical engineering, emphasizing application-specific advancements that have contributed to the field's progress. Key applications, including predictive maintenance, design optimization, structural health monitoring, quality control, and renewable energy optimization, illustrate how AI techniques, including reinforcement learning, deep learning, and neural networks, improve efficiency, reduce costs, and promote sustainable practices. This review also looks at the more general, overarching features of AI, such as its ability to adapt, to be interpreted, and to combine physics-based and data-driven models, which makes it easier to use these applications in a range of engineering settings. Despite significant advancements, challenges remain, including the model's robustness, computational demands, ethical considerations, and data quality. This paper endeavors to provide a comprehensive resource for researchers and practitioners in the mechanical engineering domain by synthesizing current advancements, identifying critical challenges, and predicting future trajectory.}
}
@article{DICARLO2007333,
title = {Untangling invariant object recognition},
journal = {Trends in Cognitive Sciences},
volume = {11},
number = {8},
pages = {333-341},
year = {2007},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2007.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S1364661307001593},
author = {James J. DiCarlo and David D. Cox},
abstract = {Despite tremendous variation in the appearance of visual objects, primates can recognize a multitude of objects, each in a fraction of a second, with no apparent effort. However, the brain mechanisms that enable this fundamental ability are not understood. Drawing on ideas from neurophysiology and computation, we present a graphical perspective on the key computational challenges of object recognition, and argue that the format of neuronal population representation and a property that we term ‘object tangling’ are central. We use this perspective to show that the primate ventral visual processing stream achieves a particularly effective solution in which single-neuron invariance is not the goal. Finally, we speculate on the key neuronal mechanisms that could enable this solution, which, if understood, would have far-reaching implications for cognitive neuroscience.}
}
@article{GRIESBAUER2025106014,
title = {London taxi drivers exploit neighbourhood boundaries for hierarchical route planning},
journal = {Cognition},
volume = {256},
pages = {106014},
year = {2025},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2024.106014},
url = {https://www.sciencedirect.com/science/article/pii/S0010027724003007},
author = {Eva-Maria Griesbauer and Pablo {Fernandez Velasco} and Antoine Coutrot and Jan M. Wiener and Jeremy G. Morley and Daniel McNamee and Ed Manley and Hugo J. Spiers},
keywords = {Spatial representation, Real-world evidence, Hierarchical representations, Spatial cognition, Wayfinding, Executive function},
abstract = {Humans show an impressive ability to plan over complex situations and environments. A classic approach to explaining such planning has been tree-search algorithms which search through alternative state sequences for the most efficient path through states. However, this approach fails when the number of states is large due to the time to compute all possible sequences. Hierarchical route planning has been proposed as an alternative, offering a computationally efficient mechanism in which the representation of the environment is segregated into clusters. Current evidence for hierarchical planning comes from experimentally created environments which have clearly defined boundaries and far fewer states than the real-world. To test for real-world hierarchical planning we exploited the capacity of London licensed taxi drivers to use their memory to construct a street by street plan across London, UK (>26,000 streets). The time to recall each successive street name was treated as the response time, with a rapid average of 1.8 s between each street. In support of hierarchical planning we find that the clustered structure of London's regions impacts the response times, with minimal impact of the distance across the street network (as would be predicted by tree-search). We also find that changing direction during the plan (e.g. turning left or right) is associated with delayed response times. Thus, our results provide real-world evidence for how humans structure planning over a very large number of states, and give a measure of human expertise in planning.}
}
@article{CHI2024117852,
title = {Artificial intelligence in metabolomics: a current review},
journal = {TrAC Trends in Analytical Chemistry},
volume = {178},
pages = {117852},
year = {2024},
issn = {0165-9936},
doi = {https://doi.org/10.1016/j.trac.2024.117852},
url = {https://www.sciencedirect.com/science/article/pii/S0165993624003352},
author = {Jinhua Chi and Jingmin Shu and Ming Li and Rekha Mudappathi and Yan Jin and Freeman Lewis and Alexandria Boon and Xiaoyan Qin and Li Liu and Haiwei Gu},
keywords = {Artificial intelligence, Metabolomics, Machine learning, Deep learning, Systems biology, Disease diagnosis, Precision medicine, Drug discovery},
abstract = {Metabolomics and artificial intelligence (AI) form a synergistic partnership. Metabolomics generates large datasets comprising hundreds to thousands of metabolites with complex relationships. AI, aiming to mimic human intelligence through computational modeling, possesses extraordinary capabilities for big data analysis. In this review, we provide a recent overview of the methodologies and applications of AI in metabolomics studies in the context of systems biology and human health. We first introduce the AI concept, history, and key algorithms for machine learning and deep learning, summarizing their strengths and weaknesses. We then discuss studies that have successfully used AI across different aspects of metabolomic analysis, including analytical detection, data preprocessing, biomarker discovery, predictive modeling, and multi-omics data integration. Lastly, we discuss the existing challenges and future perspectives in this rapidly evolving field. Despite limitations and challenges, the combination of metabolomics and AI holds great promises for revolutionary advancements in enhancing human health.}
}
@article{JINADU20106753,
title = {Globalization & State Capacity in Africa},
journal = {Procedia - Social and Behavioral Sciences},
volume = {2},
number = {5},
pages = {6753-6763},
year = {2010},
note = {The Harmony of Civilization and Prosperity for All: Selected Papers of Beijing Forum(2004-2008)},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2010.05.021},
url = {https://www.sciencedirect.com/science/article/pii/S1877042810011559},
author = {L. Adele Jinadu},
abstract = {The paper examines the impact of globalization on state capacity in Africa. It problematizes globalization as a central determining factor in building the capable state in Africa. Globalization, although it requires typologizing and contextualizing or historicizing, is used to refer to a complex set of interconnected multi-linear, multifaceted and dialectical and still unfolding historical processes, propelled by the transnationalization of finance capital, in search of new markets, and the logic of capital accumulation, and typically characterized by structural differentiation and unequal functional integration between metropolitan and dependent or satellite nations, peoples and markets. State capacity is used neither narrowly nor exclusively as human and physical resource capacity-building or capacity-enhancement, nor limited to econometric or statistical computations of gross domestic product or national income data, though it includes and requires both. Its use assumes a democratic, open, participatory, and socially inclusive political system, as important conditions for expanding and consolidating state capacity on a sustainable basis in Africa. The paper situates the problem of globalization for state capacity in Africa in the wider Pan African context. Historically, globalization has divided and balkanized African countries, carving out political, economic and cultural spheres of influence, and weakening their ability to act collectively to defend their common interests. Collective action by African countries to confront the challenges and opportunities of globalization requires new governance structures to strengthen African regional economic communities, the African Union and the New Partnership for Africa's Development, along lines that will, by democratizing decision-making and public political processes within their member-states, enhance state capacity in various sectors. Attributing the problematic character of state capacity in Africa to the massive problem of the structural condition of the African state, the paper argues that this is notably and significantly due to the contradictions arising from globalization and the dependent character of the African state, reflecting the lingering or residual colonial inheritance of dependent political and socioeconomic and psycho-cultural structures, institutions and processes, which are at the heart of the problem of state capacity in Africa. They reflect the dialectics or antinomies, the age-old or historically deep contradictory push and pull of globalization and localization or indigenization in Africa. The paper suggests that, resolving these antinomies or contradictions, requires the following: (a) Transforming contemporary globalization, on the basis of mutuality, recognition and reciprocity, emphasizing new Afrocentric epistemological foundations for thinking about African and global development, global social justice, global income redistribution, economic and socio-cultural rights, global inclusion, and global democracy. (b) Emphasizing the use of “appropriate” technologies, to ‘fit’ the lifestyles and social organizations of local communities, growing from them, requiring less reliance on outside experts and using more local expertise. (c) Re-designing new Pan-African approaches to state capacity, to strengthen the collective capacity of African continental and regional institutions to respond to globalization, turning its negative implications for Africa into opportunities to reform globalization, and make it truly global. (d) Reconceptualizing democracy, on the basis of the positive role of culture in generating and institutionalizing new modes of self-reliant, and transparent democratic governance.}
}
@article{MOINGEON2021566,
title = {Applications de l’intelligence artificielle au développement de nouveaux médicaments},
journal = {Annales Pharmaceutiques Françaises},
volume = {79},
number = {5},
pages = {566-571},
year = {2021},
issn = {0003-4509},
doi = {https://doi.org/10.1016/j.pharma.2021.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0003450921000092},
author = {P. Moingeon},
keywords = {Biotechnologies, Développement médicamenteux, Intelligence artificielle, Machines intelligentes, Médecine computationnelle, Médecine de précision, Modèle de maladie, Artificial intelligence, Biotechnologies, Computational medicine, Disease model, Drug development, Intelligent machines, Precision medicine},
abstract = {Résumé
L’intelligence artificielle (IA) recouvre les technologies qui reproduisent, par la machine, quatre dimensions de l’intelligence humaine, à savoir la perception, l’analyse, l’action et l’apprentissage. Les progrès technologiques, combinés dans ces domaines, permettent de générer et d’analyser des données massives pour modéliser la réalité d’un phénomène. Ces modèles sont ensuite réactualisés par l’accumulation de nouvelles données afin d’aider à la prise de décision et prédire le futur. Appliquée à la problématique du développement d’un médicament, l’IA permet d’établir des modèles de maladies à partir de données de profilage moléculaire de patients. Par sa puissance de calcul, l’IA intègre ces données multimodales massives dans un modèle permettant : (i) de rendre compte de l’hétérogénéité des maladies ; et (ii) d’identifier des cibles thérapeutiques importantes dans la physiopathologie. D’autres analyses computationnelles sont utilisées pour identifier des molécules thérapeutiques interagissant avec ces cibles, optimiser ces molécules ou repositionner des molécules anciennes dans de nouvelles indications. La modélisation par l’IA aide également à identifier des biomarqueurs d’efficacité, définir des combinaisons de molécules thérapeutiques pertinentes, concevoir des études cliniques innovantes avec des groupes placebo virtuels… Cette convergence révolutionnaire entre les biotechnologies, les sciences du médicament et l’IA donne aujourd’hui naissance à une médecine computationnelle de précision applicable à toutes les maladies chroniques, qui offrira des traitements parfaitement ciblés prenant en compte les spécificités du patient quant à sa physiologie, sa maladie, sa relation à l’environnement.
Summary
Artificial intelligence (AI) encompasses technologies recapitulating four dimensions of human intelligence, i.e. sensing, thinking, acting and learning. The convergence of technological advances in those fields allows to integrate massive data and build probabilistic models of a problem. The latter can be continuously updated by incorporating new data to inform decision-making and predict the future. In support of drug discovery and development, AI allows to generate disease models using data obtained following extensive molecular profiling of patients. Given its superior computational power, AI can integrate those big multimodal data to generate models allowing: (i) to represent patient heterogeneity; and (ii) identify therapeutic targets with inferences of causality in the pathophysiology. Additional computational analyses can help identifying and optimizing drugs interacting with these targets, or even repurposing existing molecules for a new indication. AI-based modeling further supports the identification of biomarkers of efficacy, the selection of appropriate combination therapies and the design of innovative clinical studies with virtual placebo groups. The convergence of biotechnologies, drug sciences and AI is fostering the emergence of a computational precision medicine predicted to yield therapies or preventive measures precisely tailored to patient characteristics in terms of their physiology, disease features and environmental risk exposure.}
}
@article{LIENERT1996845,
title = {LSD response in Eysenckian trait types identified by polypredictive CFA},
journal = {Personality and Individual Differences},
volume = {21},
number = {6},
pages = {845-850},
year = {1996},
issn = {0191-8869},
doi = {https://doi.org/10.1016/S0191-8869(96)00143-2},
url = {https://www.sciencedirect.com/science/article/pii/S0191886996001432},
author = {Gustav A. Lienert and Petra Netter},
abstract = {The four personality type combinations derived from high and low extraversion (E+E−) and high and low neuroticism (N+N−) have been related to response patterns composed of three symptoms (affective disturbances, thinking disturbances, and blackouts) scored as present (+) or absent (−) after a single oral dose of the hallucinogenic drug LSD-25. Hypotheses for expected response patterns for each personality group were derived from a data set obtained by Kohnen and Lienert (1987). Significance of associations was tested by two strategies of polyprediction configural frequency analysis (CFA): multiple uniprediction and biprediction CFA. Both strategies yielded a significant hyperpresentation of all three symptoms present in E+N+ (hysterics), merely thinking disorders in dysthymics (E−N+), merely affective symptoms in E+N− (stable extraverts), and merely blackouts in N−E− (stable introverts). Authors tried to relate these symptoms to Kretschmer's temperament types and could afterwards show by a chessboard modification of prediction CFA, that by applying two combined hypotheses for two personality types each, the significance of the predicted associations could be increased.}
}
@article{KULIHA2024161,
title = {Secure internet of medical things based electronic health records scheme in trust decentralized loop federated learning consensus blockchain},
journal = {International Journal of Intelligent Networks},
volume = {5},
pages = {161-174},
year = {2024},
issn = {2666-6030},
doi = {https://doi.org/10.1016/j.ijin.2024.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S2666603024000162},
author = {Megha Kuliha and Sunita Verma},
keywords = {Blockchain, Electronic health records, Federated learning, Healthcare monitoring, Internet of medical things, Security, Privacy, Health monitoring systems, Normalization, MIMIC-III},
abstract = {Electronic Health Records (EHRs) have become an increasingly significant source of information for healthcare professionals and researchers. Two technical challenges are addressed: motivating federated learning members to contribute their time and effort, and ensuring accurate aggregation of the global model by the centralized federated learning server. To overcome these issues and establish a decentralized solution, the integration of blockchain and federated learning proves effective, offering enhanced security and privacy for smart healthcare. The proposed approach includes a gamified element to incentivize and recognize contributions from federated learning members. This research work offers a solution involving resource management within the Internet of Medical Things (IoMT) using a newly proposed trust decentralized loop federated learning consensus blockchain. The obtained raw data is pre-processed by using handling missing values and adaptive min-max normalization. The appropriate features are selected with the aid of hybrid weighted-leader exponential distribution optimization algorithm. Because, data with multiple features exhibits varying levels of variation across each feature. The selected features are then forwarded to the training phase through the proposed pyramid squeeze attention generative adversarial networks to classify the EHR as positive and negative. The proposed classification model demonstrates high flexibility and scalability, making it applicable to a wide range of network architectures for various computer vision tasks. The introduced model provides better outcomes in terms of 98.5% in the training accuracy and 99% in the validation accuracy over Medical Information Mart for Intensive Care III (MIMIC-III) dataset, which is more efficient than the other traditional methods.}
}
@article{NAVEIRO2019133,
title = {Adversarial classification: An adversarial risk analysis approach},
journal = {International Journal of Approximate Reasoning},
volume = {113},
pages = {133-148},
year = {2019},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2019.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X18304705},
author = {Roi Naveiro and Alberto Redondo and David {Ríos Insua} and Fabrizio Ruggeri},
keywords = {Classification, Bayesian methods, Adversarial machine learning, Influence diagrams, Robustness},
abstract = {Classification techniques are widely used in security settings in which data can be deliberately manipulated by an adversary trying to evade detection and achieve some benefit. However, traditional classification systems are not robust to such data modifications. Most attempts to enhance classification algorithms in adversarial environments have focused on game theoretical ideas under strong underlying common knowledge assumptions, which are not actually realistic in security domains. We provide an alternative framework to such problems based on adversarial risk analysis which we illustrate with examples. Computational, implementation and robustness issues are discussed.}
}
@article{LAI202333,
title = {Impact of social cognitive propensity on the processing of nontransparent sentential meaning},
journal = {Journal of Pragmatics},
volume = {205},
pages = {33-62},
year = {2023},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2022.12.014},
url = {https://www.sciencedirect.com/science/article/pii/S0378216622003010},
author = {Yao-Ying Lai and Huei-ling Lai},
keywords = {Nontransparent meaning, Meaning contextualization, Combinatorial semantic processing, Individual differences, Social cognition, Autism-spectrum quotient},
abstract = {This study investigates the influence of individual social-cognitive propensity in the processing of nontransparent sentential meaning, exemplified by the morpho-syntactically unsupported iterative meaning in “The frog hopped for five minutes.” Results of our speeded questionnaire in Mandarin Chinese showed that social cognitive propensity of typically-developed individuals, indexed by autistic-like traits, significantly correlated with online response times (RTs) of naturalness rating, while the effect was absent in offline rating scores. Individuals with higher autistic-like traits (i.e., lower social skills) took longer to process sentences with nontransparent meaning for making judgments. We argue that the computation of these sentences involves meaning contextualization—construing a coherent conceptual representation by integrating multiple lexical representations and evaluating sentential-discourse context. Such context-dependent meaning processing requires sufficient context sensitivity, which varies across individuals in association with social cognitive propensity. The pattern is captured by the Dual-Process Approach to information processing and social cognition: individuals with higher autistic-like traits are prone to deliberative reasoning with lower contextual sensitivity. This cognitive bias leads to greater cost when the full comprehension demands meaning contextualization, and therefore longer RTs in evaluating appropriate interpretations. The findings show that individual variability in social cognitive propensity modulates the online computation of nontransparent sentential meaning.}
}
@article{SELVARAJ2022100471,
title = {Capture Based Trust Dependence framework for authorized node identification in mobile agent systems},
journal = {Measurement: Sensors},
volume = {24},
pages = {100471},
year = {2022},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2022.100471},
url = {https://www.sciencedirect.com/science/article/pii/S2665917422001052},
author = {Priyanka Selvaraj and Vijay Bhanu Srinivasan},
keywords = {Capture based, Dependence framework mobile agents, Network performance, Trust identification},
abstract = {A mobile agent is a self-learning machine entity that uses the system infrastructure to keep running in another remote zone, check and compile the results, interact with various locations and return to his home site after completing the relegated activities. Mobile Agent-based solutions for the testing community have grown in popularity and are now used in a variety of fields, including boardroom management, electronic commerce, renewable energy and power management. Addition to these applications, Broadband Interactive Sensors, network performance improvement, disseminated knowledge mining, multimedia, human monitoring, surveillance, affective computing, weather and environment, e-learning and semantic web administrations are only a few of the topics covered. In an extremely non trusty environment, focus should be taken to shield the portable operator from acquiring altered. Existing works on mobile agent frameworks with very surprising instruments does not offer complete security. In this paper a capture based trust dependence framework is proposed for identification of authorized or trust nodes inside mobile agents systems. Here we consider the mobile adhoc networks for computational analysis of network performance using network simulator. The framework provides efficient results in identification of authorized nodes.}
}
@article{MULET200632,
title = {Functional requirements for computer-based design support systems, derived from experimental studies},
journal = {Knowledge-Based Systems},
volume = {19},
number = {1},
pages = {32-42},
year = {2006},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2005.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0950705105000869},
author = {Elena Mulet and Rosario Vidal},
keywords = {Computer design support, Knowledge-based design systems, Design process, Computational synthesis methods, Experimental research in engineering design},
abstract = {In this paper, we examine the functions that a computational system for knowledge-based design support may undertake. We present a set of functions that bring together previous approaches and allow us to locate the work that has been developed to enhance these systems concerning those functions. We describe some new proposals, based on experimental research work, for improving some of these functions so that they can be taken into account in the development of design support systems to help the designer or group of designers reach a suitable solution in a more effective way.}
}
@article{REIHLEN2013706,
title = {Uncertainty, pluralism, and the knowledge-based theory of the firm: From J.-C. Spender’s contribution to a socio-cognitive approach},
journal = {European Management Journal},
volume = {31},
number = {6},
pages = {706-716},
year = {2013},
issn = {0263-2373},
doi = {https://doi.org/10.1016/j.emj.2013.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0263237313001011},
author = {Markus Reihlen and Torsten Ringberg},
keywords = {Knowledge-based approach, Theory of the firm, Knowledge transfer, Social constructionism, Tacit knowledge, Socio-cognitive theory, Intuition, Mental models},
abstract = {J.-C. Spender’s award-winning, knowledge-based theory of the firm is based on four premises: (1) The firm can be sufficiently understood as a system of knowledge, (2) explicit and implicit knowing can be clearly dissociated, (3) organizations are conceived as cognizing entities, and (4) intuition shaped by shared cultural practices is a superior source of managerial knowledge. This line of reasoning represents a social constructionist view of the enactment, transfer, and storage of knowledge according to which managerial knowledge is largely tacitly shaped by industry recipes and the firm’s socio-cultural conventions and other social processes. Although comprehensive in scope, we argue that a knowledge-based theory of the firm needs to integrate a cognitivist approach that includes the synergetic production of tacit and explicit knowledge, the role of reflective thinking in resolving strategic uncertainties, and the interaction between the individual and the social. This socio-cognitive theory of the firm posits that sustained competitive advantage of a firm is founded on the ability to align knowledge internally within the firm as well as externally with its stakeholders through the individual sense-making of feedback from other individuals.}
}
@article{WILKINS20258,
title = {Does DeepSeek herald AI's future?},
journal = {New Scientist},
volume = {265},
number = {3529},
pages = {8-9},
year = {2025},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(25)00207-6},
url = {https://www.sciencedirect.com/science/article/pii/S0262407925002076},
author = {Alex Wilkins},
abstract = {The success of Chinese firm DeepSeek suggests tech companies can train and run powerful AIs without consuming vast amounts of power, finds Alex Wilkins}
}
@article{KELLIHER201536,
title = {Design futures in action: Documenting experiential futures for participatory audiences},
journal = {Futures},
volume = {70},
pages = {36-47},
year = {2015},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2014.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0016328714001980},
author = {Aisling Kelliher and Daragh Byrne},
keywords = {Documentation, Multimedia, Experiential futures, Summarization, Representation, Annotation, Exhibition, Social platform},
abstract = {The futures field demonstrates a willing openness in embracing methodologies, approaches, and influences from a diversity of disciplines and perspectives. This plurality of practice is evidenced in a growing body of work that increasingly embodies futures thinking in the design of everyday material and networked experiences. The intersection of design and futures produces artifacts, applications and interactions created to provoke dialog in an accessible manner. As part of the Futures special issue on the Emerge: Artists and Scientists Redesign the Future event, this article describes the documentation and public representation of the creative outcomes from nine Emerge design futures workshops. These workshops provided a rich opportunity to study how designers and futurists collaboratively engage, implement and communicate alternative futures. The goal of the documentation effort described is to capture the experience of creating experiential futures and extend the capacity for developing social foresight through a participatory exhibit and online social platform.}
}
@article{ANDERSON2011R123,
title = {Neuroscience: What We Cannot Model, We Do Not Understand},
journal = {Current Biology},
volume = {21},
number = {3},
pages = {R123-R125},
year = {2011},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2010.12.049},
url = {https://www.sciencedirect.com/science/article/pii/S0960982210017173},
author = {William S. Anderson and Gabriel Kreiman},
abstract = {Summary
To understand computations in neuronal circuits, a model of a small patch of cortex has been developed that can describe the firing regime in the visual system remarkably well.}
}
@article{VANDERVERT2003159,
title = {How working memory and cognitive modeling functions of the cerebellum contribute to discoveries in mathematics},
journal = {New Ideas in Psychology},
volume = {21},
number = {2},
pages = {159-175},
year = {2003},
issn = {0732-118X},
doi = {https://doi.org/10.1016/S0732-118X(03)00012-6},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X03000126},
author = {Larry Vandervert},
keywords = {Einstein, Cerebellum, Intuition, Mathematics, Mental models, Working memory},
abstract = {A theory of how connections between working memory and cognitive functions of the cerebellum lead to mathematical discovery is presented. It is proposed that (a) patterns of repetitious working memory processing are learned in the cerebellum, and (b) when these cerebellar patterns are subsequently fed back to control processing in working memory, they are learned in visuospatial imagery and language as the concepts and axioms that underlie mathematical discovery. Paralleling Einstein's description of “thinking,” a working memory/cerebellar model of mathematical intuition is presented. It is concluded that the collaboration of the cerebellum and working memory constructs the only fundamental patterns (mathematics) of the joint framework that binds our cognitive consciousness with the socially verifiable operational specification of an external world.}
}
@article{BAUMANN20101561,
title = {Numerical solution of level dependent quasi-birth-and-death processes},
journal = {Procedia Computer Science},
volume = {1},
number = {1},
pages = {1561-1569},
year = {2010},
note = {ICCS 2010},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2010.04.175},
url = {https://www.sciencedirect.com/science/article/pii/S1877050910001766},
author = {Hendrik Baumann and Werner Sandmann},
keywords = {Continuous-time Markov chains, Block-tridiagonal generator matrices, Level dependent quasi-birth-and-death processes, Numerical solution, Matrix continued fractions},
abstract = {We consider the numerical computation of stationary distributions for level dependent quasi-birth-and-death processes. An algorithm based on matrix continued fractions is presented and compared to standard solution techniques. Its computational efficiency and numerical stability is demonstrated by numerical examples.}
}
@article{SUWA1997385,
title = {What do architects and students perceive in their design sketches? A protocol analysis},
journal = {Design Studies},
volume = {18},
number = {4},
pages = {385-403},
year = {1997},
note = {Descriptive models of design},
issn = {0142-694X},
doi = {https://doi.org/10.1016/S0142-694X(97)00008-2},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X97000082},
author = {Masaki Suwa and Barbara Tversky},
keywords = {architectural design, design cognition, drawings, perception, protocol analysis},
abstract = {The present research aims at examining what information architects think of and read off from their own freehand sketches, and at revealing how they perceptually interact with and benefit from sketches. We explored this in a protocol analysis of retrospective reports; each participant worked on an architectural design task while drawing freehand sketches and later reported what she/he had been thinking of during the design task. This research lies within the scope of examinations of why freehand sketches as external representation are essential for crystallizing design ideas in early design processes.}
}
@article{REINOSOCARVALHO2020389,
title = {A sprinkle of emotions vs a pinch of crossmodality: Towards globally meaningful sonic seasoning strategies for enhanced multisensory tasting experiences},
journal = {Journal of Business Research},
volume = {117},
pages = {389-399},
year = {2020},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2020.04.055},
url = {https://www.sciencedirect.com/science/article/pii/S0148296320302800},
author = {Felipe Reinoso-Carvalho and Laura Gunn and German Molina and Takuji Narumi and Charles Spence and Yuji Suzuki and Enrique {ter Horst} and Johan Wagemans},
keywords = {Crossmodal, Emotions, Flavors, Multisensory, Music, Purchase intention},
abstract = {We report a study designed to determine the most efficient means of pursuing sonic seasoning in international marketing. For the first time, music chosen to trigger specific emotional responses was directly and cross-culturally compared with music chosen as crossmodally congruent with specific taste/flavors (the latter usually referred to as ‘sonic seasoning’). The effects triggered by ‘emotional’ music were more prominent than those triggered by ‘crossmodally-corresponding’ music. Specifically, chocolate was liked more, rated as sweeter, and the purchase intent was higher, when tasted while listening to music that conveyed positive, as compared to negative, emotion. By contrast, the same chocolate was mostly rated as tasting more bitter with the negative music, as compared to the positive music. Companies looking to use sonic seasoning in marketing strategies, should therefore principally aim at intelligently classifying music based on the likely emotions that they can trigger in their customers (at least when thinking globally).}
}
@incollection{ODENBAUGH2011421,
title = {Complex Ecological Systems},
editor = {Cliff Hooker},
booktitle = {Philosophy of Complex Systems},
publisher = {North-Holland},
address = {Amsterdam},
pages = {421-439},
year = {2011},
volume = {10},
series = {Handbook of the Philosophy of Science},
issn = {18789846},
doi = {https://doi.org/10.1016/B978-0-444-52076-0.50015-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780444520760500158},
author = {Jay Odenbaugh},
abstract = {Publisher Summary
This chapter considers some of the ways in which nonlinear dynamics is changing the science of ecology. Specifically, it considers a bit of history; namely, debates over the stability of populations and communities in population and community ecology. Further, it explores arguments over population regulation in population ecology and the debate over diversity-complexity-stability in community ecology. This serves to highlight how ecological systems have been evaluated with the tools and assumptions of linear dynamical systems. Second, it turns to some conceptual issues. That is, what different concepts of stability are at work in ecology. Additionally, it provides a taxonomy of these concepts and how they are related to one another. Unfortunately, many of these concepts are mostly applicable when thinking of linear systems. As an example of nonlinear dynamics in ecology, it considers the case of deterministic chaos. Using very simple discrete difference equations suited for insect populations, for example, one can see the characteristic of sensitivity to initial conditions. Finally, it discusses the impact of complex systems analysis on issues in and around ecology. Specifically, it examines the rise of “resilience thinking” and debates over ecological laws as examples of how nonlinear dynamics is challenging ecological theory and practice.}
}
@article{BEZERRAFEITOSA2024843,
title = {Multiple criteria evaluation of hydrogen production processes for use in automotive sector},
journal = {International Journal of Hydrogen Energy},
volume = {49},
pages = {843-861},
year = {2024},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2023.09.232},
url = {https://www.sciencedirect.com/science/article/pii/S0360319923048784},
author = {Francisco Edvan {Bezerra Feitosa} and Antonella Lombardi Costa},
keywords = {Hydrogen, Gasification, Steam reforming, Electrolysis, Solar and nuclear thermochemical, MACBETH},
abstract = {This work evaluates large-scale hydrogen production processes - those capable of providing hydrogen for an eventual automotive hydrogen program in a community and along highways - with the aim to Sidentify which of the production processes of hydrogen is more attractive for an eventual automotive hydrogen program. To perform the research, the MACBETH (Measuring Attractiveness by the Category-Based Assessment Technique) method and the computational code M-MACBETH 3.2.0 are used taking into account multiple criteria. Thus, this work used the computational code M-MACBETH 3.2.0 to evaluate twelve hydrogen production processes and build rankings of attractiveness of hydrogen production processes, considering the following criteria: economic (invested capital and production cost), technical (the purity of the hydrogen produced) and environmental (the amount of energy used to produce 1 kg of hydrogen and the CO2 emissions to produce 1 kg of hydrogen). In the end, three rankings are produced considering three scenarios, which allow to conclude that: if decisions regarding hydrogen are made taking into account that economic and financial aspects have the same weight as sustainability aspects, the more attractive hydrogen plants are those that use the Solar Thermochemical Plants; if decisions are made taking into account that economic and financial aspects are more important than aspects of sustainability, the production process more attractive will be the one that uses the SR technology - Ethanol and Natural Gas Steam Reforming; and that Solar Thermochemical Plants becomes more attractive than all others alternatives when, in decision-making, sustainability environmental aspects prevail over economic and financial aspects.}
}
@article{CHUDERSKI2014258,
title = {How well can storage capacity, executive control, and fluid reasoning explain insight problem solving},
journal = {Intelligence},
volume = {46},
pages = {258-270},
year = {2014},
issn = {0160-2896},
doi = {https://doi.org/10.1016/j.intell.2014.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0160289614001020},
author = {Adam Chuderski},
keywords = {Insight problem solving, Fluid reasoning, Working memory, Storage capacity, Executive control},
abstract = {Previous studies have found discrepant results on the relationship between insight problem solving and the processes underlying analytic thinking: storage capacity, executive control (two components of working memory; WM), as well as fluid reasoning. Some research showed that WM and/or reasoning are positively related to insight, supporting the “nothing-special” account, whereas other studies demonstrated null or negative relationships favoring the “special-process” view. This study examined a large sample with a battery of insight, reasoning, and WM tasks, to estimate the pattern of links between investigated constructs using structural equation modeling. WM and reasoning together explained about two thirds of the variance in insight. Both WM components similarly contributed to insight. WM's contribution was mediated by reasoning. These results support the nothing-special view. However, after WM variance was partialed out, the link between insight and reasoning substantially weakened, that makes room for the special-process view. Both accounts can be integrated in the view that insight is “nothing special with special add-ons” – the latter understood as the processes and strategies specific only to insight problem solving.}
}
@article{KOLERS1984289,
title = {Symbol manipulation: Alternatives to the computational view of mind},
journal = {Journal of Verbal Learning and Verbal Behavior},
volume = {23},
number = {3},
pages = {289-314},
year = {1984},
issn = {0022-5371},
doi = {https://doi.org/10.1016/S0022-5371(84)90182-8},
url = {https://www.sciencedirect.com/science/article/pii/S0022537184901828},
author = {Paul A. Kolers and William E. Smythe},
abstract = {Acquisition and manipulation of symbols are the fundamental constituents of cognitive activity, but modern information processing theory has not explored their basis sufficiently. Computationalism is the one modern approach that takes an explicit position in regard to symbol manipulation. We here explore some of the virtues of that approach and show its principal deficiency, which is to construe symbolization too narrowly, thereby blocking more adequate treatments of learning and acquisition of skills. Symbols come in many kinds; the different kinds allow for different representational capabilities. A proper sorting of symbols and an understanding of their different capabilities is prerequisite, and should be of the greatest benefit, to an account of cognitive processes.}
}
@article{IGAMBERDIEV2024105346,
title = {Reflexive neural circuits and the origin of language and music codes},
journal = {BioSystems},
volume = {246},
pages = {105346},
year = {2024},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2024.105346},
url = {https://www.sciencedirect.com/science/article/pii/S0303264724002314},
author = {Abir U. Igamberdiev},
keywords = {Aristotle, Language, Consciousness, Musical code, Reflexive psychology, Self-awareness},
abstract = {Conscious activity is grounded in the reflexive self-awareness in sense perception, through which the codes signifying sensual perceptive events operate and constrain human behavior. These codes grow via the creative generation of hypertextual statements. We apply the model of Vladimir Lefebvre (Lefebvre, V.A., 1987, J. Soc. Biol. Struct. 10, 129–175) to reveal the underlying structures on which the perception and creative development of language and music codes are based. According to this model, the reflexive structure of conscious subject is grounded in three thermodynamic cycles united by the control of the basic functional cycle by the second one, and resulting in the internal action that it turn is perceived by the third cycle evaluating this action. In this arrangement, the generative language structures are formed and the frequencies of sounds that form musical phrases and patterns are selected. We discuss the participation of certain neural brain structures and the establishment of reflexive neural circuits in the ad hoc transformation of perceptive signals, and show the similarities between the processes of perception and of biological self-maintenance and morphogenesis. We trace the peculiarities of the temporal encoding of emotions in music and musical creativity, as well as the principles of sharing musical information between the performing and the perceiving individuals.}
}
@article{VIDES202258,
title = {A Subspace Method for Time Series Anomaly Detection in Cyber-Physical Systems},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {41},
pages = {58-63},
year = {2022},
note = {4th IFAC Workshop on Cyber-Physical and Human Systems CPHS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.01.103},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323001106},
author = {Fredy Vides and Esteban Segura and Carlos Vargas-Agüero},
keywords = {Anomaly detection, Hankel matrix, time series analysis, sensors, signals},
abstract = {Time series anomaly detection is an important process for system monitoring and model switching, among other applications in cyber-physical systems. In this document we present a fast subspace method for time series anomaly detection, with a relatively low computational cost, that has been designed for anomaly detection in real sensor signals corresponding to dynamical systems. We also present some general results corresponding to the theoretical foundations of our method, together with a prototypical algorithm for time series anomaly detection. Some numerical examples corresponding to applications of the prototypical algorithm are presented, and some computational tools based on the theory and algorithms presented in this paper, are provided.}
}
@article{GUO202377,
title = {Notes on the improvement of concept-cognitive learning accuracy},
journal = {International Journal of Approximate Reasoning},
volume = {156},
pages = {77-96},
year = {2023},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2023.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X23000294},
author = {Keyi Guo and Jinhai Li and Xiao Zhang},
keywords = {Granular computing, Rough set, Concept lattice, Concept-cognitive learning, Learning accuracy},
abstract = {The concept-cognitive learning (CCL) process is the specific implementation step of simulating the human brain to learn concepts, and the CCL model is its core carrier. Different CCL models constructed by different cognitive minds will produce different concept learning results. The existing CCL model based on sufficient and necessary granule approximations regards the human's approximation idea in the face of inconsistent information as a logical criterion, and aims at finding the closest concept pair of the clue as concept learning results with a certain learning accuracy. However, the existing CCL method based on sufficient and necessary granule approximations cannot guarantee that the clue must be between its lower approximation and upper approximation, which causes the fact that the learning accuracy may not effectively measure the consistency of concept learning results. What is more, although the computational process obeys logical cognitive condition, the concept learning results may not conform to the actual situation, such as the case of merely generating full concepts and empty concepts. For the first problem, we improve the learning accuracy of the existing CCL method, propose a new CCL method with learning accuracy under hybrid lattice structure, and develop CCL algorithms for the cases of objects and attributes as clues. Moreover, experiments show the effectiveness of the proposed CCL method with learning accuracy under hybrid lattice structure. For the second problem, we put forward a CCL method based on non-logical associative mechanism to handle the unreasonable situation where the concept learning results are full concepts and empty concepts. Finally, two associative CCL algorithms are explored, and experiments are conducted to show their effectiveness.}
}
@article{LAURENT2025,
title = {HUMAn, a Real-Time Evolutive Patient Model for Major Incident Simulation: Development and Validation Study},
journal = {JMIR Formative Research},
volume = {9},
year = {2025},
issn = {2561-326X},
doi = {https://doi.org/10.2196/66201},
url = {https://www.sciencedirect.com/science/article/pii/S2561326X25001945},
author = {Maxence Laurent and Arnaud Jaccard and Laurent Suppan and Elio Erriquez and Xavier Good and Eric Golay and Dominique Jaccard and Mélanie Suppan},
keywords = {physiological model, mathematical model, computer simulation, major incident management, emergency medicine, mass casualties, healthcare professional education, professional education, continuing education},
abstract = {Background
Major incidents correspond to any situation where the location, number, severity, or type of casualties requires extraordinary resources. Major incident management must be efficient to save as many lives as possible. As any paramedic or emergency medical technician may unexpectedly have to respond to major incidents, regular training is mandatory. Those trainings usually include simulations. The vast majority of major incident simulations are limited by the fact that simulated patients do not evolve during the simulation, regardless of the time elapsed and treatment decisions. Therefore, most simulations fail to incorporate the critical temporal effect of decision-making.
Objective
This study aimed to develop and validate a simplified mathematical model of physiology, capable of plausibly simulating the real-time evolution of several injuries.
Methods
A modified version of the user-centered design framework, including a relevance, development, and validation phase, was used to define the development process of the physiological model. A 12-member design and development team was established, including prehospital physicians, paramedics, and computer scientists. To determine whether the developed model was clinically realistic, 15 experienced professionals working in the prehospital field participated in the validation phase. They were asked to rate clinical and physiological parameters according to a 5-point Likert scale ranging from 1 (impossible) to 5 (absolutely realistic).
Results
The design and development team led to the development of the HUMAn model (Human is an Uncomplicated Model of Anatomy). During the relevance phase, the team defined the needed features of the model: clinically realistic, able to compute the evolution of prehospital vital signs, yet simple enough to allow real-time computation for several simulated patients on regular computers or tablets. During the development phase, iterations led to the development of a heart-lung-brain interaction model coupled to functional blocks representing the main anatomical body parts. During the validation phase, the evolution of nine simulated patients presenting pathologies devised to test the different systems and their interactions was assessed. Overall, clinical parameters of all patients had a median rating of 5 (absolutely realistic; IQR 4-5). Most (n=52, 96%) individual clinical parameters had a median rating of 5, the remainder (n=2, 4%) being rated 4. Overall physiological parameters of all patients had a median rating of 5 (absolutely realistic; IQR 3-5). The majority of individual physiological parameters (n=43, 79%) had a median rating of 5, with (n=9, 17%) rated 4, and only (n=2 ,4%) rated 3.
Conclusions
A simplified model of trauma patient evolution was successfully created and deemed clinically realistic by experienced clinicians. This model should now be included in computer-based simulations and its impact on the teaching of major incident management assessed through randomized trials.}
}
@article{TERAMOTO201893,
title = {Betti number ratios as quantitative indices for bone morphometry in three dimensions},
journal = {Computer Methods and Programs in Biomedicine},
volume = {162},
pages = {93-98},
year = {2018},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2018.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0169260718300695},
author = {Takashi Teramoto and Takeshi Kamiya and Taira Sakurai and Fuminori Kanaya},
keywords = {Computational homology, Bone morphometry, Image processing},
abstract = {Background and objective: Computational homology is an emerging mathematical tool for characterizing shapes of data. In this work, we present a methodology using computational homology for obtaining quantitative measurements of the connectivity in bone morphometry. We introduce the Betti number ratios as novel morphological descriptor for the classification of bone fine structures in three dimensions.
Methods
A total of 51 Japanese white rabbits were used to investigate the connectivity of bone trabeculae after the administration of alendronate in a tendon graft model in rabbits. They were divided into a control group C and an experimental group A. Knee joints specimens were harvested for examination of their bone trabecular structure by micro-CT. Applying the computational homology software to the reconstructed 3D image data, we extract the morphological feature of a steric bone structure as the Betti numbers set (β0, β1, β2). The zeroth Betti number β0 indicates the number of the connected components corresponding to isolated bone fragments. The first and second Betti numbers, β1 and β2, indicate the numbers of open pores and closed pores of bone trabeculae, corresponding to a 2D empty space enclosed by a 1D curve and a 3D empty space enclosed by a 2D surface, respectively.
Results
We define the Betti number ratios β1/β0 and β2/β0 to better distinguish the two groups A and C in the scatter plots. Testing the discriminant function line for 29 data points of A (22 data points of C), the 17 points (resp. 18 points) are correctly classified into group A (resp. C). The accuracy rate is 35/51. The classification results in terms of the Betti number ratios are consistent with the histomorphometric measurements observed by medical doctors. Conclusions: This study is the first application of computational homology to bone morphometry in three dimensions. We show the mathematical basis of the Betti numbers index which are useful in a statistical description of the topological features of sponge-like structures. The potential benefits associated with our method include both improved quantification and reproducibility for the stereology.}
}
@article{20214105,
title = {Introducing new group leaders: Lorenzo Calviello},
journal = {Molecular Cell},
volume = {81},
number = {20},
pages = {4105-4108},
year = {2021},
issn = {1097-2765},
doi = {https://doi.org/10.1016/j.molcel.2021.08.043},
url = {https://www.sciencedirect.com/science/article/pii/S1097276521007334},
abstract = {Lorenzo Calviello tells us about his return to Milan, Italy, to set up his lab, which aims to untangle the complex life of mRNA using a mix of computational and experimental approaches; the kind of environment he hopes to promote as part of a wider scientific culture; and the importance of heavy metal and affordable education.}
}
@article{GIGERENZER200193,
title = {Content-blind norms, no norms, or good norms? A reply to Vranas},
journal = {Cognition},
volume = {81},
number = {1},
pages = {93-103},
year = {2001},
issn = {0010-0277},
doi = {https://doi.org/10.1016/S0010-0277(00)00135-9},
url = {https://www.sciencedirect.com/science/article/pii/S0010027700001359},
author = {Gerd Gigerenzer},
keywords = {Reasoning, Probability, Norms, Rationality, Fallacy, Error},
abstract = {In the psychology of thinking, little thought is given to what constitutes good thinking. Instead, normative solutions to problems have been accepted at face value, thereby determining what counts as a reasoning fallacy. I applaud Vranas (Cognition 76 (2000) 179) for thinking seriously about norms. I do, however, disagree with his attempt to provide post hoc justifications for supposed reasoning fallacies in terms of ‘content-neutral’ norms. Norms need to be constructed for a specific situation, not imposed upon it in a content-blind way. The reason is that content-blind norms disregard relevant structural properties of the given situation, including polysemy, reference classes, and sampling. I also show that content-blind norms can, unwittingly, lead to double standards: the norm in one problem is the fallacy in the next. The alternative to content-blind norms is not no norms, but rather carefully designed norms.}
}
@article{AVEN2002195,
title = {Implementing the Bayesian paradigm in risk analysis},
journal = {Reliability Engineering & System Safety},
volume = {78},
number = {2},
pages = {195-201},
year = {2002},
issn = {0951-8320},
doi = {https://doi.org/10.1016/S0951-8320(02)00161-8},
url = {https://www.sciencedirect.com/science/article/pii/S0951832002001618},
author = {T. Aven and J.T. Kvaløy},
keywords = {Bayesian, Risk analysis, True probabilities},
abstract = {The Bayesian paradigm comprises a unified and consistent framework for analyzing and expressing risk. Yet, we see rather few examples of applications where the full Bayesian setting has been adopted with specifications of priors of unknown parameters. In this paper, we discuss some of the practical challenges of implementing Bayesian thinking and methods in risk analysis, emphasizing the introduction of probability models and parameters and associated uncertainty assessments. We conclude that there is a need for a pragmatic view in order to ‘successfully’ apply the Bayesian approach, such that we can do the assignments of some of the probabilities without adopting the somewhat sophisticated procedure of specifying prior distributions of parameters. A simple risk analysis example is presented to illustrate ideas.}
}
@article{DRESHER1990137,
title = {A computational learning model for metrical phonology},
journal = {Cognition},
volume = {34},
number = {2},
pages = {137-195},
year = {1990},
issn = {0010-0277},
doi = {https://doi.org/10.1016/0010-0277(90)90042-I},
url = {https://www.sciencedirect.com/science/article/pii/001002779090042I},
author = {B.Elan Dresher and Jonathan D. Kaye},
abstract = {One of the major challenges to linguistic theory is the solution of what has been termed the “projection problem”. Simply put, linguistics must account for the fact that starting from a data base that is both unsystematic and relatively small, a human child is capable of constructing a grammar that mirrors, for all intents and purposes, the adult system. In this article we shall address ourselves to the question of the learnability of a postulated subsystem of phonological structure: the stress system. We shall describe a computer program which is designed to acquire this subpart of linguistic structure. Our approach follows the “principles and parameters” model of Chomsky (1981a, b). This model is particularly interesting from both a computational point of view and with respect to the development of learning theories. We encode the relevant aspects of universal grammar (UG) - those aspects of linguistic structure that are presumed innate}
}
@article{SANDERS2023107790,
title = {Methodological innovations to strengthen evidence-based serious illness communication},
journal = {Patient Education and Counseling},
volume = {114},
pages = {107790},
year = {2023},
issn = {0738-3991},
doi = {https://doi.org/10.1016/j.pec.2023.107790},
url = {https://www.sciencedirect.com/science/article/pii/S0738399123001702},
author = {Justin J. Sanders and Danielle Blanch-Hartigan and Jonathan Ericson and Elise Tarbi and Donna Rizzo and Robert Gramling and Liesbeth {van Vliet}},
keywords = {Communication, Palliative care, Methodology},
abstract = {Background/Objective
A growing population of those affected by serious illness, prognostic uncertainty, patient diversity, and healthcare digitalization pose challenges for the future of serious illness communication. Yet, there is paucity of evidence to support serious illness communication behaviors among clinicians. Herein, we propose three methodological innovations to advance the basic science of serious illness communication.
Results
First, advanced computation techniques – e.g. machine-learning techniques and natural language processing – offer the possibility to measure the characteristics and complex patterns of audible serious illness communication in large datasets. Second, immersive technologies – e.g., virtual- and augmented reality – allow for experimentally manipulating and testing the effects of specific communication strategies, and interactional and environmental aspects of serious illness communication. Third, digital-health technologies – e.g., shared notes and videoconferences – can be used to unobtrusively observe and manipulate communication, and compare in-person to digitally-mediated communication elements and effects. Immersive and digital health technologies allow integration of physiological measurement (e.g. synchrony or gaze) that may advance our understanding of patient experience.
Conclusion/practice implications
New technologies and measurement approaches, while imperfect, will help advance our understanding of the epidemiology and quality of serious illness communication in an evolving healthcare environment.}
}
@article{KIRGIL2022101668,
title = {“Do your part: Stay apart”: Collective intentionality and collective (in)action in US governor's COVID-19 press conferences},
journal = {Poetics},
volume = {93},
pages = {101668},
year = {2022},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2022.101668},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X22000304},
author = {Z.M. Kirgil and A. Voyer},
keywords = {Collective intentionality, Leadership, Democrat, Republican, COVID-19},
abstract = {This mixed-methods study examines how political leaders mobilize collective intentionality during the COVID-19 pandemic in nine US States, and how collective intentionality differs across republican and democratic administrations. The results of our computational and qualitative analyses show that i) political leaders establish collective intentionality by emphasizing unity, vulnerability, action, and community boundaries; ii) political leaders’ call to collective action clashes with the inaction required by health guidelines; iii) social inequalities received little attention across all states compared to other themes; and iv) collective intentionality in democratic administrations is linked to individuals’ agency and actions, suggesting a bottom-up approach. Conversely, in republican administrations individuals’ contributions are downplayed compared to work and state-level action, indicating a top-down approach. This study demonstrates the theoretical and empirical value of collective intentionality in sociological research, and contributes to a better understanding of leadership and prosociality in times of crisis.}
}
@article{SIMS202226,
title = {Externalized memory in slime mould and the extended (non-neuronal) mind},
journal = {Cognitive Systems Research},
volume = {73},
pages = {26-35},
year = {2022},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2021.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041721000954},
author = {Matthew Sims and Julian Kiverstein},
keywords = {Extended mind, Slime mould, Navigational memory, Basal cognition, Stigmergy, Sensorimotor coordination},
abstract = {The hypothesis of extended cognition (HEC) claims that the cognitive processes that materially realise thinking are sometimes partially constituted by entities that are located external to an agent’s body in its local environment. We show how proponents of HEC need not claim that an agent must have a central nervous system, or physically instantiate processes organised in such a way as to play a causal role equivalent to that of the brain if that agent is to be capable of cognition. Focusing on the case of spatial memory, we make our argument by taking a close look at the striking example of Physarum Polycephalum plasmodium (i.e., slime mould) which uses self-produced non-living extracellular slime trails to navigate its environment. We will argue that the use of externalized spatial memory by basal organisms like Physarum is an example of extended cognition. Moreover, it is a possible evolutionary precursor to the use of internal spatial memory and recall in animals thus demonstrating how extended cognition may have emerged early in evolutionary history.}
}
@article{GONZALEZRODRIGUEZ201827,
title = {Self-Organized Linguistic Systems: From traditional AI to bottom-up generative processes},
journal = {Futures},
volume = {103},
pages = {27-34},
year = {2018},
note = {Futures of Society: The Interactions Revolution},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2018.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0016328717302161},
author = {Diego Gonzalez-Rodriguez and Jose Rodolfo Hernandez-Carrion},
keywords = {Artificial intelligence, Self-organization, Emergence, Constructed languages, Conlangs, Agent-based modelling},
abstract = {This work seeks to explore the potential of bottom-up generative processes in the context of conlang production, aiming to describe the basis of a new field of research: Self-Organized Linguistic Systems or SOLS, specified under the perspective of both self-organized systems and constructed languages. SOLS approach provides a framework for the creation of self-generated artificial languages and may serve as a starting point for the development of context-dependent or domain-specific languages. It acknowledges that the development of conlangs can happen in artificial societies of simple agents, as the output of social interactions in computational simulations under the agent-based modelling paradigm. In the proposed initial SOLS model, automatic generation of lexicon takes place in the context of a digital environment with objects, actions and agents with embodied cognition through peer-to-peer interactions. Specifically, this paper exposes how SOLS can be developed with bi-dimensional games and simulations. An initial work has been done with the xmunch-atomspace and the SciArt simulator, which constitute the first implementations of both our knowledge representation toolbox and our bi-dimensional simulator of P2P Social Dynamics. Non-interactive agent-based SOLS can allow artificial agents to independently evolve emergent languages as part of their self-organizing or adaptation processes.}
}
@article{LOMBAERS1987387,
title = {Computational techniques in operations research: A.M. Andrew Computer Language & Programming Series, Abacus, Tunbridge Wells, 1985, viii + 201 pages, £14.95},
journal = {European Journal of Operational Research},
volume = {31},
number = {3},
pages = {387-388},
year = {1987},
note = {Methodology for Public Decision-Making Interactive Decision Support Systems Queue and Game Theory},
issn = {0377-2217},
doi = {https://doi.org/10.1016/0377-2217(87)90050-6},
url = {https://www.sciencedirect.com/science/article/pii/0377221787900506},
author = {H.J.M. Lombaers}
}
@incollection{YE201842,
title = {1.05 - Open Data and Open Source GIS},
editor = {Bo Huang},
booktitle = {Comprehensive Geographic Information Systems},
publisher = {Elsevier},
address = {Oxford},
pages = {42-49},
year = {2018},
isbn = {978-0-12-804793-4},
doi = {https://doi.org/10.1016/B978-0-12-409548-9.09592-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095489095920},
author = {Xinyue Ye},
keywords = {Open data, Open GIS, Open source},
abstract = {The multiple dimensions and scales of emerging open data pose numerous challenges for the application and evaluation of public policies. At the same time, domain researchers have been relatively slow to adopt and implement new spatiotemporally explicit data analysis methods due to the availability of suitable data and the lack of extensible software packages, which becomes a major impediment to the promotion of spatiotemporal thinking and collaboration. In this regard, more attention to open data and open source geographic information system (GIS) is necessary. Free access to the data and source code allows the broader GIS and domain science communities to incorporate additional advances in theoretical perspectives and analytical methods, thus facilitating interdisciplinary collaboration of spatial science and education. A case study of comparative LISA time path is illustrated in the open source GIS context. Additionally, open source implementation of new methods can expedite comparative studies of geographical dynamics.}
}
@article{MANDEVILLE1997397,
title = {The effect of teacher certification and task level on mathematics achievement},
journal = {Teaching and Teacher Education},
volume = {13},
number = {4},
pages = {397-407},
year = {1997},
issn = {0742-051X},
doi = {https://doi.org/10.1016/S0742-051X(96)00031-5},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X96000315},
author = {Garrett K Mandeville and Qiduan Liu},
abstract = {In this study it was hypothesized that the degree of content area preparation of seventh grade mathematics teachers would differentially effect student performance as a function of the level of the mathematics tasks used to assess that performance. More specifically, students of high MATHPREP teachers were hypothesized to outperform those of low MATHPREP teachers on the higher level tasks. The sample consisted of over 9000 seventh grade students from 33 matched pairs of schools whose teachers differed on level of mathematics preparation. Aggregate measures of achievement for the students at each school on test items representing three ordered levels of thinking were obtained. The primary hypothesis was addressed using a single df interaction contrast and produced a statistically and practically significant result in the hypothesized direction. Implications for teacher preparation and the hiring practices of school administrators are considered.}
}
@article{LEE2019325,
title = {Improving process safety: What roles for Digitalization and Industry 4.0?},
journal = {Process Safety and Environmental Protection},
volume = {132},
pages = {325-339},
year = {2019},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2019.10.021},
url = {https://www.sciencedirect.com/science/article/pii/S0957582019317057},
author = {John Lee and Ian Cameron and Maureen Hassall},
keywords = {Process safety, Digital twin, Digitalization, Industry 4.0, Models, Life cycle, ISO15926},
abstract = {Process safety and risk management remain a significant challenge for the process and manufacturing industries. Digital systems have been applied over many decades to assist in process safety management throughout the lifecycle of a process plant. There has been much hype in recent years regarding Industry 4.0, digitalization and digital twins regarding the transformative potential that exists within these technologies to improve operational performance and reduce process safety accidents. In this article, a fundamental systems thinking approach is applied to the implementation of the digital twin within the process industries. The importance of having a standardized language and ontology, such as ISO15926, enables the use of reasoning engines and the ability to interconnect models and systems across the process and product lifecycle. We discuss use-cases and forms of the digital twin to improve safety within the process industries. A specific focus shows how an operator training simulator and its embedded dynamic models are applied within this environment. The article concludes with a summary of process safety related opportunities and threats associated with the application of digitalized dynamic models in industry.}
}
@article{STROJNY2024,
title = {Use of 4 Open-Ended Text Responses to Help Identify People at Risk of Gaming Disorder: Preregistered Development and Usability Study Using Natural Language Processing},
journal = {JMIR Serious Games},
volume = {12},
year = {2024},
issn = {2291-9279},
doi = {https://doi.org/10.2196/56663},
url = {https://www.sciencedirect.com/science/article/pii/S2291927924001053},
author = {Paweł Strojny and Ksawery Kapela and Natalia Lipp and Sverker Sikström},
keywords = {gaming disorder, natural language processing, machine learning, mental health, NLP, text, open-ended, response, risk, psychological, Question-based Computational Language Assessment, QCLA, transformers-based, language model analysis, Polish, Pearson, correlation, Python},
abstract = {Background
Words are a natural way to describe mental states in humans, while numerical values are a convenient and effective way to carry out quantitative psychological research. With the growing interest of researchers in gaming disorder, the number of screening tools is growing. However, they all require self-quantification of mental states. The rapid development of natural language processing creates an opportunity to supplement traditional rating scales with a question-based computational language assessment approach that gives a deeper understanding of the studied phenomenon without losing the rigor of quantitative data analysis.
Objective
The aim of the study was to investigate whether transformer-based language model analysis of text responses from active gamers is a potential supplement to traditional rating scales. We compared a tool consisting of 4 open-ended questions formulated based on the clinician's intuition (not directly related to any existing rating scales for measuring gaming disorders) with the results of one of the commonly used rating scales.
Methods
Participants recruited using an online panel were asked to answer the Word-Based Gaming Disorder Test, consisting of 4 open-ended questions about gaming. Subsequently, they completed a closed-ended Gaming Disorders Test based on a numerical scale. Of the initial 522 responses collected, we removed a total of 105 due to 1 of 3 criteria (suspiciously low survey completion time, providing nonrelevant or incomplete responses). Final analyses were conducted on the responses of 417 participants. The responses to the open-ended questions were vectorized using HerBERT, a large language model based on Google's Bidirectional Encoder Representations from Transformers (BERT). Last, a machine learning model, specifically ridge regression, was used to predict the scores of the Gaming Disorder Test based on the features of the vectorized open-ended responses.
Results
The Pearson correlation between the observable scores from the Gaming Disorder test and the predictions made by the model was 0.476 when using the answers of the 4 respondents as features. When using only 1 of the 4 text responses, the correlation ranged from 0.274 to 0.406.
Conclusions
Short open responses analyzed using natural language processing can contribute to a deeper understanding of gaming disorder at no additional cost in time. The obtained results confirmed 2 of 3 preregistered hypotheses. The written statements analyzed using the results of the model correlated with the rating scale. Furthermore, the inclusion in the model of data from more responses that take into account different perspectives on gaming improved the performance of the model. However, there is room for improvement, especially in terms of supplementing the questions with content that corresponds more directly to the definition of gaming disorder.
Trial Registration
OSF Registries osf.io/957nz; https://osf.io/957nz}
}
@article{ASEMI2025106795,
title = {Improving EEG signal-based emotion recognition using a hybrid GWO-XGBoost feature selection method},
journal = {Biomedical Signal Processing and Control},
volume = {99},
pages = {106795},
year = {2025},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2024.106795},
url = {https://www.sciencedirect.com/science/article/pii/S174680942400853X},
author = {Hanie Asemi and Nacer Farajzadeh},
keywords = {Emotion recognition, Brain signals, EEG, Prediction, Feature selection, Machine learning},
abstract = {Emotion plays a crucial role in daily life, influencing cognitive functions such as language comprehension, decision-making, attention, and concentration. With the growing integration of computer systems into our everyday activities, it is essential to understand and detect emotional states accurately. Emotion detection through EEG signals allows direct assessment of the human’s internal state and is considered an important factor in the interaction between humans and external devices. In this paper, we introduce a novel feature selection algorithm proposed to improve the accuracy of emotion classification using EEG signals, aligned with decreasing the input dimension to reduce computations, making it more suitable for real-time applications. We performed two experiments utilizing the DEAP and the MAHNOB-HCI datasets. Various features were extracted and employed for emotion classification using SVM, KNN, and XGBoost classifiers. Initially, the highest accuracy for binary emotion classification in the DEAP dataset was achieved with statistical features and the XGBoost model, reaching 78.85% for arousal and 79.02% for valence. In the MAHNOB-HCI dataset, the highest accuracy with statistical features and the XGBoost model was 67.08% for arousal and 62.24% for valence. Subsequently, we applied the grey wolf optimization algorithm as a feature selection method, optimizing the cost function based on XGBoost accuracy. This approach significantly enhanced the classification performance. For the DEAP dataset, accuracy increased to 89.63% for arousal and 89.08% for valence using statistical features. For the MAHNOB-HCI dataset, accuracy improved to 84.94% for arousal and 82.29% for valence using statistical features.}
}
@article{LEITEFILHO2024100381,
title = {Evaluating chatbot user experience (UX) through electroencephalography measures: A systematic literature review},
journal = {Computers in Human Behavior Reports},
volume = {13},
pages = {100381},
year = {2024},
issn = {2451-9588},
doi = {https://doi.org/10.1016/j.chbr.2024.100381},
url = {https://www.sciencedirect.com/science/article/pii/S2451958824000149},
author = {Jaime Ranulfo {Leite Filho} and Thiago Adriano Coleti and Marcelo Morandini},
keywords = {User experience, Chatbot, Virtual assistant, Conversational agent, Natural language interface, Brain-computer interface, Electroencephalography, EEG, Evaluating model methodology approach strategy},
abstract = {Brain activity is a biological signal with unique characteristics that can determine important patterns for recording and processing. The electroencephalogram (EEG) is the most used signal because it measures brain electrical activity, offering greater resolution and data accuracy. When associated to brain activities in user-computer interactions, it can provide information that allows analyzing adequacy and user satisfaction. Thus, the objective of this paper is to identify works that specify which information on electroencephalography assessment may be used to compose an analysis of its interactions with conversational systems. The delimited that the research problems are: (1) What information about user experience by electroencephalography can be used to compose an analysis of their interactions with conversational systems? (2) What techniques are used to present user experience information by EEG to individuals? Based on the Systematic Review method, seven studies were identified that examined commercial EEG devices for UX assessment between 2011 and 2022. The current study found that multiple emotional stimuli were used and reported. The most popular technique among researchers is event induced emotional stimulation, in which participants passively perceive emotional stimuli such as images, music and videos to evoke certain emotions.}
}
@article{KANGASHARJU2022100048,
title = {Lower secondary students’ poetry writing with the AI-based Poetry Machine},
journal = {Computers and Education: Artificial Intelligence},
volume = {3},
pages = {100048},
year = {2022},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2022.100048},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X22000030},
author = {Arja Kangasharju and Liisa Ilomäki and Minna Lakkala and Auli Toom},
keywords = {AI-based learning, Lower secondary education, Technology in education, Poetry writing, AI-supported writing},
abstract = {Despite poetry’s important role in improving linguistic skills and creative thinking, students often find poetry writing to be difficult and boring. This study is an investigation of how the digital Poetry Machine influences students’ poetry writing by applying AI techniques. It uses qualitative and quantitative analysis of the log data of poems that the seventh graders wrote with the Poetry Machine. The results show that the draft poems functioned as affordances, which the students followed as models. The drafts encouraged students to experiment with several different poetic features. The data suggest an association between the number of edited versions and the quality of the final poem. The results suggest that a co-creative AI-based tool inspires and supports those students who engage in the writing process, and the poems are developed from the first versions. More studies regarding the role of AI based digital tools in developing students’ writing competencies would be worthwhile.}
}
@article{SPRUGNOLI201799,
title = {Neural correlates of Eureka moment},
journal = {Intelligence},
volume = {62},
pages = {99-118},
year = {2017},
issn = {0160-2896},
doi = {https://doi.org/10.1016/j.intell.2017.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0160289616302756},
author = {Giulia Sprugnoli and Simone Rossi and Alexandra Emmendorfer and Alessandro Rossi and Sook-Lei Liew and Elisa Tatti and Giorgio {di Lorenzo} and Alvaro Pascual-Leone and Emiliano Santarnecchi},
keywords = {Insight, , , Cognition, fMRI, EEG, ERPs, Non-invasive brain stimulation, Neuroenhancement, NIBS, Creativity},
abstract = {Insight processes that peak in “unpredictable moments of exceptional thinking” are often referred to as Aha! or Eureka moments. During insight, connections between previously unrelated concepts are made and new patterns arise at the perceptual level while new solutions to apparently insolvable problems suddenly emerge to consciousness. Given its unpredictable nature, the definition, and behavioral and neurophysiological measurement of insight problem solving represent a major challenge in contemporary cognitive neuroscience. Numerous attempts have been made, yet results show limited consistency across experimental approaches. Here we provide a comprehensive overview of available neuroscience of insight, including: i) a discussion about the theoretical definition of insight and an overview of the most widely accepted theoretical models, including those debating its relationship with creativity and intelligence; ii) an overview of available tasks used to investigate insight; iii) an ad-hoc quantitative meta-analysis of functional magnetic resonance imaging studies investigating the Eureka moment, using activation likelihood estimation maps; iv) a review of electroencephalographic evidence in the time and frequency domains, as well as v) an overview of the application of non-invasive brain stimulation techniques to causally assess the neurobiological basis of insight as well as enhance insight-related cognition.}
}
@article{TULVER2025106081,
title = {The road to Aha: A recipe for mental breakthroughs},
journal = {Cognition},
volume = {257},
pages = {106081},
year = {2025},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2025.106081},
url = {https://www.sciencedirect.com/science/article/pii/S0010027725000216},
author = {Kadi Tulver and Karl Kristjan Kaup and Jaan Aru},
keywords = {Insight, Transformation, Problem-solving, Predictive processing, Psychotherapy, Psychedelics, Meditation, Attractor dynamics},
abstract = {We present a novel framework for understanding the diverse spectrum of mental breakthrough events, ranging from problem-solving insights to profound personal transformations. We propose that these events, while varied in expression and impact, share common underlying mechanisms of representational change. We also hypothesise that the differences in phenomenological intensity can be conceptualized along a continuum. Central to our model are three core components – tension, altered salience, and enhanced flexibility – which we identify as essential prerequisites for significant cognitive restructuring. These components interact within an iterative cycle, influencing both the emergence and nature of insight experiences. Drawing on examples from different fields, we explore how a conflict between existing models can trigger this cycle, wherein mechanisms of attention allocation and relaxation of constraints work in tandem to facilitate the emergence of insights. Furthermore, we propose that the intensity of the “aha-moment” and the breadth of its impact are contingent on how central the conflict is within one's conceptual landscape and the extent to which existing mental models are challenged. Thus, the model accounts for both the subtle, momentary insights in problem-solving and the transformative realizations that reshape core beliefs and self-perception. By synthesising insights from various domains, including psychotherapy, contemplative science, and psychedelic research, we present a theoretical account with broad scope, aiming to shed light on the complex processes that can lead to a wide array of mental breakthroughs, thereby contributing to the understanding of insight phenomena across disciplines.}
}
@article{PETIT2018135,
title = {Combining eco-social and environmental indicators to assess the sustainability performance of a food value chain: A case study},
journal = {Journal of Cleaner Production},
volume = {191},
pages = {135-143},
year = {2018},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2018.04.156},
url = {https://www.sciencedirect.com/science/article/pii/S0959652618311831},
author = {Gaëlle Petit and Caroline Sablayrolles and Gwenola {Yannou-Le Bris}},
keywords = {Life cycle assessment, Pork value chain, Sustainability, Metrics, Indicator, Framework},
abstract = {Stakeholders are increasingly demanding transparency on food value chain sustainability performance. Today there is no standard framework to meet this demand and support defining indicators to be used to conduct an overall sustainable performance assessment. This paper mobilizes existing frameworks and indicators to build new sustainable performance metrics for actors willing to work together for their value chain sustainability. Popular methods or tools for assessing dimensions of agrifood products or activities are selected and analyzed to determine how they could contribute to this metric. The analysis aims to distinguish the sustainable development pillars addressed (economic, environmental and/or social), the frames concerned (life cycle thinking or not; multi-actor or not), and the focus of performance measured (drivers, pressures, states, impacts, responses). This categorization is then used to develop a proposal for specifications adapted to food value chain sustainability performance assessment. The applicability of the framework is demonstrated through a case study in a pork agrifood value chain.}
}
@incollection{JANI202491,
title = {13.05 - Recent advances in simulation in fiber-reinforced polymer composites: Mechanical properties and applications},
editor = {Saleem Hashmi},
booktitle = {Comprehensive Materials Processing (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {91-109},
year = {2024},
isbn = {978-0-323-96021-2},
doi = {https://doi.org/10.1016/B978-0-323-96020-5.00182-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323960205001825},
author = {Hasan Rafsan Jani and Md Zillur Rahman},
keywords = {Natural fiber, Composites, FRP composites, Hybrid composites, Mechanical properties, Finite element analysis, Simulation, Industrial applications},
abstract = {This study provides an overview of the recent advances in simulation techniques applied to fiber-reinforced polymer (FRP) composites, focusing on analyzing their mechanical properties and applications. FRP composites have gained significant attention in various industries due to their excellent mechanical properties, lightweight nature, and corrosion resistance. Simulating the behavior of FRP composites allows researchers and engineers to optimize their design, understand their mechanical performance, and predict their response under different loading conditions. This study explores different theoretical models for predicting the mechanical properties of composites. This study also acknowledges the challenges of characterizing these heterogeneous materials, making computational techniques such as finite element analysis indispensable for accurate predictions. In addition, simulated mechanical properties of composites with varying fibers and matrices are explored and compared with experimental results. Furthermore, the applications of simulation in different industries for characterizing FRP composites are discussed.}
}
@article{SAMSONOVICH2012100,
title = {On a roadmap for the BICA Challenge},
journal = {Biologically Inspired Cognitive Architectures},
volume = {1},
pages = {100-107},
year = {2012},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2012.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X12000126},
author = {Alexei V. Samsonovich},
keywords = {Human-level AI, Cognitive architectures, Turing test, Newell list, Critical mass},
abstract = {The BICA Challenge is the challenge to create a general-purpose, real-life computational equivalent of the human mind using an approach based on biologically inspired cognitive architectures (BICA). To solve it, we need to understand at a computational level how natural intelligent systems develop their cognitive, metacognitive and learning functions. The solution is expected to lead us to a breakthrough to intelligent agents integrated into the human society as its members. This outcome has the potential to solve many problems of the modern world. The article starts from the roadmap proposed by Dr. James Albus for a national program unifying artificial intelligence, neuroscience and cognitive science. The BICA Challenge is introduced in this context as a waypoint on the expanded roadmap. The gap between the state of the art and challenge demands is analyzed. Specific problems and barriers are identified, an approach to overcoming them is proposed, and an ultimate practical criterion for success is formulated. It is estimated that the BICA Challenge can be solved within a decade.}
}
@article{ALBABA20191,
title = {Modeling cyber-physical human systems via an interplay between reinforcement learning and game theory},
journal = {Annual Reviews in Control},
volume = {48},
pages = {1-21},
year = {2019},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2019.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S1367578819301026},
author = {Berat Mert Albaba and Yildiray Yildiz},
keywords = {Cyber-physical human systems, Game theory, Reinforcement learning, Model validation},
abstract = {Predicting the outcomes of cyber-physical systems with multiple human interactions is a challenging problem. This article reviews a game theoretical approach to address this issue, where reinforcement learning is employed to predict the time-extended interaction dynamics. We explain that the most attractive feature of the method is proposing a computationally feasible approach to simultaneously model multiple humans as decision makers, instead of determining the decision dynamics of the intelligent agent of interest and forcing the others to obey certain kinematic and dynamic constraints imposed by the environment. We present two recent exploitations of the method to model (1) unmanned aircraft integration into the National Airspace System and (2) highway traffic. We conclude the article by providing ongoing and future work about employing, improving and validating the method. We also provide related open problems and research opportunities.}
}
@article{WANG2024104116,
title = {Revealing association rules within intricate ecosystems: A spatial co-location mining method based on Geo-Eco knowledge graph},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {133},
pages = {104116},
year = {2024},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2024.104116},
url = {https://www.sciencedirect.com/science/article/pii/S1569843224004709},
author = {Jinghan Wang and Guangyue Li and Tinghua Ai},
keywords = {Geo-eco knowledge graph, Spatial co-location pattern, Environmental similarity, Neo4j graph database},
abstract = {The analysis of association rules within ecosystems is crucial for monitoring, managing, and conserving natural resources. As widely adopted approaches for this task, geospatial methods involving spatial co-location pattern mining can reveal distribution rules and inherent associations among diverse geographical elements. Rooted in Tobler’s first law of geography, these methods focus on the impact of spatial proximity. However, apart from proximity, heterogeneity of environmental attributes such as elevation, temperature and precipitation are also essential for the formation of associations. For environmental co-location (Eco-location) pattern detection, we propose a method based on the Geo-Eco Knowledge Graph (GEKG) to mine multi-impact association rules. Firstly, we introduce the Adaptive Threshold (AT) to constrain the Delaunay triangular network, dynamically regulating adjacency relationships to generate geo-eco knowledge graph’s skeleton. For comprehensive ecosystem representation, various environmental attributes are integrated as semantic information into GEKG. In the reasoning of Eco-location patterns, we innovate beyond the traditional co-location paradigm by considering both spatial proximity and semantic similarity. Under the impact of various environmental information, sub-sets of geographically proximate entities are extracted to detect Eco-location patterns. For effective management and efficient computation, we utilize the Neo4j graph database to manage large-scale GEKG and mine Eco-location patterns with its graph search function. Experiments conducted on simulated and real-world ecological datasets show that, compared to existing techniques, our GEKG-based method can detect Eco-location patterns with greater accuracy and efficiency.}
}
@article{DAHMANI2021128847,
title = {Smart circular product design strategies towards eco-effective production systems: A lean eco-design industry 4.0 framework},
journal = {Journal of Cleaner Production},
volume = {320},
pages = {128847},
year = {2021},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2021.128847},
url = {https://www.sciencedirect.com/science/article/pii/S0959652621030432},
author = {Noureddine Dahmani and Khalid Benhida and Amine Belhadi and Sachin Kamble and Said Elfezazi and Sunil Kumar Jauhar},
keywords = {Lean design, Eco-design, Industry 4.0, Circular economy, Sustainable products, Circular business models, Smart circular product design},
abstract = {For industrial enterprises, the transformation to circular business models can be curbed both by operational tools and the lack of relevant data. Lean thinking has offered great flexibility in production processes and systems by challenging mass production practices, resulting in more “Lean” products with less waste. Lean design and Eco-design, associated with Industry 4.0 technologies, can be an efficient structured and methodological approach in developing products based on the circular economy strategies. Indeed, decisions made during the product design stage can significantly impact the sustainability of products throughout their life cycle. Hence, lean design combined with eco-design and Industry 4.0 represents an innovative model to include sustainability throughout the product life cycle. This paper explores the relationship between lean eco-design and I4.0 strategies for designing eco-efficient products based on a literature review. The proposed framework is based on the synergic use of Lean design, Eco-design, and Industry 4.0. It offers the right formula to deliver better and cleaner products using appropriate processes to support manufacturers in designing products, fulfilling customers' needs and expectations. It provides scholars, designers, and managers with valuable insights into deploying strategies to design sustainable products.}
}
@article{ACAR2010405,
title = {Designing insightful inquiring systems for sustainable organizational foresight},
journal = {Futures},
volume = {42},
number = {4},
pages = {405-416},
year = {2010},
note = {Learning the Future Faster},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2009.11.025},
url = {https://www.sciencedirect.com/science/article/pii/S0016328709001992},
author = {William Acar and Douglas A. Druckenmiller},
abstract = {This paper contributes to the theory of collaborative problem solving and strategy design by reviewing the state of the art in the application of problem solving and dialectical methods, and then linking up with analytical and computer-aided approaches. Churchman's concept of dialectical inquiry (DI) is presented, and some major derivatives of DI are reviewed, as well as an integrative method for sustainable insightful foresight developed by the authors called comprehensive situation mapping (CSM). In addition to its dialectical process side, CSM offers computational capabilities for devising and figuring out change scenarios. The theory for the manual application of CSM is summarized; in addition, its recent computerized version is presented and likely future improvements are sketched out in light of a current spate of case studies investigating the user-friendliness of its computerization.}
}
@article{VASEY2025113719,
title = {Influence of initial conditions on data-driven model identification and information entropy for ideal mhd problems},
journal = {Journal of Computational Physics},
volume = {524},
pages = {113719},
year = {2025},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2025.113719},
url = {https://www.sciencedirect.com/science/article/pii/S0021999125000026},
author = {Gina Vasey and Daniel Messenger and David Bortz and Andrew Christlieb and Brian O'Shea},
keywords = {Weak sparse identification of nonlinear dynamics, Data-driven, Ideal MHD, Shannon information entropy},
abstract = {Data-driven methods of model identification are able to discern governing dynamics of a system from data. Such methods are well suited to help us learn about systems with unpredictable evolution or systems with ambiguous governing dynamics given our current understanding. Many plasma problems of interest fall into these categories as there are a wide range of models that exist, however each model is only useful in a certain regime and often limited by computational complexity. To ensure data-driven methods align with theory, they must be consistent and predictable when acting on data whose governing dynamics are known. Weak Sparse Identification of Nonlinear Dynamics (WSINDy) is a recently developed data-driven method that has shown promise in learning governing dynamics from data with high noise levels [1]. This work examines how WSINDy acts on ideal MHD test problems as the initial conditions are varied and specifies limiting requirements for successful equation identification. It is hard to recover the governing dynamics from data that emphasize a single dominant behavior. In these low information cases, Shannon information entropy is able to pick up on the redundancies in the data that affect recoverability.}
}
@article{MEYER202013,
title = {Changing Design Education for the 21st Century},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {6},
number = {1},
pages = {13-49},
year = {2020},
note = {Design Education. Part I},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2019.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S2405872620300046},
author = {Michael W. Meyer and Don Norman},
keywords = {Design education, Design-driven transformation, Design thinking, Design doing, Major societal challenges, Complex sociotechnical systems, DesignX},
abstract = {Designers are entrusted with increasingly complex and impactful challenges. However, the current system of design education does not always prepare students for these challenges. When we examine what and how our system teaches young designers, we discover that the most valuable elements of the designer’s perspective and process are seldom taught. Instead, some designers grow beyond their education through their experience working in industry, essentially learning by accident. Many design programs still maintain an insular perspective and an inefficient mechanism of tacit knowledge transfer. Meanwhile, skills for developing creative solutions to complex problems are increasingly essential. Organizations are starting to recognize that designers bring something special to this type of work, a rational belief based upon numerous studies that link commercial success to a design-driven approach. So, what are we to do? Other learned professions such as medicine, law, and business provide excellent advice and guidance embedded within their own histories of professionalization. In this article, we borrow from their experiences to recommend a course of action for design. It will not be easy: it will require a study group to make recommendations for a roster of design and educational practices that schools can use to build a curriculum that matches their goals and abilities. And then it will require a conscious effort to bootstrap the design profession toward both a robust practitioner community and an effective professoriate, capable together of fully realizing the value of design in the 21st century. In this article, we lay out that path.}
}
@article{LI2025100894,
title = {A concise review of intelligent game agent},
journal = {Entertainment Computing},
volume = {52},
pages = {100894},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100894},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124002623},
author = {Hui Li and Xinyi Pang and Bixia Sun and Kexin Liu},
keywords = {Intelligent agent, Artificial intelligence, Monte Carlo tree, Reinforcement learning, Large language models},
abstract = {Intelligent game agents are crafted using AI technologies to mimic player behavior and make decisions autonomously. Over the past decades, the scope of intelligent agents has broadened from chess to encompass content generation, player modeling, and result prediction, reflecting the field’s evolving and multifaceted nature. In this paper, we conduct a systematic review of recent literature on intelligent methods and applications of game agents, along with general game agent frameworks. Our findings suggest that creating general intelligent agents remains a significant challenge, yet it is worthwhile to explore methods that better integrate the strengths of different techniques to build more robust and adaptable intelligent game agents.}
}
@article{LIANG201291,
title = {Psychological-Physical Force Model for Bicycle Dynamics},
journal = {Journal of Transportation Systems Engineering and Information Technology},
volume = {12},
number = {2},
pages = {91-97},
year = {2012},
issn = {1570-6672},
doi = {https://doi.org/10.1016/S1570-6672(11)60197-9},
url = {https://www.sciencedirect.com/science/article/pii/S1570667211601979},
author = {Xiao LIANG and Baohua MAO and Qi XU},
keywords = {urban traffic, bicycle, micro behavior, dynamics, psychological-physical force model, interaction},
abstract = {The core challenge in modeling bicyclist behavior dynamics is how to tackle the interaction between the lateral and the longitudinal movements. Further the bicycle transportation could be considered as multi-particle self-driven system. The combined dynamic model, psychological-physical force model (PPFM) and trajectories choice model (TCM), is proposed as a multi agent model to describe bicycle microscopic behavior dynamics. The PPFM is a continuous force model, which obeyed to the Newton's second law. By introducing the trajectories choice behavior in the tactical level, the TCM is modeled to describe the ability to individual autonomous thinking and to respond to changes ambient conditions for predefined behavior tank. Through designing computational experiments, the simulation data is collected to calibrate and validate the models. The simulation results show that the fundamental diagram obtained by simulation is dovetail into the empirical data. The PPFM is capable of describing the nonlinear interaction between individuals and the microscopic behavior of the proposed bicycle dynamic model with reasonable traffic.
摘要
自行车微观行为动力学建模的关键是如何描述自行车横向和纵向的运动关系。本文将自行车交通系统视作具有自主性的多粒子系统，提出了由心理生理力模型和轨迹选择模型构成的、描述自行车微观行为动力学特性的多主体模型。心理生理力模型为连续力模型，自行车/骑行者个体被视作是受心理力和生理力作用的、服从牛顿力学的基本粒子。在轨迹选择模型中，通过在行为模型层面引入个体运动的轨迹选择行为，预定义个体面对不同交通状况时的行为库，描述组成自行车群体中的个体独立思考和对周围环境变化做出反应的能力。通过设计计算机模拟实验并收集数据，对模型进行有效性验证。模拟结果表明：模拟得到的自行车交通流的密度-速度关系与实测数据具有良好的一致性，认为本文提出的自行车微观行为动力学模型具有交通上的合理性。}
}
@incollection{BURK2023457,
title = {Chapter 24 - Analytics architectures for the 21st century},
editor = {Gary D. Miner and Linda A. Miner and Scott Burk and Mitchell Goldstein and Robert Nisbet and Nephi Walton and Thomas Hill},
booktitle = {Practical Data Analytics for Innovation in Medicine (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {457-472},
year = {2023},
isbn = {978-0-323-95274-3},
doi = {https://doi.org/10.1016/B978-0-323-95274-3.00017-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323952743000178},
author = {Scott Burk},
keywords = {Data literacy, center of excellence, culture, data architecture, APIS, microservices, streaming data, data stores, data virtualization, 5v’s of data, master data, reference data, metadata, data governance, data management, exploratory data analysis, data prep, feature engineering, model selection, model evaluation, model deployment, model monitoring, model Ops},
abstract = {Artificial Intelligence (AI) and analytics efforts are too often just extensions or isolated additions to medical practice and research. It could be an extension of research efforts, part of a medical degree curriculum or residency program to extend scholarly pursuits. It could be hospital systems wishing to gain operation efficiency and control. It could be physician groups-of-practice trying to mitigate risk or improve financial performance. While isolated efforts may provide some benefit, leaders understand that exceptional results require a paradigm shift in thinking and the infrastructure to support the new technologies and innovation. In this chapter, we present three pillars that lay the foundation for success in medical research and results in the 21st century. Successful participants will have designed and implemented the following architectures.}
}
@article{WRIGHT2025102048,
title = {PERSONALIZED PROFILES OF ORAL HEALTH AND DISEASE: USING HIGH-DIMENSIONAL VECTOR MODELS FOR GUIDING PRECISION DENTAL CARE},
journal = {Journal of Evidence-Based Dental Practice},
volume = {25},
number = {1, Supplement },
pages = {102048},
year = {2025},
issn = {1532-3382},
doi = {https://doi.org/10.1016/j.jebdp.2024.102048},
url = {https://www.sciencedirect.com/science/article/pii/S1532338224000988},
author = {CASEY D. WRIGHT and MARCUS G. WILD and REBECCA CUTLER and KIMON DIVARIS},
keywords = {Oral health-related quality of life, Dental patient reported outcomes, Operationalization, Precision modeling},
abstract = {Operationalizing the oral health experience is an ongoing effort with various clinical and patient-reported outcomes contributing to such conceptualizations. Computational technology has afforded advances in the ability to model complex interactions between various phenomena and provides an opportunity to reconsider the way oral health is conceptualized. High-dimensional vector space modeling is introduced and discussed as a theoretical way to incorporate all relative features associated with understanding oral health, including clinical, patient-reported, and demographic information. Specifically, a novel application of high-dimensional vector space models is proposed as a vehicle to operationalize the 3P model of oral health. Additionally, this paper outlines how this approach can 1) create more precise, person-level characterizations of oral health; 2) track oral health over time, offering greater opportunities for behavioral interventions to prevent, mitigate, or treat the negative impacts of dental, oral, and craniofacial diseases; and 3) offer comparisons to dynamically tuned comparison vectors which can define “good” oral health and quantify disparities and features on which to intervene to mitigate them.}
}
@article{RAYBOURN2014471,
title = {A new paradigm for serious games: Transmedia learning for more effective training and education},
journal = {Journal of Computational Science},
volume = {5},
number = {3},
pages = {471-481},
year = {2014},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2013.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S1877750313001014},
author = {Elaine M. Raybourn},
keywords = {Transmedia learning, Serious games, Transmedia campaigns, Storytelling, Social media, Data mining, xAPI, MOOC},
abstract = {Serious games present a relatively new approach to training and education for international organizations such as NATO (North Atlantic Treaty Organization), non-governmental organizations (NGOs), the U.S. Department of Defense (DoD) and the U.S. Department of Homeland Security (DHS). Although serious games are often deployed as stand-alone solutions, they can also serve as entry points into a comprehensive training pipeline in which content is delivered via different media to rapidly scale immersive training and education for mass audiences. The present paper introduces a new paradigm for more effective and scalable training and education called transmedia learning. Transmedia learning leverages several new media trends including the peer communications of social media, the scalability of massively openonline course (MOOCs), and the design of transmedia storytelling used by entertainment, advertising, and commercial game industries to sustain audience engagement. Transmedia learning is defined as the scalable system of messages representing a narrative or core experience that unfolds from the use of multiple media, emotionally engaging learners by involving them personally in the story. In the present paper, we introduce the transmedia learning paradigm as offering more effective use of serious games for training and education. This approach is consistent with the goals of international organizations implementing approaches similar to those described by the Army Learning Model (ALM) to deliver training and education to Soldiers across multiple media. We discuss why the human brain is wired for transmedia learning and demonstrate how the Simulation Experience Design Method can be used to create transmedia learning story worlds for serious games. We describe how social media interactions and MOOCs may be used in transmedia learning, and how data mining social media and experience tracking can inform the development of computational learner models for transmedia learning campaigns. Examples of how the U.S. Army has utilized transmedia campaigns for strategic communication and game-based training are provided. Finally, we provide strategies the reader can use today to incorporate transmedia storytelling elements such as Internet, serious games, video, social media, graphic novels, machinima, blogs, and alternate reality gaming into a new paradigm for training and education: transmedia learning.}
}
@article{ACHARYA2025108652,
title = {EEGConvNeXt: A novel convolutional neural network model for automated detection of Alzheimer's Disease and Frontotemporal Dementia using EEG signals},
journal = {Computer Methods and Programs in Biomedicine},
volume = {262},
pages = {108652},
year = {2025},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2025.108652},
url = {https://www.sciencedirect.com/science/article/pii/S0169260725000690},
author = {Madhav Acharya and Ravinesh C Deo and Prabal Datta Barua and Aruna Devi and Xiaohui Tao},
keywords = {EEGConvNeXt, AD detection, Transformer-like CNN, EEG analysis, Signal processing},
abstract = {Background and objective
Deep learning models have gained widespread adoption in healthcare for accurate diagnosis through the analysis of brain signals. Neurodegenerative disorders like Alzheimer's Disease (AD) and Frontotemporal Dementia (FD) are increasingly prevalent due to age-related brain volume reduction. Despite advances, existing models often lack comprehensive multi-class classification capabilities and are computationally expensive. This study addresses these gaps by proposing EEGConvNeXt, a novel convolutional neural network (CNN) model for detecting AD and FD using electroencephalogram (EEG) signals with high accuracy.
Materials and method
In this research, we employ an open-access EEG signal public dataset containing three distinct classes: AD, FD, and control subjects. We then constructed a newly proposed EEGConvNeXt model comprised of a 2-dimensional CNN algorithm that firstly converts the EEG signals into power spectrogram-based images. Secondly, these images were used as input for the proposed EEGConvNeXt model for automated classification of AD, FD, and a control outcome. The proposed EEGConvNeXt model is therefore a lightweight model that contributes to a new image classification CNN structure based on the transformer model with four primary stages: a stem, a main model, downsampling, and an output stem.
Results
The EEGConvNeXt model achieved a classification accuracy of ∼95.70% for three-class detection (AD, FD, and control), validated using a hold-out strategy. Binary classification cases, such as AD versus FD and FD versus control, achieved accuracies exceeding 98%, demonstrating the model's robustness across scenarios.
Conclusions
The proposed EEGConvNeXt model demonstrates high classification performance with a lightweight architecture suitable for deployment in resource-constrained settings. While the study establishes a novel framework for AD and FD detection, limitations include reliance on a relatively small dataset and the need for further validation on diverse populations. Future research should focus on expanding datasets, optimizing architecture, and exploring additional neurological disorders to enhance the model's utility in clinical applications.}
}
@article{CEMPEL2013328,
title = {Application of TRIZ approach to machine vibration condition monitoring problems},
journal = {Mechanical Systems and Signal Processing},
volume = {41},
number = {1},
pages = {328-334},
year = {2013},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2013.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0888327013003610},
author = {Czesław Cempel},
keywords = {Vibration condition monitoring, TRIZ, Ideal final result –IFR, Engineering parameters, Inventive principles, Contradiction matrix},
abstract = {Up to now machine condition monitoring has not been seriously approached by TRIZ11TRIZ= Russian acronym for Inventive Problem Solving System, created by G. Altshuller ca 50 years ago. users, and the knowledge of TRIZ methodology has not been applied there intensively. However, there are some introductory papers of present author posted on Diagnostic Congress in Cracow (Cempel, in press [11]), and Diagnostyka Journal as well. But it seems to be further need to make such approach from different sides in order to see, if some new knowledge and technology will emerge. In doing this we need at first to define the ideal final result (IFR) of our innovation problem. As a next we need a set of parameters to describe the problems of system condition monitoring (CM) in terms of TRIZ language and set of inventive principles possible to apply, on the way to IFR. This means we should present the machine CM problem by means of contradiction and contradiction matrix. When specifying the problem parameters and inventive principles, one should use analogy and metaphorical thinking, which by definition is not exact but fuzzy, and leads sometimes to unexpected results and outcomes. The paper undertakes this important problem again and brings some new insight into system and machine CM problems. This may mean for example the minimal dimensionality of TRIZ engineering parameter set for the description of machine CM problems, and the set of most useful inventive principles applied to given engineering parameter and contradictions of TRIZ.}
}
@article{MISRA201964,
title = {Do religious and conscious investors make better economic decisions? Evidence from India},
journal = {Journal of Behavioral and Experimental Finance},
volume = {22},
pages = {64-74},
year = {2019},
issn = {2214-6350},
doi = {https://doi.org/10.1016/j.jbef.2019.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S2214635018301217},
author = {Rupali Misra and Sumita Srivastava and D.K. Banwet},
keywords = {Religiosity, Consciousness, Intuition, Rationality, Ambidextrous decision-making, Information processing models, Investor efficacy, Investment decision-making},
abstract = {Investment decision-making in India is different from the world and is affected by social settings. Religion lies at the core of the prevailing socio-cultural environment strengthening social-norms and belief-system, while consciousness is considered core ingredient of life which improves awareness beyond the physical plane, not attributable to cortical processes. Although socio-cultural influence on economic behaviour is quite discernible in countries following eastern-religious traditions, yet studies examining its impact on decision efficacy are scant. Present research addresses this and explores causal role of religiosity and consciousness in shaping investor’s intuitive and analytical abilities. Following a multi-step procedure with qualitative procedure – using engaged scholarship of experts in participative research model and quantitative assessment – surveying investors, the role of these innate behavioural variables in investment decision making has been examined. The paper also validates ambidextrous decision-making style in investment efficacy where intuition improves analytical thinking which further enhances investment efficacy.}
}
@article{FUSON2009343,
title = {Avoiding misinterpretations of Piaget and Vygotsky: Mathematical teaching without learning, learning without teaching, or helpful learning-path teaching?},
journal = {Cognitive Development},
volume = {24},
number = {4},
pages = {343-361},
year = {2009},
note = {Atypical Development of Numerical Cognition},
issn = {0885-2014},
doi = {https://doi.org/10.1016/j.cogdev.2009.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0885201409000707},
author = {Karen C. Fuson},
abstract = {This article provides an overview of some perspectives about special issues in classroom mathematical teaching and learning that have stemmed from the huge explosion of research in children's mathematical thinking stimulated by Piaget. It concentrates on issues that are particularly important for less-advanced learners and for those who might be having special difficulties in learning mathematics. A major goal of the article is to develop a framework for understanding what effective mathematics teaching and learning is, because doing so is so important for struggling students and for research about them. Piaget's research had a fundamental influence on the on-going tension between understanding and fluency in the classroom, supporting efforts toward increasing understanding. But in some countries, misinterpretations of Piaget led to practices that are counterproductive for children, especially struggling learners. Such misinterpretations are identified and a more balanced approach that also draws on Vygotsky is described—a learning-path developmentally-appropriate learning/teaching approach.}
}
@article{ANTONIOU2024105918,
title = {Realistic simulation of air pollution in an urban area to promote environmental policies},
journal = {Environmental Modelling & Software},
volume = {172},
pages = {105918},
year = {2024},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2023.105918},
url = {https://www.sciencedirect.com/science/article/pii/S1364815223003043},
author = {A. Antoniou and G. Ioannidis and L. Ntziachristos},
keywords = {Optimized visualization, 3D graphics software, Photo-realistic result, Pollutant reduction},
abstract = {Visualizing tools are now capable of synthesizing satellite imagery with Computational Fluid Dynamics (CFD) results. The new method is applied in Augsburg, Germany, and consists of two main phases: pre-processing and post-processing. The pre-processing phase involves creating geometry, mesh, and extracting results from simulations of traffic pollutant dispersion. In the post-processing phase, the results are combined with satellite images to produce visually optimized results. The demonstration of road traffic air pollution is based on real data from local air quality measurements and specific scenarios. The results indicate that using these visualization tools produce understandable and impressive images and videos. This enhances the public's comprehension of scientific results and raises awareness of environmental issues. An increased understanding of scientific results can reinforce the implementation of environmental policies by pressuring responsible authorities to take action. This paper provides a valuable tool for visualizing air pollution and facilitating public engagement with environmental issues.}
}
@article{KIM2019141,
title = {AI for design: Virtual design assistant},
journal = {CIRP Annals},
volume = {68},
number = {1},
pages = {141-144},
year = {2019},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2019.03.024},
url = {https://www.sciencedirect.com/science/article/pii/S0007850619300289},
author = {Sang-Gook Kim and Sang Min Yoon and Maria Yang and Jungwoo Choi and Haluk Akay and Edward Burnell},
keywords = {Design method, Machine learning, Hybrid intelligence},
abstract = {Engineering faces many wicked problems: irreducibly interdisciplinary with multiple competing objectives, and of such large scale and complexity that will require processes to deeply rely on human insights and power of computation. The resurgence of machine learning offers the possibility for new forms of human/computer collaboration where each fuels hybrid intelligence in complementary ways. A concept of virtual design assistant (VDA) is developed as a platform to bring the hybrid intelligence in solving complex design challenges. A deep learning-based abstraction process is developed to provide VDA a function to extract structured functional requirements from fragmental design specifications and customer needs.}
}
@article{LI202261,
title = {Stochastic configuration networks for self-blast state recognition of glass insulators with adaptive depth and multi-scale representation},
journal = {Information Sciences},
volume = {604},
pages = {61-79},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.04.061},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522004133},
author = {Weitao Li and Qian Zhang and Dianhui Wang and Wei Sun and Qiyue Li},
keywords = {Self-blast state, Adaptive depth and multi-scale representation, Ensemble learning, Stochastic configuration networks, Feedback mechanism},
abstract = {The operating state of insulators is directly related to the stability of power transmission line. The existing methods for insulator state recognition cannot achieve satisfactory performance. In this paper, the self-blast state recognition of glass insulators is investigated by using an adaptive learning representation. To increase the adaptability of the network to different scales, we propose a solution based on multi-scale information throughout the entire process, beginning from a low-scale to high-scale subnetworks. The multi-scale information is aggregated in parallel way to take advantage of rich information representation. Then, an imitation of the human thinking pattern is employed. Utilizing entropy-based cost function, we update the parameters of the learner model in real-time. Based on the constraint of the evaluation index, adaptive depth representation for training glass insulators that are unsatisfied with the reliability evaluation is constructed to realize the self-optimizing regulation of feature space. Correspondingly, a stochastic configuration networks (SCNs) classifier is re-constructed to fit for the update multi-hierarchies knowledge space to carry out the re-recognition process. Finally, fuzzy integration is employed to ensemble multi-hierarchies network to improve the model’s generalization. The recognition results on aerial dataset of insulators images demonstrate the effectiveness of our proposed approach.}
}
@incollection{WALTZ1997327,
title = {AI applications of massive parallelism: An experience report},
editor = {James Geller and Hiroaki Kitano and Christian B. Suttner},
series = {Machine Intelligence and Pattern Recognition},
publisher = {North-Holland},
volume = {20},
pages = {327-339},
year = {1997},
booktitle = {Parallel Processing for Artificial Intelligence 3},
issn = {0923-0459},
doi = {https://doi.org/10.1016/S0923-0459(97)80016-X},
url = {https://www.sciencedirect.com/science/article/pii/S092304599780016X},
author = {David L. Waltz},
abstract = {For nearly ten years my group and I at Thinking Machines Corporation worked at selling massively parallel computers for a variety of applications that fall broadly in the area now called “database mining.” We had an amazing team of scientists and engineers, saw trends far ahead of the rest of the world, and developed several great systems. However, we began as novices in the business arena. Sometimes we made sales, sometimes we did not; but we learned a great deal in either case. This chapter recounts the sales process and a brief history, mostly in the form of “war stories” mixed with technical details, and attempts to summarize some messages to take away, based on what we learned.}
}
@article{MOZUNI2017303,
title = {An Introduction to the Morphological Delphi Method for Design: A Tool for Future-Oriented Design Research},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {3},
number = {4},
pages = {303-318},
year = {2017},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2018.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S2405872617300710},
author = {Mehdi Mozuni and Wolfgang Jonas},
keywords = {Morphological analysis, Systemic design, Future-oriented design, Delphi method, Scenario development, Strategic foresight},
abstract = {Projecting analytical concepts is a difficult, though established process in innovation management. Designers face methodological obstacles, however, when engaging with a future system with rapidly changing factors. First, the system’s users do not yet exist. Second, continuing changes in key factors and their interactions make conceiving of relationships and delivering synthesizable data impossible. The rational core for making projections suffers from a lack of substantiation. Both morphological analysis and the Delphi method are established tools in strategic foresight. We suggest that a morphology-based Delphi method supports the process of projecting future outcomes in innovative, complex projects. In addition, each tool compensates for the other’s theoretical and functional deficits by illustrating transparent, value-based arguments in a modifiable, iterative manner.}
}
@article{CHAFEE2022R346,
title = {Prefrontal cortex},
journal = {Current Biology},
volume = {32},
number = {8},
pages = {R346-R351},
year = {2022},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2022.02.071},
url = {https://www.sciencedirect.com/science/article/pii/S0960982222003414},
author = {Matthew V. Chafee and Sarah R. Heilbronner},
abstract = {Summary
The prefrontal cortex is a well-studied but, in terms of understanding what it is for, deeply divisive part of the brain located at the front of the head. Perhaps the least controversial feature of the prefrontal cortex is its complexity. The prefrontal cortex is anatomically, functionally, and computationally complex. It is anatomically complex, containing a number of subregions each sending and receiving projections to a unique set of other cortical and subcortical areas. This interconnectivity presents a serious challenge to efforts to localize function to prefrontal cortex, because it can seem as though information flows everywhere all at once in prefrontal networks. Perhaps as a result, prefrontal cortex is also computationally complex: working memory, abstraction, sensory attention, value-based decision making, planning, and motor control are all functions that have been attributed to the prefrontal cortex. This diversity of functions is likely to reflect the diversity of brain regions that prefrontal cortex communicates with while carrying out the computations it performs to influence behavior.}
}
@article{GUPTA2024114777,
title = {Replicated multistage interconnection networks: QoS evaluation for parallel and distributed computing},
journal = {Theoretical Computer Science},
volume = {1016},
pages = {114777},
year = {2024},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2024.114777},
url = {https://www.sciencedirect.com/science/article/pii/S0304397524003943},
author = {Shilpa Gupta and G.L. Pahuja},
keywords = {Sensor networks, Distributed system, Real-time data, Efficient computing, Multistage interconnection network (MIN), Shuffle exchange network (SEN), Reliability, Cost},
abstract = {Introduction to big data becomes very important with dealing in high-performance parallel distributed computing, especially in those systems where communication among a number of processors is required. The current paper takes into consideration different topologies for the Shuffle Exchange Network (SEN) in order to attain optimal data transfer among such scenarios. SEN topologies are a key feature in connecting several processors and implementing the data transfer among them where a single processor fails to handle the load. The study hereby reports in the performance, reliability, and cost analysis of these topologies. These topologies, some of which are advocated to have better performance in many studies, include replicated networks. Our research demystifies claims made in earlier papers that replicated networks have inflated reliability and higher costs due to their additional links. Researchers are therefore guided on accurate performance data that will lead them to make optimum choices of SEN topologies for targeted applications. These findings further highlight that the trade-offs between reliability and cost must be carefully considered during network design so as to arrive at better results for big data communication and computation in parallel and distributed systems. This work provides important insights into the correct evaluation of SEN topologies, helping to correct the misleading facts and to have better network selection for various real-time application realizations.}
}