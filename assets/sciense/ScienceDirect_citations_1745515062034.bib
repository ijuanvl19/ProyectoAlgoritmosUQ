@article{PANG2022118826,
title = {Uncovering the global task-modulated brain network in chunk decomposition with Chinese characters},
journal = {NeuroImage},
volume = {247},
pages = {118826},
year = {2022},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2021.118826},
url = {https://www.sciencedirect.com/science/article/pii/S1053811921010971},
author = {Jiaoyan Pang and Hanning Guo and Xiaochen Tang and Yu Fu and Zhengwu Yang and Yongchao Li and Na An and Jing Luo and Zhijun Yao and Bin Hu},
keywords = {Cognitive pattern, Beta-series correlation, Chinese characters, Thalamus, Hippocampus},
abstract = {Chunk decomposition, which requires the mental representation transformation in accordance with behavioral goals, is of vital importance to problem solving and creative thinking. Previous studies have identified that the frontal, parietal, and occipital cortex in the cognitive control network selectively activated in response to chunk tightness, however, functional localization strategy may overlook the interaction brain regions. Based on the notion of a global brain network, we proposed that multiple specialized regions have to be interconnected to maintain goal representation during the course of chunk decomposition. Therefore, the present study applied a beta-series correlation method to investigate interregional functional connectivity in the event-related design of chunk decomposition tasks using Chinese characters, which would highlight critical nodes irrespective to chunk tightness. The results reveal a network of functional hubs with highly within or between module connections, including the orbitofrontal cortex, superior/inferior parietal lobule, hippocampus, and thalamus. We speculate that the thalamus integrates information across modular as an integrative hub while the orbitofrontal cortex tracks the mental states of chunk decomposition on a moment-to-moment basis. The superior and inferior parietal lobule collaborate to manipulate the mental representation of chunk decomposition and the hippocampus associates the relationship between elements in the question and solution phase. Furthermore, the tightness of chunks is not only associated with different processors in visual systems but also leads to increased intermodular connections in right superior frontal gyrus and left precentral gyrus. To summary up, the present study first reveals the task-modulated brain network of chunk decomposition in addition to the tightness-related nodes in the frontal and occipital cortex.}
}
@article{ZHANG2025107610,
title = {AdaptFL: Adaptive Federated Learning Framework for Heterogeneous Devices},
journal = {Future Generation Computer Systems},
volume = {165},
pages = {107610},
year = {2025},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.107610},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24005740},
author = {Yingqi Zhang and Hui Xia and Shuo Xu and Xiangxiang Wang and Lijuan Xu},
keywords = {Federated learning, Heterogeneous device, Neural architecture search, Knowledge distillation},
abstract = {With the development of the Internet of Things (IoT), Federated Learning (FL) is extensively employed in smart cities and industrial IoT, involving numerous heterogeneous devices with varying computational and storage capabilities. Traditional FL assumes that clients have enough resources to train a unified global model from the beginning to the end of training. However, it ignores the problem of uneven and real-time changes in client resources. Additionally, there are aggregation difficulties between heterogeneous client models and global model. To address these challenges, we propose an Adaptive Federated Learning Framework for Heterogeneous Devices (AdaptFL). In AdaptFL, we employ a resource-aware neural architecture search method, which searches for models based on each client’s resource conditions. It enables AdaptFL to automatically assign customized models tailored to each client’s specific resource conditions in the current round. Additionally, we employ a staged knowledge distillation strategy to facilitate efficient distribution and aggregation between the heterogeneous global model and the client models. Experimental results demonstrate that, compared to state-of-the-art model-level heterogeneous ablation methods, AdaptFL improves global test accuracy by 4% to 15% on the SVHN dataset and enhances accuracy by 5% to 14% in scenarios with heterogeneous data. Additionally, AdaptFL effectively reduces communication overhead by over 50% across all datasets. Furthermore, it offers a degree of resilience against model poisoning attacks.}
}
@article{PICCININI2004811,
title = {Functionalism, computationalism, and mental states},
journal = {Studies in History and Philosophy of Science Part A},
volume = {35},
number = {4},
pages = {811-833},
year = {2004},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2004.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0039368104000883},
author = {Gualtiero Piccinini},
keywords = {Functionalism, Computationalism, Computational functionalism, Mental states, Computational theory of mind, Functional analysis},
abstract = {Some philosophers have conflated functionalism and computationalism. I reconstruct how this came about and uncover two assumptions that made the conflation possible. They are the assumptions that (i) psychological functional analyses are computational descriptions and (ii) everything may be described as performing computations. I argue that, if we want to improve our understanding of both the metaphysics of mental states and the functional relations between them, we should reject these assumptions.}
}
@incollection{REYNA199391,
title = {Chapter 3 Fuzzy Memory and Mathematics in The Classroom},
editor = {Graham M. Davies and Robert H. Logie},
series = {Advances in Psychology},
publisher = {North-Holland},
volume = {100},
pages = {91-119},
year = {1993},
booktitle = {Memory in Everyday Life},
issn = {0166-4115},
doi = {https://doi.org/10.1016/S0166-4115(08)61096-1},
url = {https://www.sciencedirect.com/science/article/pii/S0166411508610961},
author = {Valerie F. Reyna and Charles J. Brainerd},
abstract = {Publisher Summary
This chapter presents an intuitionist account of mathematical problem solving, and some of its implications for learning and teaching. The study is based primarily on research concerning the relationship between memory and reasoning, in particular, between memory for specific problem facts and gist based reasoning—for example, Reyna, in press-b. The chapter reviews that research, including the nature of representations adults and children use to solve problems, some counterintuitive findings about factors that facilitate learning, and the implications of distinguishing between competence versus bringing that competence to bear in actual situations. The chapter focuses on computational skills (e.g., mental arithmetic) and quantitative reasoning about probability and expected value (e.g., compensation), the numerosity of sets (e.g., class-inclusion reasoning), relative magnitude (e.g., transitive inference), and weights and measures (e.g., conservation). Although many factors undoubtedly contribute to the problem of underachievement in mathematics (including low expectations) observers generally agree that, beyond basic arithmetic, mathematics is not intuitive for most learners. Recent research suggests that successful mathematical reasoners call on an intuitive appreciation of the fuzzy, qualitative relationships in problem information. Thus, by emphasizing a kind of cognitive approach that seems antithetical to mathematics, one may achieve greater success in instruction. The origins of this new approach can be traced to the foundations of mathematics itself, to Brouwer and Heyting, who argued that intuition-fluid thinking that operates on the barest senses of ideas—underlies the insights of mathematicians.}
}
@article{SPEISER2012463,
title = {Why is paper-and-pencil multiplication difficult for many people?},
journal = {The Journal of Mathematical Behavior},
volume = {31},
number = {4},
pages = {463-475},
year = {2012},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2012.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0732312312000338},
author = {Robert Speiser and Matthew H. Schneps and Amanda Heffner-Wong and Jaimie L. Miller and Gerhard Sonnert},
keywords = {Representations, Spatial schemas, Working memory, Attention, Multiplication, Algorithms},
abstract = {In school, at least in the US, we were taught to multiply by hand according to a standard algorithm. Most people find that algorithm difficult to use, and many children fail to learn it. We propose a new way to make sense of this difficulty: to treat explicit computation as perceptually supported physical and mental action. Based on recent work in neuroscience, we trace the flow of arithmetic information to emphasize demands on visual working memory and attention. We predict that algorithms that make moderate demands on memory and attention will work better than others that make stronger demands. We suggest that the judicious use of spatial schemas can reduce such cognitive demands. Experimental evidence from children in an inner-city school supports this claim. Our work suggests new ways to think about instruction. The goal should be to minimize demands that present obstacles and maximize instead what human eyes, bodies, and brains do well.}
}
@article{ISMAILOVA2021332,
title = {Equalities between Combinators to Evaluate Expressions},
journal = {Procedia Computer Science},
volume = {190},
pages = {332-340},
year = {2021},
note = {2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: Eleventh Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.06.058},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921013053},
author = {Larisa Ismailova and Viacheslav Wolfengagen and Sergey Kosikov},
keywords = {information process, model structure, indexed expressions, cognitive model, combinator-as-process},
abstract = {One of the aims of this work is to revise the known issues of how to obtain the value of the expression using an applicative computing system. Computation and/or symbolic transformations have undoubtedly become one of the dominant trends in modern computer science. In particular, this refers to the execution of the processes of inference of an object with given properties, which is studied with the formation of sets of equalities leading to a target computational model. At the same time, the mathematical theory of computation gets a target equational description, making the process of studying its properties and capabilities more suitable. A model structure based on the method of indexed expressions is developed and applied using the notions of evaluation mapping and assignments. As shown, this may not be considered as generic notion, but derived using the combinators. Thus, the semantics of computation can be derived from the construction of a combinator-as-process. This is the embedding into a system of combinators. A linking system of equalities between combinatorsl is established, which serving as a generic computational model. This allows us to look differently at the previous cognitive ideas of the semantics of computation disabling an evaluation map and assignments and enabling their replacement by a set of equalities between combinators.}
}
@article{SHAH2025104842,
title = {Deep learning multilayer architecture for analysis of three-dimensional Eyring-Powell nanofluid flow subject to viscous dissipation and joule heating},
journal = {Results in Engineering},
pages = {104842},
year = {2025},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2025.104842},
url = {https://www.sciencedirect.com/science/article/pii/S259012302500917X},
author = {Zahoor Shah and Muflih Alhazmi and Maryam Jawaid and Nafisa A. Albasheir and Mohammed M.A. Alma Zah and Nashwan Adnan Othman and Waqar Azeem Khan},
keywords = {Artificial Intelligence, Deep learning, Neural network, Eyring-Powell nanofluid, soft computing paradigm},
abstract = {Artificial intelligence has enhanced complex systems modeling by achieving exceptional precision and effectiveness. The purpose of this paper is to use the Deep Learning Multi-Layer Soft Computing paradigm (DLML-SCP) to evaluate the model 3D flow dynamics of Eyring-Powell nanofluids subject to viscous dissipation and joule heating (EPNF-3D-VJ). It can capture the complex behavior of a specialized fluid flow system with the changing of different physical parameters like Prandtl number, magnetic field parameter, radiation parameter, and Brownian motion parameter and the controlling parameters of non-Newtonian nature of the fluid, ε, δ1, and δ2, make up the source parameters of Eyring-Powell fluid. Similarity transformation in the governing partial differential equations (PDEs) ultimately leads to a reduced set of ordinary differential equations (ODEs) along with boundary conditions. Not only does it simplify the computational process, but it also boosts the model's predictive abilities. The Adam numerical technique is employed to generate a comprehensive dataset across the fluid-dynamic spectrum that covers diverse EPNF-3D-VJ cases, offering critical insights into the system's behavior. The dataset is utilized to test, train, and validate the DLML-SCP, proving its accuracy in predicting fluid system behavior. Results show the effectiveness and reliability of the model and are well aligned to the dataset. Validation results demonstrate that the DLML-SCP framework accurately predicts fluid system behavior, achieving mean square error (MSE) values between E-09 to E-10. Important findings from the performance evaluations, it can be noted that the increase in both the Prandtl number and magnetic field coefficient has led to a decrease in the temperature and velocity profiles. With an increase in the Eyring-Powell fluid parameter, the velocity profile is likely to increase. present study utilized an advanced machine learning framework that has been proven to outperform the state-of-the-art models in predicting complex dynamics of EPNF-3D-VJ. Providing strong support for a new class of physics-informed deep neural networks designed for solving fluid mechanics problems.}
}
@article{HUANG2024369,
title = {A linear-attention-combined convolutional neural network for EEG-based visual stimulus recognition},
journal = {Biocybernetics and Biomedical Engineering},
volume = {44},
number = {2},
pages = {369-379},
year = {2024},
issn = {0208-5216},
doi = {https://doi.org/10.1016/j.bbe.2024.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0208521624000299},
author = {Junjie Huang and Wanzhong Chen and Tao Zhang},
keywords = {Brain–computer interface (BCI), Convolutional neural network (CNN), Electroencephalogram (EEG), Linear attention mechanism, Visual stimulus recognition},
abstract = {The recognition task of visual stimuli based on EEG (Electroencephalogram) has become a major and important topic in the field of Brain–Computer Interfaces (BCI) research. Although the underlying spatial features of EEG can effectively represent visual stimulus information, it still remains a highly challenging task to explore the local–global information of the underlying EEG to achieve better decoding performance. Therefore, in this paper we propose a deep learning architecture called Linear-Attention-combined Convolutional Neural Network (LACNN) for visual stimuli EEG-based classification task. The proposed architecture combines the modules of Convolutional Neural Networks (CNN) and Linear Attention, effectively extracting local and global features of EEG for decoding while maintaining low computational complexity and model parameters. We conducted extensive experiments on a public EEG dataset from the Stanford Digital Repository. The experimental results demonstrate that LACNN achieves an average decoding accuracy of 54.13% and 29.83% in 6-category and 72-exemplar classification tasks respectively, outperforming the state-of-the-art methods, which indicates that our method can effectively decode visual stimuli from EEG. Further analysis of LACNN shows that the Linear Attention module improves the separability between different category features and localizes key brain region information that aligns with the paradigm principles.}
}
@incollection{LU2024173,
title = {Chapter 13 - Collection and transmission planning for large offshore wind power base},
editor = {Zongxiang Lu and Haibo Li and Ying Qiao and Le Xie and Chanan Singh},
booktitle = {Power System Flexibility},
publisher = {Academic Press},
pages = {173-191},
year = {2024},
isbn = {978-0-323-99517-7},
doi = {https://doi.org/10.1016/B978-0-323-99517-7.00016-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323995177000166},
author = {Zongxiang Lu and Haibo Li and Ying Qiao and Le Xie and Chanan Singh},
keywords = {Mid- to offshore wind power, hierarchical planning, steepest descent method, minimum spanning tree algorithm, transmission equipment selection, improved ant colony algorithm},
abstract = {Offshore wind power planning in China thus far has generally inherited the principles and thinking of onshore wind power planning. Mid- to offshore wind power access could incur a cost far higher than that of onshore wind generation, accounting for 15%–30% of the total investment. Sea state resources, corridor resources, landing conditions, and submarine cable wiring all face materially contrasting constraints to onshore wind power, necessitating dedicated planning efforts for offshore wind power. In this chapter, a planning framework for the sequential and cascaded development of mid- to offshore wind power bases is proposed, and a tiered master planning approach featuring on-site, cluster, and AC/DC transmission is established, filling the gap in transmission planning of large-scale mid- to offshore wind farm clusters. Firstly, location optimization of offshore hub substations based on a steepest descent method is established and explaining its specific implementation methods. Secondly, an improved minimum spanning tree algorithm is used to find a topology connection method for the collector system with the optimal length of submarine cable, which makes the cost optimal. Finally, the optimization model of transmission equipment selection considering the risk of high wind speed truncation is established, and the topology optimization modeling method of offshore wind power cluster transmission systems based on the improved ant colony optimization algorithm is proposed.}
}
@article{20152,
title = {Positive Gradients},
journal = {Cell Systems},
volume = {1},
number = {1},
pages = {2-3},
year = {2015},
issn = {2405-4712},
doi = {https://doi.org/10.1016/j.cels.2015.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S2405471215000137},
abstract = {The new associate vice chancellor of computational heath sciences at the University of California San Diego reflects on the coming era of big data in medicine.}
}
@article{ADABALA2005896,
title = {From virtualized resources to virtual computing grids: the In-VIGO system},
journal = {Future Generation Computer Systems},
volume = {21},
number = {6},
pages = {896-909},
year = {2005},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2003.12.021},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X03002899},
author = {Sumalatha Adabala and Vineet Chadha and Puneet Chawla and Renato Figueiredo and José Fortes and Ivan Krsul and Andrea Matsunaga and Mauricio Tsugawa and Jian Zhang and Ming Zhao and Liping Zhu and Xiaomin Zhu},
keywords = {Virtual machines, Grid-computing, Middleware, Virtual data, Network computing},
abstract = {This paper describes the architecture of the first implementation of the In-VIGO grid-computing system. The architecture is designed to support computational tools for engineering and science research In Virtual Information Grid Organizations (as opposed to in vivo or in vitro experimental research). A novel aspect of In-VIGO is the extensive use of virtualization technology, emerging standards for grid-computing and other Internet middleware. In the context of In-VIGO, virtualization denotes the ability of resources to support multiplexing, manifolding and polymorphism (i.e. to simultaneously appear as multiple resources with possibly different functionalities). Virtualization technologies are available or emerging for all the resources needed to construct virtual grids which would ideally inherit the above mentioned properties. In particular, these technologies enable the creation of dynamic pools of virtual resources that can be aggregated on-demand for application-specific user-specific grid-computing. This change in paradigm from building grids out of physical resources to constructing virtual grids has many advantages but also requires new thinking on how to architect, manage and optimize the necessary middleware. This paper reviews the motivation for In-VIGO approach, discusses the technologies used, describes an early architecture for In-VIGO that represents a first step towards the end goal of building virtual information grids, and reports on first experiences with the In-VIGO software under development.}
}
@article{CANCHIG2025100445,
title = {Enhanced selectivity of carbon quantum dots for metal ion detection through surface modification by heteroatom doping: A study on optical properties and theoretical approach},
journal = {Carbon Trends},
volume = {18},
pages = {100445},
year = {2025},
issn = {2667-0569},
doi = {https://doi.org/10.1016/j.cartre.2024.100445},
url = {https://www.sciencedirect.com/science/article/pii/S266705692400124X},
author = {María Belén Cánchig and Floralba López and Zaillmar Morales-Navarro and Alexis Debut and Karla Vizuete and Thibault Terencio and Manuel Caetano and Juan Pablo Saucedo-Vázquez},
keywords = {Carbon quantum dots, Heteroatom-doped, Ion detection, Heavy metals},
abstract = {Water contamination by toxic metal ions has become a significant issue, requiring the development of effective ion detection methods. Traditional analytical techniques often involve toxic elements or complex devices. Carbon quantum dots (CQDs) have emerged as a promising alternative for optic ion detection due to their unique properties and compatibility with living organisms. This study focuses on synthesizing and functionalizing CQDs with various heteroatoms (N, S) to enhance their optical properties and ion selectivity. CQDs were synthesized using citric acid as the carbon source and modified with l-cysteine, ethylenediamine, and diethylenetriamine. The structural and optical properties of the CQDs were determined using several techniques, including FT-IR, TEM, UV–Vis, and Fluorescence Spectroscopy. The results indicate that doping with heteroatoms significantly alters the absorption and emission properties of CQDs. Particularly, nitrogen-doped CQDs (NCQDs) exhibited the highest absorption and emission intensities, making them ideal for sensor applications. The study also demonstrated that functionalization with sulfur could modulate emission frequencies, enhancing the detection capabilities for specific ions. Fluorescence quenching studies revealed that NCQDs and S-CQDs have a high selectivity for Hg²⁺ ions, attributed both electrostatic and covalent interactions formed between the CQDs and Hg²⁺. Computational studies supported these findings, showing that the interaction with Hg²⁺ significantly affects the energy gap of the CQDs, enhancing their sensitivity. This research contributes to the field of environmental monitoring by providing a practical solution for the detection of free metal ions in water through the development of advanced CQD-based sensors.}
}
@incollection{BOSE2006649,
title = {Chapter 10 - Fuzzy Logic and Applications},
editor = {Bimal K. Bose},
booktitle = {Power Electronics And Motor Drives},
publisher = {Academic Press},
address = {Burlington},
pages = {649-729},
year = {2006},
isbn = {978-0-12-088405-6},
doi = {https://doi.org/10.1016/B978-012088405-6/50012-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780120884056500121},
author = {Bimal K. Bose},
abstract = {Publisher Summary
The chapter deals with the description of Fussy Logic (FL) principles and its application in power electronics and motor-drive systems. The FL is a discipline under Artificial Intelligence (AI). AI is basically computer emulation of human thinking (called computational intelligence). The goal of AI is to mimic human intelligence. AI also includes expert systems (ESs), artificial neural networks (ANNs), or neural networks (NNWs), and genetic algorithms (GAs). All of the main areas in AI, except ES, are defined as soft computing. The concept of FL and its characteristics emerge from Zadeh's theory propounded in 1965. Any FL application uses a knowledge base that consists of multivalued membership functions (MFs) describing the fuzzy variables and the rule table consisting of “IF… THEN… statements.” The knowledge base is developed on the basis of the behavioral nature of the system. The trial-and-error approach of FL algorithms may be time consuming, but user-friendly computer programs (such as the MATLAB-based Fuzzy Logic Toolbox) help speed the process. The applications of FL principles, include speed control of induction motor vector drives, efficiency optimization of induction motor vector drives by flux programming; and wind generation systems, linearization of the transfer characteristics of thyristor converters at discontinuous conduction, induction motor stator resistance estimations, estimation of distorted waveforms and MRAC slip gain tuning control of vector drives.}
}
@article{BOWMAN2023107339,
title = {Desperately searching for something},
journal = {Communications in Nonlinear Science and Numerical Simulation},
volume = {125},
pages = {107339},
year = {2023},
issn = {1007-5704},
doi = {https://doi.org/10.1016/j.cnsns.2023.107339},
url = {https://www.sciencedirect.com/science/article/pii/S1007570423002575},
author = {Clive E. Bowman and Peter Grindrod},
abstract = {There is a growing interest in novelty search : that is, in sampling a parameter space to search for radical or unexpected behaviour(s), occurring as a consequence of parameter choice, being input to some downstream complex system, process, or service that will not yield to analysis, without imposing any specific pre-ordained objective function, or fitness function to be optimised. We mean “parameter” in the widest sense, including system learnables, non-autonomous forcing, sequencing and all inputs. Depending upon the nature of the underlying parameter space of interest one may adopt a rather wide range of search algorithms. We do consider that this search activity has meta-objectives, though: one is of achieving diversity (efficiently reaching out across the space in some way); and one is of achieving some minimum density (not leaving out large unexplored holes). These are in tension. In general, the computational costs of both of these qualities become restrictive as the dimension of the parameter spaces increase; and consequently their balance is harder to maintain. We may also wish for a substantial random element of search to provide some luck in discovery and to avoid any naive preset sampling patterns. We consider archive-based methods within a range of spaces: finite discrete spaces, where the problem is straightforward (provided we are patient with the random element); Euclidean spaces, of increasing dimension, that become very lonely places; and infinite dimensional spaces. Our aim is to discuss a raft of distinctive search concepts, that respond to identified challenges, and rely on a rather diverse range of mathematical ideas. This arms practitioners with a range of highly practical methods. However applications requiring novelty search arise, one should avoid rushing to code-up a standard evolving search algorithm and instead give some thought to the nature and requirements of the search: there is a range of effective options available. We give some considered advice.}
}
@article{SHAW2019,
title = {Artificial Intelligence and the Implementation Challenge},
journal = {Journal of Medical Internet Research},
volume = {21},
number = {7},
year = {2019},
issn = {1438-8871},
doi = {https://doi.org/10.2196/13659},
url = {https://www.sciencedirect.com/science/article/pii/S1438887119003595},
author = {James Shaw and Frank Rudzicz and Trevor Jamieson and Avi Goldfarb},
keywords = {artificial intelligence, machine learning, implementation science, ethics},
abstract = {Background
Applications of artificial intelligence (AI) in health care have garnered much attention in recent years, but the implementation issues posed by AI have not been substantially addressed.
Objective
In this paper, we have focused on machine learning (ML) as a form of AI and have provided a framework for thinking about use cases of ML in health care. We have structured our discussion of challenges in the implementation of ML in comparison with other technologies using the framework of Nonadoption, Abandonment, and Challenges to the Scale-Up, Spread, and Sustainability of Health and Care Technologies (NASSS).
Methods
After providing an overview of AI technology, we describe use cases of ML as falling into the categories of decision support and automation. We suggest these use cases apply to clinical, operational, and epidemiological tasks and that the primary function of ML in health care in the near term will be decision support. We then outline unique implementation issues posed by ML initiatives in the categories addressed by the NASSS framework, specifically including meaningful decision support, explainability, privacy, consent, algorithmic bias, security, scalability, the role of corporations, and the changing nature of health care work.
Results
Ultimately, we suggest that the future of ML in health care remains positive but uncertain, as support from patients, the public, and a wide range of health care stakeholders is necessary to enable its meaningful implementation.
Conclusions
If the implementation science community is to facilitate the adoption of ML in ways that stand to generate widespread benefits, the issues raised in this paper will require substantial attention in the coming years.}
}
@article{QUARESIMIN20122290,
title = {Strategies for the assessment of nanocomposite mechanical properties},
journal = {Composites Part B: Engineering},
volume = {43},
number = {5},
pages = {2290-2297},
year = {2012},
issn = {1359-8368},
doi = {https://doi.org/10.1016/j.compositesb.2011.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S1359836812000030},
author = {Marino Quaresimin and Marco Salviato and Michele Zappalorto},
keywords = {A. Nano-structures, B. Mechanical properties, B. Fracture toughness, Three-stage strategy (TSS)},
abstract = {The assessment of nanocomposite mechanical properties is a challenging task. Due to their hierarchical structure, which spans from nano to macro length-scales, a different way of thinking from traditional approaches is needed to account for the characteristic phenomena of each length-scale and bridge their effects from the smaller scale to the macroscale. In the present work, some important issues of nanocomposite modelling are discussed. Then, a classification of the available modelling strategies is proposed, according to the scale from which the problem is addressed. This comprehensive analysis is thought as a necessary tool for the development of new effective approaches.}
}
@article{ZHENG2021102174,
title = {An Attention-based Bi-LSTM Method for Visual Object Classification via EEG},
journal = {Biomedical Signal Processing and Control},
volume = {63},
pages = {102174},
year = {2021},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2020.102174},
url = {https://www.sciencedirect.com/science/article/pii/S174680942030313X},
author = {Xiao Zheng and Wanzhong Chen},
keywords = {Deep learning, Attention mechanism, EEG, Bi-LSTM, Visual perception},
abstract = {Background and Objective
Despite many models have been proposed for brain visual perception and content understanding via electroencephalograms (EEGs), due to the lack of research on the inherent temporal relationship, EEG-based visual object classification still demands the improvement on its accuracy and computation complexity.
Methods
To take full advantage of the uneven visual feature saturation between time segments, an end-to-end attention-based Bi-LSTM Method is proposed, named Bi-LSTM-AttGW. Two attention strategies are introduced to Bi-LSTM framework. The attention gate replaces the forget gate in traditional LSTM. It is only relevant to the historical cell state, and not related to the current input. Hence, the attention gate can greatly reduce the number of training parameters. Moreover, the attention weighting method is applied to Bi-LSTM output, and it can explore the most decisive information.
Results
The best classification accuracy achieved by Bi-LSTM-AttGW model is 99.50%. Compared with the state-of-art algorithms and baseline models, the proposed method has great advantages in classification performance and computational complexity. Considering brain region level contribution on visual cognition task, we also verify our method using EEG signals collected from the frontal and occipital regions, that are highly correlated with visual perception tasks.
Conclusions
The results show promise towards the idea that human brain activity related to visual recognition can be more effectively decoded by neural networks with neural mechanism. The experimental results not only could provide strong support for the modularity theory about the brain cognitive function, but show the superiority of the proposed Bi-LSTM model with attention mechanism again.}
}
@article{MUSTAPHA2025103066,
title = {A survey of emerging applications of large language models for problems in mechanics, product design, and manufacturing},
journal = {Advanced Engineering Informatics},
volume = {64},
pages = {103066},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.103066},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624007171},
author = {K.B. Mustapha},
keywords = {Pre-trained language models, Large language models, Generative AI, Generative pre-trained transformer, Mechanical engineering, Engineering design, Manufacturing, Mechanics, Intelligent digital twins, Intelligent maintenance, Creativity},
abstract = {In the span of three years, the application of large language models (LLMs) has accelerated across a multitude of professional sectors. Amid this development, a new collection of studies has manifested around leveraging LLMs for segments of the mechanical engineering (ME) field. Concurrently, it has become clear that general-purpose LLMs faced hurdles when deployed in this domain, partly due to their training on discipline-agnostic data. Accordingly, there is a recent uptick of derivative ME-specific LLMs being reported. As the research community shifts towards these new LLM-centric solutions for ME-related problems, the shift compels a deeper look at the diffusion of LLMs in this emerging landscape. Consequently, this review consolidates the diversity of ME-tailored LLMs use cases and identifies the supportive technical stacks associated with these implementations. Broadly, the review demonstrates how various categories of LLMs are re-shaping concrete aspects of engineering design, manufacturing and applied mechanics. At a more specific level, it uncovered emerging LLMs’ role in boosting the intelligence of digital twins, enriching bidirectional communication within the human-cyber-physical infrastructure, advancing the development of intelligent process planning in manufacturing and facilitating inverse mechanics. It further spotlights the coupling of LLMs with other generative models for promoting efficient computer-aided conceptual design, prototyping, knowledge discovery and creativity. Finally, it revealed training modalities/infrastructures necessary for developing ME-specific language models, discussed LLMs' features that are incongruent with typical engineering workflows, and concluded with prescriptive approaches to mitigate impediments to the progressive adoption of LLMs as part of advanced intelligent solutions.}
}
@article{DUAL2024596,
title = {The Future of Durable Mechanical Circulatory Support: Emerging Technological Innovations and Considerations to Enable Evolution of the Field},
journal = {Journal of Cardiac Failure},
volume = {30},
number = {4},
pages = {596-609},
year = {2024},
issn = {1071-9164},
doi = {https://doi.org/10.1016/j.cardfail.2024.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S1071916424000411},
author = {Seraina A. Dual and Jennifer Cowger and Ellen Roche and Aditi Nayak},
keywords = {Mechanical circulatory support, left ventricular assist device, translation, innovation},
abstract = {The field of durable mechanical circulatory support (MCS) has undergone an incredible evolution over the past few decades, resulting in significant improvements in longevity and quality of life for patients with advanced heart failure. Despite these successes, substantial opportunities for further improvements remain, including in pump design and ancillary technology, perioperative and postoperative management, and the overall patient experience. Ideally, durable MCS devices would be fully implantable, automatically controlled, and minimize the need for anticoagulation. Reliable and long-term total artificial hearts for biventricular support would be available; and surgical, perioperative, and postoperative management would be informed by the individual patient phenotype along with computational simulations. In this review, we summarize emerging technological innovations in these areas, focusing primarily on innovations in late preclinical or early clinical phases of study. We highlight important considerations that the MCS community of clinicians, engineers, industry partners, and venture capital investors should consider to sustain the evolution of the field.}
}
@article{HU2018275,
title = {Can a machine have two systems for recognition, like human beings?},
journal = {Journal of Visual Communication and Image Representation},
volume = {56},
pages = {275-286},
year = {2018},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2018.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S1047320318302232},
author = {Jiwei Hu and Kin-Man Lam and Ping Lou and Quan Liu and Wupeng Deng},
keywords = {Image annotation, Multi-labeling, Hierarchical tree structure, Feature-pool selection},
abstract = {Artificial Intelligence has attracted much of researchers’ attention in recent years. A question we always ask is: “Can machines replace human beings to some extent?” This paper aims to explore the knowledge learning for an image-annotation framework, which is an easy task for humans but a tough task for machines. This paper’s research is based on an assumption that machines have two systems of thinking, each of which handles the labels of images at different abstract levels. Based on this, a new hierarchical model for image annotation is introduced. We explore not only the relationships between the labels and the features used, but also the relationships between labels. More specifically, we divide labels into several hierarchies for efficient and accurate labeling, which are constructed using our Associative Memory Sharing method, proposed in this paper.}
}
@incollection{STANOVICH2008251,
title = {The Development of Rational Thought: A Taxonomy of Heuristics and Biases},
editor = {Robert V. Kail},
series = {Advances in Child Development and Behavior},
publisher = {JAI},
volume = {36},
pages = {251-285},
year = {2008},
booktitle = {Advances in Child Development and Behavior},
issn = {0065-2407},
doi = {https://doi.org/10.1016/S0065-2407(08)00006-2},
url = {https://www.sciencedirect.com/science/article/pii/S0065240708000062},
author = {Keith E. Stanovich and Maggie E. Toplak and Richard F. West},
abstract = {Publisher Summary
The most well-known indicators of cognitive functioning—intelligence and cognitive ability tests—do not assess a critical aspect of thinking, which is the ability to think rationally. To think rationally means adopting appropriate goals, taking the appropriate action given one's goals and beliefs, and holding beliefs that are commensurate with available evidence. Standard intelligence tests do not assess such functions. Although intelligence tests assess the ability to focus on an immediate goal in the face of distraction, they do not assess whether a person has the tendency to develop goals that are rational in the first place. Likewise, intelligence tests are good measures of how well a person can hold beliefs in short-term memory and manipulate those beliefs, but they do not assess whether a person has the tendency to form beliefs rationally when presented with evidence. Similarly, intelligence tests are good measures of how efficiently a person processes information that has been provided, but they do not assess whether the person is a critical assessor of information as it is gathered in the natural environment.}
}
@article{WOLFENGAGEN2016347,
title = {Evolutionary Domains for Varying Individuals},
journal = {Procedia Computer Science},
volume = {88},
pages = {347-352},
year = {2016},
note = {7th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2016, held July 16 to July 19, 2016 in New York City, NY, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.07.447},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916317033},
author = {Viacheslav E. Wolfengagen and Larisa Yu. Ismailova and Sergey V. Kosikov and Viacheslav V. Navrotskiy and Sergey I. Kukalev and Alexander A. Zuev and Polina V. Belyatskaya},
keywords = {computational model, variable domains, individual migration, tangled individuals},
abstract = {The domains ranged by the variables using Web-resources can vary with a time. This is possible even in a runtime of Web-application giving rise to various vulnerabilities and bugs. This paper focuses at the problem mentioned as the individual migration in a problem domain. There is a lack of computational models which operate in an environment of variable domains and the contribution is to develop such a model. The advance is in establishing the mechanism for driving the dynamics of the sets and individuals. As a consequence, the behavior of the variables in query logical expression becomes predictable suppressing the possible semantic instability.}
}
@article{EKLUND201716,
title = {Two approaches to System-of-Systems from Lative Logic point of view},
journal = {Procedia Computer Science},
volume = {119},
pages = {16-21},
year = {2017},
note = {6th International Young Scientist Conference on Computational Science, YSC 2017, 01-03 November 2017, Kotka, Finland},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.11.155},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917323657},
author = {Patrik Eklund and Jari Kortelainen},
keywords = {Category theory, computational science, lative logic, System-of-Systems},
abstract = {The paper presents two approaches to model System-of-Systems on lative logic point of view. Lative logic is a general framework to construct building blocks of logic using Category Theory as its metalanguage. This approach reveals avenues to describe System-of-Systems themselves, and to model information and processes they posess, using some reasonable modelling languages in a computational manner, thus, touching foundations of computational science. After presenting some preliminary notes, the paper explains the main steps to construct lative logics, and then give two approaches to System-of-System modelling. Finally, the paper presents a survey to some applications.}
}
@article{QU2024102255,
title = {Unmanned combat aerial vehicle path planning in complex environment using multi-strategy sparrow search algorithm with double-layer coding},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {36},
number = {10},
pages = {102255},
year = {2024},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2024.102255},
url = {https://www.sciencedirect.com/science/article/pii/S1319157824003446},
author = {Liangdong Qu and Jingkun Fan},
keywords = {UCAV path planning, Double-layer coding, Complex environment, Sparrow search algorithm, Dynamic fitness regulation learning strategy},
abstract = {Unmanned combat aerial vehicles (UCAV) path planning in complex environments demands a substantial number of path points to determine feasible paths. Establishing an effective flight path for UCAVs requires numerous path points to account for fuel constraints, artillery threats, and radar avoidance. This increase in path points raises the dimensionality of the problem, which in turn degrades algorithm performance. To mitigate this issue, a double-layer coding (DLC) model is utilized to remove redundant path points, consequently lowering computational complexity and operational difficulties. Meanwhile, this paper introduces a novel enhanced sparrow search algorithm (MESSA) based on multi-strategy for UCAV path planning. The MESSA incorporates a novel dynamic fitness regulation learning strategy (DFRL), a random differential learning strategy (RDL), an elite example equilibrium learning strategy (EEEL), a dynamic elimination and regeneration strategy based on the elite example (DERE), and quadratic interpolation (QI). Furthermore, MESSA is compared against 11 state-of-the-art algorithms, demonstrating exceptional optimization performance and robustness. Additionally, the combination of MESSA with the DLC model (DLC-MESSA) is applied to solve the UCAV path planning problem. The experimental results from five complex environments indicate that DLC-MESSA outperforms other algorithms in 80% of the cases by achieving the lowest average cost, thereby demonstrating its superior robustness and computational efficiency.}
}
@article{ZENG2019138,
title = {Iterative optimal control syntheses illustrated on the Brockett integrator},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {16},
pages = {138-143},
year = {2019},
note = {11th IFAC Symposium on Nonlinear Control Systems NOLCOS 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.11.768},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319317719},
author = {Shen Zeng},
keywords = {Nonholonomic systems, Optimal control, Computational methods},
abstract = {In this paper, we investigate computational methods for synthesizing optimal control inputs for nonholonomic control systems. We use the Brockett integrator as a benchmark example to illustrate different aspects of the computational trajectory optimization problem. The main result of this paper is the establishment of a rather attractive iterative scheme for practically constructing optimal control inputs. The functionality and efficiency of the proposed iterative scheme are illustrated on different optimal steering problems centered around the Brockett integrator.}
}
@article{DIETRICH2008319,
title = {Imaging the imagination: The trouble with motor imagery},
journal = {Methods},
volume = {45},
number = {4},
pages = {319-324},
year = {2008},
note = {Neuroimaging in the sports sciences},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2008.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S1046202308000765},
author = {Arne Dietrich},
keywords = {Brain, Cortex, Exercise, Explicit, Hypofrontality, Implicit, Mental training, Neuroimaging, Sports, Imagery},
abstract = {Sports and exercise psychology finds itself in a most unfortunate situation these days. While all other branches of the psychological sciences help themselves freely to the glitzy new toys of modern neuroscience—MRI and PET, mostly—exploring the neural underpinnings of whatever cognitive function they are interested in exploring, the sport sciences are left out of the fun for the simple reason that these imaging instruments preclude motion—the very thing then that is the subject of interest to them. There are several legitimate ways around this problem but the one that seems to be most popular is, I think, not—legitimate, that is. The basic idea, unduly sharpened here, is the following. Neuroimaging studies have shown that imagined and actual motion share the same neural substrates or, alternatively, imagining an action corresponds to a subliminal activation of the same brain areas required for its execution. It follows from this, the arguments runs, that motor imagery can be used as a proxy for real motor performance, et voilà, the sports sciences can go wild with all the snazzy brain imaging tools after all—just like everyone else. This notion is, I believe, misbegotten, a house of cards that threatens to cast a long shadow over the field. The present article, then, is, to be frank, intended to put a machete to this kind of thinking. It does this by exposing this conclusion to be based on an unholy marriage of selective data reporting and gross overgeneralization. The result is a wild goose chase fueled by wishful thinking.}
}
@article{KLATZMANN2025115372,
title = {A dynamic bifurcation mechanism explains cortex-wide neural correlates of conscious access},
journal = {Cell Reports},
volume = {44},
number = {3},
pages = {115372},
year = {2025},
issn = {2211-1247},
doi = {https://doi.org/10.1016/j.celrep.2025.115372},
url = {https://www.sciencedirect.com/science/article/pii/S2211124725001433},
author = {Ulysse Klatzmann and Sean Froudist-Walsh and Daniel P. Bliss and Panagiota Theodoni and Jorge Mejías and Meiqi Niu and Lucija Rapan and Nicola Palomero-Gallagher and Claire Sergent and Stanislas Dehaene and Xiao-Jing Wang},
keywords = {consciousness, large-scale brain model, connectome, NMDA, AMPA, feedforward, feedback, computational model, ignition, global neuronal workspace, access consciousness},
abstract = {Summary
Conscious access is suggested to involve “ignition,” an all-or-none activation across cortical areas. To elucidate this phenomenon, we carry out computer simulations of a detection task using a mesoscale connectome-based model for the multiregional macaque cortex. The model uncovers a dynamic bifurcation mechanism that gives rise to ignition in a network of associative regions. A hierarchical N-methyl-D-aspartate (NMDA)/α-amino-3-hydroxy-5-methyl-4-isoxazolepropionic acid (AMPA) receptor gradient plays a critical role: fast AMPA receptors drive feedforward signal propagation, while slow NMDA receptors in feedback pathways shape and sustain the ignited network. Intriguingly, the model suggests higher NMDA-to-AMPA receptor ratios in sensory areas compared to association areas, a prediction supported by in vitro autoradiography data. Furthermore, the model accounts for diverse behavioral and physiological phenomena linked to consciousness. This work sheds light on how receptor gradients along the cortical hierarchy enable distributed cognitive functions and provides a biologically constrained computational framework for investigating the neurophysiological basis of conscious access.}
}
@article{SAMPAYO2022100348,
title = {CPSD2: A new approach for cyber-physical systems design and development},
journal = {Journal of Industrial Information Integration},
volume = {28},
pages = {100348},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2022.100348},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X22000206},
author = {M. Sampayo and P. Peças},
keywords = {Cyber-physical system, Product design and development, Business model, Operational and production needs, Design of cyber-physical systems, Industry 4.0},
abstract = {Cyber-physical systems (CPS) allow the integration of computation to physical contexts, unlocking more sophisticated capabilities in engineering systems. A methodology is needed to properly guide the entire process of creating, designing, building and implementing a CPS. Methodologies addressing the essential aspects of CPS design already exist. However, none was found that properly concerns the company's business model and its way of market interaction, in order to identify opportunities where the use of CPS could improve the performance. Therefore, this article proposes CPSD2: a CPS design and development methodology approach based on generic product design and development methods.}
}
@article{DOU2021103147,
title = {Enhancing higher-order eigenmodes of AFM using bridge/cantilever coupled system},
journal = {Micron},
volume = {150},
pages = {103147},
year = {2021},
issn = {0968-4328},
doi = {https://doi.org/10.1016/j.micron.2021.103147},
url = {https://www.sciencedirect.com/science/article/pii/S0968432821001384},
author = {Zhipeng Dou and Jianqiang Qian and Yingzi Li and Rui Lin and Tingwei Wang and Jianhai Wang and Peng Cheng and Zeyu Xu},
keywords = {Atomic force microscopy, Multi-frequency, Transfer function, Higher-order eigenmodes, Finite element simulation},
abstract = {The wide application of multi-frequency atomic force microscopy (AFM) places higher demands on the higher-order modes response of the cantilever. The response of the higher modes however is generally weaker than that of the fundamental mode in air. Researchers have proposed many methods, most of which involve cantilever modification, to enhance higher-order eigenmodes response. These previous results are proved to be effective, but the microfabrication is expensive. In this article, we propose a novel model based on bridge/cantilever coupled system to enhance the higher-order modes response of AFM cantilever. The segmented beam model provides a new thinking to explain the appearance of undesired peaks in mode analysis of cantilever. Through theoretical analysis and simulation, we find that higher resonance modes are enhanced by tuning the bridge to match the high resonances of the single clamped cantilever. The length, thickness of the coupled system and the location of excitation can affect the enhancement. In summary, this model provides a new way to improve higher mode response for multi-frequency and other high bandwidth applications of AFM.}
}
@article{MIRTSOPOULOS2023103518,
title = {Structural topology exploration through policy-based generation of equilibrium representations},
journal = {Computer-Aided Design},
volume = {160},
pages = {103518},
year = {2023},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2023.103518},
url = {https://www.sciencedirect.com/science/article/pii/S0010448523000507},
author = {Ioannis Mirtsopoulos and Corentin Fivet},
keywords = {Design space exploration, Generative design, Rule-based design, Topology, Structural design, Strut-and-tie},
abstract = {Mainstream approaches to design spatial architectural forms that are structurally relevant consist either in adapting well-known and catalogued conventional types or in searching for close-to-optimum solutions of well-defined problems. Few means exist to explore structural forms detached from these routines. The approach in this paper generates diverse non-triangulated structural topologies that do not result from optimization procedures. The process incrementally transforms interim networks of bars and forces by means of a parametric policy (–) that maintains the static equilibrium of the network at every single step, (–) that ensures growth of the network within specified (non-)convex geometric boundaries, and (–) whose high-level abstract description controls all design parameters. The successive policy application aims at decreasing the number of interim forces while increasing the number of nodes and bars in compression or tension. The entire process ends when no interim force exists anymore, which is always achievable thanks to the permanence of the static equilibrium condition. From a designer perspective, the approach opens up the generative design black box by providing geometrical and topological control and partial automation of the generation process, while not resorting to common topology patterns – e.g. triangulated bar networks. This paper describes the conceptualization and its implementation into a computational framework, named Policy-based Exploration of Equilibrium Representations (PEER). It illustrates the potential of the approach to unveil unprecedented, unexpected, but statically-valid, structural topologies. Opportunities for further development are eventually discussed.}
}
@article{VARUGHESE2023684,
title = {The intersection of space and sustainability: The need for a transdisciplinary and bi-cultural approach},
journal = {Acta Astronautica},
volume = {211},
pages = {684-701},
year = {2023},
issn = {0094-5765},
doi = {https://doi.org/10.1016/j.actaastro.2023.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S0094576523003600},
author = {Carolle Varughese and Lena Henry and Adam Morris and Sarah Bickerton and Nicholas Rattenbury and Cody Mankelow and Alice Gorman and Stevie Katavich-Barton and Priyanka Dhopade},
keywords = {Max of six for acta astronautica, Space sustainability, Transdisciplinary, Indigenous sustainability, Terrestrial sustainability, Space debris},
abstract = {Aotearoa New Zealand's emerging New Space economy provides an opportunity for key actors to focus on space and sustainability issues beyond space debris. The conflict between competing definitions and paradigms of sustainability highlights the importance of diverse values, assumptions, and drivers of change that shape the normative understanding of space sustainability issues. This paper recognises that Indigenous knowledges and practices are in parallel with systems-thinking and transdisciplinary approaches to space and sustainability. The aim of this paper is to describe how current actions can have long term impacts on using and accessing space commercially, scientifically, and culturally.}
}
@article{JI2023106379,
title = {Scalable incomplete multi-view clustering via tensor Schatten p-norm and tensorized bipartite graph},
journal = {Engineering Applications of Artificial Intelligence},
volume = {123},
pages = {106379},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106379},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623005638},
author = {Guangyan Ji and Gui-Fu Lu and Bing Cai},
keywords = {Scalable incomplete multi-view clustering, Tensorized bipartite graph, Graph completion, Tensor low-rank constraint},
abstract = {Graph-based incomplete multi-view clustering (IMVC) methods have drawn considerable attention due to their good performance in exploring the nonlinear structure of data. However, they still have the following shortcomings. First, graph construction and eigen decomposition of the Laplacian matrix included in the IMVC methods generally have high computational complexity. Second, most methods do not consider the impact of missing views and neglect the potential relationships between different views. Third, few algorithms consider both intra-view and inter-view information for clustering. Therefore, we innovatively propose a scalable incomplete multi-view clustering via the tensor Schatten p-norm and tensorized bipartite graph (SIMVC/TSTBG) method, which combines tensorized bipartite graph, graph completion, and tensor low-rank constraint into a joint framework. Concretely, we first construct bipartite graphs based on the selected m anchor points and the n data points, reducing the size of the graph from n×n to n×m(m<<n), which considerably reduces the computational complexity. Second, we adaptively complete the missing bipartite graph, which reduces the effect of missing view information on the clustering results. Third, to explore connections between missing views and mine high-order information between views, we splice the bipartite graphs into a tensor and impose a tensor low-rank constraint, i.e., the tensor Schatten p-norm, on it. At the same time, we also design an efficient algorithm to solve SIMVC/TSTBG. To our knowledge, we are the first successful practice to integrate the tensor technique with the scalable IMVC method. Compared with other IMVC methods, the results on seven datasets fully show the high efficiency and effectiveness of SIMVC/TSTBG.}
}
@article{GAO2024104747,
title = {Representing and assessing distributed situation awareness in multi-agency disaster response: A hypergraph-based methodology},
journal = {International Journal of Disaster Risk Reduction},
volume = {111},
pages = {104747},
year = {2024},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2024.104747},
url = {https://www.sciencedirect.com/science/article/pii/S2212420924005090},
author = {Chong Gao and Hui Jiang and Xiaoling Guo},
keywords = {Distributed situation awareness, Systems thinking, Higher-order interactions, Multi-agency response},
abstract = {This paper introduces a novel hypergraph-based methodology for representing and assessing distributed situation awareness (DSA) in multi-agency disaster response. The fundamental ideas and motivations of our methodology stem from the following widely acknowledged understandings and phenomenons: (a) DSA’s representation should be approached from social, information and task dimensions; (b) DSA is one of the collective behaviors that emerge from the interactions; (c) the interactions in the real world are not pairwise. Our methodology delineates the collaboration, co-activation, and co-existence interactions among social, information, and task elements as higher-order interactions. We then construct these interaction systems using hypergraph-structured data derived from disaster response scenarios. Subsequently, these systems are encoded into hypergraphs, which are validated against our dataset and proven to be practical tools for depicting higher-order interaction patterns. Analytical techniques tailored to hypergraphs are applied, yielding insights intrinsic to hypergraphs regarding DSA in emergency response. Moreover, we integrate these interaction systems into a comprehensive framework that allows for the visualization and quantitative analyses of DSA evolution dynamics. We propose several indicators of evolution, discussing their trends and implications throughout the development of the emergency response. We locate the system deficiencies by revealing a mismatch between the positions of specific elements in the network and their functions. We also identify the saturation phase in the DSA evolution process.}
}
@incollection{AI2019853,
title = {Study on the formation of chemical wave patterns for the Belousov–Zhabotinsky reaction system},
editor = {Anton A. Kiss and Edwin Zondervan and Richard Lakerveld and Leyla Özkan},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {46},
pages = {853-858},
year = {2019},
booktitle = {29th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-12-818634-3.50143-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186343501430},
author = {Jiali Ai and Wei Sun and Chi Zhai},
keywords = {far-from thermodynamic equilibrium, instabilities, reaction-diffusion system, Hopf bifurcation},
abstract = {The Belousov–Zhabotinsky (BZ) reaction system is famous because it can generate self-organized patterns, also known as “chemical waves”. Pattern formation out of an initially homogeneous system is seemingly violating the 2nd-law of thermodynamics (order is produced out of disorder), while in fact, the BZ reaction is an open, far-from thermodynamic equilibrium system, where instability is the cause of morphogenesis and Hopf bifurcation of the reaction kinetics can generate self-oscillatory state trajectories. In this paper, the evolution of the BZ reaction in a two dimensional diffusion system is studied by the numerical computation methods, for the purpose of reconstructing the chemical wave patterns. The similarity of the chemical waves to many complex systems in biology, ecology and engineering makes current study potentially significant. With the study of the pattern formation, we hope provide some thoughts on complex system theory, thermodynamics of the self-oscillatory reaction system, and numerical computation methods on complex patterns, etc.}
}
@article{GOLCUK2022159,
title = {An interval type-2 fuzzy axiomatic design method: A case study for evaluating blockchain deployment projects in supply chain},
journal = {Information Sciences},
volume = {602},
pages = {159-183},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.04.034},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522003802},
author = {İlker Gölcük},
keywords = {Blockchain technology, Fuzzy subsethood, Axiomatic design, Best-worst method, Interval type-2 fuzzy sets},
abstract = {This study is concerned with the development of the axiomatic design (AD) method under an interval type-2 fuzzy (IT2F) environment and its application in evaluating blockchain deployment projects in supply chains. Blockchain is a transformative technology that has received significant attention recently. Blockchain technology can process various business transactions by offering a reliable and decentralized infrastructure. Supply chain management is an important application area of blockchains due to its desirable properties, including data security, extended visibility, product traceability, digitalization, and disintermediation. Since blockchain technologies are in their infancy, adopting them to supply chains requires proper design methodologies. Fuzzy AD offers valuable computational mechanisms to evaluate design options in the presence of functional requirements. However, extending AD to different fuzzy extensions is not an easy task, and area-based calculations hinder its widespread applicability. In this study, an IT2F-AD method is developed based on the concept of fuzzy subsethood. The potential of the fuzzy subsethood measure as the main computation engine within type-1 and IT2F-AD is demonstrated. Finally, an integrated multiple criteria decision-making (MCDM) model is proposed by using IT2F Best-Worst Method (IT2F-BWM) and IT2F-AD. The proposed model is used to prioritize blockchain deployment projects in a real-life case study.}
}
@article{WOLTHUSEN2018178,
title = {Correlation Between Levels of Delusional Beliefs and Perfusion of the Hippocampus and an Associated Network in a Non–Help-Seeking Population},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {3},
number = {2},
pages = {178-186},
year = {2018},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2017.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S2451902217301180},
author = {Rick P.F. Wolthusen and Garth Coombs and Emily A. Boeke and Stefan Ehrlich and Stephanie N. DeCross and Shahin Nasr and Daphne J. Holt},
keywords = {Arterial spin labeling, Delusion, fMRI, Hippocampus, Perfusion, Psychosis},
abstract = {Background
Delusions are a defining and common symptom of psychotic disorders. Recent evidence suggests that subclinical and clinical delusions may represent distinct stages on a phenomenological and biological continuum. However, few studies have tested whether subclinical psychotic experiences are associated with neural changes that are similar to those observed in clinical psychosis. For example, it is unclear if overactivity of the hippocampus, a replicated finding of neuroimaging studies of schizophrenia, is also present in individuals with subclinical psychotic symptoms.
Methods
To investigate this question, structural and pulsed arterial spin labeling scans were collected in 77 adult participants with no psychiatric history. An anatomical region of interest approach was used to extract resting perfusion of the hippocampus, and 15 other regions, from each individual. A self-report measure of delusional ideation was collected on the day of scanning.
Results
The level of delusional thinking (number of beliefs [r = .27, p = .02]), as well as the associated level of distress (r = .29, p = .02), was significantly correlated with hippocampal perfusion (averaged over right and left hemispheres). The correlations remained significant after controlling for age, hippocampal volume, symptoms of depression and anxiety, and image signal-to-noise ratio, and they were confirmed in a voxelwise regression analysis. The same association was observed in the thalamus and parahippocampal, lateral temporal, and cingulate cortices.
Conclusions
Similar to patients with schizophrenia, non–help-seeking individuals show elevated perfusion of a network of limbic regions in association with delusional beliefs.}
}
@article{PAVLOVA2025103832,
title = {A developmental perspective on mind wandering and its relation to goal-directed thought},
journal = {Consciousness and Cognition},
volume = {129},
pages = {103832},
year = {2025},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2025.103832},
url = {https://www.sciencedirect.com/science/article/pii/S105381002500025X},
author = {Maria K. Pavlova},
keywords = {Attention development, Direct memory retrieval, Executive control development, Motivated attention, Spontaneous thought, Stimulus-independent thought, Task-unrelated thought},
abstract = {Mind wandering (i.e., thoughts drifting from one topic to another, with no immediate connection to the perceptual field or the ongoing task) is a widespread cognitive phenomenon. There has been increasing research interest in mind wandering in children and adolescents. However, the developmental origins of this phenomenon remain largely unknown. In the present article, I summarize the purported cognitive mechanisms of mind wandering in adults and review the empirical findings on mind wandering and automatic memory retrieval in children and adolescents. I propose a comprehensive account of the emergence of mind wandering in early and middle childhood, covering the development of its central components identified in the adult literature: motivational and emotional processes, episodic and semantic processes, perceptual decoupling, and meta-awareness. Paying special attention to the roles of developing motivation and executive control, I then address the relationship between mind wandering and goal-directed thought in children.}
}
@article{SALAHSHOORI2024123888,
title = {Simulation-based approaches for drug delivery systems: Navigating advancements, opportunities, and challenges},
journal = {Journal of Molecular Liquids},
volume = {395},
pages = {123888},
year = {2024},
issn = {0167-7322},
doi = {https://doi.org/10.1016/j.molliq.2023.123888},
url = {https://www.sciencedirect.com/science/article/pii/S0167732223026958},
author = {Iman Salahshoori and Mahdi Golriz and Marcos A.L. Nobre and Shahla Mahdavi and Rahime {Eshaghi Malekshah} and Afsaneh Javdani-Mallak and Majid {Namayandeh Jorabchi} and Hossein {Ali Khonakdar} and Qilin Wang and Amir H. Mohammadi and Seyedeh {Masoomeh Sadat Mirnezami} and Farshad Kargaran},
keywords = {Computational fluid dynamics, Drug delivery systems, Molecular simulations, Optimization, Molecular dynamics simulation, Monte Carlo simulation},
abstract = {Efficient drug delivery systems (DDSs) play a pivotal role in ensuring pharmaceuticals’ targeted and effective administration. However, the intricate interplay between drug formulations and delivery systems poses challenges in their design and optimization. Simulations have emerged as indispensable tools for comprehending these interactions and enhancing DDSs performance to address this complexity. This comprehensive review explores the latest advancements in simulation techniques for DDSs and provides a detailed analysis. The review encompasses various simulation methodologies, including molecular dynamics (MD), Monte Carlo (MC), finite element analysis (FEA), computational fluid dynamics (CFD), density functional theory (DFT), machine learning (ML), and dissipative particle dynamics (DPD). These techniques are critically examined in the context of drug delivery research. The article presents illustrative case studies involving liposomal, polymer-based, nano-particulate, and implantable DDSs, demonstrating the influential role of simulations in optimizing these systems. Furthermore, the review addresses the advantages and limitations of simulations in drug delivery research. It also identifies future directions for research and development, such as integrating multiple simulation techniques, refining and validating models for greater accuracy, overcoming computational limitations, and exploring applications of simulations in personalized medicine and innovative DDSs. Simulations employing various techniques like MD, MC, FEA, CFD, DFT, ML, and DPD offer crucial insights into drug behaviour, aiding in DDS design and optimization. Despite their advantages, including rapid and cost-effective screening, simulations require validation and addressing computational limitations. Future research should focus on integrating techniques, refining models, and exploring personalized medicine applications to enhance drug delivery outcomes. This paper underscores the indispensable contribution of simulations to drug research and development, emphasizing their role in providing valuable insights into drug behaviour, facilitating the development and optimization of DDSs, and ultimately enhancing patient outcomes. As we continue to explore and enhance simulation techniques, their impact on advancing drug discovery and improving DDSs is expected to be profound.}
}
@article{LEVENSPIEL20024691,
title = {Modeling in chemical engineering},
journal = {Chemical Engineering Science},
volume = {57},
number = {22},
pages = {4691-4696},
year = {2002},
note = {Festschrift in Honour of Dr Winn van Swaaij},
issn = {0009-2509},
doi = {https://doi.org/10.1016/S0009-2509(02)00280-4},
url = {https://www.sciencedirect.com/science/article/pii/S0009250902002804},
author = {Octave Levenspiel},
abstract = {In its 90 year life what has chemical engineering (ChE) contributed to society? Firstly, we have invented and developed processes to create new materials, more gently and more efficiently, so as to make life easier for all. Secondly, ChE has changed our accepted concepts and our ways of thinking in science and technology. Here modeling stands out as the primary development. Let us consider this.}
}
@article{CAVEDON201514,
title = {“C׳Mon dude!”: Users adapt their behaviour to a robotic agent with an attention model},
journal = {International Journal of Human-Computer Studies},
volume = {80},
pages = {14-23},
year = {2015},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2015.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S1071581915000452},
author = {Lawrence Cavedon and Christian Kroos and Damith Herath and Denis Burnham and Laura Bishop and Yvonne Leung and Catherine J. Stevens},
keywords = {Human–robot interaction, Attention model, Social interaction, Evaluation, Engagement},
abstract = {Social cues facilitate engagement between interaction participants, whether they be two (or more) humans or a human and an artificial agent such as a robot. Previous work specific to human–agent/robot interaction has demonstrated the efficacy of implemented social behaviours, such as eye-gaze or facial gestures, for demonstrating the illusion of engagement and positively impacting interaction with a human. We describe the implementation of THAMBS, The Thinking Head Attention Model and Behavioural System, which is used to model attention controlling how a virtual agent reacts to external audio and visual stimuli within the context of an interaction with a human user. We evaluate the efficacy of THAMBS for a virtual agent mounted on a robotic platform in a controlled experimental setting, and collect both task- and behavioural-performance variables, along with self-reported ratings of engagement. Our results show that human subjects noticeably engaged more often, and in more interesting ways, with the robotic agent when THAMBS was activated, indicating that even a rudimentary display of attention by the robot elicits significantly increased attention by the human. Back-channelling had less of an effect on user behaviour. THAMBS and back-channelling did not interact and neither had an effect on self-report ratings. Our results concerning THAMBS hold implications for the design of successful human–robot interactive behaviours.}
}
@article{NIEMYSKA2024168455,
title = {Discovery of a trefoil knot in the RydC RNA: Challenging previous notions of RNA topology},
journal = {Journal of Molecular Biology},
volume = {436},
number = {6},
pages = {168455},
year = {2024},
issn = {0022-2836},
doi = {https://doi.org/10.1016/j.jmb.2024.168455},
url = {https://www.sciencedirect.com/science/article/pii/S0022283624000214},
author = {Wanda Niemyska and Sunandan Mukherjee and Bartosz A. Gren and Szymon Niewieczerzal and Janusz M. Bujnicki and Joanna I. Sulkowska},
keywords = {Entanglement of biomolecules, Topology in soft matter, RNA 3D structure, RNA folding, Molecular dynamics},
abstract = {Knots are very common in polymers, including DNA and protein molecules. Yet, no genuine knot has been identified in natural RNA molecules to date. Upon re-examining experimentally determined RNA 3D structures, we discovered a trefoil knot 31, the most basic non-trivial knot, in the RydC RNA. This knotted RNA is a member of a small family of short bacterial RNAs, whose secondary structure is characterized by an H-type pseudoknot. Molecular dynamics simulations suggest a folding pathway of the RydC RNA that starts with a native twisted loop. Based on sequence analyses and computational RNA 3D structure predictions, we postulate that this trefoil knot is a conserved feature of all RydC-related RNAs. The first discovery of a knot in a natural RNA molecule introduces a novel perspective on RNA 3D structure formation and on fundamental research on the relationship between function and spatial structure of biopolymers.}
}
@article{VUNJAKNOVAKOVIC20214597,
title = {Organs-on-a-chip models for biological research},
journal = {Cell},
volume = {184},
number = {18},
pages = {4597-4611},
year = {2021},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2021.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0092867421009478},
author = {Gordana Vunjak-Novakovic and Kacey Ronaldson-Bouchard and Milica Radisic},
abstract = {Summary
We explore the utility of bioengineered human tissues—individually or connected into physiological units—for biological research. While much smaller and simpler than their native counterparts, these tissues are complex enough to approximate distinct tissue phenotypes: molecular, structural, and functional. Unlike organoids, which form spontaneously and recapitulate development, “organs-on-a-chip” are engineered to display some specific functions of whole organs. Looking back, we discuss the key developments of this emerging technology. Thinking forward, we focus on the challenges faced to fully establish, validate, and utilize the fidelity of these models for biological research.}
}
@article{KARIMISANI2025102651,
title = {Drug repositioning for Parkinson’s disease: An emphasis on artificial intelligence approaches},
journal = {Ageing Research Reviews},
volume = {104},
pages = {102651},
year = {2025},
issn = {1568-1637},
doi = {https://doi.org/10.1016/j.arr.2024.102651},
url = {https://www.sciencedirect.com/science/article/pii/S1568163724004690},
author = {Iman Karimi-Sani and Mehrdad Sharifi and Nahid Abolpour and Mehrzad Lotfi and Amir Atapour and Mohammad-Ali Takhshid and Amirhossein Sahebkar},
keywords = {Neurodegenerative diseases, Parkinson’s disease, Levodopa induced dyskinesia, Drug repositioning, Artificial intelligence, Machine learning, Deep learning},
abstract = {Parkinson’s disease (PD) is one of the most incapacitating neurodegenerative diseases (NDDs). PD is the second most common NDD worldwide which affects approximately 1–2 percent of people over 65 years. It is an attractive pursuit for artificial intelligence (AI) to contribute to and evolve PD treatments through drug repositioning by repurposing existing drugs, shelved drugs, or even candidates that do not meet the criteria for clinical trials. A search was conducted in three databases Web of Science, Scopus, and PubMed. We reviewed the data related to the last years (1975-present) to identify those drugs currently being proposed for repositioning in PD. Moreover, we reviewed the present status of the computational approach, including AI/Machine Learning (AI/ML)-powered pharmaceutical discovery efforts and their implementation in PD treatment. It was found that the number of drug repositioning studies for PD has increased recently. Repositioning of drugs in PD is taking off, and scientific communities are increasingly interested in communicating its results and finding effective treatment alternatives for PD. A better chance of success in PD drug discovery has been made possible due to AI/ML algorithm advancements. In addition to the experimentation stage of drug discovery, it is also important to leverage AI in the planning stage of clinical trials to make them more effective. New AI-based models or solutions that increase the success rate of drug development are greatly needed.}
}
@article{MAKINDE2020368,
title = {An approach to estimate the back order penalty cost of a manufacturing company},
journal = {Procedia Manufacturing},
volume = {43},
pages = {368-374},
year = {2020},
note = {Sustainable Manufacturing - Hand in Hand to Sustainability on Globe: Proceedings of the 17th Global Conference on Sustainable Manufacturing},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.175},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920307551},
author = {Olasumbo Makinde and Thomas Munyai},
keywords = {Backorder cost, Economic Order Quantity, Customer disappointment index, Inventory Models},
abstract = {The classical inventory models solely rely on accurate estimate of the back order cost, with a view to establish economic order quantity (EOQ) that must be placed by a customer. Recognizing and quantifying the adverse effects of loss of customer goodwill owing to the inability of a raw material or product supplier organisation to meet customer demands should not only focus on direct penalty cost computation, but should also incorporate change in customers’ future demand owing to this backordering phenomenon. A lot of classical and mathematical approaches focused on the computation of the back order penalty cost coefficient; which gives an organisation a clue of the customer disappointment index, and not the estimated back order cost required for EOQ computation. In light of this, this paper proposes an approach that could be utilized to accurately compute the back order penalty cost of an organisation. The approach considers: (i) the number of times backordering phenomenon have occurred in an organisation, (ii) the decision a customer takes when backordering occur once or couple of times during the ordering phases of an organisation and (iii) myriads of penalties that a customer bestow on a raw material or product supplier organisation for backordering its order, to establish the backorder cost of this organisation. The approach proposed in this study serve as a useful information to suppliers in ascertaining the raw material backorder cost based on their customer responses to backordering, with a view to ensure sustainable raw material supply.}
}
@article{KULAKOVA2024103762,
title = {Comparing third-party responsibility with intention attribution: An fMRI investigation of moral judgment},
journal = {Consciousness and Cognition},
volume = {125},
pages = {103762},
year = {2024},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2024.103762},
url = {https://www.sciencedirect.com/science/article/pii/S1053810024001296},
author = {Eugenia Kulakova and Sofia Bonicalzi and Adrian L. Williams and Patrick Haggard},
keywords = {Moral responsibility, Intention, Social cognition, Causality},
abstract = {Neuroimaging studies demonstrate that moral responsibility judgments activate the social cognition network, presumably reflecting mentalising processes. Conceptually, establishing an agent’s intention is a sub-process of responsibility judgment. However, the relationship between both processes on a neural level is poorly understood. To date, neural correlates of responsibility and intention judgments have not been compared directly. The present fMRI study compares neural activation elicited by third-party judgments of responsibility and intention in response to animated pictorial stimuli showing harm events. Our results show that the social cognition network, in particular Angular Gyrus (AG) and right Temporo-Parietal Junction (RTPJ), showed stronger activation during responsibility vs. intention evaluation. No greater activations for the reverse contrast were observed. Our imaging results are consistent with conceptualisations of intention attribution as a sub-process of responsibility judgment. However, they question whether the activation of the social cognition network, particularly AG/RTPJ, during responsibility judgment is limited to intention evaluation.}
}
@article{LEON2009539,
title = {The future of computer-aided innovation},
journal = {Computers in Industry},
volume = {60},
number = {8},
pages = {539-550},
year = {2009},
note = {Computer Aided Innovation},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2009.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0166361509001286},
author = {Noel Leon},
keywords = {Computer Aided Innovation, TRIZ, QFD},
abstract = {A new category of tools known as CAI (computer-aided innovation) is an emerging domain in the array of computer-aided technologies. CAI has been growing as a response to greater industry demands for reliability in new products. Some initial CAI ideas and concepts focused on assisting product designers in the early stage of the design process, but now a more comprehensive vision conceives CAI systems as beginning at the fuzzy front end of perceiving business opportunities and customer demands, then continuing during the creative stage in developing inventions and, further on, providing help up to the point of turning inventions into successful innovations in the marketplace. CAI methods and tools are partially inspired by Innovation Theories, such as TRIZ, QFD (Quality Function Development), Axiomatic Design, Synectics, General Theory of Innovation, Mind Mapping, Brain Storming, Lateral Thinking, and Kansei Engineering, among others. The goal of these new CAI tools under development is to assist innovators, inventors, designers, process developers and managers in their creative performance, with the expectation of changes in paradigms through the use of this new category of software tools. CAI, therefore, stands out as a departure from the usual trends. The latest approaches are presented and analyzed to derive conclusions regarding the present status and the future of these emerging tools.}
}
@article{LIU2023113460,
title = {What is the “DNA” of healthy buildings? A critical review and future directions},
journal = {Renewable and Sustainable Energy Reviews},
volume = {183},
pages = {113460},
year = {2023},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2023.113460},
url = {https://www.sciencedirect.com/science/article/pii/S1364032123003179},
author = {Hui Liu and Xiaoxiao Xu and Vivian W.Y. Tam and Peng Mao},
keywords = {Healthy building, Indoor environment, DNA framework, Sustainability, Life-cycle, Future directions},
abstract = {Since the outbreak of COVID-19, buildings that provide improved performance have aroused extensive discussion. Nowadays, the connotation of healthy building is becoming complex, performance metrics for healthy buildings vary significantly from different regions in the world and there may be information asymmetry among stakeholders. Consequently, building health performance cannot be effectively achieved. However, previous studies have launched extensive reviews on green building, and there remains a lack of comprehensive and systematic reviews on healthy buildings. To address the above issues, therefore, this research aims to (1) conduct a thorough review of healthy building research and reveal its nature; and (2) identify the current research gaps and propose possible future research directions. Content analysis using NVivo were applied to review 238 relevant publications. A DNA framework of healthy buildings, which clarifies the characteristics, triggers, guides and actions, was then constructed for better understanding of the nature of them. Subsequently, the application of DNA framework and the directions of future research were discussed. Six future research directions were finally recommended, including life-cycle thinking, standard systems improvement, policies & regulations, awareness increase, healthy building examination, and multidisciplinary integration. This research differs from previous ones because it painted a panorama of previous healthy building research. Findings of this research contribute to reveal knowledge map of healthy buildings, guide researchers to fill existing knowledge gaps, provide a standardized platform for healthy building stakeholders, and promote high-quality development of healthy buildings.}
}
@article{TOWERS201025,
title = {An ecological reading of mathematical language in a Grade 3 classroom: A case of learning and teaching measurement estimation},
journal = {The Journal of Mathematical Behavior},
volume = {29},
number = {1},
pages = {25-40},
year = {2010},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2009.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S073231230900056X},
author = {Jo Towers and Kim Hunter},
keywords = {Classroom mathematical language, Ecological thinking, Measurement, Estimation},
abstract = {In our work in teacher education and professional development, we aim to help teachers to learn to participate in, and create, classroom ecologies that support students’ learning. In this article we focus on the challenges of developing a classroom ecology that provides mathematical sustenance for students. We pay particular attention to the ways in which classroom language can impede the development of a classroom ecology—one where all students are heard and where knowing is understood as participatory. We present recommendations for teaching practice drawn from an ecological reading of the classroom discourse during a series of lessons on measurement in a Grade 3 classroom.}
}
@article{CARPENTER2020100064,
title = {Bridging Domain and Data},
journal = {Patterns},
volume = {1},
number = {4},
pages = {100064},
year = {2020},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2020.100064},
url = {https://www.sciencedirect.com/science/article/pii/S2666389920300842},
author = {Anne E. Carpenter},
abstract = {Dr. Anne Carpenter addresses her career path from cell biology toward computation. Why would a researcher move outside their comfort zone into a different field, from a domain into data science? What is the best way to bridge domain and data? What is challenging about moving from domain toward data? What is amazing about bridging domain and data?}
}
@article{KOWALCZUK2020103562,
title = {Interpretation and modeling of emotions in the management of autonomous robots using a control paradigm based on a scheduling variable},
journal = {Engineering Applications of Artificial Intelligence},
volume = {91},
pages = {103562},
year = {2020},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2020.103562},
url = {https://www.sciencedirect.com/science/article/pii/S0952197620300518},
author = {Zdzisław Kowalczuk and Michał Czubenko and Tomasz Merta},
keywords = {Emotions, Decision-making systems, Cognitive modeling, Human mind, Computational models, Fuzzy approach, Intelligent systems, Autonomous agents},
abstract = {The paper presents a technical introduction to psychological theories of emotions. It highlights a usable idea implemented in a number of recently developed computational systems of emotions, and the hypothesis that emotion can play the role of a scheduling variable in controlling autonomous robots. In the main part of this study, we outline our own computational system of emotion – xEmotion – designed as a key structural element in the developed target device, being an Intelligent System of Decision-making (ISD) for autonomous and robotic units. The ISD system has a cognitive architecture based on the principles of human psychology. The main purpose of building such a system is to prepare a framework for autonomous units used in system engineering (Kowalczuk and Czubenko, 2011; Czubenko et al., 2015). In particular, ISD is based on the concepts of cognitive psychology (in information processing) and motivation theory, which includes the system of needs (for decision-making). The xEmotion subsystem, however, focuses on modeling an alternative approach based on emotion. The xEmotion implementation covers aspects of somatic, appraisal and evolutionary theories of emotions using fuzzy sets. In this article, we also illustrate the core emotional behavior of the ISD system using simulation. The first application is a user interface for identifying emotions and predicting human behavior. The second is an eSailor simulation, which illustrates the possible behavior of the xEmotion subsystem. The last is an xDriver simulation experiment, which is to prove the validity of the concept of using emotion-based systems, according to the SVC principle. In summary, we also discuss other possible applications of the xEmotion system.}
}
@article{GARCIA2025e207,
title = {Future-proofing cities against negative city mobility and public health impacts of impending natural hazards: a system dynamics modelling study},
journal = {The Lancet Planetary Health},
volume = {9},
number = {3},
pages = {e207-e218},
year = {2025},
issn = {2542-5196},
doi = {https://doi.org/10.1016/S2542-5196(25)00026-9},
url = {https://www.sciencedirect.com/science/article/pii/S2542519625000269},
author = {Leandro Garcia and Mehdi Hafezi and Larissa Lima and Christopher Millett and Jason Thompson and Ruoyu Wang and Selin Akaraci and Rahul Goel and Rodrigo Reis and Kerry A Nice and Belen Zapata-Diomedi and Pedro C Hallal and Esteban Moro and Clifford Amoako and Ruth F Hunter},
abstract = {Summary
Background
The world faces increasing risk from more frequent and larger scale natural hazards, including infectious disease outbreaks (IDOs) and climate change-related extreme weather events (EWEs). These natural hazards are expected to have adverse mobility and public health impacts, with people living in cities especially vulnerable. Little is known about how transport systems can be optimally designed to make cities more resilient to these hazards. Our aim was to investigate how cities’ transport systems, and their resulting mobility patterns, affect their capabilities to mitigate mobility and health impacts of future large-scale IDOs and EWEs.
Methods
System dynamics modelling was used to investigate how different city mobility scenarios can affect the health and mobility impacts of four plausible future IDO and EWE (flooding) shocks in three cities: Belfast, UK; Belo Horizonte, Brazil; and Delhi, India. Three city mobility scenarios with incremental degrees of modal shift towards active travel (private motor vehicle volume reduced to 50% and 20% of total road trip volume in vision 1 and 2, and motor vehicle volume [including buses] reduced to 20% of total road trip volume in vision 3) were tested. For each city and each IDO and EWE shock, we estimated the percentage of deaths prevented in visions 1, 2, and 3, relative to the reference scenario, as well as changes in mode share over time.
Findings
In all scenarios, all cities showed reduced susceptibility to flooding, with 4–50% of deaths potentially prevented, depending on case city, city mobility, and EWE scenario. The more ambitious the transition towards healthier city mobility patterns, the greater the resilience against flooding. Only vision 3 (the most ambitious transition) showed reduced vulnerability to IDOs, with 6–19% of deaths potentially prevented. Evolution of mode shares varied greatly across cities and mobility scenarios under the IDO shocks.
Interpretation
Our results emphasise the importance of well designed, forward-thinking urban transport systems that make cities more resilient and reduce the impact of future public health-related and climate-related threats.
Funding
UK Prevention Research Partnership, UK Economic and Social Research Council, UK Medical Research Council, UK National Institute for Health and Care Research, Australian Research Council, Australian National Health and Medical Research Council, and Health and Social Care Research and Development Office Northern Ireland.}
}
@article{DUGGAN2024100102,
title = {The digital geographies of tact},
journal = {Digital Geography and Society},
volume = {7},
pages = {100102},
year = {2024},
issn = {2666-3783},
doi = {https://doi.org/10.1016/j.diggeo.2024.100102},
url = {https://www.sciencedirect.com/science/article/pii/S2666378324000242},
author = {Mike Duggan},
keywords = {Tact, Tactics, Tactility, Touch, Social behaviour, Judgement, Space, Digital media, Artificial intelligence},
abstract = {This article outlines a research agenda for the spatialities of tact produced by, through and of digital spaces. As a discipline interested in what and who characterises digital space, and in how different relations come to produce space, the article puts forward a proposition for geographers to take tact seriously as an inherently spatial concept useful for theorising the production of space in our digital society. The paper identifies three strands of tact from the literature, 1) tact and social behaviour, 2) tact and touch, 3) tact and judgement, and outlines what they can offer geography in terms of a novel framework for studying digital society. It raises questions of how and why digital spaces and practices produce new trajectories for displays of tact in everyday life, how digital spaces modulate our understanding and experiences of touch, as well as asking whether algorithmic decision making technologies such as Artificial Intelligence have a capacity for tact, and what that means for the geographies these systems shape. The work makes a contribution to the discipline's long standing interests in spatial tactics and socio-spatial behaviour, in touch and sensory geographies, and more recently to algorithmic decision making.}
}
@article{WILKINSON2024168584,
title = {Environmental impacts of earth observation data in the constellation and cloud computing era},
journal = {Science of The Total Environment},
volume = {909},
pages = {168584},
year = {2024},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2023.168584},
url = {https://www.sciencedirect.com/science/article/pii/S0048969723072121},
author = {R. Wilkinson and M.M. Mleczko and R.J.W. Brewin and K.J. Gaston and M. Mueller and J.D. Shutler and X. Yan and K. Anderson},
keywords = {Cloud computing, Satellite, Data centre, Carbon intensity, Environmental impacts},
abstract = {Numbers of Earth Observation (EO) satellites have increased exponentially over the past decade reaching the current population of 1193 (January 2023). Consequently, EO data volumes have mushroomed and data storage and processing have migrated to the cloud. Whilst attention has been given to the launch and in-orbit environmental impacts of satellites, EO data environmental footprints have been overlooked. These issues require urgent attention given data centre water and energy consumption, high carbon emissions for computer component manufacture, and difficulty of recycling computer components. Doing so is essential if the environmental good of EO is to withstand scrutiny. We provide the first assessment of the EO data life-cycle and estimate that the current size of the global EO data collection is ~807 PB, increasing by ~100 PB/year. Storage of this data volume generates annual CO2 equivalent emissions of 4101 t. Major state-funded EO providers use 57 of their own data centres globally, and a further 178 private cloud services, with considerable duplication of datasets across repositories. We explore scenarios for the environmental cost of performing EO functions on the cloud compared to desktop machines. A simple band arithmetic function applied to a Landsat 9 scene using Google Earth Engine (GEE) generated CO2 equivalent (e) emissions of 0.042–0.69 g CO2e (locally) and 0.13–0.45 g CO2e (European data centre; values multiply by nine for Australian data centre). Computation-based emissions scale rapidly for more intense processes and when testing code. When using cloud services such as GEE, users have no choice about the data centre used and we push for EO providers to be more transparent about the location-specific impacts of EO work, and to provide tools for measuring the environmental cost of cloud computation. The EO community as a whole needs to critically consider the broad suite of EO data life-cycle impacts.}
}
@incollection{VOIRONCANICIO202185,
title = {Chapter 4 - Methods and tools in geoprospective},
editor = {Emmanuel Garbolino and Christine Voiron-Canicio},
booktitle = {Ecosystem and Territorial Resilience},
publisher = {Elsevier},
pages = {85-122},
year = {2021},
isbn = {978-0-12-818215-4},
doi = {https://doi.org/10.1016/B978-0-12-818215-4.00004-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128182154000043},
author = {Christine Voiron-Canicio and Emmanuel Garbolino and Giovanni Fusco and Jean-Christophe Loubier},
keywords = {Geoprospective approach, simulations, modeling, uncertainty, land change, decision-making tool, uncertain causal model, geovizualization, graphic modeling, 3D simulation},
abstract = {This chapter illustrates the diversity of methods and tools available for developing a geoprospective approach, and, through them, the variety of ways to introduce the spatial dimension in scenarios, simulations, and collective thinking. A number of methods, such as modeling, are not specific to geoprospective. The perspective adopted is to shine the light on what their use in geoprospective entails: the specific constraints and the new questions raised relating to the weight of past evolutions, to the unforeseen, to uncertainty. In addition to the models of the land use and cover change (LUCC) type and companion modeling, this chapter gives much importance to the following new approaches which are hitherto hardly used in geoprospective: scenarios integrating various territorial scales, modeling of the decision-making process coupled with prospective spatial modeling, geoprospective based on causal probabilistic models, graphic modeling, prospective choremes, immersive and 3D simulation in landscapes of the future.}
}
@article{ZHANG2017427,
title = {Genomic Energy Landscapes},
journal = {Biophysical Journal},
volume = {112},
number = {3},
pages = {427-433},
year = {2017},
issn = {0006-3495},
doi = {https://doi.org/10.1016/j.bpj.2016.08.046},
url = {https://www.sciencedirect.com/science/article/pii/S0006349516307743},
author = {Bin Zhang and Peter G. Wolynes},
abstract = {Energy landscape theory, developed in the context of protein folding, provides, to our knowledge, a new perspective on chromosome architecture. We review what has been learned concerning the topology and structure of both the interphase and mitotic chromosomes from effective energy landscapes constructed using Hi-C data. Energy landscape thinking raises new questions about the nonequilibrium dynamics of the chromosome and gene regulation.}
}
@article{WALL201161,
title = {Structure–function relations are subtle in genetic regulatory networks},
journal = {Mathematical Biosciences},
volume = {231},
number = {1},
pages = {61-68},
year = {2011},
note = {Special issue on biological design principles},
issn = {0025-5564},
doi = {https://doi.org/10.1016/j.mbs.2011.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0025556411000150},
author = {Michael E. Wall},
keywords = {Gene regulation, Function prediction, Network motifs, , Computational biology, Synthetic biology},
abstract = {Recent studies have yielded insights into structure–function relations in genetic regulatory networks. Models of feed-forward loops show that the input–output behavior depends critically on the input signal as well as transcription interactions. Models of induction of the lac operon in Escherichia coli reveal the importance of metabolism in determining genetic regulatory network behavior. Combined experimental and computational studies of activation by MarA in E. coli show how mechanisms of transcription regulation, hidden at the level of genetic regulatory networks, can influence behavior. Together these studies illustrate that gene regulation is critically influenced by factors beyond the topology of genetic regulatory interactions. Prediction of the specific information processing roles of gene circuits is more difficult than we would like, but it is still possible. Thinking about evolution of proteins and networks might make it easier.}
}
@article{GHIMIRE2025102913,
title = {Utilizing ChatGPT to integrate world English and diverse knowledge: A transnational perspective in critical artificial intelligence (AI) literacy},
journal = {Computers and Composition},
volume = {75},
pages = {102913},
year = {2025},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2024.102913},
url = {https://www.sciencedirect.com/science/article/pii/S8755461524000896},
author = {Asmita Ghimire},
abstract = {This article proposes the implementation of a transnational post-digital pedagogy and Critical AI literacy incorporating ChatGPT in the classroom. It draws upon Scott Graham's suggestion for a multidimensional recursive writing process, emphasizing fact-checking and revision while utilizing ChatGPT. Additionally, it incorporates Suresh Canagarajah's (2019) theorization of transnational habits of writing among most international, multilingual, and marginalized students, which, according to him, are characterized by rhetorical sensitivity, depth of awareness, and linguistic knowledge. Based on these empirical and theoretical perspectives, this article proposes pausing, pondering, posing, and prioritizing as critical praxis that can be built into metacognitive activities. To explain this praxis, it showcases two kinds of metacognitive activities for fostering transnational habits among students through fact-checking processes. Similarly, it suggests designing the revision phase of writing assignments to allow students to incorporate their English language skills into the classroom. This paper identifies engaging in critical dialogue with ChatGPT and encouraging self-reflection on fact-checking and revision as effective ways to cultivate a transnational habitus among students. It concludes that adopting a transnational post-digital critical pedagogy and critical AI literacy in the writing process benefits both national and international students by promoting diverse linguistic norms and perspectives.}
}
@article{CHEN2021107754,
title = {2D multi-area coverage path planning using L-SHADE in simulated ocean survey},
journal = {Applied Soft Computing},
volume = {112},
pages = {107754},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107754},
url = {https://www.sciencedirect.com/science/article/pii/S156849462100675X},
author = {Guanzhong Chen and Yue Shen and Yixiao Zhang and Wenfeng Zhang and Dianrui Wang and Bo He},
keywords = {L-SHADE, Multi-area, Coverage path planning, Sub-area path planning, Mutation process},
abstract = {Ocean environmental surveys typically involve multi-area coverage path planning tasks. The most important problem is improving the coverage efficiency of the task. A new path planning method based on Successful History-Based Adaptive Differential Evolution variants with Linear population size reduction(L-SHADE) is presented to solve this problem. The method comprises two parts: the part of sub area coverage path planning and the part of finding the optimized sequence of sub area start points. The key idea is establishing the relationship between the starting point of each sub area and the optimized multi-area path. We implement the method through numbering the possible starting point of sub area path and proposing a computing formula. In addition, the results of L-SHADE mutation process are optimized which make L-SHADE possible to apply in multi-area coverage path planning. This method avoids area discretization and exponential growth of computational quantities, and it is suitable for complex areas as well as multi-area. The simulation results with MATLAB showed the improvement of coverage path planning task execution efficiency. Compared with the method thinking of the sub area as the center of it, our method reduced the multi-area coverage path length by 4%–7%. From the simulations and analysis, we concluded that the method is able to improve the efficiency and stability of multi-area coverage path planning.}
}
@article{MILLER2025105387,
title = {Biological mechanisms contradict AI consciousness: The spaces between the notes},
journal = {BioSystems},
volume = {247},
pages = {105387},
year = {2025},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2024.105387},
url = {https://www.sciencedirect.com/science/article/pii/S0303264724002727},
author = {William B. Miller and František Baluška and Arthur S. Reber and Predrag Slijepčević},
keywords = {Consciousness, Cellular basis of consciousness, Cognition-based evolution, Information field, Artificial intelligence, Senome},
abstract = {The presumption that experiential consciousness requires a nervous system and brain has been central to the debate on the possibility of developing a conscious form of artificial intelligence (AI). The likelihood of future AI consciousness or devising tools to assess its presence has focused on how AI might mimic brain-centered activities. Currently, dual general assumptions prevail: AI consciousness is primarily an issue of functional information density and integration, and no substantive technical barriers exist to prevent its achievement. When the cognitive process that underpins consciousness is stipulated as a cellular attribute, these premises are directly contradicted. The innate characteristics of biological information and how that information is managed by individual cells have no parallels within machine-based AI systems. Any assertion of computer-based AI consciousness represents a fundamental misapprehension of these crucial differences.}
}
@article{BROER2022115131,
title = {The Googlization of Health: Invasiveness and corporate responsibility in media discourses on Facebook's algorithmic programme for suicide prevention},
journal = {Social Science & Medicine},
volume = {306},
pages = {115131},
year = {2022},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2022.115131},
url = {https://www.sciencedirect.com/science/article/pii/S0277953622004373},
author = {Tineke Broer},
keywords = {Suicide prevention, Facebook, Content moderation, Privacy, Googlization of health},
abstract = {Big tech companies increasingly play a role in the domain of health. Also called the “Googlization of Health”, this phenomenon is often studied by drawing on the notion of ‘hostile worlds’, where market values and common goods are incommensurable. Yet, the ‘hostile worlds’ theory is not uncontested; scholars for instance argue that the justifications of big tech companies are important analytical considerations as well. Building on this literature, in this paper I report on a case study of Facebook employing AI for suicide prevention, moving beyond Facebook's justifications only to study the ways in which media commentators and their audiences discussed Facebook's programme and the values they saw as being at stake. In the results, I show how invasiveness was, in different ways and forms, a main theme in thinking about Facebook using AI to do suicide prevention. Commentators and readers alike discussed how: 1) Facebook takes corporate responsibility with this initiative, or alternatively Facebook only has commercial interests and uses the notion of ‘public good’ to transgress spheres and sectors even further, thus being invasive; 2) Facebook's AI suicide prevention programme is invasive in relation to privacy and privacy laws, or, instead, people give up their privacy willingly in exchange for entertainment; 3) The programme undermines, rather than enhances, safety; 4) Suicide prevention in itself is already invasive. These different forms of invasiveness, I argue in the conclusion, also imply responsibility for different actors, from AI itself to Facebook through to medical professionals. Moreover, they show what values are at stake in, and transformed through, Facebook's AI suicide prevention programme, going beyond the frames of privacy and surveillance capitalism.}
}
@article{GREENLEE20201043,
title = {Kinetic and Thermodynamic Control in Dynamic Covalent Synthesis},
journal = {Trends in Chemistry},
volume = {2},
number = {12},
pages = {1043-1051},
year = {2020},
issn = {2589-5974},
doi = {https://doi.org/10.1016/j.trechm.2020.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S258959742030232X},
author = {Andrew J. Greenlee and Chloe I. Wendell and Morgan M. Cencer and Summer D. Laffoon and Jeffrey S. Moore},
keywords = {dynamic, covalent, reversible, kinetic, thermodynamic},
abstract = {In recent years, dynamic covalent chemistry (DCC) has seen the synthesis of increasingly complex cyclooligomers, polymers, and diverse compound libraries. The reversible formation of covalent bonds characteristic of DCC reactions favors thermodynamic product distributions for simple unitopic reactions; however, kinetic effects are increasingly influential in reactions of multitopic precursors. In this review, we explore the interplay between thermodynamic and kinetic considerations when planning a DCC synthesis. Computational models, typically based on reaction thermodynamics, have aided in predicting DCC reaction outcomes with moderate success. A clear direction for the field is to develop more robust computational tools informed by thermodynamic and kinetic driving forces that can predict product distributions in DCC reactions.}
}
@article{EHRENFELD2019102525,
title = {Online Public Spheres in the Era of Fake News: Implications for the Composition Classroom},
journal = {Computers and Composition},
volume = {54},
pages = {102525},
year = {2019},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2019.102525},
url = {https://www.sciencedirect.com/science/article/pii/S875546151830029X},
author = {Dan Ehrenfeld and Matt Barton},
keywords = {fake news, public sphere, social media, composition, misinformation, disinformation, critical thinking, media literacy},
abstract = {This article revisits Matt Barton's 2005 article "The Future of Rational-Critical Debate in Online Public Spheres" in light of recent debates around misinformation and disinformation, data-driven influence campaigns, the blurring line between social media and news media, and the algorithmic incentivization of “fake news.” While today’s social media platforms exhibit many of the qualities that C.W. Mills and Jürgen Habermas associate with a healthy public sphere—communication between strangers is participatory, immediate, accessible, and decentralized—this article raises questions about the extent to which everyday digital writing and circulation practices align with broader democratic aspirations. The goal of this article is to explore not only what these social and technological developments mean for the health of public discourse, but also how we, as teachers of writing, can meaningfully engage with them in our classrooms. An appendix includes ideas for assignments that engage students in critical reflection about their own participation in today’s online public spheres.}
}
@article{MAVERS2002187,
title = {Interpreting the externalised images of pupils’ conceptions of ICT: methods for the analysis of concept maps},
journal = {Computers & Education},
volume = {38},
number = {1},
pages = {187-207},
year = {2002},
issn = {0360-1315},
doi = {https://doi.org/10.1016/S0360-1315(01)00074-4},
url = {https://www.sciencedirect.com/science/article/pii/S0360131501000744},
author = {Diane Mavers and Bridget Somekh and Jane Restorick},
keywords = {Concept mapping, Representations, Learning, Networked technologies, Phenomenography},
abstract = {The ImpacT2 evaluation is using image based concept mapping as one method of exploring the impact of networked technologies on students' learning. In a pre-test administered in June 2000, students in three cohorts aged 10–11, 13–14 and 15–16, produced around 2000 ‘maps’. Entitled ‘Computers in My World’, these provide a means of students externalising mental representations of networked technologies. Using a phenomenographic approach, the study aims to identify qualitatively different patterns of thinking and trends in the development of pupils' concepts. Five quanititative measures emerged from heuristic analysis of the maps: nodes, links, connectivity, ‘Spheres of Thinking’ and ‘Zones of Use’. Analysis of the pre-test maps was carried out alongside analysis of pre-test questionnaires, using SPSS. The outcomes suggest correlations between pupils' experience and the constent of their maps. Phenomenographic interviewing of selected 11 year old pupils, which entailed handling control over to interviewees through the use of open-ended questions, enabled further exploration of their experiences and understandings of those experiences. A method for in-depth interviewing of young students is described. Data suggest that pupils have sophisticated ‘secondary artifacts’ or mental models of the nature of networked technologies and their role in today's world. This has implications for the way that ICT is used in schools and for its potential as a tool for students' learning.}
}
@article{BASHIRPOURBONAB2023104459,
title = {Urban quantum leap: A comprehensive review and analysis of quantum technologies for smart cities},
journal = {Cities},
volume = {140},
pages = {104459},
year = {2023},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2023.104459},
url = {https://www.sciencedirect.com/science/article/pii/S0264275123002718},
author = {Aysan {Bashirpour Bonab} and Maria Fedele and Vincenzo Formisano and Ihor Rudko},
keywords = {Smart city, Quantum city, Smart city technologies, Urban quantum technologies, Semi-systematic literature review, Thematic analysis},
abstract = {Contemporary smart city solutions rely on standardized von Neumann architecture, in which single data units are coded as “0” or “1.” Conversely, urban quantum technologies rely on the fundamental principles of quantum physics, transcending the conventions of the current computational paradigm. On the one hand, urban quantum technologies hold managerial relevance for future smart cities. On the other hand, they are often overlooked by smart city researchers. Accordingly, their value as a breakthrough technological paradigm is still largely unexplored. In this article, we look at how quantum technologies may contribute to existing smart city solutions, including the Internet of Things, cloud computing, big data, ICT, smart transportation, artificial intelligence, and blockchain. First, through a semi-systematic review of eighty articles on quantum computing within the social science domain, we identify two relevant classes of urban quantum technologies: quantum communication and quantum computing. Second, we establish a comprehensive taxonomy of conventional smart city solutions based on the automated content analysis of 567 abstracts of articles on the technological aspects of smart cities. Third, we investigate potential associations between two classes of technologies (conventional smart city solutions and urban quantum technologies) by analyzing the semantic relationships between eighty articles on quantum technologies according to the frequency of keywords denoting different types of conventional smart city solutions. Finally, we triangulate our findings through a thematic analysis of potential uses of quantum technologies within identified categories of smart city solutions.}
}
@article{TARI2023119635,
title = {Expansion-based Hill-climbing},
journal = {Information Sciences},
volume = {649},
pages = {119635},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.119635},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523012203},
author = {Sara Tari and Matthieu Basseur and Adrien Goëffon},
keywords = {Maximum expansion, Hill-climbing algorithm, Local search, Fitness landscapes, Landscape-aware heuristics, Combinatorial optimization},
abstract = {This paper investigates the influence of adaptive walks heuristics within local searches, by studying to what extent a wiser choice among improving neighbors influences the expected quality of the attained local optima. To this aim, we specifically focus on hill-climbers and introduce the maximum expansion pivoting rule, which selects the improving neighbor having the highest number of improving neighbors. Experiments show that having one step ahead of information allows for wiser neighbor choices, leading to better local optima. As the best improvement climber, a maximum expansion climber selects a particular search trajectory among all possible first improvement trajectories, however, it significantly increases the expected quality of the trajectory. On the other hand, the computational overhead makes this heuristic less valuable when included in an iterated search process, where repeating fast random hill-climbings from random starting points still allows a better exploration of the search space. This paper, therefore, extends previous studies on the relative efficiency of hill-climbing pivoting rules, by focusing on the original maximum expansion selection criterion. Results suggest that the achievement of good local optima combined with the use of approximation techniques and problem specificities could lead to the design of effective advanced metaheuristics that exploit the maximum expansion principle.}
}
@article{SCAGLIOTTI2025113811,
title = {Normalizing flows as approximations of optimal transport maps via linear-control neural ODEs},
journal = {Nonlinear Analysis},
volume = {257},
pages = {113811},
year = {2025},
issn = {0362-546X},
doi = {https://doi.org/10.1016/j.na.2025.113811},
url = {https://www.sciencedirect.com/science/article/pii/S0362546X25000653},
author = {A. Scagliotti and S. Farinelli},
keywords = {Optimal transport, Optimal control, Γ-convergence, Linear-control neural ODEs},
abstract = {In this paper, we consider the problem of recovering the W2-optimal transport map T between absolutely continuous measures μ,ν∈P(Rn) as the flow of a linear-control neural ODE, where the control depends only on the time variable and takes values in a finite-dimensional space. We first show that, under suitable assumptions on μ,ν and on the controlled vector fields governing the neural ODE, the optimal transport map is contained in the Cc0-closure of the flows generated by the system. Then, we tackle the problem under the assumption that only discrete approximations of μN,νN of the original measures μ,ν are available: we formulate approximated optimal control problems, and we show that their solutions give flows that approximate the original optimal transport map T. In the framework of generative models, the approximating flow constructed here can be seen as a ‘Normalizing Flow’, which usually refers to the task of providing invertible transport maps between probability measures by means of deep neural networks. We propose an iterative numerical scheme based on the Pontryagin Maximum Principle for the resolution of the optimal control problem, resulting in a method for the practical computation of the approximated optimal transport map, and we test it on a two-dimensional example.}
}
@incollection{COBB200685,
title = {Constructivism},
editor = {Keith Brown},
booktitle = {Encyclopedia of Language & Linguistics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {85-87},
year = {2006},
isbn = {978-0-08-044854-1},
doi = {https://doi.org/10.1016/B0-08-044854-2/01593-5},
url = {https://www.sciencedirect.com/science/article/pii/B0080448542015935},
author = {T. Cobb},
keywords = {constructivism, constructivist, educational reform, language technologies, learner-as-linguist, objectivism, second language acquisition},
abstract = {Constructivism, the notion that knowledge must be assembled from pieces rather than assimilated whole, has been a principal learning theory in psychology for about 20 years and in psycholinguistics for 10. The theory is now making a strong entry into educational thinking, but in language education it is less in evidence. That is because applied linguists have always been constructivists, implicitly, and have already confronted some of the implementation problems facing constructivism in mathematics or science education. Nevertheless, a more explicit understanding of the constructivist approach is useful within language education, particularly in providing a framework for exploiting information technologies.}
}
@article{FUNG20243519,
title = {Chemical education in digital chemistry},
journal = {Chem},
volume = {10},
number = {12},
pages = {3519-3525},
year = {2024},
issn = {2451-9294},
doi = {https://doi.org/10.1016/j.chempr.2024.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S2451929424005369},
author = {Fun Man Fung and Magdalena Lederbauer and Yvonne S.L. Choo and Timo Gehring and Kevin Maik Jablonka and Kjell Jorner and Philippe Schwaller and Michael B. Sullivan and Andrea Volkamer and Matthew S. Sigman and Kuangbiao Liao and Charles Windle},
abstract = {In this digital age where machine learning has won the Nobel Prizes in both Physics and Chemistry, it is ever more important to give chemistry students an educational advantage that will enable them to use the tools of artificial intelligence and machine learning to enhance both their study experience and their future research. In this Voices article, chemistry education and research experts gather to share their implementation and utilization of these data-driven tools in classes and in labs.}
}
@incollection{HORVATH1995315,
title = { - Feature-based support of conceptual design of mechanical products},
editor = {Mohamed E. Elarabi and Abdalla S. Wifi and A.S. Wifi},
booktitle = {Current Advances in Mechanical Design and Production VI},
publisher = {Pergamon},
address = {Oxford},
pages = {315-322},
year = {1995},
isbn = {978-0-08-042140-7},
doi = {https://doi.org/10.1016/B978-008042140-7/50029-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780080421407500299},
author = {I. Horváth and Z. Bagoly and P. Kulcsár},
abstract = {Publisher Summary
The chapter presents a novel interpretation of conceptual design process that implies thinking in concepts rather than in functions. This chapter implements an interactive platform for early stage representation of designs. A framework, associative concept network (ACN), is elaborated to promote the development of a new computational model for concept spaces. It is also expected that higher level automation of conceptual design can be achieved based on ACNs. The reported research concentrates on the development and application of concept feature-objects (CFO). CFOs are functionally and morphologically parameterized three-dimensional skeletons that are arranged into an organ structure. Components of CFO descriptions are the physical ports, contact surfaces related to ports, bones between ports, DOF of ports, relevant physical parameters characterizing the energy transformation processes, and scientific and empirical descriptions of intentional transformations and environmental effects. Modeling entities for a given application are constructed by genetic modeling. The set of the new modeling entities can be used both for static analysis and dynamic simulation of mechanical products.}
}
@article{CHOI2023100069,
title = {Art and the artificial},
journal = {Journal of Creativity},
volume = {33},
number = {3},
pages = {100069},
year = {2023},
issn = {2713-3745},
doi = {https://doi.org/10.1016/j.yjoc.2023.100069},
url = {https://www.sciencedirect.com/science/article/pii/S2713374523000286},
author = {Suk Kyoung Choi and Steve DiPaola and Liane Gabora},
keywords = {Creativity systems, Anticipatory esthetics, AI art, Creative process, Arts-based research, Affective computing},
abstract = {This paper explores the philosophical implications of machine learning text-to-image synthesis in a practice-based phenomenology of the computational poetics of a visual art process. It is hypothesized that artificial intelligence (AI) facilitated reflective image development fosters an anticipatory esthetics in creative interactivity. The concept of AI-mediated “perspectival affordance” is introduced and its application to affective computing design emphasized. It is proposed that positioning intelligent systems as collaborative creativity tools promotes a dynamic interplay that envisions creativity as an anticipatory system, conceived of as those systems where exchange between artist and tool is mediated by future-oriented affective projection upon the system. The paper aims to establish a cognitive framework for AI-mediated creativity grounded in anticipatory interactivity, enhancing understanding of embodied cognition as mediated by AI in human-centered creativity support systems.}
}
@article{BROOKS2013947,
title = {The Primate Cerebellum Selectively Encodes Unexpected Self-Motion},
journal = {Current Biology},
volume = {23},
number = {11},
pages = {947-955},
year = {2013},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2013.04.029},
url = {https://www.sciencedirect.com/science/article/pii/S0960982213004375},
author = {Jessica X. Brooks and Kathleen E. Cullen},
abstract = {Summary
Background
The ability to distinguish sensory signals that register unexpected events (exafference) from those generated by voluntary actions (reafference) during self-motion is essential for accurate perception and behavior. The cerebellum is most commonly considered in relation to its contributions to the fine tuning of motor commands and sensorimotor calibration required for motor learning. During unexpected motion, however, the sensory prediction errors that drive motor learning potentially provide a neural basis for the computation underlying the distinction between reafference and exafference.
Results
Recording from monkeys during voluntary and applied self-motion, we demonstrate that individual cerebellar output neurons encode an explicit and selective representation of unexpected self-motion by means of an elegant computation that cancels the reafferent sensory effects of self-generated movements. During voluntary self-motion, the sensory responses of neurons that robustly encode unexpected movement are canceled. Neurons with vestibular and proprioceptive responses to applied head and body movements are unresponsive when the same motion is self-generated. When sensory reafference and exafference are experienced simultaneously, individual neurons provide a precise estimate of the detailed time course of exafference.
Conclusions
These results provide an explicit solution to the longstanding problem of understanding mechanisms by which the brain anticipates the sensory consequences of our voluntary actions. Specifically, by revealing a striking computation of a sensory prediction error signal that effectively distinguishes between the sensory consequences of self-generated and externally produced actions, our findings overturn the conventional thinking that the sensory errors coded by the cerebellum principally contribute to the fine tuning of motor activity required for motor learning.}
}
@article{SCHIFFERKANE2024104659,
title = {Converting OMOP CDM to phenopackets: A model alignment and patient data representation evaluation},
journal = {Journal of Biomedical Informatics},
volume = {155},
pages = {104659},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104659},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000777},
author = {Kayla Schiffer-Kane and Cong Liu and Tiffany J. Callahan and Casey Ta and Jordan G. Nestor and Chunhua Weng},
keywords = {Phenopackets schema, OMOP-CDM, Health data standards, Interoperability, Phenotyping, Data model},
abstract = {Objective
This study aims to promote interoperability in precision medicine and translational research by aligning the Observational Medical Outcomes Partnership (OMOP) and Phenopackets data models. Phenopackets is an expert knowledge-driven schema designed to facilitate the storage and exchange of multimodal patient data, and support downstream analysis. The first goal of this paper is to explore model alignment by characterizing the common data models using a newly developed data transformation process and evaluation method. Second, using OMOP normalized clinical data, we evaluate the mapping of real-world patient data to Phenopackets. We evaluate the suitability of Phenopackets as a patient data representation for real-world clinical cases.
Methods
We identified mappings between OMOP and Phenopackets and applied them to a real patient dataset to assess the transformation’s success. We analyzed gaps between the models and identified key considerations for transforming data between them. Further, to improve ambiguous alignment, we incorporated Unified Medical Language System (UMLS) semantic type-based filtering to direct individual concepts to their most appropriate domain and conducted a domain-expert evaluation of the mapping’s clinical utility.
Results
The OMOP to Phenopacket transformation pipeline was executed for 1,000 Alzheimer’s disease patients and successfully mapped all required entities. However, due to missing values in OMOP for required Phenopacket attributes, 10.2 % of records were lost. The use of UMLS-semantic type filtering for ambiguous alignment of individual concepts resulted in 96 % agreement with clinical thinking, increased from 68 % when mapping exclusively by domain correspondence.
Conclusion
This study presents a pipeline to transform data from OMOP to Phenopackets. We identified considerations for the transformation to ensure data quality, handling restrictions for successful Phenopacket validation and discrepant data formats. We identified unmappable Phenopacket attributes that focus on specialty use cases, such as genomics or oncology, which OMOP does not currently support. We introduce UMLS semantic type filtering to resolve ambiguous alignment to Phenopacket entities to be most appropriate for real-world interpretation. We provide a systematic approach to align OMOP and Phenopackets schemas. Our work facilitates future use of Phenopackets in clinical applications by addressing key barriers to interoperability when deriving a Phenopacket from real-world patient data.}
}
@article{BARR1986183,
title = {New Approaches in Water Balance Computations.},
journal = {Journal of Arid Environments},
volume = {11},
number = {2},
pages = {183-184},
year = {1986},
issn = {0140-1963},
doi = {https://doi.org/10.1016/S0140-1963(18)31233-3},
url = {https://www.sciencedirect.com/science/article/pii/S0140196318312333},
author = {D.I.H. Barr}
}
@article{ARIYA2025101387,
title = {Digital literacy through gaming: A comparative study of knowledge acquisition, social presence, and emotional reactions in digital and non-digital board games},
journal = {Social Sciences & Humanities Open},
volume = {11},
pages = {101387},
year = {2025},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2025.101387},
url = {https://www.sciencedirect.com/science/article/pii/S2590291125001147},
author = {Pakinee Ariya and Natchaya Wongwan and Kannikar Intawong and Kitti Puritat},
keywords = {Educational board games, Digital literacy, Digital board game, Emotional reactions, Social presence},
abstract = {The trend of educational board games is experiencing significant growth, with notable integration of technology; however, many educators continue to use both digital and physical board games. Digital board games offer convenience, accessibility, and enhanced interactivity through features like animations, sound effects, and automated rule enforcement. In contrast, physical board games emphasize tactile engagement and face-to-face social presence, fostering a stronger sense of connection among players. This study investigates the comparative impacts of digital and non-digital board games on knowledge acquisition, social presence, and emotional reactions among higher education students. Employing a quasi-experimental design, 82 students enrolled in an Information Literacy course were divided into two groups: one using digital board games and the other using non-digital board games. Pre- and post-tests measured knowledge acquisition, while the Social Presence in Gaming Questionnaire (SPGQ) and the Positive and Negative Affect Schedule (PANAS) assessed social and emotional impacts. Results indicate that both digital and non-digital games significantly enhance knowledge acquisition, with no substantial differences between the two formats. However, digital board games showed higher engagement levels, suggesting a greater potential for promoting active participation. Emotional responses were similar across both groups, with no significant differences in positive or negative affect. These findings underscore the educational value of both game formats and highlight the importance of selecting the appropriate type based on specific educational objectives.}
}
@article{NORROS201461,
title = {Developing human factors/ergonomics as a design discipline},
journal = {Applied Ergonomics},
volume = {45},
number = {1},
pages = {61-71},
year = {2014},
note = {Systems Ergonomics/Human Factors},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2013.04.024},
url = {https://www.sciencedirect.com/science/article/pii/S0003687013000975},
author = {Leena Norros},
keywords = {Design thinking, Technology-in-use, Naturalistic approach, Core-task modelling},
abstract = {This paper deals with internal challenges that the human factors/ergonomics (HFE) research faces when wishing to strengthen its contribution to development of work systems. Three established characteristics of high-quality HFE, i.e., HFE takes a systems approach, HFE is design-driven, and HFE focuses on two closely related outcomes, performance and well-being, are taken as a starting point of a methodological discussion, in which conceptual innovations, e.g. adopting the technology-in-use perspective, are proposed to support development of HFE towards the high-quality aims. The feasibility of the proposed conceptual choices is demonstrated by introducing a naturalistic HFE analysis approach including four HFE functions. The gained experience of the use of this approach in a number of complex work domains allows the conclusion that becoming design-driven appears as that most difficult quality target for HFE to reach. Creating an own design discipline identity in a multi-voiced collaboration is the key internal challenge for human factors/ergonomics.}
}
@article{SALIS202320,
title = {An Edge-Cloud based Reference Architecture to support cognitive solutions in Process Industry},
journal = {Procedia Computer Science},
volume = {217},
pages = {20-30},
year = {2023},
note = {4th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.198},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922022761},
author = {Antonio Salis and Angelo Marguglio and Gabriele {De Luca} and Silvia Razzetti and Walter Quadrini and Sergio Gusmeroli},
keywords = {Industry 4.0, process industry, smart manufacturing, reference architecture, cloud, edge computing, cognitive computing, artificial intelligence, big data analytics},
abstract = {Process Industry is one of the leading sectors of the world economy, characterized however by intense environmental impact, and very high-energy consumption. Despite a traditional low innovation pace in PI, in the recent years a strong push at worldwide level towards the dual objective of improving the efficiency of plants and the quality of products, significantly reducing the consumption of electricity and CO2 emissions has taken momentum. Digital Technologies (namely Smart Embedded Systems, IoT, Data, AI and Edge-to-Cloud Technologies) are enabling drivers for a Twin Digital-Green Transition, as well as foundations for human centric, safe, comfortable and inclusive workplaces. Currently, digital sensors in plants produce a large amount of data, which in most cases constitutes just a potential and not a real value for Process Industry, often locked-in in close proprietary systems and seldomly exploited. Digital technologies, with process modelling-simulation via digital twins, can build a bridge between the physical and the virtual worlds, bringing innovation with great efficiency and drastic reduction of waste. In accordance with the guidelines of Industrie 4.0 this work proposes a modular and scalable Reference Architecture, based on open source software, which can be implemented both in brownfield and greenfield scenarios. The ability to distribute processing between the edge, where the data have been created, and the cloud, where the greatest computational resources are available, facilitates the development of integrated digital solutions with cognitive capabilities. The reference architecture is being validated in the three pilot plants, paving the way to the development of integrated planning solutions, with scheduling and control of the plants, optimizing the efficiency and reliability of the supply chain, and balancing energy efficiency.}
}
@article{HOLMES201743,
title = {Motor cognition and neuroscience in sport psychology},
journal = {Current Opinion in Psychology},
volume = {16},
pages = {43-47},
year = {2017},
note = {Sport psychology},
issn = {2352-250X},
doi = {https://doi.org/10.1016/j.copsyc.2017.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S2352250X16301646},
author = {Paul S Holmes and David J Wright},
abstract = {Advances in technology have allowed research in cognitive neuroscience to contribute significantly to the discipline of sport psychology. In most cases, the research has become more rigorous and has directed current thinking on the mechanisms subserving a number of psychological theories and models of practice. Currently, the three most common neuroscience techniques informing sport and exercise research are electroencephalography, transcranial magnetic stimulation and functional magnetic resonance imaging. In this review, we highlight and discuss the contributions to sport psychology that have been made in recent years by applying these techniques, with a focus on the development of expertise, motor cognition, motor imagery and action observation.}
}
@article{YE2024116982,
title = {A fast cosine transformation accelerated method for predicting effective thermal conductivity},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {426},
pages = {116982},
year = {2024},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2024.116982},
url = {https://www.sciencedirect.com/science/article/pii/S004578252400238X},
author = {Changqing Ye and Shubin Fu and Eric T. Chung},
keywords = {Effective thermal conductivity, Preconditioner, Fast cosine transformation, CUDA, GPU},
abstract = {Predicting effective thermal conductivity by solving a Partial Differential Equation (PDE) defined on a high-resolution Representative Volume Element (RVE) is a computationally intensive task. In this paper, we tackle the task by proposing an efficient and implementation-friendly computational method that can fully leverage the computing power offered by hardware accelerators, namely, graphical processing units (GPUs). We first employ the Two-Point Flux-Approximation scheme to discretize the PDE and then utilize the preconditioned conjugate gradient method to solve the resulting algebraic linear system. The construction of the preconditioner originates from FFT-based homogenization methods, and an engineered linear programming technique is utilized to determine the homogeneous reference parameters. The fundamental observation presented in this paper is that the preconditioner system can be effectively solved using multiple Fast Cosine Transformations (FCT) and parallel tridiagonal matrix solvers. Regarding the fact that default multiple FCTs are unavailable on the CUDA platform, we detail how to derive FCTs from FFTs with nearly optimal memory usage. Numerical experiments including the stability comparison with standard preconditioners are conducted for 3D RVEs. Our performance reports indicate that the proposed method can achieve a 5-fold acceleration on the GPU platform over the pure CPU platform and solve the problems with 5123 degrees of freedom and reasonable contrast ratios in less than 30 s.}
}
@article{GALLISTEL201287,
title = {On rationalism and optimality: Responses to the Miller and Nevin Commentaries},
journal = {Behavioural Processes},
volume = {90},
number = {1},
pages = {87-88},
year = {2012},
note = {Society for the Quantitative Analyses of Behavior: Extinction},
issn = {0376-6357},
doi = {https://doi.org/10.1016/j.beproc.2012.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S0376635712000514},
author = {C.R. Gallistel},
keywords = {Rationalism, Optimality, Information theory, Contingency, Cue competition, Assignment of credit},
abstract = {Modern materialist rationalism is the doctrine that principles governing behaviorally important aspects of the world have become implicit in the structure of purpose-specific information-processing mechanisms through evolution by natural selection. These principles are mostly, but not entirely mathematical. Because the evolutionary process tends to optimize, the computations performed by these mechanisms tend to approximate the optimal computation. This doctrine does not imply that animals always make rational and/or optimal choices.}
}
@article{HOZ2024e40032,
title = {Educational robotics for science and mathematics teaching: Analysis of pre-service teachers' perceptions and self-confidence},
journal = {Heliyon},
volume = {10},
number = {21},
pages = {e40032},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e40032},
url = {https://www.sciencedirect.com/science/article/pii/S240584402416063X},
author = {Alejandro De la Hoz and Lina Melo and Florentina Cañada and Javier Cubero},
keywords = {Educational robotics, Pre-service teachers, Self-confidence, Perception, Mathematics education, Science education},
abstract = {Educational Robotics has had an important impact in recent years as it offers a number of advantages for students. The inclusion of robotics in any educational stage requires teachers with adequate predisposition and training, making it necessary to know the opinions of pre-service teachers. The aim of our study is to analyze the perceptions and self-confidence of 109 pre-service primary education teachers before and after an intervention based on educational robotics and challenge-based learning to teach scientific and mathematical content, in their third academic year. A quasi-experimental design was used involving pretest and posttest, using the nonparametric Mann-Whitney and Wilcoxon U tests. The results showed a significant improvement in the overall mean self-confidence. In addition, the intervention led to a more positive perception of the benefits and possibilities of robotics for teaching of scientific and mathematical content, although it also increased the difficulties of implementation due to the lack of training in this digital resource. It is concluded that interventions are required based on educational robotics that allow pre-service teachers to gain the necessary self-confidence and perception to facilitate its introduction for the teaching of scientific and mathematical content.}
}
@article{BORG202041,
title = {On “the application of science to science itself:” chemistry, instruments, and the scientific labor process},
journal = {Studies in History and Philosophy of Science Part A},
volume = {79},
pages = {41-56},
year = {2020},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2019.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0039368118300529},
author = {George Borg},
keywords = {Instrumental revolution, Labor, Scientific revolution, Structure determination, Technology, Progress, Chemistry, Mechanization},
abstract = {The “Instrumental Revolution” in chemistry refers to a transitional period in the mid-20th century during which sophisticated instrumentation based on physical principles was introduced to solve chemical problems. Historical and philosophical reflection on whether the revolution was a scientific one has been dominated by general models of scientific revolution, in particular, those proposed by Thomas Kuhn, I. B. Cohen and Ian Hacking. In this article I propose that the Industrial Revolution is a useful model for understanding the transformation wrought by the increasingly important role of machines in chemical research. Drawing on Marx's analysis of that event, I argue that that the Instrumental Revolution bears a striking resemblance to the industrial one. I offer grounds for thinking that the resemblance is not fortuitous, but rather reflects a general pattern of development involving the mechanization of the labor process. It is suggested that the cognitive consequences of radical changes in the means of production, as exemplified in the Instrumental Revolution, warrant the consideration of whether the latter is an instance of a kind of revolution in science rather than a singular episode.}
}
@article{CALDERON201860,
title = {Sunrise Hotels: An integrated managerial accounting teaching case},
journal = {Journal of Accounting Education},
volume = {44},
pages = {60-72},
year = {2018},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2018.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0748575117302439},
author = {Thomas Calderon and James W. Hesford and Nicolas Mangin and Mina Pizzini},
keywords = {Managerial accounting, Integrated learning, Case study, Teaching case},
abstract = {“Sunrise Hotels” consists of six, linked cases developed from a field study of a large hotel chain in North America. The cases are short, so they can be distributed and solved in less than a full class period, after a short lecture by the instructor. Students often see managerial topics as an unrelated collection of tools rather than as a coherent, integrated framework for decision-making and management control. Questions included with each short case guide students, and the integration developed across six cases in a single setting should help students view managerial accounting topics as inter-related tools for decision making and control.}
}
@article{CHANDRASEGARAN2013204,
title = {The evolution, challenges, and future of knowledge representation in product design systems},
journal = {Computer-Aided Design},
volume = {45},
number = {2},
pages = {204-228},
year = {2013},
note = {Solid and Physical Modeling 2012},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2012.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0010448512001741},
author = {Senthil K. Chandrasegaran and Karthik Ramani and Ram D. Sriram and Imré Horváth and Alain Bernard and Ramy F. Harik and Wei Gao},
keywords = {Knowledge representation, Knowledge capture, Knowledge management, Product design, Computational tools, Ontology, Systems engineering, Design rationale, Multidisciplinary modeling, Virtual reality, Collaborative engineering, Simulation},
abstract = {Product design is a highly involved, often ill-defined, complex and iterative process, and the needs and specifications of the required artifact get more refined only as the design process moves toward its goal. An effective computer support tool that helps the designer make better-informed decisions requires efficient knowledge representation schemes. In today’s world, there is a virtual explosion in the amount of raw data available to the designer, and knowledge representation is critical in order to sift through this data and make sense of it. In addition, the need to stay competitive has shrunk product development time through the use of simultaneous and collaborative design processes, which depend on effective transfer of knowledge between teams. Finally, the awareness that decisions made early in the design process have a higher impact in terms of energy, cost, and sustainability, has resulted in the need to project knowledge typically required in the later stages of design to the earlier stages. Research in design rationale systems, product families, systems engineering, and ontology engineering has sought to capture knowledge from earlier product design decisions, from the breakdown of product functions and associated physical features, and from customer requirements and feedback reports. VR (Virtual reality) systems and multidisciplinary modeling have enabled the simulation of scenarios in the manufacture, assembly, and use of the product. This has helped capture vital knowledge from these stages of the product life and use it in design validation and testing. While there have been considerable and significant developments in knowledge capture and representation in product design, it is useful to sometimes review our position in the area, study the evolution of research in product design, and from past and current trends, try and foresee future developments. The goal of this paper is thus to review both our understanding of the field and the support tools that exist for the purpose, and identify the trends and possible directions research can evolve in the future.}
}
@article{TAGHIKHANI2024114574,
title = {A hybrid modified PSO algorithm for the inverse p-median location problem in fuzzy random environment},
journal = {Theoretical Computer Science},
volume = {1000},
pages = {114574},
year = {2024},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2024.114574},
url = {https://www.sciencedirect.com/science/article/pii/S0304397524001890},
author = {Sepideh Taghikhani and Fahimeh Baroughi and Behrooz Alizadeh},
keywords = {Inverse p-median location problem, Fuzzy random variable, Conditional value at risk, Particle swarm optimization},
abstract = {This paper considers the inverse p-median location problem with variable edge lengths and variable vertex weights on general graphs in which the modification costs are the fuzzy random variables. We present a model for the problem in fuzzy random environment in which the objective value is computed by conditional value at risk criterion. Then, we show that the problem is NP-hard under this criterion. Therefore, a new hybrid modified particle swarm optimization algorithm is proposed to obtain the approximate optimal solution of the proposed model. Finally, computational experiments are given to illustrate high efficiency of the proposed algorithm.}
}
@incollection{KUMBALE2021306,
title = {Models for Personalized Medicine},
editor = {Olaf Wolkenhauer},
booktitle = {Systems Medicine},
publisher = {Academic Press},
address = {Oxford},
pages = {306-317},
year = {2021},
isbn = {978-0-12-816078-7},
doi = {https://doi.org/10.1016/B978-0-12-801238-3.11349-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128012383113492},
author = {Carla M. Kumbale and Jacob D. Davis and Eberhard O. Voit},
keywords = {Dynamic model, Health simplex, Health trajectory, Machine learning, Modeling, Networks, Personalized medicine, Precision medicine, Systems biology, Theranostics},
abstract = {The customization of medicine to specific individuals promises clear improvements in disease treatment, but also faces substantial challenges, many of which have their roots in the complexity of the human body. This complexity cannot be grasped with intuition alone and is not appropriately captured by reductionist methods, which have been dominating biology and medicine for the past decades. Experimental and computational systems biology have the potential of generating adequate datasets and analyzing them in a manner that captures the complexity of health and disease systems in a personalized manner. This potential has not yet fully materialized, but examples and case studies provide a glimpse of the power these approaches are likely to have in the future.}
}
@article{BARBIAN2024122160,
title = {Flow and mass transfer prediction in anisotropic TPMS-structures as extracorporeal oxygenator membranes using reduced order modeling},
journal = {Journal of Membrane Science},
volume = {690},
pages = {122160},
year = {2024},
issn = {0376-7388},
doi = {https://doi.org/10.1016/j.memsci.2023.122160},
url = {https://www.sciencedirect.com/science/article/pii/S0376738823008165},
author = {Kai P. Barbian and Lukas T. Hirschwald and John Linkhorst and Michael Neidlin and Ulrich Steinseifer and Matthias Wessling and Bettina Wiegmann and Sebastian V. Jansen},
keywords = {Membrane oxygenator, TPMS, Reduced order modeling, Gas transfer simulation, Anisotropic},
abstract = {Currently, hollow fiber membranes are the standard technology for extracorporeal membrane oxygenators. Apart from the inevitable contact of the circulating blood with the artificial material, a suboptimal flow distribution within the oxygenator favors thrombus formation which leads to a rapid loss of gas exchange capacity. The current advancement in additive manufacturing allows the design of three-dimensional membrane-structures, based on triply periodic minimal surfaces (TPMS). One of their unique advantages is local geometry variation to manipulate the flow distribution. But how this anisotropy influences the overall device performance is non-trivial and requires numerical simulation. The aim of this study was to develop a reduced order model (ROM) that is able to efficiently predict three-dimensional flow distribution and gas transfer inside TPMS-structures. We performed a parametric study using a validated micro scale computational fluid dynamics (CFD) model. Afterwards, two different modeling approaches of Sherwood-correlations and artificial neural networks (NN) were compared to characterize flow and mass transfer from the simulated data. To create the ROM, the NN modeling strategy was then implemented into a porous medium CFD model. The developed ROM was also validated. Finally, an anisotropic TPMS-membrane-structure was compared numerically with an isotropic predicate. The NN fitting strategy showed superior accuracy over the Sherwood-correlations for characterizing mass transfer in TPMS-structures. With the ROM, the gas transfer rates of oxygen and carbon dioxide from the micro CFD model could be predicted within relative root mean squared errors (RRMSE) of 2.7% and 5.1%. The pressure drop in the experiments was predicted with an RRMSE accuracy of 7.6%. In comparison with the isotropic TPMS-structure, the anisotropic structure showed a homogenized flow distribution and increased gas transfer rates of 4% to 5% for oxygen and 2.3% to 7.4% for carbon dioxide at the simulated flow rates.}
}
@article{JOHANNESJOSEFIJEN202173,
title = {An adaptive temporal-causal network model to analyse extinction of communication over time},
journal = {Cognitive Systems Research},
volume = {68},
pages = {73-83},
year = {2021},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2020.08.013},
url = {https://www.sciencedirect.com/science/article/pii/S1389041721000231},
author = {Lucas {Johannes José Fijen} and Julio {Joaquín López González} and Jan Treur},
keywords = {Extinction of communication, Network modeling, Adaptive network, Social simulation},
abstract = {The persistence of information communicated between humans is difficult to measure as it is affected by many features. This paper presents an approach to computationally model the cognitive processes of information sharing to describe persistence or extinction of communication in Twitter over time. The adaptive mental network model explains, for example, how an individual can experience information overflow on a topic, and how this affects the sharing of information. Parameter tuning by Simulated Annealing is used to identify characteristics of the network model that fit to empirical data from Twitter. The data collected is related to the independentism in Catalunya, Spain, which is considered a global issue with repercussion in Europe.}
}
@article{NNAJI2019106672,
title = {Modelling and management of smart microgrid for rural electrification in sub-saharan Africa: The case of Nigeria},
journal = {The Electricity Journal},
volume = {32},
number = {10},
pages = {106672},
year = {2019},
issn = {1040-6190},
doi = {https://doi.org/10.1016/j.tej.2019.106672},
url = {https://www.sciencedirect.com/science/article/pii/S1040619019302775},
author = {Eunice C. Nnaji and Donald Adgidzi and Michael O. Dioha and Daniel R.E. Ewim and Zhongjie Huan},
keywords = {Energy access, Off-grid rural electrification, HOMER, Simulink, Nigeria},
abstract = {Access to electricity is still a challenge in many parts of sub-Saharan Africa. In Nigeria, over 70% of the rural dwellers do not have access to electricity. The purpose of this paper is to examine the potential of a smart microgrid for off-grid rural electrification in Nigeria. A combination of design thinking and model-based design methodology is employed to select a suitable microgrid configuration and to develop a smart microgrid model. A system consisting of a solar photovoltaic array, battery energy storage and a diesel generator is selected, and the model is developed in Simulink. Demand data from 10 rural communities in Nigeria are used to validate the performance of the model and the potential for demand management is considered. The use of energy efficient light bulbs is found to reduce the peak electricity demand of the case study communities by 42 to 76%. Combining the proposed system with the use of LED bulbs makes the system to have 56 to 81% less net present cost than a system with a diesel generator alone and incandescent light bulbs. The proposed smart microgrid is found to be more suitable for off-grid rural electrification in Nigeria than diesel generators which are currently used for off-grid electrification in Nigeria.}
}
@article{WU2025113281,
title = {Graph knowledge tracing in cognitive situation: Validation of classic assertions in cognitive psychology},
journal = {Knowledge-Based Systems},
volume = {315},
pages = {113281},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113281},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125003284},
author = {Qianxi Wu and Weidong Ji and Guohui Zhou and Yingchun Yang},
keywords = {Knowledge Tracing, Cognitive situation, Hyper-Graph Neural Network, Directed Graph Convolutional Neural Network, Cognitive psychology},
abstract = {Knowledge Tracing (KT) is a fundamental and challenging task in intelligent education, aiming to trace learners’ knowledge states and learning processes, providing better support and guidance for teaching and addressing mental factors. Previous KT tasks have focused on considering learners’ exposure to extrinsic environmental factors while ignoring the influence of intrinsic psychological factors. Moreover, previous methods have adopted a single perspective in modeling learners’ knowledge states, ignoring the diversity of states in the learning process. To address these issues, we define the concept of cognitive situation through the guidance of cognitive psychology theory to help to explain the extrinsic influence and intrinsic cognition of learners within complex learning environments. Moreover, we design a Cognitive Situation-based Graph KT (CSGKT) model to quantify learners’ influences in the cognitive process by modeling schemas capturing intrinsic characteristics and extrinsic factors through Hyper-Graph Neural Networks (HGNN). Second, we utilize a Directed Graph Convolutional Neural Network (DGCNN) to capture the correlation information between knowledge concepts and structure the learner’s cognitive activities and knowledge states, adding a detailed representation of multiple states of the learning process. In addition, we use the Erase-add Gate to filter out the knowledge states that do not match the learner’s current cognitive activities to stabilize the learner’s due cognition. In our experiments, we selected nine baseline models from three mainstream approaches, including sequence-based approaches, Transformer-based approaches, and complex structure-based approaches. The experimental results show that our models outperform these baseline models. At the same time, we also verify two classic assertions in cognitive psychology, namely, the “short-term memory forgetting of knowledge concepts is mainly caused by interference rather than memory trace fading” and the “cognitive imagery and perceptual function play an equivalent role in the cognitive process”, which further support the feasibility of the model.}
}
@article{MAROWKA2020199,
title = {On the performance difference between theory and practice for parallel algorithms},
journal = {Journal of Parallel and Distributed Computing},
volume = {138},
pages = {199-210},
year = {2020},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2019.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S0743731519304964},
author = {Ami Marowka},
keywords = {Python, Teaching parallel programming, Quicksort, Performance modeling},
abstract = {The performance of parallel algorithms is often inconsistent with their preliminary theoretical analyses. Indeed, the difference is increasing between the ability to theoretically predict the performance of a parallel algorithm and the results measured in practice. This is mainly due to the accelerated development of advanced parallel architectures, whereas there is still no agreed model for parallel computation, which has implications for the design of parallel algorithms and for the manner in which parallel programming should be taught. In this study, we examined the practical performance of Cormen’s Quicksort parallel algorithm. We determined the performance of the algorithm with different parallel programming approaches and examine the capacity of theoretical performance analyses of the algorithm for predicting the actual performance. This algorithm is used for teaching theoretical and practical aspects of parallel programming to undergraduate students. We considered the pedagogic implications that may arise when the algorithm is used as a learning resource for teaching parallel programming.}
}
@article{HORVATH2024593,
title = {AI for conceptual architecture: Reflections on designing with text-to-text, text-to-image, and image-to-image generators},
journal = {Frontiers of Architectural Research},
volume = {13},
number = {3},
pages = {593-612},
year = {2024},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2024.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S2095263524000256},
author = {Anca-Simona Horvath and Panagiota Pouliou},
keywords = {Machine learning, StyleGAN2-ADA, RNN TensorFlow, VQGAN + clip, AD journal, eVolo, Conceptual design, Architectural design},
abstract = {In this paper we present a research-through-design study where we employed text-to-text, text-to-image, and image-to-image generative tools for a conceptual architecture project for the eVolo skyscraper competition. We trained these algorithms on a dataset that we collected and curated, consisting of texts about and images of architecture. We describe our design process, present the final proposal, reflect on the usefulness of such tools for early-stage design, and discuss implications for future research and practice. By analysing the results from training the text-to-text generators we could establish a specific design brief that informed the final concept. The results from the image-to-image generator gave an overview of the shape grammars of previous submissions. All results were intriguing and can assist creativity and in this way, the tools were useful for gaining insight into historical architectural data, helped shape a specific design brief, and provoked new ideas. By reflecting on our design process, we argue that the use of language when employing such tools takes a new role and that three layers of language intertwined in our work: architectural discourse, programming languages, and annotations. We present a map that unfolds how these layers came together as a contribution to making machine learning more explainable for creatives.}
}
@incollection{FOWLER20131,
title = {Chapter 1 - Introduction},
editor = {Bruce A. Fowler},
booktitle = {Computational Toxicology},
publisher = {Academic Press},
address = {San Diego},
pages = {1-4},
year = {2013},
isbn = {978-0-12-396461-8},
doi = {https://doi.org/10.1016/B978-0-12-396461-8.00001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780123964618000014},
author = {Bruce A. Fowler},
keywords = {Computational toxicology, risk assessment, chemical mixtures, emergency responses, animal–human extrapolations, data extrapolations from  test systems to human risk assessments, systems biology approaches, data mining, hypothesis generation},
abstract = {This book is intended to be an introduction to various applications of computational toxicology and to show how these approaches are currently being used effectively for risk assessment purposes in the near term. It is important to note that the field of computational toxicology is rapidly evolving and that subsequent editions of this book will take up new methods that are currently under development, such as high-throughput screening, and others that are still in a conceptual stage. There are many advantages for including computational toxicology approaches in the risk assessment process. Among these are reducing costs, minimizing use of animals in toxicology testing, improving speed in providing answers regarding chemicals in emergency situations such as the Gulf Oil spill, and dealing with the common problem of decision making for chemical mixtures. In addition, computational methods may be used for extrapolating or translating data from both in vitro and in vivo experimental animal test systems for human risk assessments of chemicals and drugs. In addition, computational methods may be used for focusing laboratory studies into productive areas by data mining the published literature and developing testable hypotheses by application of systems biology approaches to identify chemical interactions with functional molecular pathways to generate a more comprehensive picture of likely primary and secondary modes of chemical or drug activity. In summary, there is much that computational toxicology is now contributing to helping make better societal risk assessment decisions about chemicals and drugs. The future for these approaches is optimistic and limited only by human ingenuity and availability of resources.}
}
@article{ROSENBAUM2018510,
title = {Stress-related dysfunction of the right inferior frontal cortex in high ruminators: An fNIRS study},
journal = {NeuroImage: Clinical},
volume = {18},
pages = {510-517},
year = {2018},
issn = {2213-1582},
doi = {https://doi.org/10.1016/j.nicl.2018.02.022},
url = {https://www.sciencedirect.com/science/article/pii/S2213158218300561},
author = {David Rosenbaum and Mara Thomas and Paula Hilsendegen and Florian G. Metzger and Florian B. Haeussinger and Hans-Christoph Nuerk and Andreas J. Fallgatter and Vanessa Nieratschker and Ann-Christine Ehlis},
keywords = {Trier Social Stress Test (TSST), Functional near-infrared spectroscopy (fNIRS), Inferior frontal gyrus (IFG), Functional connectivity, Rumination, Cognitive control network (CCN)},
abstract = {Repetitive thinking styles such as rumination are considered to be a key factor in the development and maintenance of mental disorders. Different situational triggers (e.g., social stressors) have been shown to elicit rumination in subjects exhibiting such habitual thinking styles. At the same time, the process of rumination influences the adaption to stressful situations. The study at hand aims to investigate the effect of trait rumination on neuronal activation patterns during the Trier Social Stress Test (TSST) as well as the physiological and affective adaptation to this high-stress situation.
Methods
A sample of 23 high and 22 low ruminators underwent the TSST and two control conditions while their cortical hemodynamic reactions were measured with functional near-infrared spectroscopy (fNIRS). Additional behavioral, physiological and endocrinological measures of the stress response were assessed.
Results
Subjects showed a linear increase from non-stressful control conditions to the TSST in cortical activity of the cognitive control network (CCN) and dorsal attention network (DAN), comprising the bilateral dorsolateral prefrontal cortex (dlPFC), inferior frontal gyrus (IFG) and superior parietal cortex/somatosensory association cortex (SAC). During stress, high ruminators showed attenuated cortical activity in the right IFG, whereby deficits in IFG activation mediated group differences in post-stress state rumination and negative affect.
Conclusions
Aberrant activation of the CCN and DAN during social stress likely reflects deficits in inhibition and attention with corresponding negative emotional and cognitive consequences. The results shed light on possible neuronal underpinnings by which high trait rumination may act as a risk factor for the development of clinical syndromes.}
}
@article{ZHOU2025121348,
title = {Turning waste into energy through a solar-powered multi-generation system with novel machine learning-based life cycle optimization},
journal = {Chemical Engineering Science},
volume = {307},
pages = {121348},
year = {2025},
issn = {0009-2509},
doi = {https://doi.org/10.1016/j.ces.2025.121348},
url = {https://www.sciencedirect.com/science/article/pii/S000925092500171X},
author = {Jianzhao Zhou and Jingzheng Ren and Liandong Zhu and Chang He},
keywords = {Medical waste, Waste-to-energy, Multi-generation system, Machine learning, Comprehensive optimization},
abstract = {This study presents an innovative solar-powered multi-generation system aiming at converting waste into diverse forms of energy, including dimethyl ether (DME), hydrogen, power, and heat. Concurrently, a systematic and computationally efficient optimization framework is developed to unlock the maximum potential of this complex waste-to-energy system. The system integrates plasma gasification, DME synthesis, combining heat and power generation, solar-driven electrolysis and desalination. Life cycle assessment and techno-economic assessment have been implemented for system comprehensive optimization which is formulated as a large-scale nonlinear program (NLP) model. Based on rigorous process simulation results, a machine learning-based framework is proposed to accelerate optimization. Using medical waste treatment as a case study, the solution of the NLP problem reveals optimal levelized costs per kWh energy range from $0.1064 to $0.1304, with total life cycle carbon emissions ranging from 0.2748 to 0.5083 kg CO2-eq/kWh energy. The findings demonstrate the proposed system’s environmental sustainability and economic viability.}
}
@article{RANJBARI2023124,
title = {Waste management beyond the COVID-19 pandemic: Bibliometric and text mining analyses},
journal = {Gondwana Research},
volume = {114},
pages = {124-137},
year = {2023},
note = {Special Issue on Environmental impacts of COVID-19 pandemic},
issn = {1342-937X},
doi = {https://doi.org/10.1016/j.gr.2021.12.015},
url = {https://www.sciencedirect.com/science/article/pii/S1342937X22000272},
author = {Meisam Ranjbari and Zahra {Shams Esfandabadi} and Sneha Gautam and Alberto Ferraris and Simone Domenico Scagnelli},
keywords = {COVID-19, Plastic waste, Healthcare waste, Municipal solid waste, Wastewater, Personal protective equipment},
abstract = {The outbreak of the COVID-19 pandemic has significantly increased the demand for personal protective equipment, in particular face masks, thus leading to a huge amount of healthcare waste generated worldwide. Consequently, such an unprecedented amount of newly emerged waste has posed significant challenges to practitioners, policy-makers, and municipal authorities involved in waste management (WM) systems. This research aims at mapping the COVID-19-related scientific production to date in the field of WM. In this vein, the performance indicators of the target literature were analyzed and discussed through conducting a bibliometric analysis. The conceptual structure of COVID-19-related WM research, including seven main research themes, were uncovered and visualized through a text mining analysis as follows: (1) household and food waste, (2) personnel safety and training for waste handling, (3) sustainability and circular economy, (4) personal protective equipment and plastic waste, (5) healthcare waste management practices, (6) wastewater management, and (7) COVID-19 transmission through infectious waste. Finally, a research agenda for WM practices and activities in the post-COVID-19 era was proposed, focusing on the following three identified research gaps: (i) developing a systemic framework to properly manage the pandemic crisis implications for WM practices as a whole, following a systems thinking approach, (ii) building a circular economy model encompassing all activities from the design stage to the implementation stage, and (iii) proposing incentives to effectively involve informal sectors and local capacity in decentralizing municipal waste management, with a specific focus on developing and less-developed countries.}
}
@article{WOLFENGAGEN2020276,
title = {Capturing information processes with variable domains},
journal = {Procedia Computer Science},
volume = {169},
pages = {276-283},
year = {2020},
note = {Postproceedings of the 10th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2019 (Tenth Annual Meeting of the BICA Society), held August 15-19, 2019 in Seattle, Washington, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.177},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920303008},
author = {Viacheslav Wolfengagen and Larisa Ismailova and Sergey Kosikov},
keywords = {semantic information processing, computational model, variable domains},
abstract = {An approach to the construction of a computational model in which information processes are presented in the framework of theories without types, and they, in turn, are considered as special parts of typed theories, is proposed. Similar mixing was used in model studies for lambda-calculus. In contrast to them, in the present work, information processes correspond to parameterized metadata objects, which are variable domain constructs. Transformations of variable domains correspond to the spread of the process. Directional transformation provides the generation of metadata targets in the form of parameterized concepts. This simulates the evolving of the process, which allows the interpretation of the hidden time factor. The emerging model is purely process-based and provides a conceptual framework. The possibility of coding this framework with a system of interdependent lambda-terms is shown.}
}
@article{SPEER201099,
title = {Collegiate mathematics teaching: An unexamined practice},
journal = {The Journal of Mathematical Behavior},
volume = {29},
number = {2},
pages = {99-114},
year = {2010},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2010.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0732312310000052},
author = {Natasha M. Speer and John P. Smith and Aladar Horvath},
keywords = {Collegiate mathematics, Teaching practice},
abstract = {Though written accounts of collegiate mathematics teaching exist (e.g., mathematicians’ reflections and analyses of learning and teaching in innovative courses), research on collegiate teachers’ actual classroom teaching practice is virtually non-existent. We advance this claim based on a thorough review of peer-reviewed journals where scholarship on collegiate mathematics teaching is published. To frame this review, we distinguish between instructional activities and teaching practice and present six categories of published scholarship that consider collegiate teaching but are not descriptive empirical research on teaching practice. Empirical studies can reveal important differences among teachers’ thinking and actions, promote discussions of practice, and support learning about teaching. To support such research, we developed a preliminary framework of cognitively oriented dimensions of teaching practice based on our review of empirical research on pre-college and college teaching.}
}
@article{NGUYEN2015257,
title = {Identifiability Challenges in Mathematical Models of Viral Infectious Diseases**This work was supported by iMed - the Helmholtz Initiative on Pesonalized Medicine.},
journal = {IFAC-PapersOnLine},
volume = {48},
number = {28},
pages = {257-262},
year = {2015},
note = {17th IFAC Symposium on System Identification SYSID 2015},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2015.12.135},
url = {https://www.sciencedirect.com/science/article/pii/S2405896315027597},
author = {Van Kinh Nguyen and Esteban A. Hernandez-Vargas},
keywords = {parameter estimation, identifiability, viral infections},
abstract = {Nowadays, infections by viral pathogens are one of the biggest health threats to mankind. The development of new avenues of thinking to integrate the complexity of infectious diseases and the immune system is urgently needed. Recently mathematical modelling has emerged as a tool to interpret experimental results on quantitative grounds providing relevant insights to understand several infectious diseases. Nevertheless, modelling the complex mechanisms between viruses and the immune system can result in models with a large number of parameters to be estimated. Furthermore, experimental measurements have the problem to be sparse (in time) and highly noisy. Therefore, structural and practical identifiability are key obstacles to overcome towards mathematical models with predictive value. This paper addresses the identifiability limitations in the most common mathematical model to represent viral infections. Additionally, numerical simulations reveal how initial conditions of differential equations and fixing parameter values can alter the profile likelihood.}
}
@incollection{AGARWAL2024625,
title = {4.10 - Digital twin},
editor = {Kenneth S. Ramos},
booktitle = {Comprehensive Precision Medicine (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {625-638},
year = {2024},
isbn = {978-0-12-824256-8},
doi = {https://doi.org/10.1016/B978-0-12-824010-6.00051-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128240106000514},
author = {Sarvesh Agarwal and Vijay Pratap Singh and Paulamy Ganguly and Pujita Munnangi and Claire Collins and Sadmaan Sarker and Jason Shenoi and Scott Heston and Shruti Pandita and Tej K. Pandita and Michael Moreno and Douglas A. Baxter and Roderick I. Pettigrew and Shameer Khader and Kamlesh K. Yadav},
keywords = {Artificial intelligence, Biomarker, Clinical trial, Digital twin, Healthcare, Industrial digital twin, Manufacturing, NASA, Oncology, Personalized medicine, Supply chain},
abstract = {Since the very beginning of space exploration, NASA has been building actual size replicas of spaceships and rovers to help them troubleshoot issues when the vehicles are out in outer space. Furthermore, real-time data captured and processed during spaceship launches has helped with timely maneuvering decisions. Similar concepts are currently utilized in various industrial processes such as manufacturing, oil and gas industry, and supply chain to name a few. A good example of the use of Digital twin technology is the GPS-based navigation system where maps are overlaid with location coordinates and real-time traffic data to make decisions on the best available routes. In this chapter we describe the use of digital twin technology in various industries such as manufacturing (including drug and vaccine development), pharmaceutical, healthcare and the practice of medicine. We further describe the rapid development of computation technologies and better structuring of electronic health records which in turn has ushered the emergence of digital health in medicine. Lastly, we provide examples of the use of digital twin technology in the areas of personalized medicine (immunology, dementia, and oncology), biomarker development (Multiple sclerosis, Chron's and Cardiovascular diseases), and clinical trials (Alzheimer's and breast cancer).}
}
@incollection{BARTO199135,
title = {On the Computational Economics of Reinforcement Learning},
editor = {David S. Touretzky and Jeffrey L. Elman and Terrence J. Sejnowski and Geoffrey E. Hinton},
booktitle = {Connectionist Models},
publisher = {Morgan Kaufmann},
pages = {35-44},
year = {1991},
isbn = {978-1-4832-1448-1},
doi = {https://doi.org/10.1016/B978-1-4832-1448-1.50010-X},
url = {https://www.sciencedirect.com/science/article/pii/B978148321448150010X},
author = {Andrew G. Barto and Satinder Pal Singh},
abstract = {Following terminology used in adaptive control, we distinguish between indirect learning methods, which learn explicit models of the dynamic structure of the system to be controlled, and direct learning methods, which do not. We compare an existing indirect method, which uses a conventional dynamic programming algorithm, with a closely related direct reinforcement learning method by applying both methods to an infinite horizon Markov decision problem with unknown state-transition probabilities. The simulations show that although the direct method requires much less space and dramatically less computation per control action, its learning ability in this task is superior to, or compares favorably with, that of the more complex indirect method. Although these results do not address how the methods’ performances compare as problems become more difficult, they suggest that given a fixed amount of computational power available per control action, it may be better to use a direct reinforcement learning method augmented with indirect techniques than to devote all available resources to a computationally costly indirect method. Comprehensive answers to the questions raised by this study depend on many factors making up the economic context of the computation.}
}