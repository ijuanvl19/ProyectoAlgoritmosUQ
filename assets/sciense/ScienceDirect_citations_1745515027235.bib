@article{BURRIS1992175,
title = {Discriminator varieties and symbolic computation},
journal = {Journal of Symbolic Computation},
volume = {13},
number = {2},
pages = {175-207},
year = {1992},
issn = {0747-7171},
doi = {https://doi.org/10.1016/S0747-7171(08)80089-2},
url = {https://www.sciencedirect.com/science/article/pii/S0747717108800892},
author = {Stanley Burris},
abstract = {We look at two aspects of discriminator varieties which could be of considerable interest in symbolic computation:1.discriminator varieties are unitary (i.e., there is always a most general unifier of two unifiable terms), and2.every mathematical problem can be routinely cast in the form†p1 ≈ q1, …, pk ≈ qk implies the equation x ≈ y. Item (l) offers possibilities for implementations in computational logic, and (2) shows that Birkhoff's five rules of inference for equational logic are all one needs to prove theorems in mathematics.}
}
@article{BIRHANE2021100205,
title = {Algorithmic injustice: a relational ethics approach},
journal = {Patterns},
volume = {2},
number = {2},
pages = {100205},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100205},
url = {https://www.sciencedirect.com/science/article/pii/S2666389921000155},
author = {Abeba Birhane},
keywords = {justice, ethics, Afro-feminism, relational epistemology, data science, complex systems, enaction, embodiment, artificial intelligence, machine learning},
abstract = {Summary
It has become trivial to point out that algorithmic systems increasingly pervade the social sphere. Improved efficiency—the hallmark of these systems—drives their mass integration into day-to-day life. However, as a robust body of research in the area of algorithmic injustice shows, algorithmic systems, especially when used to sort and predict social outcomes, are not only inadequate but also perpetuate harm. In particular, a persistent and recurrent trend within the literature indicates that society's most vulnerable are disproportionally impacted. When algorithmic injustice and harm are brought to the fore, most of the solutions on offer (1) revolve around technical solutions and (2) do not center disproportionally impacted communities. This paper proposes a fundamental shift—from rational to relational—in thinking about personhood, data, justice, and everything in between, and places ethics as something that goes above and beyond technical solutions. Outlining the idea of ethics built on the foundations of relationality, this paper calls for a rethinking of justice and ethics as a set of broad, contingent, and fluid concepts and down-to-earth practices that are best viewed as a habit and not a mere methodology for data science. As such, this paper mainly offers critical examinations and reflection and not “solutions.”}
}
@incollection{LUND202435,
title = {3 - Choice Awareness strategies},
editor = {Henrik Lund},
booktitle = {Renewable Energy Systems (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
pages = {35-50},
year = {2024},
isbn = {978-0-443-14137-9},
doi = {https://doi.org/10.1016/B978-0-443-14137-9.00003-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443141379000036},
author = {Henrik Lund},
keywords = {Alternatives assessment, Choice Awareness, Feasibility studies, Institutional barriers, Market barriers, Path dependency, Public regulation, Radical technological change, Renewable energy systems, Technical alternatives},
abstract = {This chapter introduces strategies to raise the awareness of how to implement smart energy systems in fully decarbonized societies using the Choice Awareness theory. Choice Awareness is based on the understanding that existing institutional perceptions and organizational interests will often seek to eliminate certain choices from the political decision-making process, when the introduction of radical technological change is discussed. The counterstrategy is to raise public awareness that alternatives do exist and that it is possible to make a choice. Counterstrategies may involve the design of technical alternatives, feasibility studies based on institutional economic thinking, and the design of public regulation measures seen in the light of conflicting interests as well as changes in the democratic decision-making infrastructure. Each of the strategies is elaborated on in this chapter.}
}
@article{ZHANG2010S238,
title = {Biomimetic Construction of Category Mental Imagery Based on Recognition Mechanism of Visual Cortex of Human Brain},
journal = {Journal of Bionic Engineering},
volume = {7},
pages = {S238-S244},
year = {2010},
issn = {1672-6529},
doi = {https://doi.org/10.1016/S1672-6529(09)60241-9},
url = {https://www.sciencedirect.com/science/article/pii/S1672652909602419},
author = {Xianghe Zhang and Dan Wang and Luquan Ren and Pingping Liu},
keywords = {multi-dimensional space biomimetic informatics, artificial intelligence, cognitive science, mental imagery, visual cortex, object recognition},
abstract = {The principle of homology-continuity in Multi-Dimensional Biomimetic Informatics Space is applied to construct the identifying mechanism of category of deep representation of mental imagery. The model of each cerebral region involved in recognizing is established respectively and a feedforward method for establishing category mental imagery is proposed. First, the model of feature acquisition is developed based on Hubel-Wiesel model, and Gaussian function is used to simulate the simple cell receptive field to satisfy the specific function of visual cortex. Second, multiple input aggregation operation is employed to simulate the feature output of complex cells to get the invariance representation in feature space. Then, imagery basis is extracted by unsupervised learning algorithm based on the primary feature and category mental imagery is obtained by building Radial Basis Function (RBF) network. Finally, the system model is tested by training set and test set composed of real images. Experimental results show that the proposed method can establish valid deep representation of these samples, based on which the biomimetic construction of category mental imagery can be achieved. This method provides a new idea for solving imagery problem and studying imagery thinking.}
}
@article{YU2025101303,
title = {AI as a co-creator and a design material: Transforming the design process},
journal = {Design Studies},
volume = {97},
pages = {101303},
year = {2025},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2025.101303},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X25000158},
author = {Wendy Fangwen Yu},
keywords = {, , , , },
abstract = {Recent advancements in Artificial Intelligence (AI) have created new opportunities for incorporating AI into creative activities. Consequently, AI has become an increasingly significant tool in the design process, changing traditional workflows. This paper explores AI's role as both a co-creator and a design material, focusing on its impact on the ideation and evaluation stages of the design process. Through a transdisciplinary literature review, this study reveals that AI enhances creativity by providing inspirational stimuli, and supports evaluation and decision-making. However, it also introduces complexities such as cognitive overload and dependency. This paper emphasizes the need for design education reform, and training students as Designer Arbiters and Integrators to effectively collaborate with AI in a rapidly evolving technological landscape.}
}
@article{YANG20163,
title = {Modeling Urban Design with Energy Performance},
journal = {Energy Procedia},
volume = {88},
pages = {3-8},
year = {2016},
note = {CUE 2015 - Applied Energy Symposium and Summit 2015: Low carbon cities and urban energy systems},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2016.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1876610216300662},
author = {Perry Pei-Ju Yang and Jinyue Yan},
keywords = {Urban design computational model, Energy process model, Urban energy system, Urban design, Energy performance},
abstract = {Traditional urban design methods focus on the form-making process and lack performance dimensions such as energy efficiency. There are inherent differences between Urban Design as a model of decision-making for choosing form alternatives and Energy System Modeling as a model of evaluating and assessing system functions. To design a high energy performance city, the gap between the two models must be bridged. We propose a research design that combines the Urban Design Computational Model (UDCM) and the Optimization Model of Energy Process (OMEP) to demonstrate how an urban design computation can be integrated with an energy performance process and system. An evidence-based case study of community-level near zero energy districts will be needed for future work.}
}
@article{SCHUMNY1993319,
title = {Nanosystems - molecular machinery, manufacturing, and computation: by K. Eric Drexler. John Wiley & Sons, Chichester, England, 1992. ISBN 0-471-57518-6. 556 pages. Illustrated, Appendices, Glossary with detailed explanations, 337 references, extended Index.},
journal = {Computer Standards & Interfaces},
volume = {15},
number = {2},
pages = {319-320},
year = {1993},
note = {Object-Oriented Reference Models},
issn = {0920-5489},
doi = {https://doi.org/10.1016/0920-5489(93)90019-N},
url = {https://www.sciencedirect.com/science/article/pii/092054899390019N},
author = {Harald Schumny}
}
@article{CHEN2025107442,
title = {Exploring quantum neural networks for binary classification on MNIST dataset: A swap test approach},
journal = {Neural Networks},
volume = {188},
pages = {107442},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2025.107442},
url = {https://www.sciencedirect.com/science/article/pii/S0893608025003211},
author = {Kehan Chen and Jiaqi Liu and Fei Yan},
keywords = {Quantum neural networks, Swap test, Quantum activation function, Binary classification},
abstract = {In this study, we propose a novel modularized Quantum Neural Network (mQNN) model tailored to address the binary classification problem on the MNIST dataset. The mQNN organizes input information using quantum images and trainable quantum parameters encoded in superposition states. Leveraging quantum parallelism, the model efficiently processes inner product calculations of quantum neurons via the swap test, achieving constant complexity. To enhance the expressive capacity of the mQNN, nonlinear transformations, specifically quantum versions of activation functions, are integrated into the quantum network. The mQNN’s circuits are constructed from flexible quantum modules, allowing the model to adapt its structure based on varying input data types and scales for optimal performance. Furthermore, rigorous mathematical derivations are employed to validate the quantum state evolution during computation within a quantum neuron. Testing on the Pennylane platform simulates the quantum environment and confirms the mQNN’s effectiveness on the MNIST dataset. These findings highlight the potential of quantum computing in advancing image classification tasks.}
}
@article{INIGUEZLOMELI2024105106,
title = {A hardware architecture for single and multiple ellipse detection using genetic algorithms and high-level synthesis tools},
journal = {Microprocessors and Microsystems},
volume = {111},
pages = {105106},
year = {2024},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2024.105106},
url = {https://www.sciencedirect.com/science/article/pii/S0141933124001017},
author = {Francisco J. Iñiguez-Lomeli and Carlos H. Garcia-Capulin and Horacio Rostro-Gonzalez},
keywords = {Ellipse detection, Genetic algorithm, System-on-a-Chip (SoC), High-level synthesis (HLS), Hardware implementation, FPGA},
abstract = {Ellipse detection techniques are often developed and validated in software environments, neglecting the critical consideration of computational efficiency and resource constraints prevalent in embedded systems. Furthermore, programmable logic devices, notably Field Programmable Gate Arrays (FPGAs), have emerged as indispensable assets for enhancing performance and expediting various processing applications. In the realm of computational efficiency, hardware implementations have the flexibility to tailor the required arithmetic for various applications using fixed-point representation. This approach enables faster computations while upholding adequate accuracy, resulting in reduced resource and energy consumption compared to software applications that rely on higher clock speeds, which often lead to increased resource and energy consumption. Additionally, hardware solutions provide portability and are suitable for resource-constrained and battery-powered applications. This study introduces a novel hardware architecture in the form of an intellectual property core that harnesses the capabilities of a genetic algorithm to detect single and multi ellipses in digital images. In general, genetic algorithms have been demonstrated to be an alternative that shows better results than those based on traditional methods such as the Hough Transform and Random Sample Consensus, particularly in terms of accuracy, flexibility, and robustness. Our genetic algorithm randomly takes five edge points as parameters from the image tested, creating an individual treated as a potential candidate ellipse. The fitness evaluation function determines whether the candidate ellipse truly exists in the image space. The core is designed using Vitis High-Level Synthesis (HLS), a powerful tool that converts C or C＋＋functions into Register-Transfer Level (RTL) code, including VHDL and Verilog. The implementation and testing of the ellipse detection system were carried out on the PYNQ-Z1, a cost-effective development board housing the Xilinx Zynq-7000 System-on-Chip (SoC). PYNQ, an open-source framework, seamlessly integrates programmable logic with a dual-core ARM Cortex-A9 processor, offering the flexibility of Python programming for the onboard SoC processor. The experimental results, based on synthetic and real images, some of them with the presence of noise processed by the developed ellipse detection system, highlight the intellectual property core’s exceptional suitability for resource-constrained embedded systems. Notably, it achieves remarkable performance and accuracy rates, consistently exceeding 99% in most cases. This research aims to contribute to the advancement of hardware-accelerated ellipse detection, catering to the demanding requirements of real-time applications while minimizing resource consumption.}
}
@article{MEY1988743,
title = {Computation and the soul},
journal = {Journal of Pragmatics},
volume = {12},
number = {5},
pages = {743-789},
year = {1988},
issn = {0378-2166},
doi = {https://doi.org/10.1016/0378-2166(88)90056-2},
url = {https://www.sciencedirect.com/science/article/pii/0378216688900562},
author = {Jacob L. Mey and Mary Talbot},
abstract = {This article is both a review of Sperber and Wilson's Relevance and a broader critical discussion of linguistic pragmatics, from which Relevance has arisen. Four separate sections focus on Communication, Assumptions, the Metaphor of the Black Box and the Principle of Relevance itself. The authors conclude that the reductionist model of the human mind as an information-processing device, developed by Sperber and Wilson, is irredeemably asocial and therefore relevant to neither communication nor cognition.}
}
@article{AKPOLAT2025,
title = {Enhancing operational reliability for high penetration of green hydrogen production in energy islands: A power-to-X case study},
journal = {International Journal of Hydrogen Energy},
year = {2025},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2025.02.131},
url = {https://www.sciencedirect.com/science/article/pii/S0360319925006962},
author = {Alper Nabi Akpolat},
keywords = {Distributed energy resources, Energy islands, Green hydrogen production, Power electronic converters, Machine learning, Power-to-X, Reliability},
abstract = {The production, storage, and conversion of hydrogen into energy, as well as its use in areas such as green ammonia production for agriculture or the catalysis of natural gas, are of significant interest due to their stable structure and source diversity. For this purpose, energy islands (EIs) have been established near consumption points, and renewable energy (RE) obtained from photovoltaic (PV) panels and wind turbines (WTs) is used for green hydrogen production (GHP). In these EIs, hydrogen production from renewable sources shows significant growth, contributing to the power-to-X (P2X) system in terms of storage and flexibility. GHP through renewables will likely become a prominent solution for EIs within the next ten years. One of the bottlenecks here is not to reflect the adverse effects of the variable nature of renewables while transferring sustainable energy. Herein, to enhance operation sustainability, stability, and reliability of renewable-based distributed energy resources (DERs), machine learning (ML)-based techniques can be neat and auxiliary solutions. Power electronic converters (PECs) have such a duty as being the backbone of transferring energy in renewables. The crucial matter is here to keep the inputs and outputs of the converters as stable as possible. In this context, this paper outlines an ML approach to reduce the computational burden and enhance reliability. Therefore, this paper proposes the utilization of an EI to strengthen the stability and reliability of the general scheme. This work is a preliminary attempt and effective solution to establish these EIs, including GHP, considering feasibility criteria and improving reliability. Furthermore, this study examines the essential components, design criteria, challenges, and future issues for system establishment. It aims to facilitate the work of researchers in this field and further enhance the development of EIs.}
}
@article{GRIFFIN1977127,
title = {On the application of boundary conditions to time dependent computations for quasi one-dimensional fluid flows},
journal = {Computers & Fluids},
volume = {5},
number = {3},
pages = {127-137},
year = {1977},
issn = {0045-7930},
doi = {https://doi.org/10.1016/0045-7930(77)90019-6},
url = {https://www.sciencedirect.com/science/article/pii/0045793077900196},
author = {Michael D. Griffin and Anderson {John D.}},
abstract = {A study is made of the influence of boundary and initial conditions on time-dependent finite-difference solutions of quasi-one-dimensional duct flows. Several questions are addressed: (1) Under what conditions will a time-dependent solution converge to a steady-state supersonic flow, (2) Under what conditions will it converge to subsonic flow and (3) What conditions are necessary to insure a particular unique solution for subsonic flows. The results provide an orientation, or way of thinking, about the role of such conditions in time-dependent solutions of steady-state flows. The results also show that supersonic solutions are readily obtained by holding only pressure and temperature fixed at the duct inlet, and allowing velocity to float. However, subsonic solutions require pressure, temperature and velocity to be fixed at both the duct inlet and exit. If no conditions are held fixed at the exit, the results always converge to the supersonic solution, even if the fixed inlet mass flow is less than critical. In such a case, the program appears to generate additional mass flow between the inlet and throat, sufficient to choke the flow. These results also have some impact on two- and three-dimensional time-dependent solutions where subsonic flow is present on some or all portions of the flow boundaries.}
}
@article{KEMPF2023103747,
title = {Point pattern and spatial analyses using archaeological and environmental data – A case study from the Neolithic Carpathian Basin},
journal = {Journal of Archaeological Science: Reports},
volume = {47},
pages = {103747},
year = {2023},
issn = {2352-409X},
doi = {https://doi.org/10.1016/j.jasrep.2022.103747},
url = {https://www.sciencedirect.com/science/article/pii/S2352409X22004102},
author = {Michael Kempf and Gerrit Günther},
keywords = {Environmental archaeology, Quantitative archaeology, Computational methods, Spatial analysis, R, Point pattern analysis (PPA)},
abstract = {Computational methods recently gained momentum in archaeological science, particularly affecting large site distribution samples and environmental explanatory parameters. However, quantitative and environmental archaeology are still considered to be limited to a small number of experts and thus less ready to use in general research. Here, we present a case study that integrates computational methods and environmental data into archaeological spatial analyses using Point Pattern Analysis (PPA). We introduce a basic approach to model, visualise, and interpret archaeological site distributions as functions of explanatory covariates in a regional setting of the Neolithic period in the Carpathian Basin. The integration of environmental and socio-cultural variables in a multicomponent analysis allows to distinguish site location parameters and preferences across different chronological periods. Using the code to this article and open-access spatial data, the workflow can be adapted to different regional contexts and chronological periods, making it particularly suitable for spatial pattern comparison.}
}
@article{BOURDAKOU2023102881,
title = {Drug repurposing on Alzheimer's disease through modulation of NRF2 neighborhood},
journal = {Redox Biology},
volume = {67},
pages = {102881},
year = {2023},
issn = {2213-2317},
doi = {https://doi.org/10.1016/j.redox.2023.102881},
url = {https://www.sciencedirect.com/science/article/pii/S2213231723002823},
author = {Marilena M. Bourdakou and Raquel Fernández-Ginés and Antonio Cuadrado and George M. Spyrou},
keywords = {Alzheimer's disease, NRF2, , Association network,  drug repurposing, Differentially expressed genes},
abstract = {Alzheimer's disease (AD) is an age-dependent neurodegenerative disorder and the most common cause of cognitive decline. The alarming epidemiological features of Alzheimer's disease, combined with the high failure rate of candidate drugs tested in the preclinical phase, impose more intense investigations for new curative treatments. NRF2 (Nuclear factor-erythroid factor 2-related factor 2) plays a critical role in the inflammatory response and in the cellular redox homeostasis and provides cytoprotection in several diseases including those in the neurodegeneration spectrum. These roles suggest that NRF2 and its directly associated proteins may be novel attractive therapeutic targets in the fight against AD. In this study, through a systemics perspective, we propose an in silico drug repurposing approach for AD, based on the NRF2 interactome and regulome, with the aim of highlighting possible repurposed drugs for AD. Using publicly available information based on differential expressions of the NRF2-neighborhood in AD and through a computational drug repurposing pipeline, we derived to a short list of candidate repurposed drugs and small molecules that affect the expression levels of the majority of NRF2-partners. The relevance of these findings was assessed in a four-step computational meta-analysis including i) structural similarity comparisons with currently ongoing NRF2-related drugs in clinical trials ii) evaluation based on the NRF2-diseasome iii) comparison of relevance between targeted pathways of shortlisted drugs and NRF2-related drugs in clinical trials and iv) further comparison with existing knowledge on AD and NRF2-related drugs in clinical trials based on their known modes of action. Overall, our analysis yielded in 5 candidate repurposed drugs for AD. In cell culture, these 5 candidates activated a luciferase reporter for NRF2 activity and in hippocampus derived TH22 cells they increased NRF2 protein levels and the NRF2 transcriptional signatures as determined by increased expression of its downstream target heme oxygenase 1. We expect that our proposed candidate repurposed drugs will be useful for further research and clinical translation for AD.}
}
@article{WORTMANN2017173,
title = {Differentiating parametric design: Digital workflows in contemporary architecture and construction},
journal = {Design Studies},
volume = {52},
pages = {173-197},
year = {2017},
note = {Parametric Design Thinking},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2017.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X17300352},
author = {Thomas Wortmann and Bige Tunçer},
keywords = {parametric design, design automation, architectural design, software design, parametric master model},
abstract = {This paper examines Parametric Design (PD) in contemporary architectural practice. It considers three case studies: The Future of Us pavilion, the Louvre Abu Dhabi and the Morpheus Hotel. The case studies illustrate how, compared to non-parametrically and older parametrically designed projects, PD is employed to generate, document and fabricate designs with a greater level of detail and differentiation, often at the level of individual building components. We argue that such differentiation cannot be achieved with conventional Building Information Modelling and without customizing existing software. We compare the case studies' PD approaches (objected-oriented programming, functional programming, visual programming and distributed visual programming) and decomposition, algorithms and data structures as crucial factors for the practical viability of complex parametric models and as key aspects of PD thinking.}
}
@article{CHRISTENSEN2016125,
title = {Towards a formal assessment of design literacy: Analyzing K-12 students' stance towards inquiry},
journal = {Design Studies},
volume = {46},
pages = {125-151},
year = {2016},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2016.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X1530140X},
author = {Kasper Skov Christensen and Mikkel Hjorth and Ole Sejer Iversen and Paulo Blikstein},
keywords = {design education, design research, reflective practices, evaluation},
abstract = {We present a tool for quantitative assessment of K-12 students' stance towards inquiry as an important part of students' development of design literacy. On a basis of design thinking literature, we position designerly stance towards inquiry as a prerequisite for engaging with wicked problems. The Design Literacy (DeL) assessment tool contains design of a qualitative survey question, a coding scheme for assessing aspects of a designerly stance towards inquiry, and a description of how, we have validated the results through a large-scale survey administration in K-12 education. Our DeL tool is meant to provide educators, leaders, and policy makers with strong arguments for introducing design literacy in K-12 schools, which, we posit, function within in an age of measurement.}
}
@article{TABACHNECKSCHIJF1997305,
title = {CaMeRa: A computational model of multiple representations},
journal = {Cognitive Science},
volume = {21},
number = {3},
pages = {305-350},
year = {1997},
note = {Advances in analogy research: Integration of theory and data from the cognitive, computational, and neural sciences},
issn = {0364-0213},
doi = {https://doi.org/10.1016/S0364-0213(99)80026-3},
url = {https://www.sciencedirect.com/science/article/pii/S0364021399800263},
author = {Hermina J.M. Tabachneck-Schijf and Anthony M. Leonardo and Herbert A. Simon},
abstract = {This research aims to clarify, by constructing and testing a computer simulation, the use of multiple representations in problem solving, focusing on their role in visual reasoning. The model is motivated by extensive experimental evidence in the literature for the features it incorporates, but this article focuses on the system's structure. We illustrate the model's behavior by simulating the cognitive and perceptual processes of an economics expert as he teaches some well-learned economics principles while drawing a graph on a blackboard. Data in the experimental literature and concurrent verbal protocols were used to guide construction of a linked production system and parallel network, CaMeRa (Computation with Multiple Representations), that employs a “Mind's Eye” representation for pictorial information, consisting of a bitmap and associated node-link structures. Propositional list structures are used to represent verbal information and reasoning. Small individual pieces from the different representations are linked on a sequential and temporary basis to form a reasoning and inferencing chain, using visually encoded information recalled to the Mind's Eye from long-term memory and from cues recognized on an external display. CaMeRa, like the expert, uses the diagrammatic and verbal representations to complement one another, thus exploiting the unique advantages of each.}
}
@article{HORWICH201323622,
title = {Chaperonin-mediated Protein Folding},
journal = {Journal of Biological Chemistry},
volume = {288},
number = {33},
pages = {23622-23632},
year = {2013},
issn = {0021-9258},
doi = {https://doi.org/10.1074/jbc.X113.497321},
url = {https://www.sciencedirect.com/science/article/pii/S0021925820452524},
author = {Arthur L. Horwich},
keywords = {Protein Folding, Chaperone Chaperonin, Molecular Chaperone, Yeast, Protein Misfolding, Polypeptide},
abstract = {We have been studying chaperonins these past twenty years through an initial discovery of an action in protein folding, analysis of structure, and elucidation of mechanism. Some of the highlights of these studies were presented recently upon sharing the honor of the 2013 Herbert Tabor Award with my early collaborator, Ulrich Hartl, at the annual meeting of the American Society for Biochemistry and Molecular Biology in Boston. Here, some of the major findings are recounted, particularly recognizing my collaborators, describing how I met them and how our great times together propelled our thinking and experiments.}
}
@article{RODRIGUEZMENDEZ2024103804,
title = {UK net-zero policy design – from optimisation to robustness},
journal = {Environmental Science & Policy},
volume = {158},
pages = {103804},
year = {2024},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2024.103804},
url = {https://www.sciencedirect.com/science/article/pii/S1462901124001382},
author = {Quirina {Rodriguez Mendez} and Mark Workman and Geoff Darch},
keywords = {Robust Decision Making, Deep Uncertainty, Greenhouse Gas Removal, Climate modelling, United Kingdom Net-Zero target},
abstract = {The need to deal with the deep uncertainty and system complexity associated to Net-Zero pathways, especially those relying on emergent greenhouse gas removal (GGR) technologies, has resulted in a growing body of literature on alternative decision-support approaches. Exploratory modelling, and specifically Robust Decision Making (RDM), are potential approaches capable of addressing these challenges: by exploring a wide range of conceivable futures, they explicitly embrace deep uncertainties while seeking to reduce system vulnerabilities. However, though RDM methods have been well documented, there is little insight as to how such approach might be integrated into Net-Zero policy design processes. By means of a workshop (n=17) and interviews (n=13) with the UK climate policy and energy modelling communities, this contribution provides insights into the role and potential of RDM in explicitly dealing with the deep uncertainties that pervade in the establishment of a 60–100 MtCO2 UK GGR sector within three decades. The consultation process revealed that there is an appetite from the decision-making and analytical communities in integrating exploratory modelling concepts into UK policy design processes. It is recommended that to bridge the gap between theoretical RDM constructs and their broader adoption, the analytical process should include a broader set of disciplines and expertise. Specifically for the modelling community, this work suggests that in-use computational models should be adapted, rather than new tools developed. Key challenges also arise from the time and resources required, suggesting small scale place-based pilots could promote the acceptability and foster the adoption of the RDM methodology.}
}
@article{ZHANG2024101412,
title = {Predicting the Mathematics Literacy of Resilient Students from High‐performing Economies: A Machine Learning Approach},
journal = {Studies in Educational Evaluation},
volume = {83},
pages = {101412},
year = {2024},
issn = {0191-491X},
doi = {https://doi.org/10.1016/j.stueduc.2024.101412},
url = {https://www.sciencedirect.com/science/article/pii/S0191491X24000919},
author = {Yimei Zhang and Maria Cutumisu},
keywords = {Academic resilience, Machine learning, Mathematics literacy, Cultural differences},
abstract = {Mathematics is a crucial yet challenging subject for all students. Therefore, it is important to understand the role of academic resilience in mathematics, which enables students to overcome academic challenges. This study applied two machine learning algorithms, Lasso Regression (LR) and Random Forest (RF), to predict the mathematics literacy of resilient students from high-performing economies across cultures in PISA 2022. The findings indicated both RF and LR performed better in Western cultures than in Eastern cultures. Furthermore, in Eastern cultures, mathematics self-efficacy for 21st-century skills played an important role in predicting resilient students’ mathematics literacy, followed by self-efficacy towards mathematics, and mathematics anxiety. In Western cultures, self-efficacy towards mathematics was the predominant predictor, followed by mathematics self-efficacy for 21st-century skills. Theoretically, this study identifies key factors in predicting resilient students’ mathematics literacy across cultures. Methodologically, it is the first to apply ML in exploring resilient students’ mathematics literacy. Practically, it guides educators interested in developing interventions to improve resilient students’ mathematics literacy.}
}
@article{ROSSI2011820,
title = {MAPIT: a pedagogical-relational ITS},
journal = {Procedia Computer Science},
volume = {3},
pages = {820-826},
year = {2011},
note = {World Conference on Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2010.12.135},
url = {https://www.sciencedirect.com/science/article/pii/S1877050910005107},
author = {Pier Giuseppe Rossi and Simone Carletti},
keywords = {Teachers’ thinking, Intelligent tutoring system, Multi agent system, Learning management system, Tracking data, Chat-bot},
abstract = {The majority of Intelligent Tutoring System architectures are focused on supporting learners through content retrieval or in one or more given subject matters; examples of this can be found in Baghera [1], MyClass, Andes [2], Gramy, Advanced Geometry Tutor [7]. The implementation of such architectures are time-consuming and are generally not interoperable with other domains [3]. The presented research describes the experimentation of a Open Source, LMS enhanced with elements of AI aiming at supporting online teachers’ and tutors’ work by using a KB specific to relational and pedagogical aspects, not connected to a specific subject matter. Such implementation needs to be provided of an authoring tool easily and readily usable by tutors and teachers of different subjects and with medium level IT training. Starting point of our investigation has been a preliminary analysis of machine-mediated, human-human interactions (MM-HHI) and communications by using the Teachers’ thinking approach [4], [5], [6]. We considered messages exchanged between teachers/tutors and online students in three post-graduate, online courses running at the University of Macerata during 2008–2010 by the Faculty of Education. The study showed that about 30% of messages concerned structured information that could be straightforwardly retrieved by an artificial agent; almost all remaining messages were instead deeply bound to student’s learning path or required a significant input by the teacher/tutor, while the residual part of messages could — to some extents — be delegated to an intelligent agent having access to students’ tracking data in order to display visual information to users or trigger alarms to tutors. The investigation carried out prompted us for the deployment of an Open Source chat-bot system that would retrieve information already coded into the courses or originated by students through the analysis of their activity logs; the chat-bot agent uses this structured information in order to answer students’ most common questions hence relieving teachers and tutors from doing this repetitive task. The system is being implemented on a OLAT ver. 6.3 LMS loosely coupled to a JADE-based Multi Agent System in charge of processing user tracking data and running the ALICE chat-bot integrated with the platform messaging system.}
}
@article{WANG2025104139,
title = {Can attention detect AI-generated text? A novel Benford's law-based approach},
journal = {Information Processing & Management},
volume = {62},
number = {4},
pages = {104139},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2025.104139},
url = {https://www.sciencedirect.com/science/article/pii/S0306457325000767},
author = {Zhenhua Wang and Guang Xu and Ming Ren},
keywords = {AI-generated text detection, Benford's law, Attention distribution, Adherence pattern},
abstract = {AI technologies, such as the GPT-series, have garnered worldwide attention and raised concerns regarding their potential for misuse, owing to their groundbreaking text-generating capabilities, particularly in AI-generated text (AIGT). In response to the urgent need for effective detection, this study proposes BENATTEN, a novel approach that exploits the attention between human-generated text (HGT) and AIGT. We reveal that the way humans think and the probabilistic nature of AI algorithms lead to discrepancies in how they pay attention to tokens within the text they produce, with AIGT exhibiting a higher adherence to Benford's law in attention distribution compared to HGT. Extensive experiments on three general-domain datasets demonstrate the advantage of BENATTEN compared with existing methods. For instance, on the HC3 dataset, BENATTEN achieved an impressive 99.24 % accuracy, 99.69 % F1 and 99.47 % AUC, surpassing the OpenAI detector by 3.05 %, 3.48 % and 2.39 %, respectively. Also, comprehensive evaluations on seven specialized-application domain datasets have confirmed BENATTEN's robustness and its cross-platform applicability, proving its ongoing efficacy even as AI technology evolves. Further, the experiments have shown that BENATTEN exhibits remarkable resilience, effectively handling adversarial attacks and interference from other AI systems.}
}
@article{LIN2024200448,
title = {Maximizing the spread of information through content optimization},
journal = {Intelligent Systems with Applications},
volume = {24},
pages = {200448},
year = {2024},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2024.200448},
url = {https://www.sciencedirect.com/science/article/pii/S2667305324001224},
author = {Lei Lin and Yihua Du and Shibo Zhao and Wenkang Jiang and Qirui Tang and Li Xu},
keywords = {Computational social science, Human-in-the-loop, System simulation and optimization, Computational journalism, Computational advertisement},
abstract = {As data-driven prediction models advance, an increasing number of people are enjoying news personalized to their interests. The primary problem such recommendation models solve is to precisely match information with users and, in so doing, ensure that news spreads with greater efficiency. However, these techniques only help the media platform; they do not help those who produce the news. Hence, we devised a propagation framework based on a human-in-the-loop simulation that helps content authors maximize the spread of their messages through social networks. The framework works by acting on feedback provided by the simulation model. Additionally, the spread of information is formulated as a multi-objective optimization problem in which propagation is data-driven and simulated with machine learning techniques that leverage data on the historical behaviors of users. We additionally describe an implementation for this framework as an example of how the framework might be used in real life. On the practical side, the implementation uses text data from a blog to simulate the message's propagation, while, from a technical point of view, the multi-objective optimization problem is divided into an information retrieval problem and an integer programming problem, the results of which are fed back into the content editor as content operation strategies. A case study with the Sina Weibo microblog site not only validates the framework but also provides practitioners with insights into how to maximize the spread of information through social networking platforms. The results show that the proposed propagation framework is capable of increasing retweets by 7.9575 %. As an interesting aside, our experiments also show that the Weibo retweet lottery is both popular and a highly effective mechanism for increasing reposts.}
}
@article{CAMARGO20181116,
title = {A method for integrated process simulation in the mining industry},
journal = {European Journal of Operational Research},
volume = {264},
number = {3},
pages = {1116-1129},
year = {2018},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2017.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0377221717306409},
author = {Luis Felipe Riehs Camargo and Luis Henrique Rodrigues and Daniel Pacheco Lacerda and Fabio Sartori Piran},
keywords = {O.R. in natural resources, Production, Simulation, Systems thinking},
abstract = {This paper proposes a method of Integrated Process Simulation (MIPS), which considers the dynamic, stochastic and systemic characteristics of mining operations to support investment decisions in this industry. This MIPS supports development of a Decision Support System (DSS) that considers product quality, process productivity and production costs. A case study is described that used the MIPS to make better investment decisions. The MIPS has proven, in practice, to be effective in several applications; for example, in defining the maintenance policy for critical equipment in an iron ore concentration plant; the process for removing impurities and simulating the company's budget to evaluate the viability of different business plans.}
}
@incollection{OGRADY2020135,
title = {Cyber Security},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {135-141},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10532-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022955105323},
author = {Nathaniel O'Grady and Andrew C. Dwyer},
keywords = {Computing, Cybersecurity, Cyberspace, Digital, Infrastructure, Networks, Privacy, Software, Surveillance},
abstract = {As computation has become increasingly integrated into everyday life, critical infrastructure, state defense, and cybersecurity has become a new, crucial area of inquiry for geographers. This is due to the fast-changing, new securities that are being formed and enabled through, by and because of the growing role of computation. Geographers have studied cybersecurity as collectively constituted through a complex mixture of technologies, materialities, cultures, knowledges. In so doing, they have probed a range of phenomena crucial to cybersecurity; from technical processes such as encryption, malware infection, and threat detection, to the social arrangements and negotiations between various organizations and states, the implications of surveillance and big data on privacy, and how threats affect various infrastructure that support ways of life across the globe. Nevertheless, geographers do not simply consider cybersecurity as a mode of security imposed “online” or through digital technologies. Rather, in its practice, geographers have demonstrated how cybersecurity involves and invokes socio-political complications around criminality, protection, inequalities, privacy, surveillance, private enterprise, and the role of the state in the life of citizens.}
}
@article{BORSTLER2023111592,
title = {Investigating acceptance behavior in software engineering—Theoretical perspectives},
journal = {Journal of Systems and Software},
volume = {198},
pages = {111592},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111592},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222002680},
author = {Jürgen Börstler and Nauman bin Ali and Martin Svensson and Kai Petersen},
keywords = {Acceptance behavior, Dual process theory, Technology acceptance, Theory, TAM, UTAUT, TPB},
abstract = {Background:
Software engineering research aims to establish software development practice on a scientific basis. However, the evidence of the efficacy of technology is insufficient to ensure its uptake in industry. In the absence of a theoretical frame of reference, we mainly rely on best practices and expert judgment from industry-academia collaboration and software process improvement research to improve the acceptance of the proposed technology.
Objective:
To identify acceptance models and theories and discuss their applicability in the research of acceptance behavior related to software development.
Method:
We analyzed literature reviews within an interdisciplinary team to identify models and theories relevant to software engineering research. We further discuss acceptance behavior from the human information processing perspective of automatic and affect-driven processes (“fast” system 1 thinking) and rational and rule-governed processes (“slow” system 2 thinking).
Results:
We identified 30 potentially relevant models and theories. Several of them have been used in researching acceptance behavior in contexts related to software development, but few have been validated in such contexts. They use constructs that capture aspects of (automatic) system 1 and (rational) system 2 oriented processes. However, their operationalizations focus on system 2 oriented processes indicating a rational view of behavior, thus overlooking important psychological processes underpinning behavior.
Conclusions:
Software engineering research may use acceptance behavior models and theories more extensively to understand and predict practice adoption in the industry. Such theoretical foundations will help improve the impact of software engineering research. However, more consideration should be given to their validation, overlap, construct operationalization, and employed data collection mechanisms when using these models and theories.}
}
@article{LEMOEL2020110,
title = {Towards a multi-level understanding in insect navigation},
journal = {Current Opinion in Insect Science},
volume = {42},
pages = {110-117},
year = {2020},
note = {Neuroscience * Biomechanics of Insect Flight and Bio-inspired engineering},
issn = {2214-5745},
doi = {https://doi.org/10.1016/j.cois.2020.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S2214574520301310},
author = {Florent {Le Moël} and Antoine Wystrach},
abstract = {To understand the brain is to understand behaviour. However, understanding behaviour itself requires consideration of sensory information, body movements and the animal’s ecology. Therefore, understanding the link between neurons and behaviour is a multi-level problem, which can be achieved when considering Marr’s three levels of understanding: behaviour, computation, and neural implementation. Rather than establishing direct links between neurons and behaviour, the matter boils down to understanding two transitions: the link between neurons and brain computation on one hand, and the link between brain computations and behaviour on the other hand. The field of insect navigation illustrates well the power of such two-sided endeavour. We provide here examples revealing that each transition requires its own approach with its own intrinsic difficulties, and show how modelling can help us reach the desired multi-level understanding.}
}
@article{TALL1999223,
title = {What Is the Object of the Encapsulation of a Process?},
journal = {The Journal of Mathematical Behavior},
volume = {18},
number = {2},
pages = {223-241},
year = {1999},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(99)00029-2},
url = {https://www.sciencedirect.com/science/article/pii/S0732312399000292},
author = {David Tall and Michael Thomas and Gary Davis and Eddie Gray and Adrian Simpson},
abstract = {Several theories have been proposed to describe the transition from process to object in mathematical thinking. Yet, what is the nature of this “object” produced by the “encapsulation” of a process? Here, we outline the development of some of the theories (including Piaget, Dienes, Davis, Greeno, Dubinsky, Sfard, Gray, and Tall) and consider the nature of the mental objects (apparently) produced through encapsulation and their role in the wider development of mathematical thinking. Does the same developmental route occur in geometry as in arithmetic and algebra? Is the same development used in axiomatic mathematics? What is the role played by imagery?}
}
@article{SHRYANE2020112806,
title = {Is cognitive behavioural therapy effective for individuals experiencing thought disorder?},
journal = {Psychiatry Research},
volume = {285},
pages = {112806},
year = {2020},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2020.112806},
url = {https://www.sciencedirect.com/science/article/pii/S0165178119302793},
author = {Nick Shryane and Richard Drake and Anthony P. Morrison and Jasper Palmier-Claus},
keywords = {Psychosis, Cognitive behavioural therapy, Thought disorder, Randomized Controlled Trial},
abstract = {Various clinical guidelines recommend cognitive behavioural therapy (CBT) to treat psychosis without reference to patients’ thought disorder. However, there is a risk that disorganized thinking hampers CBT. We tested the prediction that thought disorder would interfere with the effectiveness of CBT for hallucinations and delusions, compared to treatment as usual and supportive counselling, in secondary data from two large, single blind randomised controlled trials. We fitted latent growth curve models separately for the development of frequency and distress of symptoms. CBT was significantly more successful than counselling in reducing delusional frequency in the short term and hallucinatory distress at any point, even in those with relatively high thought disorder. We found little evidence that clinicians should restrict CBT in this subgroup of patients. Nevertheless, the findings highlight the importance of effective initial treatment of thought disorder in maximising the benefit of CBT for psychosis, particularly for reducing distress from hallucinations.}
}
@article{HUANG200870,
title = {Investigating the cognitive behavior of generating idea sketches through neural network systems},
journal = {Design Studies},
volume = {29},
number = {1},
pages = {70-92},
year = {2008},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2007.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X07000750},
author = {Yinghsiu Huang},
keywords = {drawings, computer supported design, visual reasoning, neural network},
abstract = {Design can be regarded as a seeing–moving–seeing process, where designers repeatedly see and generate ideas that are based on what they have done. The crucial point of design thinking is how designers recognize ambiguous shapes from sketches and then transfer them into different shapes. This study attempts to conduct cognitive experiments to elucidate the sketching process and to simulate two types of sketching behavior used by neural network systems. When exhibiting the first type of sketching behavior, designers are able to transform their original sketches to satisfy requirements. Simulating this type of visual cognitive behavior by neural networks could help computers modify shapes to meet design requirements, as human designers do. When demonstrating the second type of sketching behavior, designers are able to see an ambiguous shape as different complete shapes so as to associate divergent design ideas. Another set of neural networks investigated in this study could also associate different shapes by adjusting the TSL and produce different idea sketches from the same shape.}
}
@article{BUTLER2021170,
title = {Expert performance and crowd wisdom: Evidence from English Premier League predictions},
journal = {European Journal of Operational Research},
volume = {288},
number = {1},
pages = {170-182},
year = {2021},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2020.05.034},
url = {https://www.sciencedirect.com/science/article/pii/S037722172030480X},
author = {David Butler and Robert Butler and John Eakins},
keywords = {OR in sports, Prediction, Experts},
abstract = {This paper analyses the forecasting accuracy of experts vis-à-vis laypeople over three seasons of English Premier League matches. We find that former professional football players have superior forecasting ability when compared to laypeople. The results give partial support to the view that a crowd forecast offers the greatest precision. Pundits generate a positive return while both the crowd and laypeople generate losses. As the prediction of multiple score outcomes represents a computationally difficult task, both groups display forecasting biases including a preference toward specific score forecasts. The results are relevant for those concerned with gambling behaviour if the forecasting strategies adopted here generalise to match betting markets.}
}
@article{GARG2024101391,
title = {Molecular Mechanics Demonstrate S-COMT as promising therapeutic receptor when analyzed with secondary plant metabolites},
journal = {Journal of the Indian Chemical Society},
volume = {101},
number = {11},
pages = {101391},
year = {2024},
issn = {0019-4522},
doi = {https://doi.org/10.1016/j.jics.2024.101391},
url = {https://www.sciencedirect.com/science/article/pii/S0019452224002711},
author = {Deepanshu Garg and Aarya Vashishth and Maharsh Jayadeep Jayawant and Virupaksha A. Bastikar},
keywords = {S-COMT receptor, Depression, Plant secondary metabolites, Molecular docking, Molecular dynamic simulation},
abstract = {Major depressive disorder (MDD) and other psychiatric conditions are debilitating illnesses affecting millions globally. Catechol-O-methyltransferase (COMT), an enzyme that regulates dopamine and norepinephrine breakdown in the brain, has emerged as a potential therapeutic target for these disorders. This study explores the inhibitory potential of plant secondary metabolites against S-COMT using computational techniques. COMT exists in two isoforms: membrane-bound COMT (MB-COMT), primarily found in brain neurons, and soluble COMT (S-COMT), present in peripheral tissues. S-COMT, particularly in the prefrontal cortex, is crucial for regulating neurotransmitters and maintaining cognitive function. Studies suggest S-COMT variants might be linked to the development of depression, schizophrenia, and other psychiatric disorders. Current COMT inhibitors often suffer from limitations, necessitating the exploration of novel therapeutic strategies. This study employed in-silico methods to investigate plant secondary metabolites as potential S-COMT inhibitors. Here, we describe the S-COMT protein structure retrieval and validation, followed by molecular docking simulations to identify plant compounds with the strongest binding affinity to the receptor's active site. Key amino acid residues involved in these interactions were also analyzed. Furthermore, molecular dynamics simulations were conducted to assess the stability of the top-scoring protein-ligand complexes over a 100-ns timeframe. The results explored the stability of ligand binding within the active site and its impact on the overall conformation of the S-COMT receptor. Our findings highlight promising therapeutic potential for these plant-derived compounds. Further in vitro and in vivo studies are warranted to validate their efficacy and safety for potential clinical applications in treating S-COMT-related disorders.
Subjects
Bioinformatics and Computational Biology, Proteomics, Neurogenerative Diseases.}
}
@article{ARORA1990131,
title = {Computational design optimization: A review and future directions},
journal = {Structural Safety},
volume = {7},
number = {2},
pages = {131-148},
year = {1990},
issn = {0167-4730},
doi = {https://doi.org/10.1016/0167-4730(90)90063-U},
url = {https://www.sciencedirect.com/science/article/pii/016747309090063U},
author = {Jasbir S. Arora},
keywords = {optimization methods, nonlinear problems, review, computational aspects, engineering design},
abstract = {A mathematical model for design optimization of engineering systems is defined. Computational algorithms to treat the model are reviewed and their features are discussed. The attributes of a good algorithm are given. Sequential quadratic programming algorithms that generate and use the approximate Hessian of the Lagrange function to calculate the search direction are the most recent methods. They are the most reliable methods among the available ones. Several other computational aspects, such as robust implementation of algorithms, use of a knowledge base, interactive use of optimization, and use of a database and database management system, are discussed. Recent developments in the field and future directions are presented.}
}
@article{LIU2022189,
title = {Granular cabin: An efficient solution to neighborhood learning in big data},
journal = {Information Sciences},
volume = {583},
pages = {189-201},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.11.034},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521011543},
author = {Keyu Liu and Tianrui Li and Xibei Yang and Xin Yang and Dun Liu and Pengfei Zhang and Jie Wang},
keywords = {Computational efficiency, Granular computing, Neighborhood learning, Neighborhood rough set},
abstract = {Neighborhood Learning (NL) is a paradigm covering theories and techniques of neighborhood, which facilitates data organization, representation and generalization. While delivering impressive performances across various fields such as granular computing, cluster analysis, NL is argued to be computationally demanding, thereby limiting its utility and applicability. In this study, a simple and generic scheme named granular cabin is proposed for drastically speeding up the algorithmic implementation of NL. Specifically, this scheme is deployed to Neighborhood Rough Set (NRS) which is a typical NL methodology. And three major applications of NRS are concerned including approximation computation, neighborhood classification and feature selection. Extensive experiments demonstrate that NRS methodology enhanced by granular cabin consumes much less time. This study offers a promising solution that ensures the great potential of NL in big data.}
}
@article{FATH2005485,
title = {Elucidating public perceptions of environmental behavior: a case study of Lake Lanier},
journal = {Environmental Modelling & Software},
volume = {20},
number = {4},
pages = {485-498},
year = {2005},
note = {Vulnerability of Water Quality in Intensively Developing Urban Watersheds},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2004.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364815204000611},
author = {Brian D. Fath and M.B. Beck},
keywords = {Cultural theory, Integrated environmental assessment, Stakeholder participation},
abstract = {Participation of stakeholders in stewardship of the aquatic environment, including participation from members of the general public, has become much more widespread than was the case a decade or so ago. With this shift, from a former predominantly technocratic stance to something of a democratic stance on the style of management, it becomes important to elucidate public perceptions of environmental behavior. The paper examines this issue: from a rather specific perspective, where the role of time is significant; with a specific purpose in mind—for defining illustrative stakeholder aspirations for the future, whose plausibility is to be assessed against a computational model of lake behavior; and for a specific case study, Lake Lanier in the Chattahoochee watershed of Georgia, USA. Perturbations and variation in the behavior of the aquatic environment span many time frames, from the very short-term response associated with storms, infrastructure failure, transient pollution events, and so on, to the much longer-term, for instance, the biogeochemical ‘ageing’ of a lake over many decades and more. Our analysis is devoted to data from a survey of stakeholder imagination and perceptions of how the future state of Lake Lanier may evolve in the relatively short term (2–5 years) and in the long term, defined as 25+ years (the span of a generation). Overall, stakeholders are pessimistic and fear that things will be worse in the longer term. Guided largely by thinking on the perspectives of the social solidarities of Cultural Theory, extraction and analysis of sub-samples of the survey responses show that this outlook over the two frames of time is persistent, irrespective of what are, in principle, rather different ‘global’ attitudes towards the man-environment relationship. Of interest inter alia to the foresight generating procedure, by which the ‘reachability’ of stakeholder-derived futures for the lake is to be assessed using a computational model of the relevant parts of the science base, is the question of whether the same small number of priorities for further research on lake behavior is robust in the face of the rich variety of aspirations for the future inevitable in a democratic community of stakeholders.}
}
@article{CORDA2021100834,
title = {The secret of planets’ perihelion between Newton and Einstein},
journal = {Physics of the Dark Universe},
volume = {32},
pages = {100834},
year = {2021},
issn = {2212-6864},
doi = {https://doi.org/10.1016/j.dark.2021.100834},
url = {https://www.sciencedirect.com/science/article/pii/S2212686421000650},
author = {Christian Corda},
abstract = {Three different approaches show that, contrary to a longstanding conviction older than 160 years, the advance of Mercury’s perihelion can be achieved in Newtonian gravity with a very high precision by correctly analyzing the situation without neglecting Mercury’s mass. General relativity remains more precise than Newtonian physics, but Newtonian framework is more powerful than researchers and astronomers were thinking till now, at least for the case of Mercury. The Newtonian formula of the advance of planets’ perihelion breaks down for the other planets. The predicted Newtonian result is indeed too large for Venus and Earth. Therefore, it is also shown that corrections due to gravitational and rotational time dilation, in an intermediate framework which analyzes gravity between Newton and Einstein, solve the problem. By adding such corrections, a result consistent with the one of general relativity is indeed obtained. Thus, the most important results of this paper are two: (i) It is not correct that Newtonian theory cannot predict the anomalous rate of precession of the perihelion of planets’ orbit. The real problem is instead that a pure Newtonian prediction is too large. (ii) Perihelion’s precession can be achieved with the same precision of general relativity by extending Newtonian gravity through the inclusion of gravitational and rotational time dilation effects. This second result is in agreement with a couple of recent and interesting papers of Hansen, Hartong and Obers. Differently from such papers, in the present work the importance of rotational time dilation is also highlighted. Finally, it is important to stress that a better understanding of gravitational effects in an intermediate framework between Newtonian theory and general relativity, which is one of the goals of this paper, could, in principle, be crucial for a subsequent better understanding of the famous Dark Matter and Dark Energy problems.}
}
@article{CHEN201217,
title = {Data-Brain driven systematic human brain data analysis: A case study in numerical inductive reasoning centric investigation},
journal = {Cognitive Systems Research},
volume = {15-16},
pages = {17-32},
year = {2012},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2010.12.014},
url = {https://www.sciencedirect.com/science/article/pii/S138904171100012X},
author = {Jianhui Chen and Ning Zhong and Peipeng Liang},
keywords = {Data-Brain, Systematic human brain data analysis, Provenance, Brain Informatics},
abstract = {As a crucial step in understanding human intelligence, Brain Informatics (BI) focuses on thinking centric investigations of human cognitive functions with respect to multiple activated brain areas and neurobiological processes for a given task. Although it has been recognized that systematic human brain data analysis is an important issue of BI methodology, the existing expert-driven multi-aspect data analysis excessively depends on individual capabilities and cannot be widely adopted in BI community. In this paper, we propose a Data-Brain driven approach for systematic brain data analysis, which is implemented by using the Data-Brain, Data-Brain based BI provenances and Global Learning Scheme for BI. Furthermore, a human numerical inductive reasoning centric investigation is described to demonstrate significance and usefulness of the proposed approach. Such a Data-Brain driven approach reduces the dependency on individual capabilities and provides a practical way for realizing the systematic human brain data analysis of BI methodology.}
}
@article{BESOLD201597,
title = {Towards integrated neural–symbolic systems for human-level AI: Two research programs helping to bridge the gaps},
journal = {Biologically Inspired Cognitive Architectures},
volume = {14},
pages = {97-110},
year = {2015},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2015.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X15000468},
author = {Tarek R. Besold and Kai-Uwe Kühnberger},
keywords = {Research program, Neural–symbolic integration, Complexity theory, Cognitive architectures, Agent architectures},
abstract = {After a human-level AI-oriented overview of the status quo in neural–symbolic integration, two research programs aiming at overcoming long-standing challenges in the field are suggested to the community: The first program targets a better understanding of foundational differences and relationships on the level of computational complexity between symbolic and subsymbolic computation and representation, potentially providing explanations for the empirical differences between the paradigms in application scenarios and a foothold for subsequent attempts at overcoming these. The second program suggests a new approach and computational architecture for the cognitively-inspired anchoring of an agent’s learning, knowledge formation, and higher reasoning abilities in real-world interactions through a closed neural–symbolic acting/sensing–processing–reasoning cycle, potentially providing new foundations for future agent architectures, multi-agent systems, robotics, and cognitive systems and facilitating a deeper understanding of the development and interaction in human-technological settings.}
}
@article{WEI2021189,
title = {Multi-core-, multi-thread-based optimization algorithm for large-scale traveling salesman problem},
journal = {Alexandria Engineering Journal},
volume = {60},
number = {1},
pages = {189-197},
year = {2021},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2020.06.055},
url = {https://www.sciencedirect.com/science/article/pii/S1110016820303227},
author = {Xin Wei and Liang Ma and Huizhen Zhang and Yong Liu},
keywords = {Multi-core, Multi-thread, Traveling Salesman Problem, Optimization Algorithm},
abstract = {With the rapid development of general hardware technology, microcomputers with multi-core CPUs have been widely applied in commercial services and household usage in the last ten years. Multi-core chips could, theoretically, lead to much better performance and computational efficiency than single-core chips. But so far, they have not shown general advantages for users, other than for operating systems and some specialized software. It is not easy to transform traditional single-core-based algorithms into multi-core-, multi-thread-based algorithms that can greatly improve efficiency, because of difficulties in computation and scheduling of hardware kernels, and because some programming languages cannot support multi-core, multi-thread programming. Therefore, a kind of multi-core-, multi-thread-based fast algorithm was designed and coded with Delphi language for the medium- and large-scale traveling salesman problem instances from TSPLIB, which can fully speed up the searching process without loss of quality. Experimental results show that the algorithm proposed can, under the given hardware limitations, take full advantage of multi-core chips and effectively balance the conflict between increasing problem size and computational efficiency and thus acquire satisfactory solutions.}
}
@article{MURRAY2019928,
title = {Center Finding in E. coli and the Role of Mathematical Modeling: Past, Present and Future},
journal = {Journal of Molecular Biology},
volume = {431},
number = {5},
pages = {928-938},
year = {2019},
issn = {0022-2836},
doi = {https://doi.org/10.1016/j.jmb.2019.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S0022283619300269},
author = {Seán M. Murray and Martin Howard},
keywords = {bacterial cell division positioning, plasmid segregation, MinCDE system, ParABS system, mathematical modeling},
abstract = {We review the key role played by mathematical modeling in elucidating two center-finding patterning systems in Escherichia coli: midcell division positioning by the MinCDE system and DNA partitioning by the ParABS system. We focus particularly on how, despite much experimental effort, these systems were simply too complex to unravel by experiments alone, and instead required key injections of quantitative, mathematical thinking. We conclude the review by analyzing the frequency of modeling approaches in microbiology over time. We find that while such methods are increasing in popularity, they are still probably heavily under-utilized for optimal progress on complex biological questions.}
}
@article{DIGIACOMO2025115393,
title = {Perspectives on the role of “-Omics” in predicting response to immunotherapy},
journal = {European Journal of Cancer},
volume = {220},
pages = {115393},
year = {2025},
issn = {0959-8049},
doi = {https://doi.org/10.1016/j.ejca.2025.115393},
url = {https://www.sciencedirect.com/science/article/pii/S0959804925001741},
author = {Anna Maria {Di Giacomo} and Sumit Subudhi and Wim Vos and Massimo Andreatta and Santiago Carmona and Will McTavish and Barbara Seliger and Ramy Ibrahim and Michael Lahn and Michael Smith and Alexander Eggermont and Bernard A. Fox and Michele Maio},
keywords = {Immunotherapy, Systems biology, “omics”-based biology, Epigenetics, Tumor microenvironment, Dark matter},
abstract = {The annual Immuno-Oncology “Think Tank” held in October 2023 in Siena reviewed the rapidly evolving systems-biological approaches which are now providing a deeper understanding of tumor and tumor microenvironment heterogeneity. Based on this understanding opportunities for novel therapies may be identified to overcome resistance to immunotherapy. There is increasing evidence that malignant disease processes are not limited to purely intracellular or genetic events but constitute a dynamic interaction between the host and disease. Tumor responses are influenced by many host tissue determinants across different cellular compartments, which can now be investigated by high-throughput molecular profiling technologies, often labelled with a suffix “-omics”. “Omics” together with ever increasing computational power, fast developments in machine learning, and high-resolution detection tools offer an unrivalled opportunity to connect high-dimensional data and create a holistic view of disease processes in cancer. This review describes advances in several state-of-the-art “-omics” approaches with perspectives on how these can be applied to the clinical development of new immunotherapeutic strategies and ultimately adopted in clinical practice.}
}
@article{SCHULTZ2010174,
title = {Models and methods in motion: Declining the dogma dance},
journal = {Futures},
volume = {42},
number = {2},
pages = {174-176},
year = {2010},
note = {Epistemological pluralism in futures studies},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2009.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0016328709001736},
author = {Wendy Schultz},
abstract = {I take a communicative pragmatist and realist approach to futures studies. This implies a sensitivity to understanding what the audience can absorb and using futures methods effectively to create spaces for new futures. While Wilber's work affords us with new insights to engage with methodology, is not the only path. Indeed, it is intellectual bigotry to demand that everyone master the tools one personally deems most appropriate. Critical conversations about futures must remain open, where post-modernist and integral thinking widen our horizons, they are welcomed, where they straitjacket our thoughts, they are not.}
}
@article{WHEELER2020192,
title = {Ideology and predictive processing: coordination, bias, and polarization in socially constrained error minimization},
journal = {Current Opinion in Behavioral Sciences},
volume = {34},
pages = {192-198},
year = {2020},
note = {Political Ideologies},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2020.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S2352154620300632},
author = {Nathan E Wheeler and Suraiya Allidina and Elizabeth U Long and Stephen P Schneider and Ingrid J Haas and William A Cunningham},
abstract = {Recent models of cognition suggest that the brain may implement predictive processing, in which top-down expectations constrain incoming sensory data. In this perspective, expectations are updated (error minimization) only if sensory data sufficiently deviate from these expectations (prediction error). Although originally applied to perception, predictive processing is thought to generally characterize cognitive architecture, including the social cognitive processes involved in ideological thinking. Scaling up these simple computational principles to the social sphere outlines a path by which group members may adopt shared ideologies and beliefs to predict behavior and cooperate with each other. Because ideological judgments are of specific interest to others in our political groups, we may increasingly regulate each other’s thinking, sharing the process of error minimization. In this paper, we outline how this process of shared error minimization may lead to shared ideologies and beliefs that allow group members to predict and cooperate with each other, and how, as a consequence, political polarization and extremism may result.}
}
@article{PACE2023105433,
title = {Exploring future research and innovation directions for a sustainable blue economy},
journal = {Marine Policy},
volume = {148},
pages = {105433},
year = {2023},
issn = {0308-597X},
doi = {https://doi.org/10.1016/j.marpol.2022.105433},
url = {https://www.sciencedirect.com/science/article/pii/S0308597X22004808},
author = {Lisa A. Pace and Ozcan Saritas and Alan Deidun},
keywords = {Foresight, Blue economy, Interdisciplinary science, Marine science, Sustainable development, Stakeholder participation},
abstract = {The blue economy integrates commercial, research and innovation activities across diverse industrial sectors. Achieving a sustainable blue economy requires unlocking the potential of science and innovation to develop innovative ocean sustainability solutions. This study explores the role of foresight in co-creating alternative, preferred futures for a sustainable blue economy looking towards 2030 and in establishing an interdisciplinary dialogue about research and innovation opportunities to achieve these futures. To this end, a foresight exercise is conducted with marine scientists and researchers in 6 countries in Europe. The exercise is designed in three stages: scanning, scenario-building and strategic orientation, and uses a combination of foresight methods to encourage creative thinking and exploration. The scenarios developed in the study describe alternative future worlds built on the establishment of self-sustaining communities and engaged societies; the diffusion of digitalisation and growth of blue biotechnologies; booming ecosystem services and open and collaborative research infrastructures that impact different sectors of the blue economy. A portfolio of research and innovation areas is developed that aims to inspire new research directions in four domains: (i) integrated ocean management tools; (ii) closed loop, circular polyculture systems; (iii) co-creation of innovation and transdisciplinary research; and (iv) open access and collaborative databases supporting ecosystem services. The study highlights the role of foresight in bridging across disciplinary perspectives and industry sectors. Foresight can be used to complement Decision-Support Systems and other quantitative approaches for research agenda-setting and for decision-making on policies addressing sustainability in the marine sciences. The process contributes to futures skills-building at institutional level and helps establish a futures mindset for strategic planning.}
}
@article{MCGILL2021113697,
title = {Evaluation of public health interventions from a complex systems perspective: A research methods review},
journal = {Social Science & Medicine},
volume = {272},
pages = {113697},
year = {2021},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2021.113697},
url = {https://www.sciencedirect.com/science/article/pii/S0277953621000290},
author = {Elizabeth McGill and Vanessa Er and Tarra Penney and Matt Egan and Martin White and Petra Meier and Margaret Whitehead and Karen Lock and Rachel {Anderson de Cuevas} and Richard Smith and Natalie Savona and Harry Rutter and Dalya Marks and Frank {de Vocht} and Steven Cummins and Jennie Popay and Mark Petticrew},
keywords = {Systems thinking, Complexity science, Evaluation methodologies, Public health, Practice},
abstract = {Introduction
Applying a complex systems perspective to public health evaluation may increase the relevance and strength of evidence to improve health and reduce health inequalities. In this review of methods, we aimed to: (i) classify and describe different complex systems methods in evaluation applied to public health; and (ii) examine the kinds of evaluative evidence generated by these different methods.
Methods
We adapted critical review methods to identify evaluations of public health interventions that used systems methods. We conducted expert consultation, searched electronic databases (Scopus, MEDLINE, Web of Science), and followed citations of relevant systematic reviews. Evaluations were included if they self-identified as using systems- or complexity-informed methods and if they evaluated existing or hypothetical public health interventions. Case studies were selected to illustrate different types of complex systems evaluation.
Findings
Seventy-four unique studies met our inclusion criteria. A framework was developed to map the included studies onto different stages of the evaluation process, which parallels the planning, delivery, assessment, and further delivery phases of the interventions they seek to inform; these stages include: 1) theorising; 2) prediction (simulation); 3) process evaluation; 4) impact evaluation; and 5) further prediction (simulation). Within this framework, we broadly categorised methodological approaches as mapping, modelling, network analysis and ‘system framing’ (the application of a complex systems perspective to a range of study designs). Studies frequently applied more than one type of systems method.
Conclusions
A range of complex systems methods can be utilised, adapted, or combined to produce different types of evaluative evidence. Further methodological innovation in systems evaluation may generate stronger evidence to improve health and reduce health inequalities in our complex world.}
}
@article{CORPONI2021,
title = {Frontal lobes dysfunction across clinical clusters of acute schizophrenia},
journal = {Revista de Psiquiatría y Salud Mental},
year = {2021},
issn = {1888-9891},
doi = {https://doi.org/10.1016/j.rpsm.2021.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S1888989121001324},
author = {Filippo Corponi and Yana Zorkina and Daniel Stahl and Andrea Murru and Eduard Vieta and Alessandro Serretti and Аnna Morozova and Alexander Reznik and Georgiy Kostyuk and Vladimir Pavlovich Chekhonin},
keywords = {Schizophrenia, Frontal lobe, Precision medicine, Cluster analysis, Machine learning},
abstract = {Introduction
Schizophrenia is a clinical construct comprising manifold phenotypes underlying heterogeneous biological underpinnings. The Positive and Negative Syndrome Scale (PANSS) represents the standard tool in the clinical characterization of patients affected by schizophrenia, allowing to detect different clinical profiles within the disorder. Frontal lobes are a key area of brain dysfunction in schizophrenia. We investigated whether different clinical profiles in acute schizophrenia show differences in frontal lobes dysfunction or not.
Methods
We defined PANSS-derived principal components in a sample of 516 acute patients. These components were used as clustering variables in a finite-mixture model. Frontal lobe impairment, measured with the frontal assessment battery (FAB) score, was adjusted for disease duration and compared across the clinical clusters with ANCOVA. A supervised-learning approach was then implemented to reveal most informative PANSS items.
Results
A three-cluster solution emerged: a first profile with high-moderate expression for the positive and excitability/hostility component; a second profile scoring high on depression/anxiety and low on other components; a third profile, comprising the majority of the study population (74%), with a heavy affection on the negative-disorganization dimensions. After controlling for disease duration, frontal lobe impairment significantly differed across the aforementioned clusters, with the third cluster being the most affected. Two PANSS items presented the highest predictive value for FAB total score.
Conclusions
Among negative and disorganization symptoms, “difficulty in abstract thinking” and “lack of spontaneity/flow in conversation” are specifically mapped to higher levels of frontal lobes dysfunction, hinting at similar features with other neurological disorders involving frontal lobes.}
}
@article{WANG2020256,
title = {Fine-grained neural decoding with distributed word representations},
journal = {Information Sciences},
volume = {507},
pages = {256-272},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2019.08.043},
url = {https://www.sciencedirect.com/science/article/pii/S0020025519307820},
author = {Shaonan Wang and Jiajun Zhang and Haiyan Wang and Nan Lin and Chengqing Zong},
keywords = {Neural decoding, fMRI word decoding, Word class, Stimuli paradigm, Word embedding models, Informative voxels},
abstract = {fMRI word decoding refers to decode what the human brain is thinking by interpreting functional Magnetic Resonance Imaging (fMRI) scans from people watching or listening to words, representing a sort of mind-reading technology. Existing works decoding words from imaging data have been largely limited to concrete nouns from a relatively small number of semantic categories. Moreover, such studies use different word-stimulus presentation paradigms and different computational models, lacking a comprehensive understanding of the influence of different factors on fMRI word decoding. In this paper, we present a large-scale evaluation of eight word embedding models and their combinations for decoding fine-grained fMRI data associated with three classes of words recorded from three stimulus-presentation paradigms. Specifically, we investigate the following research questions: (1) How does the brain-image decoder perform on different classes of words? (2) How does the brain-image decoder perform in different stimulus-presentation paradigms? (3) How well does each word embedding model allow us to decode neural activation patterns in the human brain? Furthermore, we analyze the most informative voxels associated with different classes of words, stimulus-presentation paradigms and word embedding models to explore their neural basis. The results have shown the following: (1) Different word classes can be decoded most effectively with different word embedding models. Concrete nouns and verbs are more easily distinguished than abstract nouns and verbs. (2) Among the three stimulus-presentation paradigms (picture, sentence and word clouds), the picture paradigm achieves the highest decoding accuracy, followed by the sentence paradigm. (3) Among the eight word embedding models, the model that encodes visual information obtains the best performance, followed by models that encode textual and contextual information. (4) Compared to concrete nouns, which activate mostly vision-related brain regions, abstract nouns activate broader brain regions such as the visual, language and default-mode networks. Moreover, both the picture paradigm and the model that encodes visual information have stronger associations with vision-related brain regions than other paradigms and word embedding models, respectively.}
}
@article{PEYRACHE2024255,
title = {A homothetic data generated technology},
journal = {European Journal of Operational Research},
volume = {316},
number = {1},
pages = {255-267},
year = {2024},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2024.01.031},
url = {https://www.sciencedirect.com/science/article/pii/S037722172400050X},
author = {Antonio Peyrache},
keywords = {Data envelopment analysis, Input homotheticity, Free disposal hull, Efficiency},
abstract = {I propose a method for constructing an enlargement of a variable returns to scale production technology that will satisfy homotheticity. The method can be used both with DEA and FDH single output (or single input) technologies and it is computationally fast. The method is constructed by adding a restriction to the axiomatically delineated homothetic reference technologies which requires these reference technologies to be subsets of the minimal reference technology that satisfies constant returns to scale. Within this set it is possible to identify a homothetic technology that satisfies the property of minimum extrapolation.}
}
@incollection{KARALIS2024215,
title = {Chapter 6 - Artificial intelligence in drug discovery and clinical practice},
editor = {Natassa Pippa and Costas Demetzos and Maria Chountoulesi},
booktitle = {From Current to Future Trends in Pharmaceutical Technology},
publisher = {Academic Press},
pages = {215-255},
year = {2024},
isbn = {978-0-323-91111-5},
doi = {https://doi.org/10.1016/B978-0-323-91111-5.00006-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323911115000068},
author = {Vangelis D. Karalis},
keywords = {Artificial intelligence, Drug discovery, Clinical practice, Machine learning},
abstract = {Artificial intelligence (AI) is the imitation of human intelligence by computers. It is the science of creating intelligent machines capable of performing tasks equivalent or superior to those performed by humans. The process involves collecting data, formulating rules for its use, making approximate or definitive determinations, and self-correcting. In particular, artificial intelligence and machine learning (ML) have attracted considerable attention in a variety of sectors, including pharmaceutical sciences, and have led to a rapid rise in new applications for machine learning in numerous areas of pharmaceutical sciences. In computational chemistry, deep learning models have been used to predict drug-target interactions, develop new compounds, and predict pharmacokinetics. AI, robotics, and advanced computing have applications in drug repurposing, quality-by-design, 3D printing, and nanomedicine. When used properly, AI techniques can improve patient treatment, detection and reduction of risk factors, and identification of complications.}
}
@article{GRUNAU2025107014,
title = {General Polyhedral Approximation of two-stage robust linear programming for budgeted uncertainty},
journal = {Computers & Operations Research},
volume = {179},
pages = {107014},
year = {2025},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2025.107014},
url = {https://www.sciencedirect.com/science/article/pii/S0305054825000425},
author = {Lukas Grunau and Tim Niemann and Sebastian Stiller},
keywords = {Robust optimization, Two-stage robust optimization, Linear programming, Approximation algorithm, Transportation location problem},
abstract = {We consider two-stage robust linear programs with uncertain righthand side. We develop a General Polyhedral Approximation (GPA), in which the uncertainty set U is substituted by a finite set of polytopes derived from the vertex set of an arbitrary polytope that dominates U. The union of the polytopes need not contain U. We analyze and computationally test the performance of GPA for the frequently used budgeted uncertainty set U (with m rows). For budgeted uncertainty affine policies are known to be best possible approximations (if coefficients in the constraints are nonnegative for the second-stage decision). In practice calculating affine policies typically requires inhibitive running times. Therefore an approximation of U by a single simplex has been proposed in the literature. GPA maintains the low practical running times of the simplex based approach while improving the quality of approximation by a constant factor. The generality of our method allows to use any polytope dominating U (including the simplex). We provide a family of polytopes that allows for a trade-off between running time and approximation factor. The previous simplex based approach reaches a threshold at Γ>m after which it is not better than a quasi nominal solution. Before this threshold, GPA significantly improves the approximation factor. After the threshold, it is the first fast method to outperform the quasi nominal solution. We exemplify the superiority of our method by a fundamental logistics problem, namely, the Transportation Location Problem, for which we also specifically adapt the method and show stronger results.}
}
@article{KLIGER20217,
title = {Dynamic Archeology or Distant Reading: Literary Study Between Two Formalisms},
journal = {Russian Literature},
volume = {122-123},
pages = {7-28},
year = {2021},
note = {Digital Humanities and Russian and East European Studies},
issn = {0304-3479},
doi = {https://doi.org/10.1016/j.ruslit.2021.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0304347921000429},
author = {Ilya Kliger},
keywords = {Computational Literary Studies, Distant Reading, Literary Form, Russian Formalism, OPOIAZ},
abstract = {Scholars working within computational literary studies often invoke Russian Formalism as a methodologically like-minded school of thought and a repository of useful insights, which can at last be tested with the help of recently developed digital techniques. Yet the two formalisms diverge starkly when it comes to three of their most fundamental categories of analysis: first, in their respective conceptions of literary form itself; next, in their notions of history and of what it means to tell the history of form; and finally, in the ways in which they construe the relationship between literature and society as a whole, or, in other words, in their corresponding sociologies of literary form. This paper, then, is a contribution to creating the conditions for the possibility of a genuine exchange between the two formalisms here at issue by focusing, first and foremost, on what divides them.}
}
@article{AVEN201633,
title = {On the use of conservatism in risk assessments},
journal = {Reliability Engineering & System Safety},
volume = {146},
pages = {33-38},
year = {2016},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2015.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S0951832015002938},
author = {Terje Aven},
keywords = {Conservatism, Risk assessments, Knowledge},
abstract = {It is common to use conservatism in risk assessments, replacing uncertain quantities with values that lead to a higher level of risk. It is argued that the approach represents a practical method for dealing with uncertainties and lack of knowledge in risk assessment. If the computed probabilities meet the pre-defined criteria with the conservative quantities, there is strong support for the “real risk” to meet these criteria. In this paper we look more closely into this practice, the main aims being to clarify what it actually means and what the implications are, as well as providing some recommendations. The paper concludes that conservatism should be avoided in risk assessments – “best judgements” should be the ruling thinking, to allow for meaningful comparisons of options. By incorporating sensitivity analyses and strength of knowledge judgements for the background knowledge on which the assigned probabilities are based, the robustness of the conclusions can be more adequately assessed.}
}
@incollection{COHEN20253,
title = {Chapter 1 - The evolution of machine learning: Past, present, and future},
editor = {Chhavi Chauhan and Stanley Cohen},
booktitle = {Artificial Intelligence in Pathology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
pages = {3-14},
year = {2025},
isbn = {978-0-323-95359-7},
doi = {https://doi.org/10.1016/B978-0-323-95359-7.00001-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323953597000017},
author = {Stanley Cohen},
keywords = {Capsule, Core memory, Graphical, Instruction set, Neural networks, Neuromorphic computing, Probability, Statistics support vectors},
abstract = {The earliest computers were designed to perform complex calculations, and their architecture allowed for the storage of not only data but also instructions as to how to manipulate that data. This evolved to the point where the computer-processed data according to a structure model of the real world, expressible in mathematical terms. The computer did not learn but was merely following instructions. The next step was to create a set of instructions that would allow the computer to learn from experience, i.e., to extract its own rules from large amounts of data and use those rules for classification and prediction. This was the beginning of machine learning and has led to the field collectively defined as artificial intelligence (AI). A major breakthrough came with the implementation of algorithms that were loosely modeled on brain architecture, with multiple interconnecting units sharing weighted puts among them, organized in computational layers (deep learning). AI has already revolutionized many aspects of modern life and is finding application in biomedical research and clinical practice at an accelerating rate.}
}
@article{COWLEY2019104025,
title = {Wide coding: Tetris, Morse and, perhaps, language},
journal = {Biosystems},
volume = {185},
pages = {104025},
year = {2019},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2019.104025},
url = {https://www.sciencedirect.com/science/article/pii/S0303264719301820},
author = {S J Cowley},
keywords = {Organic codes, Distributed language, Adaptors, Wide cognition, Reading, Languaging},
abstract = {Code biology uses protein synthesis to pursue how living systems fabricate themselves. Weight falls on intermediary systems or adaptors that enable translated DNA to function within a cellular apparatus. Specifically, code intermediaries bridge between independent worlds (e.g. those of RNAs and proteins) to grant functional lee-way to the resulting products. Using this Organic Code (OC) model, the paper draws parallels with how people use artificial codes. As illustrated by Tetris and Morse, human players/signallers manage code functionality by using bodies as (or like) adaptors. They act as coding intermediaries who use lee-way alongside “a small set of arbitrary rules selected from a potentially unlimited number in order to ensure a specific correspondence between two independent worlds” (Barbieri, 2015). As with deep learning, networked bodily systems mesh inputs from a coded past with current inputs. Received models reduce ‘use’ of codes to a run-time or program like process. They overlook how molecular memory is extended by living apparatuses that link codes with functioning adaptors. In applying the OC model to humans, the paper connects Turing’s (1937) view of thinking to Wilson’s (2004) appeal to wide cognition. The approach opens up a new view of Kirsh and Maglio’s (1994) seminal studies on Tetris. As players use an interface that actualizes a code or program, their goal-directed (i.e. ‘pragmatic’) actions co-occur with adaptor-like ‘filling in’ (i.e. ‘epistemic’ moves). In terms of the OC model, flexible functions derive from, not actions, but epistemic dynamics that arise in the human-interface-computer system. Second, I pursue how a Morse radio operator uses dibs and dabs that enable the workings of an artificial code. While using knowledge (‘the rules’) to resemiotize by tapping on a transmission key, bodily dynamics are controlled by adaptor-like resources. Finally, turning to language, I sketch how the model applies to writing and reading. Like Morse operators, writers resemiotize a code-like domain of alphabets, spelling-systems etc. by acting as (or like) bodily adaptors. Further, in attending to a text-interface (symbolizations), a reader relies on filling-in that is (or feels) epistemic. Given that humans enact or mimic adaptor functions, it is likely that the OC model also applies to multi-modal language.}
}
@article{HAREL201758,
title = {Field-based hypotheses on advancing standards for mathematical practice},
journal = {The Journal of Mathematical Behavior},
volume = {46},
pages = {58-68},
year = {2017},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2017.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0732312317300457},
author = {Guershon Harel},
keywords = {Common Core State Standards in Mathematics (CCSSM), Standards for mathematical practice},
abstract = {The Common Core State Standards in Mathematics (CCSSM, 2010) are organized around two types of standards: the standards for mathematical content and standards for mathematical practice. The central goal of this paper is to present cognitive and instructional analyses of standards for mathematical practice through a discussion of field-based activities with inservice secondary mathematics teachers and students. A potential value of the study is that it provides researchers with specific field-based hypotheses on advancing standards for mathematical practice.}
}
@incollection{MAGGIONI2010255,
title = {Knowledge Domains and Domain Learning},
editor = {Penelope Peterson and Eva Baker and Barry McGaw},
booktitle = {International Encyclopedia of Education (Third Edition)},
publisher = {Elsevier},
edition = {Third Edition},
address = {Oxford},
pages = {255-264},
year = {2010},
isbn = {978-0-08-044894-7},
doi = {https://doi.org/10.1016/B978-0-08-044894-7.00483-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780080448947004838},
author = {L. Maggioni and P.A. Alexander},
keywords = {Discipline, Domain, History, Knowledge, Learning, Mathematics, Reading, Science, Writing},
abstract = {The roots of current disciplines and domains of study reach well back in history. An exploration of their development shows that these areas of knowledge have not only reflected cultural changes, but have also influenced societies, especially through formal educational systems. Besides being characterized by their focus on a particular part of the world, disciplines are also distinguished by a specific way of thinking about their respective domains of study. Psychological research has identified several features of these pathways to knowledge (e.g., reading, writing, history, mathematics, and science) that generally define the landscape of academic practice.}
}
@article{OMRAN2022114806,
title = {Valorization of agro-industrial biowaste to green nanomaterials for wastewater treatment: Approaching green chemistry and circular economy principles},
journal = {Journal of Environmental Management},
volume = {311},
pages = {114806},
year = {2022},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2022.114806},
url = {https://www.sciencedirect.com/science/article/pii/S0301479722003796},
author = {Basma A. Omran and Kwang-Hyun Baek},
keywords = {Green synthesis, Zero-cost, Nanomaterials, Wastewater treatment, Sustainability},
abstract = {Water pollution is one of the most critical issues worldwide and is a priority in all scientific agendas. Green nanotechnology presents a plethora of promising avenues for wastewater treatment. This review discusses the current trends in the valorization of zero-cost, biodegradable, and readily available agro-industrial biowaste to produce green bio-nanocatalysts and bio-nanosorbents for wastewater treatment. The promising roles of green bio-nanocatalysts and bio-nanosorbents in removing organic and inorganic water contaminants are discussed. The potent antimicrobial activity of bio-derived nanodisinfectants against water-borne pathogenic microbes is reviewed. The bioactive molecules involved in the chelation and tailoring of green synthesized nanomaterials are highlighted along with the mechanisms involved. Furthermore, this review emphasizes how the valorization of agro-industrial biowaste to green nanomaterials for wastewater treatment adheres to the fundamental principles of green chemistry, circular economy, nexus thinking, and zero-waste manufacturing. The potential economic, environmental, and health impacts of valorizing agro-industrial biowaste to green nanomaterials are highlighted. The challenges and future outlooks for the management of agro-industrial biowaste and safe application of green nanomaterials for wastewater treatment are summarized.}
}
@article{MANZOLLI2022112211,
title = {A review of electric bus vehicles research topics – Methods and trends},
journal = {Renewable and Sustainable Energy Reviews},
volume = {159},
pages = {112211},
year = {2022},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2022.112211},
url = {https://www.sciencedirect.com/science/article/pii/S1364032122001344},
author = {Jônatas Augusto Manzolli and João Pedro Trovão and Carlos Henggeler Antunes},
keywords = {Electric bus, Electric mobility, Research gaps, Sustainability, Fleet operation, Energy management},
abstract = {The transportation sector accounts for a significant share of greenhouse gas emissions. Hence, the electrification of this sector is a crucial contributor to the mitigation of global warming. Recent studies suggest that electric vehicles will be economically paired with internal combustion engine vehicles in the near future. However, relying on private vehicle decarbonization only cannot deliver comprehensive space management efficiency solutions in urban environments. Therefore, it is essential to invest in the technological development and deployment of electric buses for public transportation, directly enhancing the quality of life in large cities. From this perspective, this review examines a wide range of scientific literature on electric bus research using science mapping methods and content analysis to support critical thinking unveiling the main research streams, methods, and gaps of the field. The analysis indicates that future research on electric buses will be mainly devoted to sustainability (encompassing economic, environmental and quality of service dimensions), energy management strategies, and fleet operation.}
}
@article{KAKOOEE20241466,
title = {Impact of Pavlovian Approach Bias on Bidirectional Planning in Spatial Navigation Tasks},
journal = {Procedia Computer Science},
volume = {246},
pages = {1466-1478},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.593},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924026449},
author = {Reza Kakooee and Mohammad TH Beheshti and Mehdi Keramati},
keywords = {Reinforcement Learning, Computational Modeling, Bidirectional Planning, Decision-Making, Pavlovian Bias},
abstract = {Bidirectional planning refers to a form of goal-directed decision-making process that combines forward and backward planning. Forward planning expands decision trees from the current state towards simulated futures, while backward planning starts the tree expansion from specific goal points in the opposite direction. Previous research has highlighted the impact of Pavlovian approach bias on behavior, showing that animals move towards appetitive outcomes regardless of the appropriateness of such behavior for achieving those outcomes. However, it remains unexplored whether the Pavlovian approach influences behavior by biasing backward planning. This research introduces a spatial navigation task to investigate the involvement of backward planning in humans’ action-selection process and to determine whether the Pavlovian approach biases behavior through backward planning. The results reveal the behavioral signature of backward planning in humans and show that Pavlovian approach bias can influence both forward and backward planning, leading to decisions that are not necessarily instrumentally more efficient. Additionally, we developed a bidirectional planning algorithm based on reinforcement learning to simulate the participants’ decisions. The simulation results suggest that the observed behavioral patterns can be parsimoniously explained by assuming that the Pavlovian approach bias acts as a pruning mechanism when expanding decision trees in both forward and backward directions.}
}
@incollection{ROCAVERT202065,
title = {Arts Bias},
editor = {Mark Runco and Steven Pritzker},
booktitle = {Encyclopedia of Creativity (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Oxford},
pages = {65-68},
year = {2020},
isbn = {978-0-12-815615-5},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.23612-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093245236122},
author = {Carla Rocavert},
keywords = {Algorithm, Arts, Bias, Creativity, Capitalism, Elite, Neoliberalism, Permanence, Technology, Utility},
abstract = {This entry posits that current debates around ‘arts bias’ are indicative of evolving definitions of creativity. It discusses themes of utility and permanence to illuminate tensions between historical conceptions of artistic creativity and newer fields, especially those which are driving the global economy toward an increasingly technologically-oriented paradigm under neoliberal capitalism. The arrival of computational creativity and the practice of applying algorithmic data technologies to artmaking are discussed.}
}
@article{RUTER2000519,
title = {Analysis, finite element computation and error estimation in transversely isotropic nearly incompressible finite elasticity},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {190},
number = {5},
pages = {519-541},
year = {2000},
issn = {0045-7825},
doi = {https://doi.org/10.1016/S0045-7825(99)00286-8},
url = {https://www.sciencedirect.com/science/article/pii/S0045782599002868},
author = {Marcus Rüter and Erwin Stein},
abstract = {In this paper we present constitutive models for nearly incompressible, transversely isotropic materials in finite hyperelasticity, particularly for reinforced rubber-like materials, which are of essential engineering interest. The theory is developed using a convected curvilinear coordinate system based on a mixed two-field displacement–pressure energy functional. Furthermore, an a posteriori error estimator without multiplicative constants is derived for non-linear anisotropic problems, which measures the discretization error in the first Piola–Kirchhoff stresses in the L2-norm by solving local Neumann problems with equilibrated tractions. Illustrative numerical examples demonstrate the anisotropic material behaviour of reinforced materials and the efficiency of using adaptive finite element methods.}
}
@article{MCCOWN201233,
title = {Farmers use intuition to reinvent analytic decision support for managing seasonal climatic variability},
journal = {Agricultural Systems},
volume = {106},
number = {1},
pages = {33-45},
year = {2012},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2011.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X11001557},
author = {R.L. McCown and P.S. Carberry and N.P. Dalgliesh and M.A. Foale and Z. Hochman},
keywords = {Decision support, Simulation, Information system, Cognitive system, Intuition, Climatic risk},
abstract = {The FARMSCAPE Information System emerged in a long-running research program aimed at making simulation models useful to Australian farmers in managing climatic variability. This paper is about how well it has worked. This is reported in relation to two standards: (1) the value to thinking and action expressed by farmers and their consultants, (2) correspondence with theory about learning and judgement in uncertain external environments. The former utilises recorded narrative interviews with participants over many years. The latter uses a cognitive framework drawn from theory of judgment and decision making featuring the relationship between intuition and analysis (McCown, 2011). The cognitive theory framework makes sense of several evaluation surprises. The first was high enthusiasm by largely-intuitive farmers for an analytic approach to soil water in conjunction with a newly-appreciated “bucket” metaphor for water balance. The second surprise was the virtual absence of soil water measurement 10years later. This had been replaced by various intuitive estimates, calibrated to maintain a heuristic relationship with regard to the “bucket” as a resource. Farmers and their advisers were facilitated in using simulation for thought experiments and planning under climatic uncertainty. Benchmarking enabled problem solving in documented conditions. Scenario analysis using historical climate records supported thought experiments by providing probability distributions that were valued for shaping expectations as a “history of the future”. In retrospective evaluation interviews, researchers were surprised to find that yield forecasting and tactical decision making, anticipated to be analyses that were both site- and season-specific forecasts, had served farmers as “management gaming” simulations to aid formulating action rules for such conditions, thus reducing the need for an on-going decision-aiding service. Equipped with their soil monitoring techniques and with their heuristic rules, farmers still reserved a place for simulation “when you’ve got a planting situation out of the ordinary.”}
}
@article{RUBIOFERNANDEZ2025269,
title = {Tracking minds in communication},
journal = {Trends in Cognitive Sciences},
volume = {29},
number = {3},
pages = {269-281},
year = {2025},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2024.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S1364661324003127},
author = {Paula Rubio-Fernandez and Marlene D. Berke and Julian Jara-Ettinger},
keywords = {social cognition, Theory of Mind, language, communication},
abstract = {How does social cognition help us communicate through language? At what levels does this interaction occur? In classical views, social cognition is independent of language, and integrating the two can be slow, effortful, and error-prone. But new research into word level processes reveals that communication is brimming with social micro-processes that happen in real time, guiding even the simplest choices like how we use adjectives, articles, and demonstratives. We interpret these findings in the context of advances in theoretical models of social cognition and propose a communicative mind-tracking framework, where social micro-processes are not a secondary process in how we use language – they are fundamental to how communication works.}
}
@article{DENHAM2022105526,
title = {Visualization and modeling of forest fire propagation in Patagonia},
journal = {Environmental Modelling & Software},
volume = {158},
pages = {105526},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105526},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222002262},
author = {Mónica M. Denham and Sigfrido Waidelich and Karina Laneri},
keywords = {Simulation, Modeling, Forest fire behavior, High-performance computing, GPGPU},
abstract = {Fire propagation is a big concern all over the world. Visualization is a valuable tool to test possible different scenarios for fire spread, specially for designing strategies for fire control, mitigation and management. We present a parallel High-Performance Computing (HPC) forest fire simulator with an interactive and intuitive user interface that offers several functionalities to the user. The visualization interface allows to choose the propagation model of preference, the scenario of interest, as well as numerous simulation features including firebreaks and ignition points. We show some of the outputs for two different mathematical models for fire spreading. The simulator was developed with an open source philosophy in the framework of Faster Than Real Time (FTRT) applications thinking on its possible use in the field during a forest fire propagation. It can be run in Linux (Ubuntu) and Windows Operating Systems and for portability purposes the simulator was also implemented on a NVIDIA Jetson Nano.}
}
@article{ZMIGROD202034,
title = {The role of cognitive rigidity in political ideologies: theory, evidence, and future directions},
journal = {Current Opinion in Behavioral Sciences},
volume = {34},
pages = {34-39},
year = {2020},
note = {Political Ideologies},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2019.10.016},
url = {https://www.sciencedirect.com/science/article/pii/S2352154619301147},
author = {Leor Zmigrod},
abstract = {A contentious debate in political psychology has centred on the role of cognitive rigidity in shaping individuals’ political ideologies and worldviews. Early theories in the 1950s posited that strict ideological doctrines may tend to attract individuals with dispositions towards mental rigidity. This question has persisted: Does psychological rigidity foster a tendency towards ideological extremism? This review evaluates the empirical landscape with respect to the rigidity-of-the-extreme and the rigidity-of-the-right hypotheses and offers conceptual and methodological recommendations for future research avenues. The evidence suggests that cognitive rigidity is linked to ideological extremism, partisanship, and dogmatism across political and non-political ideologies. Advances in the measurement of ideological extremity and cognitive rigidity will facilitate further elucidation regarding how exactly the two hypotheses may be reconciled and why they have been historically placed in a potentially false competition. This synthesis suggests that a scientifically rigorous understanding of the cognitive roots of ideological thinking may be essential for developing effective antidotes to intolerance and intergroup hostility.}
}
@article{KWON2019109608,
title = {Towards codification of thunderstorm/downburst using gust front factor: Model-based and data-driven perspectives},
journal = {Engineering Structures},
volume = {199},
pages = {109608},
year = {2019},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2019.109608},
url = {https://www.sciencedirect.com/science/article/pii/S0141029619306315},
author = {Dae Kun Kwon and Ahsan Kareem},
keywords = {Wind loads, Nonstationary process, Gust front, Gust front factor, Downburst, Thunderstorm, Codes and standards},
abstract = {Winds associated with gust fronts originating from a thunderstorm/downburst exhibit rapid changes during a short time period which may be accompanied by changes in direction. For several decades, a number of studies have been focused on identifying the characteristics of such nonstationary gust front winds in a variety of manners such as experimental/numerical methods and full-scale measurements. Yet, beginning the dialogue on any guidelines for design practice has thus far not evolved, in part due to a limited consensus on such characteristics among studies in conjunction with paucity of available data needed for vetting and corroborating, which is further impacted by the presence of nonstationarity. In an effort to establishing a new design procedure for this type of wind load effect on structures, the gust front factor (GFF) framework has been proposed by authors that encapsulates both the kinematic and dynamic features of gust front induced wind effects on structures, which distinguish themselves from those experienced in conventional boundary layer flows. This study revisits the gust front factor framework seeking to take the next step toward a possible initial framework for codification of gust front winds from model-based and data-driven perspectives. A modular and extensible web-enabled framework to estimate gust front related wind load effects is envisaged to rationally and holistically quantify design loads. This would promote design practice to enhance disaster resilience of the built environment. In this context, a closed-form expression concerning nonstationary fluctuations for a case of a long pulse duration is derived to facilitate rapid evaluation of nonstationary turbulence effects. A preliminary uncertainty analysis is also carried out to assess the influence of uncertainties associated with the load effects of gust front winds and the reliability of GFF. In addition, a comparison of the model-based gust front factor with a recently introduced thunderstorm response spectrum technique to assess their relative performance is carried out. In view of the lessons learned from the history of the gust loading factor on codes and standards, a possible living codification concept through a learning and updating invoking the emerging “Design Thinking” approach is discussed.}
}
@article{ZHANG2022101060,
title = {The neural encoding of productive phonological alternation in speech production: Evidence from Mandarin Tone 3 sandhi},
journal = {Journal of Neurolinguistics},
volume = {62},
pages = {101060},
year = {2022},
issn = {0911-6044},
doi = {https://doi.org/10.1016/j.jneuroling.2022.101060},
url = {https://www.sciencedirect.com/science/article/pii/S0911604422000045},
author = {Jie Zhang and Caicai Zhang and Stephen Politzer-Ahles and Ziyi Pan and Xunan Huang and Chang Wang and Gang Peng and Yuyu Zeng},
keywords = {Tone sandhi, Mandarin Chinese, Speech production, Event-related potentials, Phonological alternation, Word frequency},
abstract = {The understanding of alternation is a key goal in phonological research. But little is known about how phonological alternations are implemented in speech production. The current study tested the hypothesis that the production of words that undergo a highly productive alternation, Mandarin Tone 3 sandhi, is supported by a computation mechanism, which predicts that this alternation is subserved by neural activity in a time-window associated with post-lexical phonological and phonetic encoding regardless of word frequency. ERPs were recorded while participants sub-vocally produced high- and low-frequency disyllabic words that do or do not require sandhi. Sandhi words elicited more positive ERPs than non-sandhi words over left anterior channels around 336–520 ms after participants saw the cue instructing them to initiate sub-vocal production, but this effect was not significantly modulated by word frequency. These findings are consistent with predictions of the computation mechanism and have implications for current psycholinguistic models of speech production. (150 words)}
}
@article{WANG2022269,
title = {Intelligent Attack Analysis for IRS Communications with Incomplete Information},
journal = {Procedia Computer Science},
volume = {202},
pages = {269-276},
year = {2022},
note = {International Conference on Identification, Information and Knowledge in the internet of Things, 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.04.035},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922005695},
author = {Han Wang and Tianlin Zhu and Dapeng Li and Rui Jiang and Xiaoming Wang and Youyun Xu},
keywords = {IRS system, monitoring, attacking tactic, greedy, robust attack},
abstract = {Intelligent Reflection Surface (IRS) will be widely used in future communication system construction to reduce construction costs and improve coverage. However, IRS systems are generally equipped with controllers to receive wireless signal instructions, this increases the vulnerability of future communications. In this paper, we present an attack tactic to provide a way of thinking for the future defense deployment. At the beginning, the hacker cannot know the whole communication system, then it continuously attacks the IRS system, monitor the communication system, and sequentially learns new information about the system in each attacking round in order to attack more effectively in the next round. A two-layer optimal mathematical model is presented to describe the BS and the hacker’s decision. And the two-layer optimization which is difficult to solve is transformed into a single layer linear optimization by using equivalent transformation and dual transformation. A series of mathematical experiments are used to test different scenarios applicable to different monitoring style, and verify that the tactic proposed in this paper can effectively interfere with the system.}
}
@article{LEI2025110929,
title = {Fusion of heterogeneous industrial wireless networks: A survey},
journal = {Computer Networks},
volume = {257},
pages = {110929},
year = {2025},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2024.110929},
url = {https://www.sciencedirect.com/science/article/pii/S1389128624007618},
author = {Jiale Lei and Piao Jiang and Linghe Kong and Chi Xu and Chenren Xu and Kai Lin and Yueping Cai and Yanzhao Su and Weiping Ding and Zhen Wang and Bangyu Li and Xiaoguang Chen and Feng Gao and Weibo Wang and Jiadi Yu},
keywords = {Industrial wireless networks, 5G-U, Network fusion, Industrial internet of things},
abstract = {With the surge of wireless communication technology in smart factories, competition between different signals for limited unauthorized spectrum has led to heavy network conflicts and congestion. New technologies such as 5G unlicensed (5G-U) join industrial wireless networks (IWN) to provide advanced network access properties, making the issue more troublesome. This paper explores the advantages of fusion thinking from a new perspective of integrating heterogeneous IWNs, and systematically analyzes the key technologies that support IWN fusion, filling the gap in existing literature on IWN fusion systems. The main contribution of this paper includes proposing a technical framework based on the classic IWN architecture, fully studying the technologies and extensive research work that contribute to achieving IWN fusion from a bottom-up perspective. Moreover, open issues and prospects are enumerated to inspire valuable research works. Our research not only provides substantial contributions to the integration of the latest technologies, but also has important potential impacts on the future development of smart factory network infrastructure.}
}
@article{SAFARZYNSKA20121011,
title = {Evolutionary theorizing and modeling of sustainability transitions},
journal = {Research Policy},
volume = {41},
number = {6},
pages = {1011-1024},
year = {2012},
note = {Special Section on Sustainability Transitions},
issn = {0048-7333},
doi = {https://doi.org/10.1016/j.respol.2011.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S0048733312000595},
author = {Karolina Safarzyńska and Koen Frenken and Jeroen C.J.M. {van den Bergh}},
keywords = {Coevolution, Evolutionary economics, Group selection, Lock-in, Niche, Regime, Social learning, Transition, Transition management},
abstract = {This paper argues that evolutionary thinking and modeling can contribute to the emerging research on sustainability transitions and their management. Evolutionary theory provides a range of concepts and mechanisms that are useful in making existing theorizing about transitions more precise and complete. In particular, we will discuss how the multi-level, multi-phase, co-evolutionary, and social learning dynamics underlying transitions can be addressed in evolutionary models. In addition, evolutionary theorizing offers suggestions for extending current theoretical frameworks of transitions. Group selection provides a good example. We review the small set of formal evolutionary models of sustainability transitions, and show that existing formal evolutionary models of technological, social and institutional change can provide useful inputs to transition research and management.}
}
@article{KOKIS200226,
title = {Heuristic and analytic processing: Age trends and associations with cognitive ability and cognitive styles},
journal = {Journal of Experimental Child Psychology},
volume = {83},
number = {1},
pages = {26-52},
year = {2002},
issn = {0022-0965},
doi = {https://doi.org/10.1016/S0022-0965(02)00121-2},
url = {https://www.sciencedirect.com/science/article/pii/S0022096502001212},
author = {Judite V. Kokis and Robyn Macpherson and Maggie E. Toplak and Richard F. West and Keith E. Stanovich},
abstract = {Developmental and individual differences in the tendency to favor analytic responses over heuristic responses were examined in children of two different ages (10- and 11-year-olds versus 13-year-olds), and of widely varying cognitive ability. Three tasks were examined that all required analytic processing to override heuristic processing: inductive reasoning, deductive reasoning under conditions of belief bias, and probabilistic reasoning. Significant increases in analytic responding with development were observed on the first two tasks. Cognitive ability was associated with analytic responding on all three tasks. Cognitive style measures such as actively open-minded thinking and need for cognition explained variance in analytic responding on the tasks after variance shared with cognitive ability had been controlled. The implications for dual-process theories of cognition and cognitive development are discussed.}
}
@article{RUCH2024115840,
title = {Alterations in performance and discriminating power of the death/suicide implicit association test across the lifespan},
journal = {Psychiatry Research},
volume = {335},
pages = {115840},
year = {2024},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2024.115840},
url = {https://www.sciencedirect.com/science/article/pii/S0165178124001252},
author = {Donna A. Ruch and Jeffrey A. Bridge and Jaclyn Tissue and Sean P. Madden and Hanga Galfavy and Marianne Gorlyn and Arielle H. Sheftall and Katalin Szanto and John G. Keilp},
keywords = {Death/suicide implicit association, Suicide, Suicide risk assessment, Suicide prevention/early detection},
abstract = {The Death/Suicide Implicit Association Test (d/s-IAT) has differentiated individuals with prior and prospective suicide attempts in previous studies, however, age effects on test results remains to be explored. A three-site study compared performance on the d/s-IAT among participants aged 16–80 years with depression and prior suicide attempt (n = 82), with depression and no attempts (n = 80), and healthy controls (n = 86). Outcome measures included the standard difference (D) score, median reaction times, and error rates. Higher D scores represent a stronger association between death/suicide and self, while lower scores represent a stronger association between life and self. The D scores differed significantly among groups overall. Participants with depression exhibited higher scores compared to healthy controls, but there was no difference between participants with and without prior suicide attempts(F[2,242]=8.76, p<.001). Response times for participants with prior attempts differed significantly from other groups, with no significant differences in error rates. The D score was significantly affected by age (β =-0.007, t = 3.65, p<.001), with slowing of response times in older ages. Results suggest reaction time d/s-IAT D scores may not distinguish implicit thinking about suicide as response times slow with age, but slowed response times may be sensitive to suicide risk potentially indicating basic information processing deficits.}
}
@article{WANG2024111131,
title = {Three-way clustering: Foundations, survey and challenges},
journal = {Applied Soft Computing},
volume = {151},
pages = {111131},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.111131},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623011493},
author = {Pingxin Wang and Xibei Yang and Weiping Ding and Jianming Zhan and Yiyu Yao},
keywords = {Cluster analysis, Two-way clustering, Three-way decision, Three-way clustering},
abstract = {Clustering, as an unsupervised data mining technique, allows us to classify similar objects into the same cluster according to certain criteria. It helps us identify patterns between objects, reveal the associations between objects, and discover hidden structures. Traditional two-way clustering (2W clustering) algorithms represent one cluster by one set and only two types of relationships are considered between a sample and a cluster, namely, belonging to and not belonging to. Two-way decision is not always feasible especially in situations that are characterized by uncertainty and lack of information. Guided by the principle of three-way decision (3WD) as thinking in threes, three-way clustering (3W clustering) addresses the information uncertainty problem using core and the fringe regions to character a cluster. The universe is split into three sections by these two sets, which capture three kinds of relationships between objects and a cluster, namely, belonging to, partially belonging to, and not belonging-to. Compared with 2W clustering methods, 3W clustering incorporates the fringe region to describe the uncertain relationship between objects and clusters, which provides more information about the clustering structure. This survey points out the historical developments of three-way clustering and makes an overview of the achievements in the field of three-way clustering. In addition, to reap a clearer grasp of the development and research significance of three-way clustering, we divide the existing three-way clustering approaches into two categories and present the bibliometric analysis of related approaches. Finally, we point out some challenges and future research topics in three-way clustering. It is hoped that this review can serve as a reference and provide convenience for scholars and practitioners in the field of three-way clustering.}
}
@article{NOURANI2015891,
title = {Predictive Control, Competitive Model Business Planning, and Innovation ERP},
journal = {Procedia Computer Science},
volume = {65},
pages = {891-900},
year = {2015},
note = {International Conference on Communications, management, and Information technology (ICCMIT'2015)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.09.048},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915028781},
author = {Cyrus F. Nourani and Codrina Lauth},
keywords = {Competitive Models, Innovation Management, ERP. Multiplayer Games, Game Trees Computing, Predictive Modeling, Planning, Competitive Models, Dynamic Programming},
abstract = {New optimality principles are put forth based on competitive model business planning. A Generalized MinMax local optimum dynamic programming algorithm is presented and applied to business model computing where predictive techniques can determine local optima. Based on a systems model an enterprise is not viewed as the sum of its component elements, but the product of their interactions. The paper starts with introducing a systems approach to business modeling. A competitive business modeling technique, based on the author's planning techniques is applied. Systemic decisions are based on common organizational goals, and as such business planning and resource assignments should strive to satisfy higher organizational goals. It is critical to understand how different decisions affect and influence one another. Here, a business planning example is presented where systems thinking technique, using Causal Loops, are applied to complex management decisions. Predictive modeling specifics are briefed. A preliminary optimal game modeling technique is presented in brief with applications to innovation and R&D management. Conducting gap and risk analysis can assist with this process. Example application areas to e-commerce with management simulation models are examined.}
}
@article{ZHANG2017123,
title = {Collective decision optimization algorithm: A new heuristic optimization method},
journal = {Neurocomputing},
volume = {221},
pages = {123-137},
year = {2017},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2016.09.068},
url = {https://www.sciencedirect.com/science/article/pii/S0925231216311183},
author = {Qingyang Zhang and Ronggui Wang and Juan Yang and Kai Ding and Yongfu Li and Jiangen Hu},
keywords = {Collective decision optimization algorithm, Artificial neural networks, Meta-heuristic, Decision-making},
abstract = {Recently, inspired by nature, diversiform successful and effective optimization methods have been proposed for solving many complex and challenging applications in different domains. This paper proposes a new meta-heuristic technique, collective decision optimization algorithm (CDOA), for training artificial neural networks. It simulates the social behavior of human based on their decision-making characteristics including experience-based phase, others'-based phase, group thinking-based phase, leader-based phase and innovation-based phase. Different corresponding operators are designed in the methodology. Experimental results carried out on a comprehensive set of benchmark functions and two nonlinear function approximation examples demonstrate that CDOA is competitive with respect to other state-of-art optimization algorithms.}
}
@article{YANG2025123101,
title = {Shared energy storage with multi-microgrids: Coordinated development and economic-social-environmental comprehensive assessment under supply–demand uncertainties},
journal = {Renewable Energy},
pages = {123101},
year = {2025},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2025.123101},
url = {https://www.sciencedirect.com/science/article/pii/S0960148125007633},
author = {Xiaohui Yang and Xinlan Yi and Hongye Wang and Longxi Li},
keywords = {Shared energy storage, Coordinated planning, Economic-social-environmental value assessment, Price strategy, Supply and demand uncertainties, Multi-microgrid},
abstract = {Coordinated development of multi-microgrids and shared energy storage optimizes resource allocation, enhances renewable energy utilization, and mitigates environmental impacts. However, stakeholder conflicts and supply–demand uncertainties pose significant challenges. This study proposes a bi-level interaction framework for coordinated planning, optimizing shared energy storage pricing via genetic algorithms to determine optimal leasing, scheduling, and configuration strategies, exhibiting strong scalability and adaptability in large-scale systems. Wasserstein metric-distributed robust optimization and stochastic planning address supply–demand uncertainties, enhancing robustness and reducing the model’s over-conservatism. Leveraging complementary charging and discharging behaviors, the model minimizes storage investment, improves resource utilization, stabilizes the system, and achieves over 95% renewable energy absorption, fostering a sustainable business model. Given the diversification of energy storage technologies, a rigorous value assessment method is essential. This study constructs an economic-social-environmental evaluation framework for shared energy storage based on life cycle thinking, externality theory, and sharing economy principles. Employing the analytic hierarchy process, the framework quantifies stakeholder benefits, responsibility allocation, and profit potential, including arbitrage from off-peak storage and peak-time discharge, grid pressure alleviation, and enhanced renewable energy integration. Finally, analysis of subsidy policies and the positive externalities of expanding peak-to-valley differentials offers policy insights for cleaner, more resilient, and sustainable energy systems.}
}
@article{CARVALHO2016169,
title = {Origins and evolution of enactive cognitive science: Toward an enactive cognitive architecture},
journal = {Biologically Inspired Cognitive Architectures},
volume = {16},
pages = {169-178},
year = {2016},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2015.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X15000535},
author = {Leonardo Lana de Carvalho and Denis James Pereira and Sophia Andrade Coelho},
keywords = {Cognitive science, Enaction, Complex systems, Cognitive architecture},
abstract = {This paper presents a historical perspective on the origin of the enactive approach to cognitive science, starting chronologically from cybernetics, with the aim of clarifying its main concepts, such as enaction, autopoiesis, structural coupling and natural drift; thus showing their influences in computational approaches and models of cognitive architecture. Works of renowned authors, as well as some of their main commentators, were addressed to report the development of enactive approach. We indicate that the enactive approach transcends its original context within biology, and at a second moment within connectionism, changes the understanding of the relationships so far established between the body and the environment, and the ideas of conceptual relationships between the mind and the body. The influence on computational theories is of great importance, leading to new artificial intelligence systems as well as the proposition of complex, autopoietic and alive machines. Finally, the article stresses the importance of the enactive approach in the design of agents, understanding that previous approaches have very different cognitive architectures and that a prototypical model of enactive cognitive architecture is one of the largest challenges today.}
}
@incollection{WARE20081,
title = {Chapter 1 - Visual Queries},
editor = {Colin Ware},
booktitle = {Visual Thinking},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {1-22},
year = {2008},
isbn = {978-0-12-370896-0},
doi = {https://doi.org/10.1016/B978-0-12-370896-0.00001-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780123708960000019},
author = {Colin Ware},
abstract = {Publisher Summary
This book about graphic design provides a channel for clear communication that supports visual thinking and acts as an interface to the vast information resources of the modern world. Visual thinking is a process that has the allocation of attention as its very essence. Attention, however, is multifaceted. Making an eye movement is an act of attending. Eye movements are executed to satisfy the need for information and can be thought of as a sequence of visual queries on the visual world. The idea of the visual query is shorthand for what one does when obtaining information either from the world at large or from some kind of information display. Understanding what visual queries are easily executed is a critical skill for the designer. The special skill of designers is not so much skill with drawing or graphic design software, although these are undoubtedly useful, but the talent to analyze a design in terms of its ability to support the visual queries of others. One reason why design is difficult is that the designer already has the knowledge expressed in the design and has seen it develop from inception and therefore cannot see it with fresh eyes. The solution is to be analytic and this is where this book is intended to have value. Effective design should start with a visual task analysis, determine the set of visual queries to be supported by a design, and then use color, form, and space to efficiently serve those queries.}
}
@article{VANCOUVER20081,
title = {Integrating self-regulation theories of work motivation into a dynamic process theory},
journal = {Human Resource Management Review},
volume = {18},
number = {1},
pages = {1-18},
year = {2008},
issn = {1053-4822},
doi = {https://doi.org/10.1016/j.hrmr.2008.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1053482208000028},
author = {Jeffrey B. Vancouver},
keywords = {Self-regulation, Control theory, Goals, Computational modeling, Dynamic processes},
abstract = {Instead of merely combining theories of self-regulation, the current paper articulates a dynamic process theory of the underlying cognitive subsystems that explain relationships among long-used constructs like goals, expectancies, and valence. Formal elements of the theory are presented in an attempt to encourage the building of computational models of human actors, thinkers, and learners in organizational contexts. Discussion focuses on the application of these models for understanding the dynamics of individuals interacting in their organizations.}
}
@incollection{BRAME201915,
title = {Chapter 2 - Course Design: Making Choices About Constructing Your Course},
editor = {Cynthia J. Brame},
booktitle = {Science Teaching Essentials},
publisher = {Academic Press},
pages = {15-28},
year = {2019},
isbn = {978-0-12-814702-3},
doi = {https://doi.org/10.1016/B978-0-12-814702-3.00002-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128147023000020},
author = {Cynthia J. Brame},
keywords = {Undergraduate, science, education, course design, learning goals, learning objectives, guiding questions, formative assessment},
abstract = {Designing or redesigning a course can be a creative and rewarding effort, but it is always a challenge. Science is characterized by continuous change and an ever-growing (and already large!) body of knowledge, and our courses often seek to help students understand the core knowledge, experimental tools, and ways of thinking in a field. It’s a big task. Further, a course may play a particular role in the curriculum, serving as a prerequisite, a capstone, or the course in which students learn a particular skill. How do you pick on what to focus, and how do you organize your course to help your students be able to transfer their knowledge to a new setting? How can you design the course to help your students build a conceptual framework that can expand and grow as their understanding grows? This chapter describes six principles to guide your course design and provides suggestions for more detailed resources.}
}
@article{LIGABO2023102155,
title = {Practical way to apply fourth-generation assessment tools integrated into creating meaningful learning experiences in biology at high school},
journal = {Evaluation and Program Planning},
volume = {96},
pages = {102155},
year = {2023},
issn = {0149-7189},
doi = {https://doi.org/10.1016/j.evalprogplan.2022.102155},
url = {https://www.sciencedirect.com/science/article/pii/S0149718922001094},
author = {Mateus Ligabo and Fabiana Carvalho Silva and Ana Carolina da S.A. Carvalho and Durval Rodrigues and Rita C.L.B. Rodrigues},
keywords = {Concept maps, Meaningful learning, Hermeneutic-dialectic circle, Hofstede's cultural dimensions, Fourth-generation assessment},
abstract = {The learning process for a Biology topic regarding organisms and animal kingdom diversity was investigated through an innovative Interactive Didactic Sequence (IDS) which integrated the idea of “concept maps” with the Hermeneutic-Dialectic Circle (HDC). HDC is a tool for data collection and a reference for pluralist-constructivist thinking, considered a form of fourth-generation evaluation. Hofstede's cultural dimensions were also integrated into the investigation in order to facilitate mediation in an evaluative context. Students' performances (N = 25) from a São Paulo-Brazil public school were statistically evaluated. Their cultural profile was determined via the Hofstede Value Survey Model 1994 questionnaire. The elaborative process of arranging concept maps was individual (CM-individual) and integrated with HDC in groups (CM-HDC). Concept map assessment methods were based off existing literature. An improvement in students' performances (p < 0.05) that presented concept maps integrated to HDC in a more complex structure when compared to individually-built maps was observed. Employment of HDC helped form motivational/interactive dialogues between students and teachers, which, in turn, assisted in achieving greater learning through the use of concept maps. The application of the fourth-generation evaluation was improved via knowledge regarding students' cultural profiles.}
}
@article{MACLEOD2019101201,
title = {Mesoscopic modeling as a cognitive strategy for handling complex biological systems},
journal = {Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences},
volume = {78},
pages = {101201},
year = {2019},
issn = {1369-8486},
doi = {https://doi.org/10.1016/j.shpsc.2019.101201},
url = {https://www.sciencedirect.com/science/article/pii/S1369848618300839},
author = {Miles MacLeod and Nancy J. Nersessian},
keywords = {Mesoscopic modeling, Middle-out strategy, Systems biology, Model building, Mental modeling, Distributed cognition, Bounded rationality},
abstract = {In this paper we aim to give an analysis and cognitive rationalization of a common practice or strategy of modeling in systems biology known as a middle-out modeling strategy. The strategy in the cases we look at is facilitated through the construction of what can be called mesoscopic models. Many models built in computational systems biology are mesoscopic (midsize) in scale. Such models lack the sufficient fidelity to serve as robust predictors of the behaviors of complex biological systems, one of the signature goals of the field. This puts some pressure on the field to provide reasons for why and how these practices are warranted despite not meeting the stated goals of the field. Using the results of ethnographic study of problem-solving practices in systems biology, we aim to examine the middle-out strategy and mesoscopic modeling in detail and to show that these practices are rational responses to complex problem solving tasks on cognitive grounds in particular. However making this claim requires us to update the standard notion of bounded rationality to take account of how human cognition is coupled to computation in these contexts. Our account fleshes out the idea that has been raised by some philosophers on the “hybrid” nature of computational modeling and simulation. What we call “coupling” both extends modelers’ capacities to handle complex systems, but also produces various cognitive and computational constraints which need to be taken into account in any computational problem solving strategy seeking to maintain insight and control over the models produced.}
}
@incollection{KLATT200719,
title = {Perspectives for process systems engineering – a personal view from academia and industry},
editor = {Valentin Pleşu and Paul Şerban Agachi},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {24},
pages = {19-32},
year = {2007},
booktitle = {17th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/S1570-7946(07)80027-7},
url = {https://www.sciencedirect.com/science/article/pii/S1570794607800277},
author = {Karsten-Ulrich Klatt and Wolfgang Marquardt},
keywords = {Review, critical assessment, emerging fields, modeling, design, optimization, control, operations, numerical algorithms, software.},
abstract = {Process systems engineering (PSE) has been an active research field for almost 50 years. Modeling, simulation and optimization technologies have been developed to a mature state. These technologies have been penetrating all fields of chemical engineering in academia as well as in industrial practice. Systems thinking has been established in industrial practice largely through powerful commercial process simulation software and through mandatory courses in most chemical engineering programs. This contribution reflects on the past, present and future of PSE. Special emphasis will be on the perspectives of this field from an academic and industrial point of view.}
}
@incollection{WHITTEN201953,
title = {Chapter 4 - Guided Cognition Effects in Learning Mathematics},
editor = {William B. Whitten and Mitchell Rabinowitz and Sandra E. Whitten},
booktitle = {Guided Cognition for Learning},
publisher = {Academic Press},
pages = {53-108},
year = {2019},
isbn = {978-0-12-817538-5},
doi = {https://doi.org/10.1016/B978-0-12-817538-5.00004-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128175385000043},
author = {William B. Whitten and Mitchell Rabinowitz and Sandra E. Whitten},
keywords = {Advance organizers, Consolidators, Effective homework, Efficient homework, Guided Cognition design, Homework, Long-term or long-lasting learning, Middle school mathematics learning},
abstract = {This chapter reports 11 experiments that were done to determine whether Guided Cognition-designed homework facilitates learning middle school mathematics, and if so, to determine how it helps and what is learned. Experiments were performed in two middle schools and included 8th graders in two experiments and 7th graders in nine experiments. Mathematics topics ranged from fractions to integers to geometry. As in the literature experiments, students were in their normal school environment following their regular curriculum and were unaware that their learning was being observed. Guided Cognition design was found to be effective for learning mathematics. Working story problems that were enriched with cognitive events such as role play, divergent thinking, visualizing and illustrating, and relating to prior experience raised scores on unexpected quizzes by about a letter grade. Another unexpected quiz found that the improvements in problem-solving performance persisted for 6months. Guided Cognition homework was also found to be efficient in that students who worked eight problems and then performed four cognitive events performed as well on unexpected quizzes as students who worked 24 problems in the same time interval. Another pair of experiments determined that modest gains could be made from merely reading completed examples of cognitive events, but that these gains were not long-lasting. Performing the cognitive events was found to be most effective for long-term performance. Another experiment found that experiencing cognitive events after working some mathematics problems can help consolidate knowledge of how to work such problems.}
}
@article{BRANDT20051578,
title = {Mental spaces and cognitive semantics: A critical comment},
journal = {Journal of Pragmatics},
volume = {37},
number = {10},
pages = {1578-1594},
year = {2005},
note = {Conceptual Blending Theory},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2004.10.019},
url = {https://www.sciencedirect.com/science/article/pii/S0378216605000603},
author = {Per Aage Brandt},
keywords = {Mental Space Theory, Truth conditions, Spinoza, Semantic domains, Mental architecture, Material anchors},
abstract = {The article criticizes the negative influence of modern analytic, anti-semantic and anti-phenomenological thinking on cognitive semantics, and the errors or weaknesses of analysis it induces in current Mental Space Theory (MST). It also shows how a less inhibited theory of meaning, mental spaces and blending could develop more useful analyses of empirical occurrences, such as the artifacts called ‘material anchors’ and works of art — here exemplified by a painting by Matisse.}
}
@article{FEIZIZADEH2024103764,
title = {Spatiotemporal mapping of urban trade and shopping patterns: A geospatial big data approach},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {128},
pages = {103764},
year = {2024},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2024.103764},
url = {https://www.sciencedirect.com/science/article/pii/S1569843224001183},
author = {Bakhtiar Feizizadeh and Davoud Omarzadeh and Thomas Blaschke},
keywords = {Shopping pattern mapping, GIS, Geospatial big data, Data-driven approaches, Spatially explicit, GIScience, Novel methodology},
abstract = {The economic viability of an urban area in terms of trade and shopping significantly impacts its residents’ quality of life and is crucial for any sustainable development initiative. Geographic information systems (GIS) are well established, but the use of GIS technology within finance and trade analysis is still in its infancy. In this article, we highlighted the potential of GIS technology and big data analytics and demonstrated the importance of thinking in spatial terms for analysing patterns within the trade and finance industries. We studied spatiotemporal trade and shopping patterns in the city of Tabriz using data generated by customer purchase transactions obtained from 5200 stores, shopping, business and service centres. We employed time series transaction data collected from the points of sale in stores, shopping, service and business centres located in different areas of the city. We applied four well known geospatial big data driven approaches including machine learning nearest neighbour, kernel density estimation, space–time pattern mining and spatiotemporal coupling tele-coupling for detecting and mapping of spatial trade hotspot patterns. The results of this study indicated the potential of GIScience methods for the explicit spatial mapping of trade and shopping patterns. The results revealed that the city centre, particularly the Bazaar of Tabriz, acts as the city’s heart of trade, and we identify additional major business hotspots. Furthermore, the results allow for studying the impacts of unbalanced urban development in Tabriz, where the wealthy suburbs with high quality of life, such as Valiasr and Elguli, host the major shopping hotspots. The spatial patterns obtained enable local stakeholders, decision makers and authorities to develop strategic plans for urban sustainable development in Tabriz. The geospatial big data approach used can stimulate novel and progressive research. Results of this study demonstrate methodological advancements in GIScience by ’spatializing’ individual purchase data and therefor proposing an explicit geospatial big data analysis approach.}
}
@article{LITT1993459,
title = {Single neuron computation: T. McKenna, J. Davis and S.F. Zornetzer (Eds.) (Academic Press, San Diego, CA, 1992, 664 p., Price US $59.95)},
journal = {Electroencephalography and Clinical Neurophysiology},
volume = {87},
number = {6},
pages = {459-460},
year = {1993},
issn = {0013-4694},
doi = {https://doi.org/10.1016/0013-4694(93)90160-W},
url = {https://www.sciencedirect.com/science/article/pii/001346949390160W},
author = {Brian Litt}
}
@article{OSORIOMORA2025837,
title = {A risk-averse latency location-routing problem with stochastic travel times},
journal = {European Journal of Operational Research},
volume = {321},
number = {3},
pages = {837-850},
year = {2025},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2024.10.041},
url = {https://www.sciencedirect.com/science/article/pii/S0377221724008440},
author = {Alan Osorio-Mora and Francisco Saldanha-da-Gama and Paolo Toth},
keywords = {Routing, Cumulative routing, Sampling, Variable neighborhood search},
abstract = {In this paper, a latency location-routing problem with stochastic travel times is investigated. The problem is cast as a two-stage stochastic program. The ex-ante decision comprises the location of the depots. The ex-post decision regards the routing, which adapts to the observed travel times. A risk-averse decision-maker is assumed, which is conveyed by adopting the latency CVaRα as the objective function. The problem is formulated mathematically. An efficient multi-start variable neighborhood search algorithm is proposed for tackling the problem when uncertainty is captured by a finite set of scenarios. This procedure is then embedded into a sampling mechanism so that realistic instances of the problem can be tackled, namely when the travel times are represented by random vectors with an infinite support. An extensive computational analysis is conducted to assess the methodological developments proposed and the relevance of capturing uncertainty in the problem. Additional insights include the impact of the risk level in the solutions.}
}
@article{PHONAPICHAT20143169,
title = {An Analysis of Elementary School Students’ Difficulties in Mathematical Problem Solving},
journal = {Procedia - Social and Behavioral Sciences},
volume = {116},
pages = {3169-3174},
year = {2014},
note = {5th World Conference on Educational Sciences},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2014.01.728},
url = {https://www.sciencedirect.com/science/article/pii/S1877042814007459},
author = {Prathana Phonapichat and Suwimon Wongwanich and Siridej Sujiva},
keywords = {Mathematical problem solving, mathematical difficulties, mathematical skills, elementary school students},
abstract = {The main purpose of mathematics teaching is to enable students to solve problems in daily life. Unfortunately, according to the latest national test results, most students lack mathematical problem solving skills. This proves to be one of the reasons why overall achievement in mathematics is considered quite low. It also reflects that students have difficulties in comprehending mathematical problems affecting the process of problem-solving. Therefore, in order to allow teachers to establish a proper teaching plan suitable for students’ learning process, this research aims to analyze the difficulties in mathematical problem solving among elementary school students. Samples are divided into two groups, elementary school students and mathematics teachers. Data collection was conducted by structured interview, documentary analysis, and survey tests. Data analysis was conducted by descriptive statistics, and content analysis. The results suggest that there are several difficulties in problem solving, namely 1) Students have difficulties in understanding the keywords appearing in problems, thus cannot interpret them in mathematical sentences. 2) Students are unable to figure out what to assume and what information from the problem is necessary to solving it, 3) Whenever students do not understand the problem, they tend to guess the answer without any thinking process, 4) Students are impatient and do not like to read mathematical problems, and 5) Students do not like to read long problems. Therefore, the results found in this research will lead to the creation and the development of mathematical problem solving diagnostic tests for teachers, in order to improve students’ mathematical problem solving skills.}
}
@article{TONNANG2022100964,
title = {Advances in data-collection tools and analytics for crop pest and disease management},
journal = {Current Opinion in Insect Science},
volume = {54},
pages = {100964},
year = {2022},
issn = {2214-5745},
doi = {https://doi.org/10.1016/j.cois.2022.100964},
url = {https://www.sciencedirect.com/science/article/pii/S2214574522000992},
author = {Henri EZ Tonnang and Daisy Salifu and Bester T Mudereri and Joel Tanui and Andrew Espira and Thomas Dubois and Elfatih M Abdel-Rahman},
abstract = {Innovative methods in data collection and analytics for pest and disease management are advancing together with computational efficiency. Tools, such as the open-data kit, research electronic data capture, fall armyworm monitoring, and early warning- system application and remote sensing have aided the efficiency of all types of data collection, including text, location, images, audio, video, and others. Concurrently, data analytics have also evolved with the application of artificial intelligence and machine learning (ML) for early warning and decision-support systems. ML has repeatedly been used for the detection, diagnosis, modeling, and prediction of crop pests and diseases. This paper thus highlights the innovations, implications, and future progression of these technologies for sustainability.}
}
@article{WANG2025113691,
title = {Effect of interlayer spacing on the mechanical properties of the graphene oxide/thermoplastic polyurethane nanocomposite},
journal = {Computational Materials Science},
volume = {250},
pages = {113691},
year = {2025},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2025.113691},
url = {https://www.sciencedirect.com/science/article/pii/S0927025625000345},
author = {Yuyang Wang and Guofu Yin and Junpeng Liu and Jiao Li and Chunqiang Wei and Minjie Li},
keywords = {Thermoplastic Polyurethane, Graphene Oxide, Interlayer Spacing, Mechanical Properties, Molecular Dynamics},
abstract = {In this paper, in order to investigate the effect of graphene oxide (GO) interlayer spacing on the overall mechanical properties of GO/thermoplastic polyurethane (TPU) nanocomposite, the uniaxial tensile simulation on the computational model of GO/TPU nanocomposite with monolayer and bilayer graphene sheets were carried out. During the tensile process, the variation of potential energy, void percentage, tensile strain contour, and density distribution with tensile strain were applied to analyze the underlying microscopic mechanism of stress change of monolayer GO/TPU system. The results show that when the system was within the stress yield region, the micro rearrangement motion and relative sliding of TPU chains play a major role. Furthermore, when the system enters into the stress softening region until stress failure, the void formation and nuclear within the system dominates the stress change, which is attributed to the interfacial structure of GO/TPU. Subsequently, based on the above-mentioned results in the case of monolayer GO/TPU result, the effects of different layer spacings on the overall mechanical properties of GO/TPU nanocomposite system were discussed by varying the spacing of GO layers. The results show that with the increase of GO layer spacing, the elastic modulus and yield strength of the system show a tendency of increasing and then decreasing, and the failure strain is the opposite, and when the GO dispersion is better, it can also play the role of delayed damage failure of the system.}
}
@article{CHERRIER2023104497,
title = {Household heterogeneity in macroeconomic models: A historical perspective},
journal = {European Economic Review},
volume = {158},
pages = {104497},
year = {2023},
issn = {0014-2921},
doi = {https://doi.org/10.1016/j.euroecorev.2023.104497},
url = {https://www.sciencedirect.com/science/article/pii/S0014292123001265},
author = {Beatrice Cherrier and Pedro Garcia Duarte and Aurélien Saïdi},
keywords = {History of macroeconomics, Heterogeneous agents, Bewley models, Permanent income hypothesis, Aggregation, Equity premium puzzle, Precautionary savings},
abstract = {In this paper, we trace the rise of heterogeneous household models in mainstream macroeconomics from the turn of the 1980s to the early 2000s, when these models evolved into an identifiable and consistent literature. We show that different communities across the US and Europe considered heterogeneous agents for various reasons and developed models that differed in their theoretical and empirical strategies. Minnesota economists primarily focused on incorporating stochastic heterogeneity into general equilibrium models. Other researchers refined growth models or tried to find alternatives to the permanent income hypothesis, leading them to explore more structural heterogeneity. We also document the computational challenges that some of these communities faced, how they gradually became aware of each other's work, and how they faced criticisms from macro- and microeconomists, many of them trained in European countries and dissatisfied with the theoretical and empirical aggregation strategies underlying these models.}
}
@article{OLIVIER2024103956,
title = {DynBioSketch: A tool for sketching dynamic visual summaries in biology, and its application to infection phenomena},
journal = {Computers & Graphics},
volume = {122},
pages = {103956},
year = {2024},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2024.103956},
url = {https://www.sciencedirect.com/science/article/pii/S0097849324000918},
author = {Pauline Olivier and Tara Butler and Pascal Guehl and Jean-Luc Coll and Renaud Chabrier and Pooran Memari and Marie-Paule Cani},
keywords = {Sketch-based modeling, Interactive geometric modeling, Sketch-based animation, Narration},
abstract = {Having simple methods of illustration is essential to scientific thinking. To complement the abstract sketches regularly used in cell biology, we propose DynBioSketch, an easy-to-use digital modeling and animation tool, enabling biologists to resort to less simplified representations when necessary without having to call professional artists. DynBioSketch is an interactive sketching system dedicated to the design and communication of biological phenomena at the cellular scale that can be illustrated in a few minutes of animation. Our model integrates 3D modeling, pattern-based design of 3D shape distributions, and sketch-based animation. These elements can be combined to create complex scenarios such as the infection phenomenon on which we focus, allowing a narrative design adapted to communication between researchers or educational applications in biology. Our results, along with a user study conducted with biology researchers, highlight the potential of DynBioSketch in enabling the direct design of dynamic visual summaries that convey relevant information, as shown in our infection case study. By bridging the gap between abstract representations used by experts and more illustrative depictions, DynBioSketch opens a new avenue for communicating biological concepts.}
}
@article{CHEN2025121691,
title = {A novel attribute reduction algorithm based on granular sequential three-way decision},
journal = {Information Sciences},
volume = {694},
pages = {121691},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121691},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524016050},
author = {Yuliang Chen and Yunlong Cheng and Binbin Luo and Yabin Shao and Mingfu Zhao and Qinghua Zhang},
keywords = {Granular computing, Sequential three-way decision, Granular rough sets, Attribute reduction},
abstract = {Attribute reduction plays a crucial role in knowledge discovery, and sequential three-way decision (S3WD) provides a new method for attribute reduction. However, the three regions of the S3WD model are usually represented as three sets, which leads to two disadvantages. On one hand, it is difficult to obtain the condition of a decision rule when multiple equivalence classes are merged into a set because different equivalence classes have different descriptions. On the other hand, if the boundary region of the upper level of S3WD is a set, one has to partition the upper level with all the acquired attributes rather than the newly added attribute. That is, there is double counting. Therefore, this paper focuses on how to retain the topology of equivalence classes in S3WD, and how to use this topology to enhance semantic interpretation and improve computational efficiency. To this end, a granular version of S3WD, called granular sequential three-way decision (GS3WD), is first developed to retain the information structure of equivalence classes. And then, three acceleration strategies and an efficient granular sequential three-way reduction (GS3WR) are proposed. Finally, a concept tree can be generated simultaneously in the process of GS3WR, and the decision rules with multi-granularity can be extracted from this concept tree directly. Experimental results show that GS3WR can obtain the same core attributes and reducts as the representative attribute reduction algorithms in rough sets and the computational efficiency is improved by hundreds of times.}
}
@article{LENG20242963,
title = {An Improved YOLOv8-Based Method for Real-Time Detection of Harmful Tea Leaves in Complex Backgrounds},
journal = {Phyton-International Journal of Experimental Botany},
volume = {93},
number = {11},
pages = {2963-2981},
year = {2024},
issn = {0031-9457},
doi = {https://doi.org/10.32604/phyton.2024.057166},
url = {https://www.sciencedirect.com/science/article/pii/S0031945724001643},
author = {Xin Leng and Jiakai Chen and Jianping Huang and Lei Zhang and Zongxuan Li},
keywords = {Harmful tea leaves, YOLO-DBD, Focal-CIoU Loss, dynamic head, Bi-Level Routing Attention},
abstract = {Tea, a globally cultivated crop renowned for its unique flavor profile and health-promoting properties, ranks among the most favored functional beverages worldwide. However, diseases severely jeopardize the production and quality of tea leaves, leading to significant economic losses. While early and accurate identification coupled with the removal of infected leaves can mitigate widespread infection, manual leaves removal remains time-consuming and expensive. Utilizing robots for pruning can significantly enhance efficiency and reduce costs. However, the accuracy of object detection directly impacts the overall efficiency of pruning robots. In complex tea plantation environments, complex image backgrounds, the overlapping and occlusion of leaves, as well as small and densely harmful leaves can all introduce interference factors. Existing algorithms perform poorly in detecting small and densely packed targets. To address these challenges, this paper collected a dataset of 1108 images of harmful tea leaves and proposed the YOLO-DBD model. The model excels in efficiently identifying harmful tea leaves with various poses in complex backgrounds, providing crucial guidance for the posture and obstacle avoidance of a robotic arm during the pruning process. The improvements proposed in this study encompass the Cross Stage Partial with Deformable Convolutional Networks v2 (C2f-DCN) module, Bi-Level Routing Attention (BRA), Dynamic Head (DyHead), and Focal Complete Intersection over Union (Focal-CIoU) Loss function, enhancing the model’s feature extraction, computation allocation, and perception capabilities. Compared to the baseline model YOLOv8s, mean Average Precision at IoU 0.5 (mAP0.5) increased by 6%, and Floating Point Operations Per second (FLOPs) decreased by 3.3 G.}
}
@article{WANG2023120782,
title = {A closed-loop analysis approach for ensuring stormwater source control design solution to achieve the intended goals},
journal = {Water Research},
volume = {247},
pages = {120782},
year = {2023},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2023.120782},
url = {https://www.sciencedirect.com/science/article/pii/S0043135423012228},
author = {Sheng Wang and Lidan Feng and Yezi Yuan},
keywords = {Sponge city, Stormwater source control, Closed-loop analysis, Bioretention, Runoff frequency spectrum},
abstract = {Stormwater source controls have been adopted worldwide to address hydrological and environmental impairments caused by the spread of impervious surfaces in cities. Current design method in China uses 30-year daily rainfall records to generate relationship of rainfall volume capture ratio (αg) and daily design storm, and then uses design storm to propose design solution. However, source control performance differs from rain to rain, and hence the design solution's actual effect may deviate from αg. Borrowing closed-loop feedback concept from business domain, this study proposes closed-loop analysis (CLA) which uses design solution's 30-year simulated result as data feedback to check design solution's effectiveness and then make improvements if necessary. It consists of four methods: 1) hourly design storm statistical method, for addressing the weakness of current daily design storm; 2) design solution model credibility examination method, for guaranteeing credibility of 30-year simulated results for CLA; 3) appropriate design storms determination method for source control without underdrain; 4) additional design parameters optimization method for source control with underdrain. Taking Xiamen city for example, case study results shows that design solution's 30-year simulated results were consistent/comparable with sizing calculation formula that was used to propose design solution, and therefore they were credible for CLA. Appropriate design storms ensured design solutions without underdrain to achieve the intended αg±3 %. Optimal design parameters combinations ensured design solutions with underdrain to achieve αg but also restore natural runoff events with pre- and post-development runoff frequency spectra similarity being 0.670–0.691. Based on stormwater mathematical model, CLA can drive source control design computation to a new methodological stage.}
}
@article{SIGALA2018151,
title = {New technologies in tourism: From multi-disciplinary to anti-disciplinary advances and trajectories},
journal = {Tourism Management Perspectives},
volume = {25},
pages = {151-155},
year = {2018},
issn = {2211-9736},
doi = {https://doi.org/10.1016/j.tmp.2017.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S2211973617301435},
author = {Marianna Sigala},
abstract = {Technologies transform tourism management and marketing from a static and utilitarian sense (whereby managers and tourists use technologies as tools) to a transformative conceptualization whereby tourism markets and actors both shape and are shaped by technology. This paper unravels the transformative power of technologies on: the tourism actors and resources (both the traditional but also new actors, i.e. the technology agents); the ways actors interact to (co-)create but also (co-)destruct tourism value; and the context in which tourism actors interact from a linear supply chain tourism ‘industry’ to a complex socio-technical smart tourism ecosystem. To study such complex phenomena and transformations, the paper emphasises that research should not only adopt a multi-disciplinary approach, but it also needs to follow an anti-disciplinary thinking whereby new knowledge and constructs do not simply fall within existing paradigms, disciplinary silos and mindsets once developed by studying the ‘pure’ humans and their behaviours.}
}
@article{FU2013729,
title = {Expert representation of design repository space: A comparison to and validation of algorithmic output},
journal = {Design Studies},
volume = {34},
number = {6},
pages = {729-762},
year = {2013},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2013.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X13000495},
author = {Katherine Fu and Joel Chan and Christian Schunn and Jonathan Cagan and Kenneth Kotovsky},
keywords = {computer supported design, design by analogy, design methods, engineering design},
abstract = {Development of design-by-analogy tools is a promising design innovation research avenue. Previously, a method for computationally structuring patent databases as a basis for an automated design-by-analogy tool was introduced. To demonstrate its strengths and weaknesses, a computationally-generated structure is compared to four expert designers' mental models of the domain. Results indicate that, compared to experts, the computationally-generated structure is sensible in clustering of patents and organization of clusters. The computationally-generated structure represents a space in which experts can find common ground/consensus – making it promising to be intuitive/accessible to broad cohorts of designers. The computational method offers a resource-efficient way of usefully conceptualizing the space that is sensible to expert designers, while maintaining an element of unexpected representation of the space.}
}
@article{WILSON1997575,
title = {Computation and controversy: Value conflicts and social choices: R. KLING (Ed.) 2nd ed. Academic Press, New York (1996). xxiv + 961 pp., ISBN 0-12-415040-3},
journal = {Information Processing & Management},
volume = {33},
number = {4},
pages = {575-577},
year = {1997},
issn = {0306-4573},
doi = {https://doi.org/10.1016/S0306-4573(97)82727-6},
url = {https://www.sciencedirect.com/science/article/pii/S0306457397827276},
author = {Tom Wilson}
}
@article{MOGILNER2019R915,
title = {Alex Mogilner},
journal = {Current Biology},
volume = {29},
number = {19},
pages = {R915-R917},
year = {2019},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2019.07.077},
url = {https://www.sciencedirect.com/science/article/pii/S0960982219309571},
author = {Alex Mogilner}
}