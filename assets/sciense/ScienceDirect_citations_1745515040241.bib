@article{HONG2024422,
title = {AF-FTTSnet: An end-to-end two-stream convolutional neural network for online quality monitoring of robotic welding},
journal = {Journal of Manufacturing Systems},
volume = {74},
pages = {422-434},
year = {2024},
issn = {0278-6125},
doi = {https://doi.org/10.1016/j.jmsy.2024.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0278612524000724},
author = {Yuxiang Hong and Xingxing He and Jing Xu and Ruiling Yuan and Kai Lin and Baohua Chang and Dong Du},
keywords = {Welding quality monitoring, Visual sensing, Molten pool, Defect prediction, Two-stream network},
abstract = {Online welding quality monitoring (WQM) is crucial for intelligent welding, and deep learning approaches considering spatiotemporal features for WQM tasks show great potential. However, one of the important challenges for existing approaches is to balance the spatiotemporal representation learning capability and computational efficiency, which makes it challenging to adapt welding processes with complex and drastic molten pool dynamic behavior. This paper proposes a novel approach for WQM using molten pool visual sensing and deep learning considering spatiotemporal features, the proposed deep learning network called attention fusion based frame-temporality two-stream network (AF-FTTSnet). Firstly, a passive vision sensor is used to acquire continuous dynamic molten pool images. Meanwhile, temporal difference images are computed to provide novel features and temporal representations. Then, a two-stream feature extraction module is designed to concurrently extract rich spatiotemporal features from molten pool images and temporal difference images. Finally, an attention fusion module with the ability to automatically identify and weight the most relevant features is designed to achieve optimal fusion of the two-stream features. The shop welding experimental results indicate that the proposed AF-FTTSnet model can effectively and robustly recognize five typical welding states during helium arc welding, with an accuracy of 99.26%. This model has been demonstrated to exhibit significant performance improvements compared to mainstream temporal sequence models. Available: https://github.com/Just199806/TSCNN/tree/master.}
}
@article{SALINGER1994139,
title = {Massively parallel finite element computations of three-dimensional, time-dependent, incompressible flows in materials processing systems},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {119},
number = {1},
pages = {139-156},
year = {1994},
issn = {0045-7825},
doi = {https://doi.org/10.1016/0045-7825(94)00081-6},
url = {https://www.sciencedirect.com/science/article/pii/0045782594000816},
author = {Andrew G. Salinger and Qiang Xiao and Yuming Zhou and Jeffrey J. Derby},
abstract = {A parallel implementation of the Galerkin finite element method for three-dimensional, incompressible flows is presented. The inherent element-by-element parallelism of the method is exploited to make efficient use of the architecture of the CM-5 computer. Our implementation features a mixed formulation to expand the primitive variables using triquadratic brick elements with linear, discontinuous pressure basis functions, and the GMRES method with diagonal preconditioning is employed to solve the linear system at each Newton iteration. Transitions among flow states in the classical Taylor-Couette system, which are representative of the complexity of flows found in materials processing systems, are computed as benchmark solutions, and preliminary results are presented for flow in a large-scale, solution crystal growth system. Sustained calculation rates of up to 6 GigaFLOPS are achieved on 512 processors of the CM-5.}
}
@article{LAI2023101343,
title = {Optimization of urban and rural ecological spatial planning based on deep learning under the concept of sustainable development},
journal = {Results in Engineering},
volume = {19},
pages = {101343},
year = {2023},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2023.101343},
url = {https://www.sciencedirect.com/science/article/pii/S259012302300470X},
author = {Yilin Lai},
keywords = {Sustainable development, Spatial planning, Remote sensing images, CNN, GPU},
abstract = {At present, the speed of urbanization in China is constantly accelerating. At the same time, due to the severe situation of tight resource constraints, severe environmental pollution, and ecosystem degradation, vigorously promoting the construction of ecological civilization has become a key planning direction. However, traditional urban and rural ecological spatial planning is influenced by factors such as region, terrain, and spatial scale, which cannot adapt to the current spatial planning requirements. To achieve sustainable urban and rural ecological spatial planning, we propose a method that uses the optimized remote sensing images and convolutional neural networks to achieve spatial planning. In the analysis of the application effect of the usage method, the experimental results show that increasing the amount of data such as image size can improve the execution performance of the computer when the computer is not fully utilizing its resources and its computational volume fails to saturate the computational capacity. The parallel configuration designed in this experiment can accelerate the performance of the computer better, and the acceleration effect becomes more obvious as the difficulty of the algorithm increases. The Faster RCNN algorithm proposed in this experiment has the highest retrieval accuracy in the Flickr30K dataset and MS-COCO dataset compared with other algorithms. In Flickr30k data set, compared with other models in the table, the model used in this paper has the highest retrieval accuracy. The retrieval accuracy of R@1, R@5, R@10 increased by 23.1%, 8.1% and 5.3%, respectively. In MS-COCO data set, the retrieval accuracy increased by 19.2%, 13.1% and 8.3% respectively. The above results confirm that the combination of remote sensing images and convolutional neural network technology can perform simple ecological planning of a city's urban and rural areas, which proves that the method proposed in this experiment has practicality.}
}
@incollection{ASCHEID200711,
title = {Chapter 2 - Opportunities for Application-Specific Processors: The Case of Wireless Communications},
editor = {Paolo Lenne and Rainer Leupers},
booktitle = {Customizable Embedded Processors},
publisher = {Morgan Kaufmann},
address = {Burlington},
pages = {11-37},
year = {2007},
series = {Systems on Silicon},
issn = {18759661},
doi = {https://doi.org/10.1016/B978-012369526-0/50003-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780123695260500036},
author = {Gerd Ascheid and Heinrich Meyr},
abstract = {Publisher Summary
A paradigm change in designing complex systems-on-chip (SoCs) occurs roughly every 12 years because of the exponentially increasing number of transistors on a chip. This paradigm change is characterized by a move to a higher level of abstraction. Instead of thinking in register-transfer level (RTL) blocks and wires, computing elements and interconnect are needed to be thought. The next design discontinuity will lead to different solutions, depending on the application. The following core propositions for wireless communications are made: future SoC for wireless communications will be heterogeneous, reconfigurable Multi-Processor System-on-Chip (MPSoC). They will contain computational elements that cover the entire spectrum, from fixed functionality blocks to domain-specific DSPs and general-purpose processors. A key role will be played by ASIPs. ASIPs exploit the full architectural space (memory, interconnect, instruction set, parallelism), so they are optimally matched to a specific task. The heterogeneous computational elements will communicate via a network-on-chip (NoC), as the conventional bus structures do not scale. These MPSoC platforms will be designed by a cross-disciplinary team. This chapter substantiates this proposition. It begins by analyzing the properties of future wireless communication systems and observes that the systems are computationally demanding. Furthermore, they need innovative architectural concepts to be energy efficient. The chapter discusses the canonical structure of a digital receiver for wireless communication and addresses the design of ASIPs.}
}
@article{JONES2013122,
title = {Understanding the integral: Students’ symbolic forms},
journal = {The Journal of Mathematical Behavior},
volume = {32},
number = {2},
pages = {122-141},
year = {2013},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2012.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732312312000612},
author = {Steven R. Jones},
keywords = {Calculus, Integral, Student understanding, Undergraduate mathematics education, Symbolic form, Accumulation},
abstract = {Researchers are currently investigating how calculus students understand the basic concepts of first-year calculus, including the integral. However, much is still unknown regarding the cognitive resources (i.e., stable cognitive units that can be accessed by an individual) that students hold and draw on when thinking about the integral. This paper presents cognitive resources of the integral that a sample of experienced calculus students drew on while working on pure mathematics and applied physics problems. This research provides evidence that students hold a variety of productive cognitive resources that can be employed in problem solving, though some of the resources prove more productive than others, depending on the context. In particular, conceptualizations of the integral as an addition over many pieces seem especially useful in multivariate and physics contexts.}
}
@article{YUAN2025129490,
title = {Global attention network with rain prior for real time single image deraining},
journal = {Neurocomputing},
volume = {625},
pages = {129490},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129490},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225001626},
author = {Yuan Yuan and Xuanbin Guo and Dandan Ma},
keywords = {Rain removal, Deep learning, Attention mechanism, Activation function},
abstract = {Poor visibility caused by rainy image can have a negative impact on the performance of computer vision applications. While several image deraining algorithms have been popularly adopted, most of them suffer from two main limitations: (1) they cannot well handle real and complex rain scenes by only focusing on one type of rain in images (e.g. raindrops or rain streaks) whereas the reality often coexists with both types, (2) they face significant difficulties in practical application because of ignoring the speed of inference. To address the above problems, we propose a global attention network (GANet) that can quickly and effectively separate rain streaks and raindrops. Inspired by the fact that rain in images often appears white, we leverage this prior to obtain an initial rain-free background image to guide neural network-based image deraining. Moreover, a new global attention block (GAB) is designed to simultaneously extract the rain features from spatial and channel dimensions. By cascading multiple GABs, the proposed method can effectively obtain the features of rain streaks and raindrops and progressively separates the rain-free image. Furthermore, owing to the nonlinear properties of GAB, the activation functions are omitted, which can speed up the inference time. And the depth-wise and point-wise convolutions are employed to promote computation efficiency as well. Extensive experiments on raindrop and rain streak datasets demonstrate that our method outperforms state-of-the-art methods, achieving up to 37.53 dB PSNR on Rain100L with an inference speed of 39 FPS, which is 2–30 times faster than competitors.}
}
@article{YANG2022101239,
title = {Identifying keyword sleeping beauties: A perspective on the knowledge diffusion process},
journal = {Journal of Informetrics},
volume = {16},
number = {1},
pages = {101239},
year = {2022},
issn = {1751-1577},
doi = {https://doi.org/10.1016/j.joi.2021.101239},
url = {https://www.sciencedirect.com/science/article/pii/S1751157721001103},
author = {Jinqing Yang and Yi Bu and Wei Lu and Yong Huang and Jiming Hu and Shengzhi Huang and Li Zhang},
keywords = {Sleeping beauty, Delayed recognition, Knowledge diffusion trajectory, Survival analysis},
abstract = {Knowledge diffusion is a significant driving force behind discipline development and technological innovation. Keyword is a unique knowledge diffusion trajectory, in which the sleeping beauty phenomenon sometimes appears. In this paper, we first put forward the concept of Keyword Sleeping Beauties (KSBs) on the basis of the scientific literature phenomenon of sleeping beauties. Then, we construct a parameter-free identification method to distinguish KSBs based on beauty coefficient criteria. Furthermore, we analyze the intrinsic and extrinsic influencing factors to explore the awakening mechanism of KSBs. The experiment results show that sleeping beauty phenomena also exist in the keyword diffusion trajectory and 284 KSBs are identified. The depth of sleep has a positive correlation with awakening intensity, while the length of sleep has a negative correlation with awakening intensity. In the two years of pre-awakening, KSBs tend to appear in the journals with a higher impact factor. In addition, the adoption frequency and the number of KSBs both increase obviously in the one year of pre-awakening. The findings of this paper enrich the patterns of knowledge diffusion and extend academic thinking on the sleeping beauty in science.}
}
@article{WU2024112197,
title = {A multi-strategy three-way decision approach for tri-state risk loss under q-rung orthopair fuzzy environment},
journal = {Applied Soft Computing},
volume = {167},
pages = {112197},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.112197},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624009712},
author = {Ping Wu and Yihua Zhong and Chuan Chen and Yanlin Wang and Chao Min},
keywords = {Three-way decision, q-rung orthopair fuzzy sets, Tri-state risk loss, Multi-strategy perspective, Threshold theorem},
abstract = {Addressing the decision-making challenge arising from the uncertainty of human cognition, three-way decision (3WD) and q-rung orthopair fuzzy sets (q-ROFSs) are integrated in this paper to propose a multi-strategy three-way decision approach (MS3WDA) for tri-state risk loss (TSRL) under q-rung orthopair fuzzy environment. Based on the ternary thinking of human cognition, the risk loss with hesitation state is considered and constructed under q-rung orthopair fuzzy environment. The TSRL with hesitation state is further constructed by combining the q-rung orthopair fuzzy (q-ROF) information. The conditional probability adopted by the original object classes is improved and extended by the three components of q-ROFSs. Next, the TSRL with q-ROF information and three components of q-ROFSs are integrated with decision-theoretic rough sets (DTRSs) to establish a novel 3WD model. Some relevant properties are also analyzed and discussed for the developed 3WD model. Then, its multi-strategy decision method is proposed based on the multi-strategy perspective. The related strategies with five different levels are designed by considering three different risk appetite perspectives and four different aspects of q-ROF information. The relevant threshold theorems are also given and proved to further provide the theoretical support for our MS3WDA. According to the five different strategies, we further derive the corresponding decision rules of MS3WDA. The key steps and specific algorithm are summarized for MS3WDA. Finally, a case study is provided to demonstrate the practicability and feasibility of MS3WDA. Meanwhile, the rationality, robustness and superiority of MS3WDA are further validated by the sensitivity analysis and comparative analysis.}
}
@article{ADHIKARI20121374,
title = {Multi-commodity network flow models for dynamic energy management – Smart Grid applications},
journal = {Energy Procedia},
volume = {14},
pages = {1374-1379},
year = {2012},
note = {2011 2nd International Conference on Advances in Energy Engineering (ICAEE)},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2011.12.1104},
url = {https://www.sciencedirect.com/science/article/pii/S1876610211045243},
author = {R.S. Adhikari and N. Aste and M. Manfren},
keywords = {Dynamic energy management, Smart Grid, Multi-commodity network flow models},
abstract = {The strong interconnection between human activities, energy use and pollution reduction strategies in contemporary society has determined the necessity of collecting scientific knowledge from different fields to provide useful methods and models to foster the transition towards more sustainable energy systems. This is a challenging task in particular for contemporary communities where an increasing demand for services is combined with rapidly changing lifestyles and habits. The Smart Grid concept is the result of a confluence of issues and a convergence of objectives, which include national energy security, climate change, pollution reduction, grid reliability, etc. While thinking about a paradigm shift in energy systems, drivers, characteristics, market segments, applications and other interconnected aspects must be taken into account simultaneously. In this context, the use of multi-commodity network flow models for dynamic energy management aims at finding a compromise between model usefulness, accuracy, flexibility, solvability and scalability in Smart Grid applications.}
}
@article{UDDIN2021106,
title = {Application of Theory in Chronic Pain Rehabilitation Research and Clinical Practice},
journal = {The Open Sports Sciences Journal},
volume = {14},
pages = {106-113},
year = {2021},
issn = {1875-399X},
doi = {https://doi.org/10.2174/1875399X02114010106},
url = {https://www.sciencedirect.com/science/article/pii/S1875399X2100013X},
author = {Zakir Uddin and Joy C. MacDermid and Fatma A. Hegazy and Tara L. Packham},
keywords = {Chronic pain , Hypersensitivity , Theory , Rehabilitation , Disability , T-cell},
abstract = {Introduction
Chronic pain has multiple aetiological factors and complexity. Pain theory helps us to guide and organize our thinking to deal with this complexity. The objective of this paper is to critically review the most influential theory in pain science history (the gate control theory of pain) and focus on its implications in chronic pain rehabilitation to minimize disability.
Methods
In this narrative review, all the published studies that focused upon pain theory were retrieved from Ovoid Medline (from 1946 till present), EMBAS, AMED and PsycINFO data bases.
Results
Chronic pain is considered a disease or dysfunction of the nervous system. In chronic pain conditions, hypersensitivity is thought to develop from changes to the physiological top-down control (inhibitory) mechanism of pain modulation according to the pain theory. Pain hypersensitivity manifestation is considered as abnormal central inhibitory control at the gate controlling mechanism. On the other hand, pain hypersensitivity is a prognostic factor in pain rehabilitation. It is clinically important to detect and manage hypersensitivity responses and their mechanisms.
Conclusion
Since somatosensory perception and integration are recognized as a contributor to the pain perception under the theory, then we can use the model to direct interventions aimed at pain relief. The pain theory should be leveraged to develop and refine measurement tools with clinical utility for detecting and monitoring hypersensitivity linked to chronic pain mechanisms.}
}
@incollection{MARR1988534,
title = {A computational theory of human stereo vision††M.I.T. Psychology Department, 79 Amherst Street, Cambridge Ma 02139, U.S.A.},
editor = {Allan Collins and Edward E. Smith},
booktitle = {Readings in Cognitive Science},
publisher = {Morgan Kaufmann},
pages = {534-547},
year = {1988},
isbn = {978-1-4832-1446-7},
doi = {https://doi.org/10.1016/B978-1-4832-1446-7.50046-7},
url = {https://www.sciencedirect.com/science/article/pii/B9781483214467500467},
author = {D. MARR and T. POGGIO}
}
@incollection{RENNE2022147,
title = {Chapter 8 - Measuring and assessing resilience},
editor = {John L. Renne and Brian Wolshon and Anurag Pande and Pamela Murray-Tuite and Karl Kim},
booktitle = {Creating Resilient Transportation Systems},
publisher = {Elsevier},
pages = {147-192},
year = {2022},
isbn = {978-0-12-816820-2},
doi = {https://doi.org/10.1016/B978-0-12-816820-2.00005-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128168202000050},
author = {John L. Renne and Brian Wolshon and Anurag Pande and Pamela Murray-Tuite and Karl Kim},
keywords = {Resilience, Measurement, Assessment, Goals, Metrics, Data collection},
abstract = {The ability to effectively apply resilience-oriented thinking into practice starts with understanding, measuring, and evaluating the benefits and costs of resilience. With this knowledge it also becomes possible to comparatively assess potential planning, design, and maintenance options to plan and allocate financial and personnel resources most effectively to address needs. Other key components of practical and meaningful measurements and assessments of resilience are establishing metrics that quantify its performance and knowing what and how much data to collect. Then, understanding what these data mean so that goals, objectives, and expectations of resilience can be set, both within transportation organizations and for the consumers of the services they provide. Unfortunately, there is no universal agreement on what resilience even is, let alone how to systematically measure and assess it. However, recent reviews of practice and research show that ideas and methods to evaluate and assess resilience are evolving at a rapid pace, both within and outside of transportation. This chapter presents a summary of these ideas and compares and contrasts the effort they require to implement and the benefits they are expected to bring.}
}
@article{LUO2023101957,
title = {A design model of FBS based on interval-valued Pythagorean fuzzy sets},
journal = {Advanced Engineering Informatics},
volume = {56},
pages = {101957},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.101957},
url = {https://www.sciencedirect.com/science/article/pii/S147403462300085X},
author = {Yuhan Luo and Minna Ni and Feng Zhang},
keywords = {FBS model, Pythagorean fuzzy sets, AHP, HOQ},
abstract = {The FBS (Function-Behaviour-Structure) model is a research model that stimulates creative thinking of designers in the design process. In order to reduce the influence of user requirement ambiguity on design results in the product design process and improve the accuracy of user requirements in the function-behavior-structure (FBS) design model, this paper proposes an interval-valued Pythagorean fuzzy set-based FBS model integrating AHP and HOQ methods. Firstly, the design model will use IVPF-AHP method to study user requirements and use interval-valued Pythagorean linguistic terms to replace the traditional scoring method of AHP to get the weight of each user requirement. Secondly, the conversion between user requirements and functions will be realized by IVPF-HOQ method, which converts customer requirements into functional characteristics and calculates the weights of each functional characteristic. Finally, the design focus will be filtered according to the order of importance of the functional characteristics, which will be used as functions to guide the development of the FBS model. In this paper, the feasibility and effectiveness of the proposed method will be verified by an application example of a hand-held fluorescence spectrometer. The results show that the proposed FBS model can effectively reduce the subjectivity and ambiguity in the decision-making process, improve the accuracy and information richness of user requirements, and effectively highlight the focus of the design study. The innovation of the proposed method is to provide a more objective and accurate innovative design method for user requirements through the integration of AHP, HOQ and FBS to effectively explore and analyze user requirements. The use of IVPFS to deal with fuzzy information in the design process in a more flexible manner can reduce the ambiguity of requirements when user data is small, and effectively improve the limitations of the FBS design model which is more subjective.}
}
@article{FALZON2006629,
title = {Using Bayesian network analysis to support centre of gravity analysis in military planning},
journal = {European Journal of Operational Research},
volume = {170},
number = {2},
pages = {629-643},
year = {2006},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2004.06.028},
url = {https://www.sciencedirect.com/science/article/pii/S0377221704005156},
author = {Lucia Falzon},
keywords = {Military, Decision analysis, Probabilistic models, Bayesian networks},
abstract = {Centre of gravity (COG) analysis is an integral and cognitively demanding aspect of military operational planning. It involves identifying the enemy and friendly COG and subsequently determining the critical vulnerabilities that have to be degraded or negated to influence the COG of each side. This paper describes a modelling framework based on the causal relationships among the critical capabilities and requirements for an operation. The framework is subsequently used as a basis for the construction, population and analysis of Bayesian networks to support a rigorous and systematic approach to COG analysis. The importance of this work is that it uses existing planning process concepts to facilitate the construction of comprehensive models in which uncertainties and subjective judgements are clearly represented, thus enabling future re-use and traceability. The visual representation of the COG causal structure helps to clarify thinking and provides a way to record and impart this thinking. Moreover, it gives planners the capability to perform impact analysis, that is, to determine which actions are most likely to achieve a desirable end-state. The paper discusses the methodology, development and implementation of the COG Network Effects Tool (COGNET) suite for model population and model checking as well as impact analysis.}
}
@article{HASSABIS2007299,
title = {Deconstructing episodic memory with construction},
journal = {Trends in Cognitive Sciences},
volume = {11},
number = {7},
pages = {299-306},
year = {2007},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2007.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S1364661307001258},
author = {Demis Hassabis and Eleanor A. Maguire},
abstract = {It has recently been observed that the brain network supporting recall of episodic memories shares much in common with other cognitive functions such as episodic future thinking, navigation and theory of mind. It has been speculated that ‘self-projection’ is the key common process. However, in this Opinion article, we note that other functions (e.g. imagining fictitious experiences) not explicitly connected to either the self or a subjective sense of time, activate a similar brain network. Hence, we argue that the process of ‘scene construction’ is better able to account for the commonalities in the brain areas engaged by an extended range of disparate functions. In light of this, we re-evaluate our understanding of episodic memory, the processes underpinning it and other related cognitive functions.}
}
@article{CRANFORD2022100638,
title = {Navigating the “Kessel Run” of digital materials acceleration},
journal = {Patterns},
volume = {3},
number = {11},
pages = {100638},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100638},
url = {https://www.sciencedirect.com/science/article/pii/S2666389922002707},
author = {Steve Cranford},
abstract = {Computational methods such as machine learning, artificial intelligence, and big data in physical sciences, particularly materials science, have been exponentially growing in terms of progress, method development, and number of studies and related publications. This aggregate momentum of the community is palpable, and many exciting discoveries are likely on the horizon. But, like all endeavors, some thought should be given to the current trajectory of the field, ensuring the full potential of the new digital space.}
}
@article{AGNOLI2020116385,
title = {Predicting response originality through brain activity: An analysis of changes in EEG alpha power during the generation of alternative ideas},
journal = {NeuroImage},
volume = {207},
pages = {116385},
year = {2020},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2019.116385},
url = {https://www.sciencedirect.com/science/article/pii/S1053811919309760},
author = {Sergio Agnoli and Marco Zanon and Serena Mastria and Alessio Avenanti and Giovanni Emanuele Corazza},
keywords = {EEG, Alpha power, Originality, Idea generation, Divergent-thinking, Temporal dynamics, Creativity},
abstract = {Growing neurophysiological evidence points to a role of alpha oscillations in divergent thinking (DT). In particular, studies have shown a consistent EEG alpha synchronization during performance on the Alternative Uses Task (AUT), a well-established DT task. However, there is a need for investigating the brain dynamics underlying the production of a sequence of multiple, alternative ideas at the AUT and their relationship with idea originality. In twenty young adults, we investigated changes in alpha power during performance on a structured version of the AUT, requiring to ideate four alternative uses for conventional objects in distinct and sequentially balanced time periods. Data analysis followed a three-step approach, including behaviour aspects, physiology aspects, and their mutual relationship. At the behavioural level, we observed a typical serial order effect during DT production, with an increase of originality associated with an increase in ideational time and a decrease in response percentage over the four responses. This pattern was paralleled by a shift from alpha desynchronization to alpha synchronization across production of the four alternative ideas. Remarkably, alpha power changes were able to explain response originality, with a differential role of alpha power over different sensor sites. In particular, alpha synchronization over frontal, central, and temporal sites was able to predict the generation of original ideas in the first phases of the DT process, whereas alpha synchronization over centro-parietal sites persistently predicted response originality during the entire DT production. Moreover, a bilateral hemispheric effect in frontal sites and a left-lateralized effect in central, temporal, and parietal sensor sites emerged as predictors of the increase in response originality. These findings highlight the temporal dynamics of DT production across the generation of alternative ideas and support a partially distinct functional role of specific cortical areas during DT.}
}
@incollection{GARDNER2024103,
title = {Chapter 5 - Smart design for socially engaging environments},
editor = {Nicole Gardner},
booktitle = {Scaling the Smart City},
publisher = {Elsevier},
pages = {103-128},
year = {2024},
series = {Smart Cities},
isbn = {978-0-443-18452-9},
doi = {https://doi.org/10.1016/B978-0-443-18452-9.00006-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443184529000069},
author = {Nicole Gardner},
keywords = {Cyber-physical system, Design, Interaction, Physical computing, Play, Smart city, Smart cities, Social engagement, Social capital, Social interaction, Urban play, Urban technology},
abstract = {Smart city initiatives typically aim to optimize the efficiency of essential urban infrastructure and urban service delivery and performance. This chapter considers how smart technologies can also be deployed in ways to catalyze social interactions among citizens in urban public realm spaces to create socially engaging environments. Drawing on a range of concepts such as social cohesion, social capital, and object-centered sociality, this chapter considers how existing and speculative urban technology projects that combine spatial design thinking and physical computing can scaffold and amplify opportunities for social engagement. It considers how urban technology projects that mobilize tactics of proximity, curiosity, and play can create new and different ways for people to relate to each other in urban space.}
}
@incollection{SANTOS2024,
title = {Data analysis on Decision-Making},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-443-13701-3.00018-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443137013000189},
author = {Eulália Santos and Margarida F. Oliveira},
keywords = {Artificial intelligence, Business, Data analysis, Decision making, Logistics, Machine learning, Mathematical modeling operations research, Mathematical programming, Optimization, Statistic, Strategic management, Technology},
abstract = {Today, data analysis plays a vital role in identifying market trends and supporting strategic decision-making in organizations. To make an effective decision in order to obtain positive results, it is necessary not only to carefully analyze various pieces of information but also to use artificial intelligence and critical thinking. Mathematics plays an essential role in making effective decisions and providing tools and methods for analyzing, modeling and solving both simple and more complex problems.}
}
@article{EBBY200573,
title = {The powers and pitfalls of algorithmic knowledge: a case study},
journal = {The Journal of Mathematical Behavior},
volume = {24},
number = {1},
pages = {73-87},
year = {2005},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2004.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0732312304000768},
author = {Caroline Brayer Ebby},
keywords = {Algorithms, Computation, Learning, Student understanding, Sociocultural perspective, Reform curriculum},
abstract = {This study examines one child's use of computational procedures over a period of 3 years in an urban elementary school where teachers were using a standards-based curriculum. From a sociocultural perspective, the use of standard algorithms to solve mathematical problems is viewed as a cultural tool that both enables and constrains particular practices. As this student appropriated and mastered procedures for addition, subtraction, multiplication and division, she could solve problems that involved fairly straightforward computations or where she could easily model the action to determine an appropriate computation. At the same time, her use of these algorithms, along with other readily available tools, such as her fingers or multiplication tables, constrained her ability to reflect on the tens-structure of the number system, an effect that had serious consequences for her overall mathematical achievement. The results of this study suggest that even when not directly introduced, algorithms have such strong currency that they can mediate more reform-oriented instruction.}
}
@incollection{KURGANSKAYA2024760,
title = {Multi-scale modeling of crystal-fluid interactions: State-of-the-art, challenges and prospects},
editor = {Klaus Wandelt and Gianlorenzo Bussetti},
booktitle = {Encyclopedia of Solid-Liquid Interfaces (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {760-792},
year = {2024},
isbn = {978-0-323-85670-6},
doi = {https://doi.org/10.1016/B978-0-323-85669-0.00034-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323856690000349},
author = {I. Kurganskaya and R.D. Rohlfs and A. Luttge},
keywords = {Crystal-water interface, Electric double layer, Grand canonical Monte Carlo, Kinetic Monte Carlo, Kinetics, Mineral–water interface, Parameterization, Reaction pathways, Reaction probability, Reaction rates, Statistical mechanics of interfaces, Stepwave, Stochastic model, Upscaling, Voronoi},
abstract = {We describe theoretical and conceptual approaches to treat crystal-fluid interactions across the scales in the communities studying mineral-fluid interactions for a variety of purposes, from understanding fundamental principles to geological reservoir characterization and environmental mitigation. We delineate basics of theory, recent breakthroughs, and challenges in modeling approaches from the atomistic scale to the mesoscale. Quantum Mechanics, Molecular Dynamics, Kinetic Monte Carlo and Voronoi computational geometry are covered. We discuss possible theoretical and conceptual developments to overcome those challenges toward more reliable predictive models. A special attention is given to the development of interfaces between the techniques addressing different scales.}
}
@article{GRANJO202021,
title = {Enhancing the autonomy of students in chemical engineering education with LABVIRTUAL platform},
journal = {Education for Chemical Engineers},
volume = {31},
pages = {21-28},
year = {2020},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2020.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S174977282030018X},
author = {José F.O. Granjo and Maria G. Rasteiro},
keywords = {Web platform, Virtual labs, Chemical processes, Autonomous learning},
abstract = {Engineering educators have been developing different approaches to supplement scientific background and further develop the ability for autonomous and critical thinking in students. In 2009, the University of Coimbra has made available on-line a virtual platform with a wide scope, directed towards Chemical Engineering education. The platform is divided into four different educational topics: Unit Operations and Separations, Chemical Reaction, Process Systems Engineering and Biological Processes. These sections include simulators, applications, and case studies to help understanding chemical/biochemical processes and improve their autonomy. This paper presents an assessment of the use of that platform by two different groups of students in the school years of 2015/2016 and 2018/2019: a group from the 3rd-year of Chemical Engineering, and another one from a Project Design course (2nd cycle, MSc of Chemical Engineering). A case study addressing the synthesis of phthalic anhydride by o-xylene oxidation on a fixed-bed catalytic reactor is also given to show the use of existing simulators in LABVIRTUAL.}
}
@article{SZYMANSKI2021,
title = {Words Are Essential, but Underexamined, Research Tools for Microbes and Microbiomes},
journal = {mSystems},
volume = {6},
number = {4},
year = {2021},
issn = {2379-5077},
doi = {https://doi.org/10.1128/msystems.00769-21},
url = {https://www.sciencedirect.com/science/article/pii/S2379507721002683},
author = {Erika Szymanski},
keywords = {discourse, engineering, metaphor, microbiome, science and technology studies, synthetic biology, synthetic yeast},
abstract = {Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive.
ABSTRACT
Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive. My research addresses this deficit by examining how metaphors for handling microbes shape possibilities for working with yeast and bacteria in synthetic biology, microbiome research, and other fields that reconfigure what microbes can be. Though poised to reexamine assumptions, these fields routinely rest on metaphors and other language tools that quietly embed ways of thinking that may work against wider aims—for example, imagining bacteria as imperfect machines that should therefore be rendered increasingly passive and controllable. Researchers, therefore, need to examine how language tools structure their observations and expectations so that the tools they choose are appropriate for the work they want to do.}
}
@article{RANDALL1991219,
title = {Review of linear least squares computations: by R.W. Farebrother},
journal = {Linear Algebra and its Applications},
volume = {153},
pages = {219-223},
year = {1991},
issn = {0024-3795},
doi = {https://doi.org/10.1016/0024-3795(91)90221-H},
url = {https://www.sciencedirect.com/science/article/pii/002437959190221H},
author = {John H. Randall}
}
@article{GARDECKI2018138,
title = {Innovative Internet of Things-reinforced Human Recognition for Human-Machine Interaction Purposes},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {6},
pages = {138-143},
year = {2018},
note = {15th IFAC Conference on Programmable Devices and Embedded Systems PDeS 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.07.143},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318308875},
author = {Arkadiusz Gardecki and Michal Podpora and Aleksandra Kawala-Janik},
keywords = {Human-Machine Interaction, Internet of Things, Human Recognition, Humanoid Robots, Human Identification},
abstract = {Accurate and reliable human recognition and parametrisation have always been an important challenge in efficient Man-Machine Interaction. A humanoid robot is able to offer a much richer and more natural behaviour and human-like communication, but only if the robot possesses sufficient knowledge about the interlocutor, such as inter alia: gender, age, mood, behaviour data, interaction history. In this paper authors introduced an innovative conception in Human-Machine Interaction, where instead of thinking about an interaction as an event (which uses and produces information) an innovative point of view was proposed, where the interaction is just an event in a continuous flow of information. The difference, once perceived, results in an astounding change of conception, as well as a whole new set of ideas. The human detection, information acquisition, human recognition – can be performed earlier, before a human reaches the humanoid robot, also the history of interactions and possible interests of the interlocutor can be predicted before they would even start the conversation. This paper contains a detailed analysis of the proposed environment-based approach to interaction, as well as the Internet of Things-reinforced information acquisition.}
}
@article{NSSSN2024106769,
title = {VNSMAS: A constraint-based portfolio profit maximization},
journal = {Computers & Operations Research},
volume = {170},
pages = {106769},
year = {2024},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2024.106769},
url = {https://www.sciencedirect.com/science/article/pii/S0305054824002417},
author = {Usha Devi N.S.S.S.N. and R. Mohan},
keywords = {GAN, Reinforcement learning, Stock, Fuzzy},
abstract = {Stock trading has a more significant influence on the global economy. Stock trading with portfolio optimization became challenging due to the complexity of analyzing the high variance in time series stock data. Efficient portfolio management increases profit and avoids risky situations when investing. The present work aims to model a Variable Neighborhood Search Multi-Agent System for Portfolio Optimization (VNSMASPPO) to optimize the profit on defined trading constraints on buying, selling, and holding trading decisions. This work proposes a novel Variable Neighborhood Search-based Multi-Agent System (VNASMAS) algorithm for profit computation with a constraint-based multi-agent system. The stock price history experimental data sets are collected from 8th August 2016 to 31st March 2023 with 14,567 records. The proposed model achieved an RMSE of 10.11, MAE of 2.75, and MAPE of 0.017, outperforming the literature models. VNSMASPPO maximizes the portfolio profit and is a reliable, adaptable approach.}
}
@incollection{CALVERT201369,
title = {Chapter 3 - Social Dimensions of Microbial Synthetic Biology},
editor = {Colin Harwood and Anil Wipat},
series = {Methods in Microbiology},
publisher = {Academic Press},
volume = {40},
pages = {69-86},
year = {2013},
booktitle = {Microbial Synthetic Biology},
issn = {0580-9517},
doi = {https://doi.org/10.1016/B978-0-12-417029-2.00003-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780124170292000030},
author = {Jane Calvert and Emma Frow},
keywords = {Anticipation, Governance, Public engagement, Public good, Responsible innovation, Social dimensions, Science and technology studies, Synthetic biology},
abstract = {In this chapter, we outline a number of foundational ideas that underpin our approach to the study of the social, ethical, legal and philosophical dimensions of synthetic biology. We describe these through a series of important shifts that have taken place over the past few decades of social science research. We suggest a move away from discussions centred around ethical ‘implications’, speculations about the future and concerns about risk, regulation and public acceptance, towards a conversation that talks in terms of social ‘dimensions’, anticipating the future, managing uncertainty, tools of governance and research for the public good. We argue that these seemingly subtle changes in vocabulary open up a new and productive space for thinking about the social dimensions of synthetic biology.}
}
@article{VASILE201177,
title = {Entry points, interests and attitudes. An integrative approach of learning},
journal = {Procedia - Social and Behavioral Sciences},
volume = {11},
pages = {77-81},
year = {2011},
note = {Teachers for the Knowledge Society},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2011.01.037},
url = {https://www.sciencedirect.com/science/article/pii/S1877042811000395},
author = {Cristian Vasile},
keywords = {multiple intelligence, entry points, personality, interests},
abstract = {The relationship between personality and intelligence is of a major importance in the learning process. Interests and attitudes are related to the entry points on emotional ground. In some educational systems the focus on cognitive abilities and cognitive functions increased, amplified by the neuroscience and the computational approach. The cognitive approach should be enriched with major aspects from the global human psychological system like interests/motivation, emotional profile, attitudes and so on. The focus on cognition only, or the computational view should be completed with personality approaches and behavior regulation, all of these influencing without doubt the intelligence.}
}
@article{FOSGERAU2021109911,
title = {Some remarks on CCP-based estimators of dynamic models},
journal = {Economics Letters},
volume = {204},
pages = {109911},
year = {2021},
issn = {0165-1765},
doi = {https://doi.org/10.1016/j.econlet.2021.109911},
url = {https://www.sciencedirect.com/science/article/pii/S0165176521001889},
author = {Mogens Fosgerau and Emerson Melo and Matthew Shum and Jesper R.-V. Sørensen},
keywords = {Dynamic discrete choice, Random utility, Linear programming, Convex analysis, Convex optimization},
abstract = {This note provides several remarks relating to the conditional choice probability (CCP) based estimation approaches for dynamic discrete-choice models. Specifically, the Arcidiacono and Miller (2011) estimation procedure relies on the ”inverse-CCP” mapping ψp from CCPs to choice-specific value functions. Exploiting the convex-analytic structure of discrete choice models, we discuss two approaches for computing this mapping, using either linear or convex programming, for models where the utility shocks can follow arbitrary parametric distributions. Furthermore, the ψ function is generally distinct from the ”selection adjustment” term (i.e. the expectation of the utility shock for the chosen alternative), so that computational approaches for computing the latter may not be appropriate for computing ψ.}
}
@article{BEYNON2008476,
title = {Experimenting with computing},
journal = {Journal of Applied Logic},
volume = {6},
number = {4},
pages = {476-489},
year = {2008},
note = {The Philosophy of Computer Science},
issn = {1570-8683},
doi = {https://doi.org/10.1016/j.jal.2008.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S157086830800044X},
author = {Meurig Beynon and Steve Russ},
keywords = {Empirical Modelling, Observation, Experiment, Computing, Theory, Radical empiricism, Dependency, Agency},
abstract = {We distinguish two kinds of experimental activity: post-theory and exploratory. Post-theory experiment enjoys computer support that is well-aligned to the classical theory of computation. Exploratory experiment, in contrast, arguably demands a broader conception of computing. Empirical Modelling (EM) is proposed as a more appropriate conceptual framework in which to provide computational support for exploratory experiment. In the process, it promises to provide integrated computational support for both exploratory and post-theory experiment. We first sketch the motivation for EM and illustrate its potential for supporting experimentation, then briefly highlight the semantic challenge it poses and the philosophical implications.}
}
@article{MARTINI2022105446,
title = {R_IC: A novel and versatile implementation of the index of connectivity in R},
journal = {Environmental Modelling & Software},
volume = {155},
pages = {105446},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105446},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222001529},
author = {Lorenzo Martini and Tommaso Baggio and Loris Torresani and Stefano Crema and Marco Cavalli},
keywords = {Sediment connectivity, Geomorphometry, R_IC, Open-source},
abstract = {Sediment connectivity is the capability of a system to regulate the exchange of sediment in catchments. The Index of Connectivity (IC) has become a widely used tool, offering a practical way to assess sediment connectivity from hillslopes to downstream channels. We present a novel implementation of IC in R environment to expand the audience of users and encourage alternative applications of the index. The R_IC is an open-source and freely available tool composed by three codes. Standard R_IC runs the IC and it represents the core of the other variants. Custom R_IC offers a more flexible script, allowing the computation of alternative weighting factors and the possibility of running a further profile IC analysis. Batch R_IC performs batch processing of the index. For each code variant, a geomorphological application is presented to illustrate how the R_IC could be used in watershed management and practical issues related to sediment dynamics.}
}
@incollection{KIM2009332,
title = {Spatial Data Mining, Geovisualization},
editor = {Rob Kitchin and Nigel Thrift},
booktitle = {International Encyclopedia of Human Geography},
publisher = {Elsevier},
address = {Oxford},
pages = {332-336},
year = {2009},
isbn = {978-0-08-044910-4},
doi = {https://doi.org/10.1016/B978-008044910-4.00526-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780080449104005265},
author = {C. Kim},
keywords = {Exploratory spatial data analysis, Geovisualization, Knowledge discovery, Spatial autocorrelation, Spatial data mining, Spatial outliers, Spatial uncertainty, Visual data mining},
abstract = {Geovisualization in spatial data mining is one of the main methods that has recently been the subject of knowledge discovery research in geographic information science. Geovisualization is often referred to as knowledge discovery in that it produces previously unseen patterns from a larger set of data. Due to the increase in geospatial data, any techniques that can shift through large sets of data quickly and efficiently are in high demand. Geovisualization uses visual representations to facilitate thinking, understanding, and knowledge construction about human and physic environments, at geographic scales of measurement. It augments human visual ability in perceiving high complex structures, and detecting, exploring, and exploiting significant patterns. It integrates scientific visualization with traditional cartography, and can be utilized at data pre-processing, spatial data mining, and knowledge construction. The main purpose of geovisualization, however, is on insight rather than maps. Research needs in geovisualization are extensive as follows: geovisulation in spatiotemporal databases, the automated discovery of spatial knowledge, geovisualization in remote-sensing data and spatial object-oriented databases, effective geovisualizations of spatial relationships, and efficient geocomputation.}
}
@article{JUST20121292,
title = {Autism as a neural systems disorder: A theory of frontal-posterior underconnectivity},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {36},
number = {4},
pages = {1292-1313},
year = {2012},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2012.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0149763412000334},
author = {Marcel Adam Just and Timothy A. Keller and Vicente L. Malave and Rajesh K. Kana and Sashank Varma},
keywords = {Autism, Connectivity, Underconnectivity, 4CAPS, Computational model, fMRI},
abstract = {The underconnectivity theory of autism attributes the disorder to lower anatomical and functional systems connectivity between frontal and more posterior cortical processing. Here we review evidence for the theory and present a computational model of an executive functioning task (Tower of London) implementing the assumptions of underconnectivity. We make two modifications to a previous computational account of performance and brain activity in typical individuals in the Tower of London task (Newman et al., 2003): (1) the communication bandwidth between frontal and parietal areas was decreased and (2) the posterior centers were endowed with more executive capability (i.e., more autonomy, an adaptation is proposed to arise in response to the lowered frontal-posterior bandwidth). The autism model succeeds in matching the lower frontal-posterior functional connectivity (lower synchronization of activation) seen in fMRI data, as well as providing insight into behavioral response time results. The theory provides a unified account of how a neural dysfunction can produce a neural systems disorder and a psychological disorder with the widespread and diverse symptoms of autism.}
}
@article{BLAND2025,
title = {Quantal response equilibrium as a structural model for estimation: The missing manual},
journal = {Games and Economic Behavior},
year = {2025},
issn = {0899-8256},
doi = {https://doi.org/10.1016/j.geb.2025.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0899825625000211},
author = {James R. Bland and Theodore L. Turocy},
keywords = {Quantal response, Estimation, Computation, Experiments},
abstract = {One of the original objectives of the (logit) quantal response equilibrium (LQRE) model was to provide a method for structural estimation of behavior in games, when behavior deviated from Nash equilibrium predictions. To date, only Chapter 6 of the book on quantal response equilibrium by Goeree et al. (2016) focuses on how such estimation can be implemented. We build on that chapter to provide here a more detailed treatment of the methodological issues of implementing maximum likelihood estimation of QRE. We compare the equilibrium correspondence and empirical payoff approaches to estimation, and identify some considerations in interpreting the results of those approaches when applied to the same data on the same game. We also provide a more detailed “field guide” to using numerical continuation methods to accomplish estimation, including guidance on how to tailor implementations to games with different structures.}
}
@article{KRUSKOPF2024104574,
title = {Future teachers’ self-efficacy in teaching practical and algorithmic ICT competencies – Does background matter?},
journal = {Teaching and Teacher Education},
volume = {144},
pages = {104574},
year = {2024},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2024.104574},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X24001069},
author = {Milla Kruskopf and Rekar Abdulhamed and Mette Ranta and Heidi Lammassaari and Kirsti Lonka},
keywords = {Teaching self-efficacy, Self-efficacy, ICT competence, Digital competence, Programming, 21st century competencies, Teacher students},
abstract = {Future teachers need to be confidently equipped to teach 21st century ICT skills. We investigated teaching self-efficacy (TSE) in ICT competencies among teacher students. We confirmed distinct ICT competencies among two cohorts from teacher training programs (n = 347; n = 428): practical (i.e., device and data management), and algorithmic (i.e., programming, and data security). Regression analyses indicated TSE-biases regarding younger age, male gender, and a background in natural sciences, with significant interactions between age, gender, and having learned such ICT-skills already in school. The findings point to a need for tailored strategies in teacher education to mitigate TSE disparities.}
}
@incollection{KOSSLYN1988615,
title = {Seeing and Imagining in the Cerebral Hemispheres: A Computational Approach},
editor = {Allan Collins and Edward E. Smith},
booktitle = {Readings in Cognitive Science},
publisher = {Morgan Kaufmann},
pages = {615-642},
year = {1988},
isbn = {978-1-4832-1446-7},
doi = {https://doi.org/10.1016/B978-1-4832-1446-7.50052-2},
url = {https://www.sciencedirect.com/science/article/pii/B9781483214467500522},
author = {Stephen M. Kosslyn}
}
@article{PUTICA2024105836,
title = {Reconceptualizing complex posttraumatic stress disorder: A predictive processing framework for mechanisms and intervention},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {164},
pages = {105836},
year = {2024},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2024.105836},
url = {https://www.sciencedirect.com/science/article/pii/S0149763424003051},
author = {Andrea Putica and James Agathos},
keywords = {Complex Posttraumatic Stress Disorder (C-PTSD), Predictive processing, Trauma, Interoceptive inference, Active inference},
abstract = {In this article, we introduce a framework for interpreting Complex Posttraumatic Stress Disorder (C-PTSD) through predictive processing, a neuroscience concept explaining the brain’s interpretation and prediction of sensory information. While closely related to PTSD, C-PTSD encompasses additional symptom clusters marked by disturbances in self-organization (DSO), such as negative self-concept, affect dysregulation, and relational difficulties, typically resulting from prolonged traumatic stressors. Our model leverages advances in computational psychiatry and neuroscience, offering a mechanistic explanation for these symptoms by illustrating how prolonged trauma disrupts the brain's predictive processing. Specifically, altered predictive mechanisms contribute to C-PTSD's symptomatology, focusing on DSO: (1) Negative self-concept emerges from maladaptive priors that bias perception towards self-criticism, misaligning expected and actual interoceptive states; (2) Misalignment between predicted and actual interoceptive signals leads to affect dysregulation, with sensitivity to bodily cues; and (3) Relationship challenges arise from skewed social prediction errors, fostering mistrust and withdrawal. This precision-focused approach sheds light on the dynamics underpinning C-PTSD and highlights potential intervention targets aimed at recalibrating the predictive processing system.}
}
@article{THOMPSON2024939,
title = {Leveraging marine biotechnology for an All-Atlantic sustainable blue economy},
journal = {Trends in Biotechnology},
volume = {42},
number = {8},
pages = {939-941},
year = {2024},
issn = {0167-7799},
doi = {https://doi.org/10.1016/j.tibtech.2023.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S0167779923003670},
author = {Cristiane Thompson and Alice C. Ortmann and Thulani Makhalanyane and Fabiano Thompson},
keywords = {All Atlantic, food security, biotechnology, low-carbon aquaculture, integrated multitrophic aquaculture, biofloc technology},
abstract = {Despite the lack of research, development, and innovation funds, especially in South Atlantic countries, the Atlantic is suited to supporting a sustainable marine bioeconomy. Novel low-carbon mariculture systems can provide food security, new drugs, and climate mitigation. We suggest how to develop this sustainable marine bioeconomy across the entire Atlantic.}
}
@incollection{PANDEY202563,
title = {Chapter 4 - Impact of quantum computing on healthcare data security},
editor = {Gayathri Nagasubramanian and S. Rakesh Kumar and Valentina {Emilia Balas}},
booktitle = {Quantum Computing for Healthcare Data},
publisher = {Academic Press},
pages = {63-90},
year = {2025},
series = {Advances in Biomedical Informatics},
isbn = {978-0-443-29297-2},
doi = {https://doi.org/10.1016/B978-0-443-29297-2.00002-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443292972000022},
author = {Manoj Kumar Pandey and Jyoti Upadhyay and Naresh Kumar Kar and Velliangiri Sarveshwaran},
keywords = {Quantum computing, security, cryptography, healthcare, challenges, sustainable development goals},
abstract = {With the potential to completely transform computation, quantum computing (QC) is a young topic at the vanguard of scientific inquiry and technological advancement. QC promises to bring about dramatic improvements in data security and processing capabilities when it is integrated into healthcare systems. Conventional encryption techniques like RSA and ECC are based on discrete logarithms and integer factorization, two cryptographic issues that are currently unsolvable but can be solved by QC. This trend, however, also makes it more likely that current cryptographic systems will be subject to quantum attacks, which will force the creation and use of encryption methods that are resistant to quantum attacks. Additionally, by employing quantum-resistant hashing techniques, QC enables improved data integrity verification, guaranteeing the veracity and validity of medical data. The applied application of quantum-resistant cryptography techniques and the integration of quantum secure protocols into the current healthcare infrastructure still facing some difficulties therefore anyhow these encouraging advancements in this technique. This chapter focuses on the effect of the QC on the security of healthcare data with particular importance on how it revolutionized encryption, data integrity, privacy protection, and other related issues.}
}
@article{PAIVIO2014141,
title = {Intelligence, dual coding theory, and the brain},
journal = {Intelligence},
volume = {47},
pages = {141-158},
year = {2014},
issn = {0160-2896},
doi = {https://doi.org/10.1016/j.intell.2014.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0160289614001305},
author = {Allan Paivio},
keywords = {IQ theories, IQ tests, Conceptual/empirical flaw, DCT a unified theory, IQ neuroscience},
abstract = {The distinction between verbal and nonverbal cognitive abilities has been a defining feature of psychometric theories of intelligence for the past century. Despite their popularity, however, these theories have not included functional connections between verbal and nonverbal systems that are necessary if they are to explain performance in intellectual tasks involving interactions between language and nonverbal knowledge. This functional gap limits the capacity of psychometric theories to explain and predict fundamental aspects of individual differences in cognitive abilities that have long been studied experimentally. This article summarizes the history, nature, and possible causes of the problem, and then concludes with a neuroscientifically-enhanced, multimodal dual coding approach to intelligence that focuses on the synergistic interactivity of verbal and nonverbal systems.}
}
@article{KANDEMIR2025104990,
title = {Pre-service mathematics teachers' modelling processes within model eliciting activity through digital technologies},
journal = {Acta Psychologica},
volume = {256},
pages = {104990},
year = {2025},
issn = {0001-6918},
doi = {https://doi.org/10.1016/j.actpsy.2025.104990},
url = {https://www.sciencedirect.com/science/article/pii/S0001691825003038},
author = {Mehmet Ali Kandemir and Nurullah Eryilmaz},
keywords = {Digital technologies, Mathematical modelling, Model eliciting activity (MEA), Pre-service mathematics teachers (PSTs), Technology use, Mathematical content knowledge (MCK)},
abstract = {This study examines the modelling approaches of high school pre-service mathematics teachers (PSTs) and the types and purposes of digital technologies they use at different stages of the mathematical modelling process. Conducted during the 2018–2019 academic year, the study involved 26 PSTs working in eight groups as part of a course on computer technologies in mathematics education. The participants engaged in a model eliciting activity (MEA) focused on the obesity problem, integrating digital technologies and mathematical content knowledge. Findings indicate that while PSTs effectively utilized the internet, spreadsheets, calculators, and mathematical software for problem-solving, three distinct purposes of technology use emerged. However, challenges included overreliance on technological outputs, limiting critical evaluation and validation of models, and difficulties in transferring mathematical content knowledge to the modelling process. These results highlight the need for explicit instructional support in teacher education programs to enhance PSTs' critical engagement with digital tools and strengthen their ability to integrate mathematical knowledge in real-world problem-solving.}
}
@article{HUHN201642,
title = {Cognitive framing in action},
journal = {Cognition},
volume = {151},
pages = {42-51},
year = {2016},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2016.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S0010027716300439},
author = {John M. Huhn and Cory Adam Potts and David A. Rosenbaum},
keywords = {Action, Cognitive framing, Heuristics, Object manipulation, Motor control, Bimanual actions},
abstract = {Cognitive framing effects have been widely reported in higher-level decision-making and have been ascribed to rules of thumb for quick thinking. No such demonstrations have been reported for physical action, as far as we know, but they would be expected if cognition for physical action is fundamentally similar to cognition for higher-level decision-making. To test for such effects, we asked participants to reach for a horizontally-oriented pipe to move it from one height to another while turning the pipe 180° to bring one end (the “business end”) to a target on the left or right. From a physical perspective, participants could have always rotated the pipe in the same angular direction no matter which end was the business end; a given participant could have always turned the pipe clockwise or counter-clockwise. Instead, our participants turned the business end counter-clockwise for left targets and clockwise for right targets. Thus, the way the identical physical task was framed altered the way it was performed. This finding is consistent with the hypothesis that cognition for physical action is fundamentally similar to cognition for higher-level decision-making. A tantalizing possibility is that higher-level decision heuristics have roots in the control of physical action, a hypothesis that accords with embodied views of cognition.}
}
@incollection{ZIEGLERRODRIGUEZ2025169,
title = {Chapter 6 - Life cycle assessment of constructed wetlands: measuring their contribution to sustainable development},
editor = {Asheesh Kumar Yadav and Jan Vymazal and Yaqian Zhao and Pratiksha Srivastava},
booktitle = {Emerging Developments in Constructed Wetlands},
publisher = {Elsevier},
pages = {169-193},
year = {2025},
isbn = {978-0-443-14078-5},
doi = {https://doi.org/10.1016/B978-0-443-14078-5.00006-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443140785000064},
author = {Kurt Ziegler-Rodriguez and Marianna Garfí},
keywords = {Sustainable development, life cycle assessment, constructed wetland, biological waste treatment, water management},
abstract = {Life cycle thinking has led to the development of a series of methodologies that evaluate the sustainability of any process, product, or activity, considering the three aspects of sustainable development: the environmental, economic, and social pillars. These methodologies called the (Environmental) Life Cycle Assessment, the Social Life Cycle Assessment and the Life Cycle Costing, have the peculiarity to consider the whole life cycle of a product or process, from the extraction of raw materials to their end of life. At the same time, sustainable development has led to the strengthening of disciplines and novel technologies such as circular bioeconomy, industrial ecology, and nature-based solutions. In this context, constructed wetlands have been gaining popularity since they are a low-cost alternative for urban and industrial wastewater treatment in small communities. The performed life cycle assessments of these technologies have shown that, regardless of the model, configuration, or type of waste treated, they have low environmental impacts compared with conventional solutions (e.g., activated sludge system) due to low energy requirements, no chemicals consumption, and avoidance of off-site management and transportation practices. In terms of costs, constructed wetlands can drastically reduce the costs associated with wastewater treatment and management. However, more efforts should be made in order to define the social benefits of this technology (e.g., local employment generation, landscape improvement) and the quality of the recovered resources (e.g., treated water, fertilizer).}
}
@article{ASOULIN201998,
title = {Phrase structure grammars as indicative of uniquely human thoughts},
journal = {Language Sciences},
volume = {74},
pages = {98-109},
year = {2019},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2019.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0388000119300117},
author = {Eran Asoulin},
keywords = {Thought, Cognition, Phrase structure grammars, Chomsky hierarchy, Animal cognition},
abstract = {I argue that the ability to compute phrase structure grammars is indicative of a particular kind of thought. This type of thought that is only available to cognitive systems that have access to the computations that allow the generation and interpretation of the structural descriptions of phrase structure grammars. The study of phrase structure grammars, and formal language theory in general, is thus indispensable to studies of human cognition, for it makes explicit both the unique type of human thought and the underlying mechanisms in virtue of which this thought is made possible.}
}
@article{1985174,
title = {Learning to use a word processor: by doing, by thinking, and by knowing: John M. Carroll and Robert L. Mack: Rep. RC 9481, IBM T.J. Watson Research Center, Yorktown Heights, New York 10598, U.S.A., (July 1982)},
journal = {Decision Support Systems},
volume = {1},
number = {2},
pages = {174},
year = {1985},
issn = {0167-9236},
doi = {https://doi.org/10.1016/0167-9236(85)90071-5},
url = {https://www.sciencedirect.com/science/article/pii/0167923685900715}
}
@incollection{VANLOAN1992247,
title = {Chapter 6 A survey of matrix computations},
series = {Handbooks in Operations Research and Management Science},
publisher = {Elsevier},
volume = {3},
pages = {247-321},
year = {1992},
booktitle = {Computing},
issn = {0927-0507},
doi = {https://doi.org/10.1016/S0927-0507(05)80203-8},
url = {https://www.sciencedirect.com/science/article/pii/S0927050705802038},
author = {Charles {Van Loan}},
abstract = {Publisher Summary
This chapter presents three-level introduction to the field of matrix computations. The chapter discusses analytic and computational tools that underpin numerical linear algebra. Low dimension examples are the rule with appropriate generalizations to follow. The central themes include (a) the language of matrix factorizations, (b) the art of introducing zeros into a matrix, (c) the exploitation of structure, and (d) the distinction between problem sensitivity and algorithmic stability. Matrix factorizations that play a central role in numerical linear algebra are also presented in the chapter. The chapter also discusses factorization. For each factorization, algorithms are surveyed, associated mathematical properties, and applications are discussed. One factorization (Chotesky) is used to illustrate various aspects of high performance matrix computations. Successful computing requires the design of codes that pay careful attention to the flow of data during execution.}
}
@incollection{BALANAY201949,
title = {3 - Tools for circular economy: Review and some potential applications for the Philippine textile industry},
editor = {Subramanian Senthilkannan Muthu},
booktitle = {Circular Economy in Textiles and Apparel},
publisher = {Woodhead Publishing},
pages = {49-75},
year = {2019},
series = {The Textile Institute Book Series},
isbn = {978-0-08-102630-4},
doi = {https://doi.org/10.1016/B978-0-08-102630-4.00003-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780081026304000030},
author = {Raquel Balanay and Anthony Halog},
keywords = {Circular economy, Industrial sustainability, Life cycle thinking, Sustainable development, Systems modelling, Textile industry},
abstract = {Instituting circular economy (CE) for sustainability is the aim of taking stock of various analytical/assessment tools. A review of these tools reveals a continuing endeavor to incorporate in the procedures the systems and life cycle thinking and the triple bottom-line framework of sustainable development (economic, social, and environmental). Over time, the CE tools have been modified with the incorporation of some unique attributes in the cases being studied. Life cycle assessment (LCA) remains the popular and the only standardized procedure to analyze CE issues in industries, specifically in the environmental aspect. However, consistency, measurement, and aggregation issues are the major setbacks of having an integrated LCA for economic, social, and environmental impacts. The alternative tools used across the world to study the economic, social, and environmental aspects of CE have increased in both number and sophistication. Optimization and systems models have been increasingly used on a case-based format. Although the downside is the less standardized approach with less chances of comparability in terms of results, these models have been designed appropriately to tackle challenges associated with intricate, multifaceted, and encompassing sustainability and CE issues to improve policy development. In the textile industry, LCA as a popular tool is only used for environmental sustainability assessment but not much in social and economic aspects. The Philippine textile industry still has to catch up in the application of those tools for sustainability assessment. A framework has been suggested for the country's roadmap/guide to attain circularity in textile industry operations.}
}
@article{SIMS1991383,
title = {Computers and experiments in stress analysis: Eds G. M. Carlomagno and C. A. Brebbia Computational Mechanics Publications, Southampton, UK},
journal = {Engineering Structures},
volume = {13},
number = {4},
pages = {383-384},
year = {1991},
issn = {0141-0296},
doi = {https://doi.org/10.1016/0141-0296(91)90027-A},
url = {https://www.sciencedirect.com/science/article/pii/014102969190027A},
author = {P. Sims}
}
@article{RABOY201736,
title = {An introductory microeconomics in-class experiment to reinforce the marginal utility/price maximization rule and the integration of modern theory},
journal = {International Review of Economics Education},
volume = {24},
pages = {36-49},
year = {2017},
issn = {1477-3880},
doi = {https://doi.org/10.1016/j.iree.2016.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S1477388016300494},
author = {David G. Raboy},
keywords = {Experimental economics, Modern microeconomics, Principles classes, Alternative pedagogy},
abstract = {This paper presents an in-class experiment used as a teaching tool in an introductory microeconomics class at the undergraduate college level. It is directed at a critical but challenging concept for principles students—constrained utility maximization and a methodology to intuit preferences. The experimental project is nested in the literature pertaining to the current transition in microeconomic theory motivated by contributions from behavioral economics and transactions-cost economics, among other elements; modern pedagogical models; experimental economics; and experiments as in-classroom teaching tools. While not dispositive as to the general efficacy of in-class experiments, the paper provides an example of an alternative instructional approach which is helpful to principles students under strictly defined protocols. The benefits to students include heightened understanding of the core subject topic, greater interest in the subject matter, a closer connection to real-world economics, and enhanced critical thinking capabilities.}
}
@article{GU2023120105,
title = {Detection of Attention Deficit Hyperactivity Disorder in children using CEEMDAN-based cross frequency symbolic convergent cross mapping},
journal = {Expert Systems with Applications},
volume = {226},
pages = {120105},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120105},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423006073},
author = {Danlei Gu and Aijing Lin and Guancen Lin},
keywords = {Cross-frequency coupling, Convergent cross mapping, Complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN), Attention Deficit Hyperactivity Disorder (ADHD), Electroencephalogram (EEG), Dispersion},
abstract = {The cross-frequency coupling relationship of EEG signals is of great significance to identify abnormal EEG signals and diagnose diseases. This paper proposes a new algorithm, CEEMDAN (complete ensemble empirical mode decomposition with adaptive noise)-based cross-frequency symbolic convergent cross mapping (CEEMDAN CF-SCCM). In the numerical simulation test, we have confirmed that CEEMDAN CF-SCCM is an effective method to quantify the cross-frequency information transmission of complex system signals from the three dimensions of robustness to noise, coupling sensitivity, and data length sensitivity. It can successfully distinguish the driving factors and response factors in the phase–amplitude interaction and has good robustness to noise. With this approach, we examined differences in cross-frequency phase–amplitude coupling between ADHD patients and normal individuals over classical brain frequency bands (Alpha, Beta, Delta, Gamma, Theta). According to the position of the electrodes, the brain was divided into four regions: front, back, left, and right, and the phase–amplitude coupling between different frequencies in each region was compared. Compared with the normal group, there was more information transmission in the anterior region of Delta waves and Theta waves. The front and left sides of the brain are responsible for thinking, mental and auditory functions. This information helps us gain insight into the brain dynamics of ADHD patients and contributes to the diagnosis of the disease.}
}
@incollection{NICHELLI2016379,
title = {Chapter 23 - Consciousness and Aphasia},
editor = {Steven Laureys and Olivia Gosseries and Giulio Tononi},
booktitle = {The Neurology of Conciousness (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {379-391},
year = {2016},
isbn = {978-0-12-800948-2},
doi = {https://doi.org/10.1016/B978-0-12-800948-2.00023-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128009482000236},
author = {Paolo Nichelli},
keywords = {language impairment, anarthria, dynamic aphasia, fMRI, neurophysiological measures},
abstract = {Different language impairments allow us to investigate how much the use of language can influence the content of conscious awareness and therefore of thinking and reasoning. Pure anarthria (different from mutism) and verbal short-term memory deficits are associated with an impairment of the effect of covert speech on the content of working memory. Dynamic aphasia impairs the processes involved in the transition between thinking and speaking. However, even the most severe agrammatic patients can retain reasoning about others’ beliefs that according to some theories can only take place in explicit sentences of a natural language. Error monitoring is also impaired in many aphasic patients and in some of them is associated with complete lack of error awareness (anosognosia for aphasia). In patients with impaired consciousness whenever language examination is impossible or unreliable, fMRI and neurophysiological measures such as event-related potentials can provide a window for examining residual language capabilities.}
}
@article{FIROOZI2025104593,
title = {Developing urban infrastructure: Strategic integration of solar-heated pavement systems for enhanced resilience and sustainability},
journal = {Results in Engineering},
volume = {25},
pages = {104593},
year = {2025},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2025.104593},
url = {https://www.sciencedirect.com/science/article/pii/S2590123025006711},
author = {Ali Akbar Firoozi and Ali Asghar Firoozi and D.O. Oyejobi and Siva Avudaiappan and Erick Saavedra Flores},
keywords = {Solar-Heated pavements, Urban resilience, Sustainable infrastructure, Smart city technologies, Thermal energy storage, Green urban planning},
abstract = {This research examines the integration and optimization of solar-heated pavement systems in urban environments, emphasizing their potential to enhance urban resilience and sustainability. Utilizing advanced materials and smart technologies, these pavements maintain safe, ice-free surfaces during winter, reducing reliance on traditional snow removal methods and their environmental impacts. The methodology focuses on a combination of experimental setups and computational simulations to analyze the design features, material advancements, and strategic integration of these systems. Quantitative findings from case studies across diverse climates demonstrate a significant reduction in energy consumption by up to 40 % and maintenance costs by 60 %, highlighting the economic and environmental benefits. The manuscript advocates broader implementation and recommends further research to optimize system efficiency and applicability in urban planning initiatives.}
}
@incollection{SHAYNAROSENBAUM201787,
title = {2.06 - Episodic and Semantic Memory},
editor = {John H. Byrne},
booktitle = {Learning and Memory: A Comprehensive Reference (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {87-118},
year = {2017},
isbn = {978-0-12-805291-4},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.21037-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093245210377},
author = {R. {Shayna Rosenbaum} and Alice S.N. Kim and Stevenson Baker},
keywords = {Aging, Amnesia, Autobiographical memory, Autonoetic consciousness, Child development, Default mode network, Familiarity, fMRI, Future imagining, Hippocampus, Medial temporal lobe, Mental time travel, Patient K.C., Personal semantic memory, Recollection, Spatial memory, Temporal neocortex},
abstract = {Much of the richness in human life derives from episodic memory, mental representations of detailed experiences from our personal pasts. To make sense of those experiences, knowledge about the world and oneself must also exist in a form that is free of context – known as semantic memory. This chapter revisits and builds on Tulving's distinction between episodic and semantic memory, with a focus on their differences, similarities, and interactions, informed by cognitive, neuropsychological, and neuroimaging studies. Extensions of this distinction into spatial memory, and beyond memory into future thinking, are considered in the context of process views of memory organization.}
}
@incollection{CATTANEO2015220,
title = {Mental Imagery: Visual Cognition},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {220-227},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.57024-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008097086857024X},
author = {Zaira Cattaneo and Juha Silvanto},
keywords = {Brain stimulation, Creative thinking, Depictivist, Imagery, Imagery debate, Mathematics, Memory, Neuroimaging, Occipital cortex, Perception, Propositional, Reasoning, Vision, Visual cognition, Working memory},
abstract = {Mental imagery can be defined as a quasi-perceptual experience occurring in the absence of perceptual input. The present article provides a review of the key processes involved in mental imagery, the relationship of imagery to working memory, and of the debate on the underlying format of mental images. We also review the functional significance of imagery in a range of cognitive processes, such as memory, creative thinking, reasoning, and problem solving. Finally, the brain basis of mental imagery and its overlap with the cortical regions involved in visual perception are discussed.}
}
@article{MEARA2000345,
title = {Vocabulary and neural networks in the computational assessment of texts written by second-language learners},
journal = {System},
volume = {28},
number = {3},
pages = {345-354},
year = {2000},
issn = {0346-251X},
doi = {https://doi.org/10.1016/S0346-251X(00)00016-6},
url = {https://www.sciencedirect.com/science/article/pii/S0346251X00000166},
author = {Paul Meara and Catherine Rodgers and Gabriel Jacobs},
keywords = {Neural network, Computational assessment, Vocabulary, French},
abstract = {This paper explores the potential of a neural network in language assessment. Many examination systems rely on subjective judgments made by examiners as a way of grading the writing of non-native speakers. Some research (e.g. Engber, 1995. The relationship of lexical proficiency to the quality of ESL compositions. Journal of Second Language Writing 4(2), 139–155) has shown that these subjective judgements are influenced to a very large extent by the lexical choices made by candidates. We took Engber's basic model, but automated the evaluation of lexical content. A group of non-native speakers of French were asked to produce a short text in response to a picture stimulus. The texts were graded by French native speaker teachers. We identified a number of words which occurred in about half the texts, and coded each text for the occurrence and non-occurrence of each word. We then trained a neural network to grade the texts on the basis of these codings. The results suggest that it might be possible to teach a neural network to mimic the judgements made by human markers.}
}
@article{WHITACRE201747,
title = {Integer comparisons across the grades: Students’ justifications and ways of reasoning},
journal = {The Journal of Mathematical Behavior},
volume = {45},
pages = {47-62},
year = {2017},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2016.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0732312316301742},
author = {Ian Whitacre and Beti Azuz and Lisa L.C. Lamb and Jessica Pierson Bishop and Bonnie P. Schappelle and Randolph A. Philipp},
keywords = {Integers, Negative numbers, Children’s mathematical thinking, Order, Magnitude},
abstract = {This study is an investigation of students’ reasoning about integer comparisons—a topic that is often counterintuitive for students because negative numbers of smaller absolute value are considered greater (e.g., −5>−6). We posed integer-comparison tasks to 40 students each in Grades 2, 4, and 7, as well as to 11th graders on a successful mathematics track. We coded for correctness and for students’ justifications, which we categorized in terms of 3 ways of reasoning: magnitude-based, order-based, and developmental/other. The 7th graders used order-based reasoning more often than did the younger students, and it more often led to correct answers; however, the college-track 11th graders, who responded correctly to almost every problem, used a more balanced distribution of order- and magnitude-based reasoning. We present a framework for students’ ways of reasoning about integer comparisons, report performance trends, rank integer-comparison tasks by relative difficulty, and discuss implications for integer instruction.}
}
@incollection{VALLERO20211,
title = {Chapter 1 - Systems science},
editor = {Daniel A. Vallero},
booktitle = {Environmental Systems Science},
publisher = {Elsevier},
pages = {1-24},
year = {2021},
isbn = {978-0-12-821953-9},
doi = {https://doi.org/10.1016/B978-0-12-821953-9.00014-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128219539000143},
author = {Daniel A. Vallero},
keywords = {Systems science, Scientific method, Data-intensive discovery, Computational methods, Emergence, Fuzziness, Ecotone, Ecocline, Environmental risk, Biosolids},
abstract = {This chapter manifests how some of the connotations of systems apply to environmental science. The discussion begins with the history and evolution of scientific methods and paradigms, especially the agreement on the scientific method, spatial and temporal complexity, and the first principles of thermodynamics and motion. These are compared to modern environmental applications, including computational methods, governance, and emergence. Scientific and technical communication approaches needed for environmental systems science are described.}
}
@article{KARADAG2025,
title = {A new frontier in design studio: AI and human collaboration in conceptual design},
journal = {Frontiers of Architectural Research},
year = {2025},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2025.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S2095263525000226},
author = {Derya Karadağ and Betül Ozar},
keywords = {Artificial intelligence, Design process, Conceptual development, Interior design education, Text-to-image generation},
abstract = {This study explores the role of artificial intelligence (AI) in the conceptual design phase of interior design education, focusing on AI's potential to help students visualise and refine creative ideas. Conducted within a design studio course, the research integrates text-to-image generators, particularly Midjourney to support students' design processes. Implemented in the fourth week of a 14-week course, a structured workshop introduced students to Midjourney, with surveys conducted both at this stage and during the final submission to capture changes in student perspectives. Using a two-phase case study involving a workshop, surveys, and interviews among senior undergraduate students in the bachelor's program of the Interior Architecture and Environmental Design Department, the study assesses the impact of AI prompts, from simple keywords to detailed narratives, on concept development and project outcomes. Findings indicate that AI broadens design possibilities, facilitates iterative ideation, and improves conceptual precision through high-fidelity visualizations. While students view AI as a valuable addition to their creative process, they also express concerns about ethics and the need to balance AI's benefits with preserving design authenticity. This research contributes to the broader discussion on AI's role in design, advocating for a balanced integration that respects both technological potential and human creativity.}
}
@article{MALEKSHAHIAN2025,
title = {Bridging the Skills Gap: Enhancing Employability for Chemical Engineering Graduates},
journal = {Education for Chemical Engineers},
year = {2025},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2025.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S174977282500017X},
author = {Maryam Malekshahian and Jessica Dautelle and Salman Shahid},
keywords = {Employability Skills, Chemical Engineering Education, Transferable Skills, Curriculum Development, Industry Readiness},
abstract = {Extensive research underscores a persistent skills gap among graduates across various disciplines. However, identifying the precise skill gaps in engineering education remains challenging due to inconsistencies in existing research, and studies specifically addressing employability skills in chemical engineering are limited. This study aims to address these knowledge gaps by identifying the critical employability skills necessary for chemical engineering graduates. The study employs a multi-method approach, incorporating a systematic literature review, surveys of students, alumni, and employers, and a statistical analysis of job advertisements for graduate positions. The objective is to establish a comprehensive understanding of required competencies and evaluate the alignment between employer expectations and graduate competencies. A structured skill framework was developed, encompassing 15 primary skill groups and over 75 sub-skills. Comparative analysis of employer perceptions and job advertisement data highlighted discrepancies in perceived versus stated skill priorities. However, competencies such as communication, interpersonal skills, self-management, and adaptability were consistently recognised as essential across sectors. Significant skill gaps were observed in areas such as communication, problem-solving, literacy, interpersonal, self-management, and business acumen. Survey findings indicate that engineering students often overestimate their technical proficiency while underestimating the importance of transferable skills such as resilience, ethics, and integrity. Conversely, employers consistently emphasise the need for a well-rounded skillset that integrates technical expertise with strong communication and management capabilities. This disconnect underscores the need for educational programmes to promote greater self-awareness among students and ensure their skill development aligns with industry demands. These results align with existing literature, reinforcing the importance of embedding transferable skills within engineering curricula to better prepare graduates for professional success.}
}
@incollection{TAYLOR1995227,
title = {Chapter 13 - Computational needs for process tomography},
editor = {R.A. Williams and M.S. Beck},
booktitle = {Process Tomography},
publisher = {Butterworth-Heinemann},
address = {Oxford},
pages = {227-249},
year = {1995},
isbn = {978-0-08-093801-1},
doi = {https://doi.org/10.1016/B978-0-08-093801-1.50017-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780080938011500174},
author = {R.W. Taylor}
}
@article{SANCHIS2023102162,
title = {Towards a general equilibrium theory of allocation of time for the digital revolution era},
journal = {Technology in Society},
volume = {72},
pages = {102162},
year = {2023},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2022.102162},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X22003037},
author = {Raúl G. Sanchis},
keywords = {Household economics, Time allocation, Consumer behaviour, Firm behaviour, General equilibrium},
abstract = {The Digital Revolution we are witnessing has started a new era in modern societies and economies. Time inputs, whether these are from human beings and non-human, electronical or mechanical devices are increasingly more important, especially –but not uniquely– in most advanced economies and societies. Existing economic theory strives to accommodate time inputs into mainstream economic theory. This paper contributes to the existing literature on time allocation theoretical models by suggesting a general equilibrium framework likely to respond to some existing challenges in modern economies. In the general equilibrium modelling process, some improvements are made to time allocation models from the consumer side which concern the inclusion of non-human time inputs and multitasking, and a novel development on a producer theory of allocation of time is designed to determine the underpinnings of a computationally tractable general equilibrium theory of allocation of time. Both the solution and usefulness of this work will require the help of cutting-edge computational techniques in future work.}
}
@article{AKANDA2025112329,
title = {Understanding comment practices in Scratch: A study of comments in a block-based visual programming language},
journal = {Journal of Systems and Software},
volume = {222},
pages = {112329},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112329},
url = {https://www.sciencedirect.com/science/article/pii/S016412122400373X},
author = {Wahiduzzaman Akanda and James Clause},
keywords = {Comment, Text-based programming, Visual programming, Scratch, Taxonomy},
abstract = {Comments are vital for software documentation. They provide necessary insights and assist developers in understanding and maintaining the software. Due to their importance, comments have been extensively studied, and much has been learned about them. These existing studies have predominantly focused on text-based languages. Conversely, block-based visual programming languages, particularly Scratch, are becoming increasingly popular. Some studies regarding comments related to the Scratch online community focus on topics such as fostering online community and engagement, sentiment analysis, etc. However, they overlook the visual aspects and the qualitative analysis of comments within code in Scratch projects. This is a meaningful limitation, and this research project studies comments and their pattern in Scratch projects from both textual and visual perspectives. We examined comments collected from different Scratch projects. Each comment was manually annotated based on textual and visual attributes, producing a taxonomy model of comments for a visual programming language. The classification results were analyzed to understand better the practice of commenting in Scratch. Our result revealed that Scratch projects produced noisier(i.e., less understandable) comments than text-based programming languages like Java. In addition, the study also revealed several limitations and shortcomings that could be addressed to improve the commenting experience in Scratch.}
}
@article{CHAE2025105759,
title = {Enhancing nuclear power plant diagnostics: A comparative analysis of XAI-based feature selection methods for abnormal and emergency scenario detection},
journal = {Progress in Nuclear Energy},
volume = {185},
pages = {105759},
year = {2025},
issn = {0149-1970},
doi = {https://doi.org/10.1016/j.pnucene.2025.105759},
url = {https://www.sciencedirect.com/science/article/pii/S014919702500157X},
author = {Young Ho Chae and Seung Geun Kim and Jeonghun Choi and Seo Ryong Koo and Jonghyun Kim},
abstract = {This study introduces the application of explainable artificial intelligence (XAI) techniques to enhance nuclear power plant diagnostics through effective feature selection. We compared various XAI methods, including gradient-based techniques, layer-wise relevance propagation, DeepSHAP, integrated gradients, local interpretable model-agnostic explanation(LIME), and saliency maps, with traditional approaches such as principal component analysis (PCA). By applying these methods to data from an IAEA iPWR simulator, which includes 35 abnormal and emergency scenarios with 116 state variables, we demonstrated the superiority of XAI-based methods in selecting features that effectively distinguish between different plant conditions. Our approach successfully reduced the input dimensionality from 116 to 20 features while maintaining high diagnostic accuracy. XAI methods, particularly saliency map and DeepSHAP, outperformed traditional techniques by revealing distinct patterns for various abnormal situations. This reduction in dimensionality offers several benefits, including enhanced cybersecurity, improved human–machine interfaces, and increased computational efficiency. The findings have significant implications for developing more accurate, efficient, and interpretable diagnostic systems in nuclear power plants, potentially improving safety and operational effectiveness. Future work will focus on validating these methods across diverse plant designs and integrating this approach with advanced AI techniques for real-time adaptive diagnostics.}
}
@article{CUI2019305,
title = {Developing reflection analytics for health professions education: A multi-dimensional framework to align critical concepts with data features},
journal = {Computers in Human Behavior},
volume = {100},
pages = {305-324},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2019.02.019},
url = {https://www.sciencedirect.com/science/article/pii/S0747563219300718},
author = {Yi Cui and Alyssa Friend Wise and Kenneth L. Allen},
keywords = {Reflection, Learning analytics, Natural language processing, Professional education, Dental education, Health professions education},
abstract = {Reflection is a key activity in self-regulated learning (SRL) and a critical part of health professions education that supports the development of effective lifelong-learning health professionals. Despite widespread use and plentiful theoretical models, empirical understanding of and support for reflection in health professions education remains limited due to simple manual assessment and rare feedback to students. Recent moves to digital reflection practices offer opportunities to computationally study and support reflection as a part of SRL. The critical task in such an endeavor, and the goal of this paper, is to align high-level reflection qualities that are valued conceptually with low-level features in the data that are possible to extract computationally. This paper approaches this goal by (a) developing a unified framework for conceptualizing reflection analytics in health professions education and (b) empirically examining potential data features through which these elements can be assessed. Synthesizing the prior literature yields a conceptual framework for health professions reflection comprised of six elements: Description, Analysis, Feelings, Perspective, Evaluation, and Outcome. These elements then serve as the conceptual grounding for the computational analysis in which 27 dental students’ reflections (in six reflective statement types) over the course of 4 years were examined using selected LIWC (Linguistic Inquiry and Word Count) indices. Variation in elements of reflection across students, years, and reflection-types supports use of the multi-dimensional analysis framework to (a) increase precision of research claims; (b) evaluate whether reflection activities are engaged in as intended; and (c) diagnose aspects of reflection in which specific students need support. Implications for the development of health professions reflection analytics that can contribute to SRL and promising areas for future research are discussed.}
}
@incollection{YELLA2022770,
title = {2.32 - Magic bullets: Drug repositioning and drug combinations},
editor = {Terry Kenakin},
booktitle = {Comprehensive Pharmacology},
publisher = {Elsevier},
address = {Oxford},
pages = {770-788},
year = {2022},
isbn = {978-0-12-820876-2},
doi = {https://doi.org/10.1016/B978-0-12-820472-6.00116-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012820472600116X},
author = {Jaswanth K. Yella and Anil G. Jegga},
keywords = {Artificial intelligence, Computer-aided drug synthesis, COVID-19, De novo drug discovery, Drug combinations, Drug repurposing, Drug synergy, Machine learning, Network analysis},
abstract = {Discovery and development of novel pharmaceuticals continue to be a very costly, time-consuming and uncertain process impacting negatively not only the research and development of pharmaceutical industry but also health care. Although the number of novel drugs approved each year has grown over by as much as 60% compared to the past decade, there are still many diseases that do not have any approved drug. Recent technological advances in the biomedical, genomics, and computational science domains accompanied by multisource and multidimensional data opened new opportunities and challenges. The drug discovery paradigm is increasingly shifting from hypothesis-driven to data-driven approaches. While the search for the magic bullets of medicine continues, the magic—crunching the data deluge into knowledge and hypotheses nuggets—is mostly driven by machines and machine intelligence. This review will primarily focus on three facets of computational drug discovery approaches, namely, drug repositioning, de novo drug discovery, and drug combinations, and reflect on computational approaches which are reproducible and seem most promising for the machine learning-driven drug discovery. Finally, using COVID-19 as an example, we discuss how the computational approaches are aiding and accelerating the process of discovery of magic bullet(s) for this dreadful pandemic.}
}
@article{SELVERSTON1988109,
title = {A consideration of invertebrate central pattern generators as computational data bases},
journal = {Neural Networks},
volume = {1},
number = {2},
pages = {109-117},
year = {1988},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(88)90013-5},
url = {https://www.sciencedirect.com/science/article/pii/0893608088900135},
author = {Allen I Selverston},
abstract = {The essential features of real neural networks are discussed with respect to their usefulness for connectionist modeling. These features are broken down into cellular and synaptic properties and related to a form of neural circuit known as central pattern generators. The gastric and pyloric rhythm of the lobster stomatogastric system are presented as possible computational data bases for modeling studies.}
}
@article{ROTHMCDUFFIE2018173,
title = {Middle school mathematics teachers’ orientations and noticing of features of mathematics curriculum materials},
journal = {International Journal of Educational Research},
volume = {92},
pages = {173-187},
year = {2018},
issn = {0883-0355},
doi = {https://doi.org/10.1016/j.ijer.2018.09.019},
url = {https://www.sciencedirect.com/science/article/pii/S0883035518305512},
author = {Amy {Roth McDuffie} and Jeffrey Choppin and Corey Drake and Jon Davis},
keywords = {Curriculum, Curriculum analysis, Teacher orientation, Middle school mathematics, Teacher noticing},
abstract = {We report findings on teachers’ noticing of features in the teacher resources of mathematics curriculum programs. Based on prior analysis, we selected teachers using one of two curriculum types: delivery mechanism or thinking device. The participating teachers and the curriculum programs aimed to align with the Common Core Standards for Mathematics, and thus, they ostensibly held a common aim for instruction. We analyzed 147 lesson planning interviews with 20 middle school mathematics teachers. We found that teachers attended to similar features of teacher resources; however, patterns for interpreting and planning decisions varied based on teachers’ orientations and curriculum type.}
}
@article{FERRES2025,
title = {AI in the Era of GPT: Transforming the Future of Work and Discovery},
journal = {Journal of the American College of Radiology},
year = {2025},
issn = {1546-1440},
doi = {https://doi.org/10.1016/j.jacr.2025.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S1546144025001097},
author = {Juan M.Lavista Ferres and Elliot K. Fishman and Linda C. Chu and Felipe Lopez-Ramirez and Charles K. Crawford and Steven P. Rowe}
}
@article{KUZNETSOV20206378,
title = {Harmonic balance analysis of pull-in range and oscillatory behavior of third-order type 2 analog PLLs},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {6378-6383},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.1773},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320323818},
author = {N.V. Kuznetsov and M.Y. Lobachev and M.V. Yuldashev and R.V. Yuldashev and G. Kolumbán},
keywords = {Phase-locked loop, third-order PLL, type 2 PLL, nonlinear analysis, harmonic balance method, describing function, global stability, birth of oscillations, hold-in range, pull-in range, lock-in range, Egan conjecture},
abstract = {The most important design parameters of each phase-locked loop (PLL) are the local and global stability properties, and the pull-in range. To extend the pull-in range, engineers often use type 2 PLLs. However, the engineering design relies on approximations which prevent a full exploitation of the benefits of type 2 PLLs. Using an exact mathematical model and relying on a rigorous mathematical thinking this problem is revisited here and the stability and pull-in properties of the third-order type 2 analog PLLs are determined. Both the local and global stability conditions are derived. As a new idea, the harmonic balance method is used to derive the global stability conditions. That approach offers an extra advantage, the birth of unwanted oscillations can be also predicted. As a verification it is shown that the sufficient conditions of global stability derived by the harmonic balance method proposed here and the well-known direct Lyapunov approach coincide with each other, moreover, the harmonic balance predicts the birth of oscillations in the gap between the local and global stability conditions. Finally, an example when the conditions for local and global stability coincide, is considered.}
}
@article{BAUER2024100002,
title = {What if? Numerical weather prediction at the crossroads},
journal = {Journal of the European Meteorological Society},
volume = {1},
pages = {100002},
year = {2024},
issn = {2950-6301},
doi = {https://doi.org/10.1016/j.jemets.2024.100002},
url = {https://www.sciencedirect.com/science/article/pii/S2950630124000024},
author = {Peter Bauer},
keywords = {Numerical weather prediction, Machine learning, High-performance computing},
abstract = {This paper provides an outlook on the future of operational weather prediction given the recent evolution in science, computing and machine learning. In many parts, this evolution strongly deviates from the strategy operational centres have formulated only several years ago. New opportunities in digital technology have greatly accelerated progress, and the full integration of computational science in numerical weather prediction centres is common knowledge now. Within the last few years, a vast machine learning research community has emerged for creating new and tailor-made products, accelerating processing and – most of all – creating emulators for the entire production of global forecasts that outperform traditional systems at the spatial resolution of the training data. In this context, the role of both numerical models and observations is changing from being equation to data driven. Model simulations and reanalyses are becoming the new currency for training machine learning, and operational centres are in a powerful position as they generate these datasets based on decades worth of experience. This environment creates incredible opportunities to progress much faster than in the past but also uncertainties about what the strategic implications on defining cost-effective and sustainable research and operations are, and how to achieve sufficient high-performance computing and data handling capacities. It will take individual national public services a while to understand what to focus on and how to coordinate their substantial investments in staff and infrastructure at institutional, national and international level. This paper addresses this new situation operational weather prediction finds itself in through formulating the most likely “what if?” scenarios for the near future. It also provides an outline for how weather centres could adapt.}
}
@article{SALEM2025141924,
title = {Novel eco-friendly nicotinonitrile derivative as a corrosion inhibitor for carbon steel: Synthesis, inhibitive efficiency, and DFT analysis},
journal = {Journal of Molecular Structure},
volume = {1335},
pages = {141924},
year = {2025},
issn = {0022-2860},
doi = {https://doi.org/10.1016/j.molstruc.2025.141924},
url = {https://www.sciencedirect.com/science/article/pii/S0022286025006106},
author = {Aya M. Salem and Ahmed Nassef and Ahmed M. Wahba and Samar M. Mohammed},
keywords = {Corrosion, Inhibition, C-steel, HCl, Nicotinonitrile derivatives, Langmuir isotherm},
abstract = {Two new variations of Nicotinonitrile were synthesized, namely: "4-(4-Chlorophenyl)-3-cyano-6-(thien-2-yl)-1H-pyridin-2-one (3A) and 4-(4-Chlorophenyl)-2-oxo-1-(prop‑2-yn-1-yl)-6-(thien-2-yl)-1,2-dihydropyridine-3-carbonitrile (4A). The chemical structures were examined and confirmed using IR and 1H NMR. This method's notable features include being solvent-free, catalyst-free, economical, and having great yields without the need for a catalyst. These compounds were then evaluated as corrosion inhibitors for carbon steel (CS) in 1 M HCl media. Both weight loss (WL) and electrochemical methods such as potentiodynamic polarization (PDP) and electrochemical impedance spectroscopy (EIS) were employed for the investigation. The synthesis and assessment of a novel series of organic compounds with related chemical structures as hydrochloric acid corrosion inhibitors of C-steel is a novel aspect of this study. The results showed that the Nicotinonitrile derivatives were effective corrosion inhibitors, with inhibition efficiencies ( %η) of 86.4 % and 90.7 % for 3A and 4A, respectively, at a concentration of 15×10−5 M. Theoretical computations are applied using the density functional theory. The experiments' findings show that these compounds are effective corrosion inhibitors, and that the concentration of the substances increases the inhibition efficiency. When compared to 3A, the 4A molecule has the maximum efficiency. Monte Carlo simulations and quantum chemical calculations were also performed to analyse and discuss the behaviour of these derivatives. Surface analysis using Scanning Electron Microscopy (SEM) and Energy Dispersive X-ray (EDX) was conducted to verify the results obtained from atomic force microscope measurements. Excellent agreement is found between the outcomes of theoretical computations and experimental measurements.}
}
@incollection{ERNST2021265,
title = {Chapter 22 - Pharmaceutical toxicology},
editor = {Martin Wehling},
booktitle = {Principles of Translational Science in Medicine (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Boston},
pages = {265-279},
year = {2021},
isbn = {978-0-12-820493-1},
doi = {https://doi.org/10.1016/B978-0-12-820493-1.00008-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204931000088},
author = {Steffen W. Ernst and Richard Knight and Jenny Royle and Laura Stephenson},
keywords = {Regulatory toxicology, discovery toxicology, dose-resonse relationship, pharmaceutial safety, drug develoment, risk assessment},
abstract = {This chapter aims to highlight the core principles of pharmaceutical toxicology. It is an interrelated discipline that needs to be applied at all stages of the drug development process to appropriately characterise the safety profile of a drug compound and acknowledge the uncertainties associated with models available. With a strategic mindset, the preclinical safety activities aim to build a comprehensive profile of the drug so that potential hazards can be identified and the risks for healthy trial subjects or patients quantified, and, if necessary, suitable means for eliminating or reducing unacceptable risks can be put in place. We focus on the 2 distinct phases of pharmaceutical toxicology:  Discovery toxicology and regulatory toxicology, to explain how the thinking is built upon at each stage and how the mindset shifts from enabling the selection of an optimally derisked clinical candidate through to thorough risk characterisation and management of those risks for clinical development. Considering attrition due to safety reasons, whether clinical or preclinical, is one of the main reasons for drug project failure, safety assessments should be viewed with equal importance as drug efficacy assessments.}
}
@article{HAN2025104938,
title = {Synergizing artificial intelligence and probiotics: A comprehensive review of emerging applications in health promotion and industrial innovation},
journal = {Trends in Food Science & Technology},
volume = {159},
pages = {104938},
year = {2025},
issn = {0924-2244},
doi = {https://doi.org/10.1016/j.tifs.2025.104938},
url = {https://www.sciencedirect.com/science/article/pii/S0924224425000743},
author = {Xin Han and Qingqiu Liu and Yun Li and Meng Zhang and Kaiyang Liu and Lai-Yu Kwok and Heping Zhang and Wenyi Zhang},
keywords = {Artificial intelligence, Gastrointestinal health, Personalized medicine, Probiotic metabolite},
abstract = {Background
Probiotics play a vital role in human health, garnering significant scientific and public interest. The integration of artificial intelligence (AI) into probiotic research and applications promises to revolutionize strain discovery, health outcomes, and food industry innovations.
Scope and approach
This review explores the intersection of AI and probiotics, focusing on AI-powered machine learning models that revolutionize strain screening, biomarker prediction, and metabolite analysis. Artificial intelligence enables early diagnosis and personalized nutrition by predicting biomarkers for conditions like inflammatory bowel disease and irritable bowel syndrome. It also identifies key probiotic metabolites, such as antimicrobial peptides, exopolysaccharides, and phenolic compounds, advancing fermentation technology and probiotic efficacy. Challenges, including data quality, computational demands, and experimental validation, are also discussed.
Key findings and conclusions
Artificial intelligence outperforms conventional methods, offering rapid, high-precision screening, scalable data analysis, and automated strain optimization. Case studies demonstrate AI models achieving over 97% accuracy in bacterial identification and accelerated metabolite discovery. However, challenges like data quality, computational costs, and model interpretability remain. Overcoming these will strengthen the role of AI in precision nutrition, functional food development, and personalized medicine. This review concludes with future perspectives, emphasizing the potential of AI to revolutionize gut microbiome research and probiotic-based therapeutics.}
}
@incollection{GOI2024353,
title = {13 - Perspective on photonic neuromorphic computing},
editor = {Min Gu and Elena Goi and Yangyundou Wang and Zhengfen Wan and Yibo Dong and Yuchao Zhang and Haoyi Yu},
booktitle = {Neuromorphic Photonic Devices and Applications},
publisher = {Elsevier},
pages = {353-375},
year = {2024},
series = {Photonic Materials and Applications Series},
isbn = {978-0-323-98829-2},
doi = {https://doi.org/10.1016/B978-0-323-98829-2.00009-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323988292000098},
author = {Elena Goi and Min Gu},
keywords = {Neuromorphic photonics, photonic memories, all-optical AI microscopy, hybrid platforms},
abstract = {Bioinspired neuromorphic algorithms can process information more rapidly and more accurately than conventional algorithms, in the attempt to achieve brain-like capacity and efficiency in tasks that are challenging for traditional computers but easy for humans. With the development of applications more performing than ever, the computational requirements for running neuromorphic models are increasing exponentially, motivating efforts to develop new, specialized hardware for fast and efficient execution. Neuromorphic photonics, the implementation of neuromorphic information processing with optoelectronic hardware, is a new computational paradigm based on photons aiming to achieve brain-like information processing in the optical domain, and an interdisciplinary field that is expanding in a multitude of directions. In this chapter, we first revise what we believe are currently the main theoretical and technical challenges in the field and then give a broad perspective on the new directions and opportunities that, in our opinion, represent the current frontiers of neuromorphic photonics.}
}
@article{ROSSITER20237555,
title = {A suite of MATLAB livescript files to support learning of elementary control and feedback concepts},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {7555-7560},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.657},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323010315},
author = {J.A. Rossiter},
keywords = {Virtual laboratories, independent learning, visualisation, livescripts},
abstract = {This paper builds on a body of work in the community which is focussed on sharing learning and teaching resources, especially those which might support a first course in control. Here attention is given to some of the mathematical, analytical and numerical computations which are required to support simple system and feedback analysis and design. The aim is to provide resources which allow students to focus on core concepts and understanding so that the numerical computations are not an obstacle to their investigations. More specifically, this paper focuses on a number of MATLAB livescript files which have been produced to help students visualise the impact of parameter and design choices on system behaviour, while simultaneously empowering them to understand the source code and thus upskill them for the future. The paper gives an overview of the livescripts available so users can decide whether these could be useful in their own context; all are freely available on the author's website (Rossiter, 2021).}
}
@article{DOSHI2020103202,
title = {Recursively modeling other agents for decision making: A research perspective},
journal = {Artificial Intelligence},
volume = {279},
pages = {103202},
year = {2020},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2019.103202},
url = {https://www.sciencedirect.com/science/article/pii/S000437021930027X},
author = {Prashant Doshi and Piotr Gmytrasiewicz and Edmund Durfee},
keywords = {Decision theory, Game theory, Hierarchical beliefs, Multiagent systems, Recursive modeling, Theory of mind},
abstract = {Individuals exhibit theory of mind, attributing beliefs, intent, and mental states to others as explanations of observed actions. Dennett's intentional stance offers an analogous abstraction for computational agents seeking to understand, explain, or predict others' behaviors. These recognized theories provide a formal basis to ongoing investigations of recursive modeling. We review and situate various frameworks for recursive modeling that have been studied in game- and decision- theories, and have yielded methods useful to AI researchers. Sustained attention given to these frameworks has produced new analyses and methods with an aim toward making recursive modeling practicable. Indeed, we also review some emerging uses and the insights these yielded, which are indicative of pragmatic progress in this area. The significance of these frameworks is that higher-order reasoning is critical to correctly recognizing others' intent or outthinking opponents. Such reasoning has been utilized in academic, business, military, security, and other contexts both to train and inform decision-making agents in organizational and strategic contexts, and also to more realistically predict and best respond to other agents' intent.}
}
@article{ZHANG20211358,
title = {Deep learning-based evaluation of factor of safety with confidence interval for tunnel deformation in spatially variable soil},
journal = {Journal of Rock Mechanics and Geotechnical Engineering},
volume = {13},
number = {6},
pages = {1358-1367},
year = {2021},
issn = {1674-7755},
doi = {https://doi.org/10.1016/j.jrmge.2021.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1674775521001268},
author = {Jinzhang Zhang and Kok Kwang Phoon and Dongming Zhang and Hongwei Huang and Chong Tang},
keywords = {Deep learning, Convolutional neural network (CNN), Tunnel safety, Confidence interval, Random field},
abstract = {The random finite difference method (RFDM) is a popular approach to quantitatively evaluate the influence of inherent spatial variability of soil on the deformation of embedded tunnels. However, the high computational cost is an ongoing challenge for its application in complex scenarios. To address this limitation, a deep learning-based method for efficient prediction of tunnel deformation in spatially variable soil is proposed. The proposed method uses one-dimensional convolutional neural network (CNN) to identify the pattern between random field input and factor of safety of tunnel deformation output. The mean squared error and correlation coefficient of the CNN model applied to the newly untrained dataset was less than 0.02 and larger than 0.96, respectively. It means that the trained CNN model can replace RFDM analysis for Monte Carlo simulations with a small but sufficient number of random field samples (about 40 samples for each case in this study). It is well known that the machine learning or deep learning model has a common limitation that the confidence of predicted result is unknown and only a deterministic outcome is given. This calls for an approach to gauge the model's confidence interval. It is achieved by applying dropout to all layers of the original model to retrain the model and using the dropout technique when performing inference. The excellent agreement between the CNN model prediction and the RFDM calculated results demonstrated that the proposed deep learning-based method has potential for tunnel performance analysis in spatially variable soils.}
}
@article{MORETTI1980145,
title = {Computational aerodynamics using mini computers},
journal = {Computers & Fluids},
volume = {8},
number = {1},
pages = {145-153},
year = {1980},
note = {Special Issue: Computers in Aerodynamics},
issn = {0045-7930},
doi = {https://doi.org/10.1016/0045-7930(80)90037-7},
url = {https://www.sciencedirect.com/science/article/pii/0045793080900377},
author = {Gino Moretti},
abstract = {The importance of minicomputers as a research tool in gasdynamics is explained, and a few examples are given to show their efficiency.}
}
@article{HU2024112598,
title = {Unraveling the dynamics of stacking fault nucleation in ceramics: A case study of aluminum nitride},
journal = {Computational Materials Science},
volume = {231},
pages = {112598},
year = {2024},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2023.112598},
url = {https://www.sciencedirect.com/science/article/pii/S092702562300592X},
author = {Yixuan Hu and Yumeng Zhang and Simanta Lahkar and Xiaodong Wang and Qi An and Kolan {Madhav Reddy}},
keywords = {Ceramics, Aluminum nitride, Deformation, Stacking faults, Generalized stacking fault energy},
abstract = {Stacking fault (SF), originating from the emission of partial dislocations, wields significant influence over the structural and physicochemical traits of ceramic materials. Yet, the intricate atomic dynamics driving SF nucleation remain obscured. Here, we introduce an improved methodology for computing the generalized stacking fault energy (GSFE) in ceramics, integrating uneven Degrees of Freedom (DOFs) for distinct lattice sites. This refinement has yielded substantial energy advantages over the traditional rigid shift method inherited from metallic systems. Our findings underscore that the relaxation of nonmetallic N atoms within the SF region is pivotal for achieving a more realistic SF simulation. This, in turn, unveils the involvement of N atom migration within the SF region between different aluminum tetrahedral sites during SF nucleation. By alleviating the energy barrier, this relaxation contrasts with previous simulations where nonmetallic elements remained more rigid. This work demonstrates the atomic dynamics of SF nucleation in ceramics and breaks the conventional wisdom of uniformly applying constraints for GSFE computations.}
}
@article{KHISTY200577,
title = {Possibilities of steering the transportation planning process in the face of bounded rationality and unbounded uncertainty},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {13},
number = {2},
pages = {77-92},
year = {2005},
note = {Handling Uncertainty in the Analysis of Traffic and Transportation Systems (Bari, Italy, June 10–13 2002)},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2005.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X05000161},
author = {C. Jotin Khisty and Turan Arslan},
keywords = {Paradigm shift, Planning, Rationality, Systemicity, Transportation, Uncertainty},
abstract = {This paper describes and discusses the possibilities of steering the transportation planning process in the face of bounded rationality and unbounded uncertainty: (a) through the introduction of the concept of ‘systemicity’; (b) by expanding the spectrum of the existing planning paradigm currently in use; (c) by reducing complexity through the application of tests of adequacy, dependency, suitability, and adaptability; (d) through the introduction of soft systems thinking; and (e) by using ‘abductive’ in addition to deductive and inductive inferencing. It is concluded that the application of these strategies, adjustments, and tests to the existing planning procedure will hopefully enrich and strengthen our planning effort and make it more robust.}
}
@article{JIN2025127620,
title = {LDBMamba: Language-guided Dual-Branch Mamba for hyperspectral image domain generalization},
journal = {Expert Systems with Applications},
volume = {280},
pages = {127620},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127620},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425012424},
author = {Chenyang Jin and Xuyang Teng and Minghui Chu and Yuqi Hao and Senlong Qin and Xiaohui Li and Xiaodong Yu},
keywords = {Mamba, Cross-scene, Hyperspectral image classification, Domain generalization, Multimodal},
abstract = {Recent advancements in domain generalization (DG) for cross-scene hyperspectral image (HSI) classification have succeeded considerably with CNN-based and ViT-based methods. However, CNNs often fail to capture global features, and ViTs, although effective, are hindered by high computational demands and slow inference speeds. These problems limit their practicality in cross-scene HSI applications. The recently introduced Mamba model, which is developed from space-state model, strikes a compelling balance between training efficiency and inference speed, offering a promising alternative for HSI classification. Motivated by Mamba, we propose the Language-guided Dual-Branch Mamba (LDBMamba), specifically designed for cross-scene HSI classification. LDBMamba integrates four key components: Spatial Local-Global Scan (SLGS), Spectral Limited-Board Scan (SLBS), Spatial-Spectral-Star-Fusion Inhibited Mamba (S3FIM), and Contrastive Learning with Label-based and Text-based Prior Knowledge Generation Rule (LTR). SLGS and SLBS transform HSI image blocks into spatial and spectral sequences respectively, which can be effectively modeled by the IM module within S3FIM. The S3F component then fuses and enhances these features. In addition, the incorporation of prior knowledge generated by the LTR can enhance the model’s ability to learn domain-invariant representations through contrastive learning. Comprehensive experiments across three benchmark datasets demonstrate that our model outperforms state-of-the-art CNN-based and ViT-based DG methods and provide a promising direction for cross-scene HSI classification.}
}
@article{REN2025109484,
title = {A multi-criteria decision-making method based on discrete Z-numbers and Aczel-Alsina aggregation operators and its application on early diagnosis of depression},
journal = {Engineering Applications of Artificial Intelligence},
volume = {139},
pages = {109484},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109484},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624016427},
author = {Dong Ren and Xiuqin Ma and Hongwu Qin and Siyue Lei and Xuli Niu},
keywords = {Multi-criteria decision-making, Fuzzy sets, Discrete Z-numbers, Aczel-alsina aggregation operator},
abstract = {In mental health diagnostics, the questionnaire is an effective and cost-effective method. However, the traditional questionnaire test methods for depression and anxiety have great ambiguity. The discrete Z-numbers (DZs) provide solutions for describing and resolving complex fuzzy issues in the intelligent multi-criteria decision-making (MCDM) process. However, large-scale datasets are not suited for the present MCDM techniques due to their extremely high computational cost. Additionally, these techniques are less stable and flexible. To address the above issues, a novel MCDM method is introduced, which is based on the DZs theory and the Aczel-Alsina (AA) aggregation operator (AO) for large-scale datasets. To begin with, centroid points are calculated for DZs, and a series of novel AOs are introduced. And then a score function with a parameter is introduced to balance the influence between the possibility restriction and the fuzzy restriction of DZs. Thirdly, a new MCDM method under DZs is presented based on the proposed AA AOs and score function. Finally, to support the early diagnosis of depression and anxiety, we apply our method to the real-life online Depression, Anxiety, and Stress Scale (DASS) which can be transformed into DZs by our proposed preprocessing method. According to experimental results, our method is applicable to large-scale datasets and has much lower complexity as well as higher flexibility and stability.}
}
@article{JAHEL2023122624,
title = {The future of social-ecological systems at the crossroads of quantitative and qualitative methods},
journal = {Technological Forecasting and Social Change},
volume = {193},
pages = {122624},
year = {2023},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2023.122624},
url = {https://www.sciencedirect.com/science/article/pii/S0040162523003098},
author = {Camille Jahel and Robin Bourgeois and Jérémy Bourgoin and William's Daré and Marie {De Lattre-Gasquet} and Etienne Delay and Patrice Dumas and Christophe {Le Page} and Marc Piraux and Rémi Prudhomme},
keywords = {Quantitative, Qualitative, Anticipation, Foresight, Power relationship, Discontinuities},
abstract = {Urgent calls to transform societies toward more sustainability make the practice of anticipation more and more necessary. The progressive development of computational technologies has opened room for a growing use of quantitative methods to explore the future of social-ecological systems, in addition to qualitative methods. This warrants investigating issues of power relationships and discontinuities and unknowns that arise when mingling quantitative and qualitative anticipatory methods. We first reflected on the semantics attached to these methods. We then conducted a comparative analysis on the way the articulation of quantitative and qualitative methods was conducted, based on an in-depth analysis of a set of eleven anticipatory projects completed by several external case studies. We propose insights to classify projects according to the timing (successive, iterative or convergent) and the purpose of the articulation (imagination, refinement, assessment and awareness raising). We use these insights to explore methodological implications and power relationships and then discuss the ways to inform or frame anticipatory projects that seek to combine these methods.}
}
@article{ESCOUFLAIRE2024129,
title = {Automated text classification of opinion vs. news French press articles. A comparison of transformer and feature-based approaches},
journal = {Language & Communication},
volume = {99},
pages = {129-140},
year = {2024},
issn = {0271-5309},
doi = {https://doi.org/10.1016/j.langcom.2024.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0271530924000624},
author = {Louis Escouflaire and Antonin Descampe and Cédrick Fairon},
keywords = {Subjectivity, Transformers, Feature-based model, Text classification, Discourse analysis, Explainability},
abstract = {This study explores Natural Language Processing (NLP) methods for distinguishing between press articles belonging to the journalistic genres of ‘objective’ news and ‘subjective’ opinion. Two classification models are compared: CamemBERT, a French transformer model fine-tuned for the task, and a machine learning model using 32 linguistic features. Trained on 8000 Belgian French articles, both models are evaluated on 1000 Canadian French articles. Results show CamemBERT’s superiority but highlight potential for hybrid approaches and emphasizes the need for robust and transparent methods in NLP. The research contributes to understanding NLP’s role in journalism by addressing challenges of point of view detection in press discourse.}
}
@article{TERAN2017384,
title = {Dynamic Profiles Using Sentiment Analysis for VAA's Recommendation Design},
journal = {Procedia Computer Science},
volume = {108},
pages = {384-393},
year = {2017},
note = {International Conference on Computational Science, ICCS 2017, 12-14 June 2017, Zurich, Switzerland},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.05.265},
url = {https://www.sciencedirect.com/science/article/pii/S187705091730902X},
author = {Luis Terán and Jose Mancera},
keywords = {Voting Advice Applications, Dynamic Profiles, Recommender Systems, Decision-Making, Elections},
abstract = {In the context of elections, the Internet opens new and promising possibilities for parties and candidates looking for a better political strategy and visibility. In this way they can also organize their election campaign to gather funds, to mobilize support, and to enter into a direct dialogue with the electorate. This paper presents an ongoing research of recommender systems applied on e-government, particularly it is an extension of so-called voting advice applications (VAA’s). VAA’s are Web applications that support voters, providing relevant information on candidates and political parties by comparing their political interests with parties or candidates on different political issues. Traditional VAA’s provide recommendations of political parties and candidates focusing on static profiles of users. The goal of this work is to develop a candidate profile based on different parameters, such as the perspective of voters, social network activities, and expert opinions, to construct a more accurate dynamic profile of candidates. Understanding the elements that compose a candidate profile will help citizens in the decision-making process when facing a lack of information related to the behavior and thinking of future public authorities. At the end of this work, a fuzzy-based visualization approach for a VAA design is given using as a case study the National Elections of Ecuador in 2013.}
}
@article{ZHOU2024124298,
title = {Hyperspectral imaging combined with blood oxygen saturation for in vivo analysis of small intestinal necrosis tissue},
journal = {Spectrochimica Acta Part A: Molecular and Biomolecular Spectroscopy},
volume = {315},
pages = {124298},
year = {2024},
issn = {1386-1425},
doi = {https://doi.org/10.1016/j.saa.2024.124298},
url = {https://www.sciencedirect.com/science/article/pii/S1386142524004645},
author = {Yao Zhou and LeChao Zhang and DanFei Huang and Yong Zhang and LiBin Zhu and Xiaoqing Chen and Guihua Cui and Qifan Chen and XiaoJing Chen and Shujat Ali},
keywords = {Hyperspectral imaging, Tissue oxygenation, Small intestine tissue, Isosbestic points},
abstract = {Acute mesenteric ischemia (AMI) is a clinically significant vascular and gastrointestinal condition, which is closely related to the blood supply of the small intestine. Unfortunately, it is still challenging to properly discriminate small intestinal tissues with different degrees of ischemia. In this study, hyperspectral imaging (HSI) was used to construct pseudo-color images of oxygen saturation about small intestinal tissues and to discriminate different degrees of ischemia. First, several small intestine tissue models of New Zealand white rabbits were prepared and collected their hyperspectral data. Then, a set of isosbestic points were used to linearly transform the measurement data twice to match the reference spectra of oxyhemoglobin and deoxyhemoglobin, respectively. The oxygen saturation was measured at the characteristic peak band of oxyhemoglobin (560 nm). Ultimately, using the oxygenated hemoglobin reflectance spectrum as the benchmark, we obtained the relative amount of median oxygen saturation in normal tissues was 70.0 %, the IQR was 10.1 %, the relative amount of median oxygen saturation in ischemic tissues was 49.6 %, and the IQR was 14.6 %. The results demonstrate that HSI combined with the oxygen saturation computation method can efficiently differentiate between normal and ischemic regions of the small intestinal tissues. This technique provides a powerful support for internist to discriminate small bowel tissues with different degrees of ischemia, and also provides a new way of thinking for the diagnosis of AMI.}
}
@article{ZOU2025106959,
title = {LCFFNet: A Lightweight Cross-scale Feature Fusion Network for human pose estimation},
journal = {Neural Networks},
volume = {183},
pages = {106959},
year = {2025},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106959},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024008888},
author = {Xuelian Zou and Xiaojun Bi},
keywords = {Human pose estimation, 2d dynamic multi-scale convolution, Contextual semantic information, Adaptive feature fusion},
abstract = {Human pose estimation is one of the most critical and challenging problems in computer vision. It is applied in many computer vision fields and has important research significance. However, it is still a difficult challenge to strike a balance between the number of parameters and computing load of the model and the accuracy of human pose estimation. In this study, we suggest a Lightweight Cross-scale Feature Fusion Network (LCFFNet) to strike a balance between accuracy and computational load and parameter volume. The Lightweight HRNet-Like (LHRNet) network, Cross-Resolution-Aware Semantics Module (CRASM), and Adapt Feature Fusion Module (AFFM) make up LCFFNet. To be more precise, first, we suggest a lightweight LHRNet network that includes Dynamic Multi-scale Convolution Basic (DMSC-Basic block) block, Basic block, and DMSC-Basic block submodules in the network’s three high-resolution subnetwork stages. The proposed dynamic multi-scale convolution in DMSC-Basic block can reduces the amount of model parameters and complexity of the LHRNet network, and has the ability to extract variable pose features. In order to maintain the model’s ability to express features, the Basic block is introduced. As a result, the LHRNet network not only makes the model more lightweight but also enhances its feature expression capabilities. Second, we propose a CRASM module to enhance contextual semantic information while reducing the semantic gap between different scales by fusing features from different scales. Finally, the augmented semantic feature map’s spatial resolution is finally restored from bottom to top using our suggested AFFM, and adaptive feature fusion is used to increase the positioning accuracy of important sites. Our method successfully predicts keypoints with 74.2 % AP, 89.9 % PCKh@0.5 and 66.9 % AP on the MSCOCO 2017, MPII and Crowdpose datasets, respectively. Our model reduces the number of parameters by 89.0 % and the computational complexity by 87.5 % compared with HRNet. The proposed network performs as well as current large-model human pose estimation networks while outperforming state-of the-art lightweight networks.}
}
@article{CARUSO1996135,
title = {Reported earliest memory age: Relationships with personality and coping variables},
journal = {Personality and Individual Differences},
volume = {21},
number = {1},
pages = {135-142},
year = {1996},
issn = {0191-8869},
doi = {https://doi.org/10.1016/0191-8869(96)00021-9},
url = {https://www.sciencedirect.com/science/article/pii/0191886996000219},
author = {John C. Caruso and Charles L. Spirrison},
abstract = {The present study examined the viability of motivated retrieval failure as a mechanism for childhood amnesia. A total of 115 undergraduates completed the Constructive Thinking Inventory, the NEO Personality Inventory, and answered questions regarding their earliest memory in order to assess the relationships between personality and coping variables and the subject's reported age of earliest memory. The personality and coping inventories were divided into four groups defined by the stated age of each subject's earliest memory. Analyses indicated that scores on two Constructive Thinking Inventory scales (Emotional Coping and Categorical Thinking) and two NEO Personality Inventory scales (Neuroticism and Openness to Experience) were significantly different between groups. These differences were consistent with the motivated retrieval failure hypothesis. Subscales of these four scales were then analyzed individually to further examine the differences between groups. Results are discussed in the context of personality assessment and the repression of early memories.}
}
@article{OUDMAN2018214,
title = {Effects of different cue types on the accuracy of primary school teachers' judgments of students' mathematical understanding},
journal = {Teaching and Teacher Education},
volume = {76},
pages = {214-226},
year = {2018},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2018.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X17302305},
author = {Sophie Oudman and Janneke {van de Pol} and Arthur Bakker and Mirjam Moerbeek and Tamara {van Gog}},
keywords = {Teacher judgment, Judgment accuracy, Cue utilization, Primary education, Mathematics education, Decimals},
abstract = {To gain insight into how teachers' judgment accuracy can be improved, we investigated effects of cue-type availability. While thinking aloud, 21 teachers judged their fourth grade students' (n = 176) decimal magnitude understanding. Sensitivity (correctly judging what students did understand) did not improve from availability of both answer cues (students' answers to prior practice problems) and student cues (knowledge of students triggered by knowing their names), and was lower when only answer cues were available, compared to only student cues. Specificity (correctly judging what students did not understand) was higher when only answer cues were available, compared to only student cues or both student and answer cues.}
}
@article{MARTIN1993141,
title = {Neural connections, mental computation: Lynn Nadel, Lynn A. Cooper, Peter Culicover and R. Michael Harnish, eds.},
journal = {Artificial Intelligence},
volume = {62},
number = {1},
pages = {141-151},
year = {1993},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(93)90052-D},
url = {https://www.sciencedirect.com/science/article/pii/000437029390052D},
author = {Benjamin Martin}
}
@incollection{MILLER2017141,
title = {8 - Doctoral and professional programs},
editor = {Susan M. Miller and Walter H. Moos and Barbara H. Munk and Stephen A. Munk},
booktitle = {Managing the Drug Discovery Process},
publisher = {Woodhead Publishing},
address = {Boston},
pages = {141-169},
year = {2017},
isbn = {978-0-08-100625-2},
doi = {https://doi.org/10.1016/B978-0-08-100625-2.00008-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780081006252000088},
author = {Susan M. Miller and Walter H. Moos and Barbara H. Munk and Stephen A. Munk},
keywords = {Critical thinking, Basic/applied/clinical, Problem identification, Research design, Teams, PhD/PharmD, Postdoc/postdoctoral, Writing/publishing.},
abstract = {In this chapter on graduate and professional education, we explore Doctoral and professional programs, posing a number of key questions you should ask yourself. Where to apply to graduate school or a postdoc, and why? With whom should you work? A PhD or a PharmD, and to postdoc or not? What must you do to be successful? Moreover, we touch on traits important to becoming an independent researcher and ask whether success in graduate school or a postdoctoral fellowship requires different skills than undergraduate degrees. Critical thinking habits and skills underpin this discussion. We outline possible career choices, touching on the knowledge and expertise used by drug hunters, and also ask what might be of most value to potential employers. Each of us is different, and what's best for you is something you will have to decipher, but hopefully only after you consult with family, friends, and advisors or mentors. Regardless, “the big leap” is coming, so get ready.}
}
@article{CARPENTER1992457,
title = {Chapter 4 Cognitively guided instruction: Building on the knowledge of students and teachers},
journal = {International Journal of Educational Research},
volume = {17},
number = {5},
pages = {457-470},
year = {1992},
issn = {0883-0355},
doi = {https://doi.org/10.1016/S0883-0355(05)80005-9},
url = {https://www.sciencedirect.com/science/article/pii/S0883035505800059},
author = {Thomas P. Carpenter and Elizabeth Fennema},
abstract = {This chapter summarizes the results of a series of correlational, experimental, and case studies on Cognitively Guided Instruction (CGI), a program designed to help teachers understand children's thinking and use this knowledge to make instructional decisions. Results of the studies show that teachers' knowledge and beliefs about students' thinking are related to students' achievement. There were significant differences between CGI classes and control classes on the emphasis on problem solving and low level skills, the freedom given to students to construct their own strategies for solving problems, the teachers' knowledge of their students thinking, and the students' achievement in both problem solving and skills.}
}
@article{EVANS201123659,
title = {Advancing Science through Mining Libraries, Ontologies, and Communities*},
journal = {Journal of Biological Chemistry},
volume = {286},
number = {27},
pages = {23659-23666},
year = {2011},
issn = {0021-9258},
doi = {https://doi.org/10.1074/jbc.R110.176370},
url = {https://www.sciencedirect.com/science/article/pii/S0021925819487164},
author = {James A. Evans and Andrey Rzhetsky},
keywords = {Biophysics, Computation, Computer Modeling, Drug Design, Epigenetics, Computational Biology, Information Cascade, Sociology of Science, Text Mining},
abstract = {Life scientists today cannot hope to read everything relevant to their research. Emerging text-mining tools can help by identifying topics and distilling statements from books and articles with increased accuracy. Researchers often organize these statements into ontologies, consistent systems of reality claims. Like scientific thinking and interchange, however, text-mined information (even when accurately captured) is complex, redundant, sometimes incoherent, and often contradictory: it is rooted in a mixture of only partially consistent ontologies. We review work that models scientific reason and suggest how computational reasoning across ontologies and the broader distribution of textual statements can assess the certainty of statements and the process by which statements become certain. With the emergence of digitized data regarding networks of scientific authorship, institutions, and resources, we explore the possibility of accounting for social dependences and cultural biases in reasoning models. Computational reasoning is starting to fill out ontologies and flag internal inconsistencies in several areas of bioscience. In the not too distant future, scientists may be able to use statements and rich models of the processes that produced them to identify underexplored areas, resurrect forgotten findings and ideas, deconvolute the spaghetti of underlying ontologies, and synthesize novel knowledge and hypotheses.}
}
@article{GADZHIEV2025101314,
title = {Creating A dynamic cognovisor – Brain activity recognition using principal Component analysis and Machine learning models},
journal = {Cognitive Systems Research},
volume = {89},
pages = {101314},
year = {2025},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101314},
url = {https://www.sciencedirect.com/science/article/pii/S1389041724001086},
author = {Ismail M. Gadzhiev and Alexander S. Makarov and Vadim L. Ushakov and Vyacheslav A. Orlov and Georgy A. Ivanitsky and Sergei A. Dolenko},
keywords = {Brain Activity, Cognitive States, Machine Learning, Principal Component Analysis},
abstract = {This study explores the feasibility of developing a dynamic cognovisor capable of recognizing cognitive states and transitions using fMRI data. Data were collected from 31 participants performing spatial and verbal tasks during fMRI scanning and were preprocessed using a nine-step algorithm for artifact removal and denoising. Three types of classification problems were examined, with machine learning methods and dimensionality reduction techniques applied to classify activity states. The best-performing models were identified for each classification problem, providing insights into their applicability. Notably, binary classification of resting versus active states achieved good quality with relatively simple methods. A key finding underscores the importance of accounting for temporal history of the signal prior to the prediction moment to improve model performance.}
}
@incollection{SCHNEEGANS2008241,
title = {13 - Dynamic Field Theory as a Framework for Understanding Embodied Cognition},
editor = {Paco Calvo and Antoni Gomila},
booktitle = {Handbook of Cognitive Science},
publisher = {Elsevier},
address = {San Diego},
pages = {241-271},
year = {2008},
series = {Perspectives on Cognitive Science},
issn = {15564495},
doi = {https://doi.org/10.1016/B978-0-08-046616-3.00013-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008046616300013X},
author = {Sebastian Schneegans and Gregor Schöner},
abstract = {Publisher Summary
Embodied cognition is an approach to cognition that has roots in motor behavior. This approach emphasizes that cognition typically involves acting with a physical body on an environment in which that body is immersed. The approach of embodied cognition postulates that understanding cognitive processes entails understanding their close link to the motor surfaces that may generate action and to the sensory surfaces that provide sensory signals about the environment. To a certain extent, the embodiment stance implies a mistrust of the abstraction inherent in much information processing thinking, in which the interface between cognitive processes and their sensorimotor support is drawn at a level that is quite removed from both the sensory and the motor systems. New theoretical tools are needed to address cognition within the embodiment perspective. This chapter reviews one set of theoretical concepts which is believed to be particularly suited to address the constraints of embodiment and situatedness. It refers to this set of concepts as Dynamical Systems Thinking.}
}
@incollection{STEIN202139,
title = {Chapter 2 - Brain–minds: What’s the best metaphor?},
editor = {Dan J. Stein},
booktitle = {Problems of Living},
publisher = {Academic Press},
pages = {39-59},
year = {2021},
isbn = {978-0-323-90239-7},
doi = {https://doi.org/10.1016/B978-0-323-90239-7.00005-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323902397000055},
author = {Dan J. Stein},
keywords = {Psychiatry, Philosophy, Realism, Psychiatric classification, Pluralism, Erklären, Verstehen, Epistemic humility, Practical wisdom},
abstract = {This chapter addresses the question of how best to think about the brain–mind from both philosophical and psychiatric perspectives. The section on philosophy of mind notes the positions of physicalism, dualism, and functionalism, and proposes that emergent materialism has particular advantages. The section on psychiatry notes the positions of behaviourism and existentialism. Two key metaphors of the brain–mind are then critiqued: the hydraulic model of psychoanalysis, and the computational model of cognitive science. A third metaphor, that of ‘wetware’, which emphasizes that the brain–mind cannot simply be divided into hardware and software, but rather that it must be approached as a complex psychobiological phenomenon, is proposed. Several advantages of this metaphor are discussed, including that it is consistent with emergent materialism and a view of the brain–mind as embodied and embedded in social activity, as well as with current cognitive-affective and psychiatric science.}
}
@article{VANOPHEUSDEN2019127,
title = {Tasks for aligning human and machine planning},
journal = {Current Opinion in Behavioral Sciences},
volume = {29},
pages = {127-133},
year = {2019},
note = {Artificial Intelligence},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2019.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S2352154619300622},
author = {Bas {van Opheusden} and Wei Ji Ma},
abstract = {Research on artificial intelligence and research on human intelligence rely on similar conceptual foundations and have long inspired each other [1,2•]. However, achieving concrete synergy has been difficult, with one obstacle being a lack of alignment of the tasks used in both fields. Artificial intelligence research has traditionally focused on tasks that are challenging to solve, often using human performance as a benchmark to surpass [3, 4, 5, 6, 7]. By contrast, cognitive science and psychology have moved towards tasks that are simple enough to allow for detailed computational modeling of people’s choices. These divergent objectives have led to a divide in the complexity of tasks studied, both in perception and cognition. The purpose of this paper is to explore the middle ground: are there tasks that are reasonably attractive to both fields and could provide fertile ground for synergy?}
}
@incollection{BLISS19921,
title = {REASONING SUPPORTED BY COMPUTATIONAL TOOLS},
editor = {MICHAEL R. KIBBY and J. ROGER HARTLEY},
booktitle = {Computer Assisted Learning: Selected Contributions from the CAL '91 Symposium},
publisher = {Pergamon},
address = {Amsterdam},
pages = {1-9},
year = {1992},
isbn = {978-0-08-041395-2},
doi = {https://doi.org/10.1016/B978-0-08-041395-2.50007-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780080413952500078},
author = {JOAN BLISS and JON OGBORN and RICHARD BOOHAN and JONATHAN BRIGGS and TIM BROSNAN and DEREK BROUGH and HARVEY MELLAR and ROB MILLER and CAROLINE NASH and CATHY RODGERS and BABIS SAKONIDIS},
abstract = {Abstract
This paper sets out the work of the Tools for Exploratory Learning Programme within the ESRC Initiative Information Technology in Education. The research examines young secondary children's reasoning with computational tools. We distinguish between exploratory and expressive modes of learning, that is, interaction with another's model and creation of one's own model, respectively. The research focuses on reasoning, rather than learning, along three dimensions: quantitative, qualitative, and semi-quantitative. It provides a 3 × 2 classification of tasks according to modes of learning and types of reasoning. Modelling tools were developed for the study and descriptions of these are given. The research examined children's reasoning with tools in all three dimensions looking more exhaustively at the semi-quantitative. Pupils worked either in an exploratory mode or an expressive mode on one of the following topics: Traffic, Health and Diet, and Shops and Profits. They spent 3-4 h individually with a researcher over 2 weeks, carrying out four different activities: reasoning without the computer; learning to manipulate first the computer then later the tool and finally carrying out a task with the modelling tool. Pupils were between 12 and 14 yr. Research questions both about children's reasoning when working with or creating models and about the nature of the tools used are discussed. Finally an analytic scheme is set out which describes the nature of the causal and non-causal reasoning observed together with some tentative results.}
}
@article{GOVIL2022103125,
title = {Validation of agile methodology as ideal software development process using Fuzzy-TOPSIS method},
journal = {Advances in Engineering Software},
volume = {168},
pages = {103125},
year = {2022},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2022.103125},
url = {https://www.sciencedirect.com/science/article/pii/S0965997822000357},
author = {Nikhil Govil and Ashish Sharma},
keywords = {Software Development Process, Decision support system, Fuzzy logic, Agile Software Development, Fuzzy TOPSIS, Multi-Criteria Decision Making},
abstract = {Agile methodologies have been an emerging choice of software professionals for the past decade and a half. However, apart from this, some other SDLC models are also available for selection in front of software developers to develop any software. Usually, project managers select any of these models to develop software through their past experiences. There is no logical basis for this selection to be completely correct, as a result of which there is always a risk of software failure or over budget if an inappropriate model has opted. Keeping this problem of software industries in mind, an ideal SDLC model has been identified mathematically in this article. In this article, we applied the Fuzzy TOPSIS method that validates Agile software development as an ideal choice. We have taken a total of six software development processes that are being applied globally. Feedback from five experienced decision-makers has been taken in the form of linguistic terms and further converted into fuzzy values to perform the computation of the closeness coefficient rank of each experimented alternative software development process.}
}
@article{SHIN2023104897,
title = {Pedagogical discourse markers in online algebra learning: Unraveling instructor's communication using natural language processing},
journal = {Computers & Education},
volume = {205},
pages = {104897},
year = {2023},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2023.104897},
url = {https://www.sciencedirect.com/science/article/pii/S0360131523001744},
author = {Jinnie Shin and Renu Balyan and Michelle P. Banawan and Tracy Arner and Walter L. Leite and Danielle S. McNamara},
keywords = {Pedagogical communication, Online learning, Video lectures, Natural language processing},
abstract = {Despite the proliferation of video-based instruction and its benefits—such as promoting student autonomy and self-paced learning—the complexities of online teaching remain a challenge. To be effective, educators require extensive training in digital teaching methodologies. As such, there's a pressing need to examine and comprehend the intricacies of instructors' communication patterns within this context. This research addresses the pressing need to understand pedagogical discourse in online video lectures in Algebra classes by employing computational linguistic tools and natural language processing (NLP). Using transcripts from 125 Algebra 1 video lectures—comprising 4962 instances of pedagogical discourse—from five instructors at Math Nation, a virtual math learning environment, we analyzed the conveyance of linguistic, attitudinal, and emotional nuances. With the aid of 26 Coh-Metrix and SÉANCE features, we classified educators' language choices, achieving an accuracy of 86.7%. Furthermore, variations in language choices, as signified by discourse markers, were examined through a K-means clustering approach. The resulting 17 clusters were grouped into interpersonal, structural, and cognitive pedagogic functions. Through this exploration, we demonstrate the promising potential of NLP in efficiently deciphering pedagogical communication patterns in video lectures. These insights open a new avenue for research, aimed at assessing the efficacy of digital instruction by scrutinizing pedagogical discourse characteristics in computer-based learning environments.}
}