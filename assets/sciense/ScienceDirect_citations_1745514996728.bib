@article{QIN2025105181,
title = {Deconstructing customer satisfaction recipes: A dynamic configurational framework leveraging the power of online reviews in tourism contexts},
journal = {Tourism Management},
volume = {110},
pages = {105181},
year = {2025},
issn = {0261-5177},
doi = {https://doi.org/10.1016/j.tourman.2025.105181},
url = {https://www.sciencedirect.com/science/article/pii/S0261517725000512},
author = {Yong Qin and Chaoguang Luo and Eric W.T. Ngai},
keywords = {Customer satisfaction, Online reviews, Panel fsQCA, Three-factor theory, Tourism contexts},
abstract = {The complex relationship between product or service attribute performance and customer satisfaction in online review environments has been widely discussed. However, attribute configuration for enhancing customer satisfaction under holistic thinking, particularly considering temporal interactions among attributes, remains underexplored. To address this gap, this study develops a dynamic configurational framework based on complexity and three-factor theories to deconstruct attribute recipes and their temporal evolution for improving customer satisfaction in tourism contexts. By extracting key service attributes and affects directly from tourist reviews, it identifies patterns that consistently achieve high satisfaction over time. This is the first study to integrate online review mining with panel fuzzy-set qualitative comparative analysis from a customer experience perspective. Findings reveal the existence of multiple equivalent causal pathways, with distinct satisfaction pathways for different tourist segments. Practically, normative causal recipes assist policymakers in optimizing tourism resource allocation and providing strategic insights to dynamically respond to tourist demands.}
}
@incollection{DIX2003381,
title = {CHAPTER 14 - Upside-Down ∀s and Algorithms—Computational Formalisms and Theory},
editor = {John M. Carroll},
booktitle = {HCI Models, Theories, and Frameworks},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {381-429},
year = {2003},
series = {Interactive Technologies},
isbn = {978-1-55860-808-5},
doi = {https://doi.org/10.1016/B978-155860808-5/50014-9},
url = {https://www.sciencedirect.com/science/article/pii/B9781558608085500149},
author = {Alan Dix}
}
@article{CAI2004135,
title = {Why do U.S. and Chinese students think differently in mathematical problem solving?: Impact of early algebra learning and teachers’ beliefs},
journal = {The Journal of Mathematical Behavior},
volume = {23},
number = {2},
pages = {135-167},
year = {2004},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2004.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732312304000124},
author = {Jinfa Cai},
abstract = {This paper reports two studies that examined the impact of early algebra learning and teachers’ beliefs on U.S. and Chinese students’ thinking. The first study examined the extent to which U.S. and Chinese students’ selection of solution strategies and representations is related to their opportunity to learn algebra. The second study examined the impact of teachers’ beliefs on their students’ thinking through analyzing U.S. and Chinese teachers’ scoring of student responses. The results of the first study showed that, for the U.S. sample, students who have formally learned algebraic concepts are as likely to use visual representations as those who have not formally learned algebraic concepts in their problem solving. For the Chinese sample, students rarely used visual representations whether or not they had formally learned algebraic concepts. The findings of the second study clearly showed that U.S. and Chinese teachers view students’ responses involving concrete strategies and visual representations differently. Moreover, although both U.S. and Chinese teachers value responses involving more generalized strategies and symbolic representations equally high, Chinese teachers expect 6th graders to use the generalized strategies to solve problems while U.S. teachers do not. The research reported in this paper contributed to our understanding of the differences between U.S. and Chinese students’ mathematical thinking. This research also established the feasibility of using teachers’ scoring of student responses as an alternative and effective way of examining teachers’ beliefs.}
}
@article{LOTURCO2022104059,
title = {The knowledge and skill content of production complexity},
journal = {Research Policy},
volume = {51},
number = {8},
pages = {104059},
year = {2022},
note = {Special Issue on Economic Complexity},
issn = {0048-7333},
doi = {https://doi.org/10.1016/j.respol.2020.104059},
url = {https://www.sciencedirect.com/science/article/pii/S0048733320301372},
author = {Alessia {Lo Turco} and Daniela Maggioni},
keywords = {Occupational complexity, Services, Regional growth, STEM},
abstract = {In this paper we investigate the labour content of complex products. By exploiting O*NET information on the skill and knowledge required by occupations, we find that the product complexity measure suggested by Hausmann and Hidalgo (2009) is highly intensive in STEM knowledge and in Science, Mathematics and Critical Thinking skill requirements. We then propose a new measure of occupational complexity based on these occupational features. Among other advantages, this indicator has the merit to measure complexity for service industries that, so far, has never been measured. In an empirical model of the growth of USA Metropolitan Areas (MSAs), we find that MSAs whose initial industrial structure embeds a higher level of occupational complexity experience higher real per capita GDP growth over the 2001–2017 period. The occupational complexity measure is a stronger predictor of growth than other metrics of industries’ occupational and task content as well as compared to indicators of local occupational and industrial composition. When we separately compute occupational complexity of service and manufacturing industries and delve into their specific role for long run growth, we find a prominent role of the occupation complexity embedded in local services with respect to the one embedded in local manufacturing. Our baseline evidence is corroborated in the context of the NUTS3 regions of France over the period 2010–2017.}
}
@article{HERNIMAN2021373,
title = {Interrelationships between depressive symptoms and positive and negative symptoms of recent onset schizophrenia spectrum disorders: A network analytical approach},
journal = {Journal of Psychiatric Research},
volume = {140},
pages = {373-380},
year = {2021},
issn = {0022-3956},
doi = {https://doi.org/10.1016/j.jpsychires.2021.05.038},
url = {https://www.sciencedirect.com/science/article/pii/S0022395621003071},
author = {Sarah E. Herniman and Lisa J. Phillips and Stephen J. Wood and Sue M. Cotton and Edith J. Liemburg and Kelly A. Allott},
keywords = {Comorbidity, Co-occurrence, Early psychosis, Depression, Affect},
abstract = {Objective
There is a need to better understand the interrelationships between positive and negative symptoms of recent-onset schizophrenia spectrum disorders (SSD) and co-occurring depressive symptoms. Aims were to determine: (1) whether depressive symptoms are best conceptualised as distinct from, or intrinsic to, positive and negative symptoms; and (2) bridging symptoms.
Methods
Network analysis was applied to data from 198 individuals with depressive and psychotic symptoms in SSD from the Psychosis Recent Onset GRoningen Survey (PROGR-S). Measures were: Montgomery–Åsberg Depression Rating Scale and Positive and Negative Syndrome Scale.
Results
Positive symptoms were just as likely to be associated with depressive and negative symptoms, and had more strong associations with depressive than negative symptoms. Negative symptoms were more likely to be associated with depressive than positive symptoms, and had more strong associations with depressive than positive symptoms. Suspiciousness and stereotyped thinking bridged between positive and depressive symptoms, and apparent sadness and lassitude between negative and depressive symptoms.
Conclusions
Depressive symptoms might be best conceptualised as intrinsic to positive and negative symptoms pertaining to deficits in motivation and interest in the psychotic phase of SSD. Treatments targeting bridges between depressive and positive symptoms, and depressive and such negative symptoms, might prevent or improve co-occurring depressive symptoms, or vice-versa, in the psychotic phase of SSD.}
}
@article{GREGORY198254,
title = {Current design thinking: 24 papers from Design 79, I Chem E Midlands Branch (available from (ChemE Rugby) 336 pp, £15},
journal = {Design Studies},
volume = {3},
number = {1},
pages = {54},
year = {1982},
issn = {0142-694X},
doi = {https://doi.org/10.1016/0142-694X(82)90084-9},
url = {https://www.sciencedirect.com/science/article/pii/0142694X82900849},
author = {Sydney Gregory}
}
@article{LI202231,
title = {Application Analysis of Artificial Intelligent Neural Network Based on Intelligent Diagnosis},
journal = {Procedia Computer Science},
volume = {208},
pages = {31-35},
year = {2022},
note = {7th International Conference on Intelligent, Interactive Systems and Applications},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922014478},
author = {Yukun Li},
keywords = {Intelligent diagnosis, artificial intelligence network, automobile fault diagnosis, the neural network},
abstract = {In recent years, the continuous development of computer science and AI technology makes the application prospect of artificial intelligence in fault diagnosis emerge. As a simulation technology of human thinking pattern, intelligent diagnosis technology can check and manage the monitoring target in real time to ensure the accuracy of data information. This paper introduces the basic principles of key artificial intelligence technologies in the field of sports, such as convolutional neural network, object detection, object tracking and action recognition. Then it analyzes the application status of intelligent diagnosis technology and artificial intelligence network under intelligent diagnosis, and puts forward the application of artificial intelligence neural network in automobile fault diagnosis based on examples. In the construction of the neural network system, the real-time collection of vehicle operation data can be analyzed, once the fault is found, the driver can be notified in time to avoid safety accidents. The author summarizes the existing research results on the application of artificial intelligence algorithm in intelligent diagnosis, in order to provide help for the subsequent research.}
}
@article{KONG20241462,
title = {On locality of quantum information in the Heisenberg picture for arbitrary states},
journal = {Chinese Journal of Physics},
volume = {89},
pages = {1462-1473},
year = {2024},
issn = {0577-9073},
doi = {https://doi.org/10.1016/j.cjph.2024.04.028},
url = {https://www.sciencedirect.com/science/article/pii/S0577907324001655},
author = {Otto C.W. Kong},
keywords = {Quantum information, Quantum locality, Deutsch–Hayden descriptors, Noncommutative values of observables},
abstract = {The locality issue of quantum mechanics is a key issue to a proper understanding of quantum physics and beyond. What has been commonly emphasized as quantum nonlocality has received an inspiring examination through the notion of the Heisenberg picture of quantum information. Deutsch and Hayden established a local description of quantum information in a setting of quantum information flow in a system of qubits. With the introduction of a slightly modified version of what we call the Deutsch–Hayden matrix values of observables, together with our recently introduced parallel notion of the noncommutative values from a more fundamental perspective, we clarify all the locality issues based on such values as quantum information carried by local observables in any given arbitrary state of a generic composite system. Quantum information as the ‘quantum’ values of observables gives a transparent conceptual picture of all that. Spatial locality for a projective measurement is also discussed. The pressing question is if and how such information for an entangled system can be retrieved through local processes which can only be addressed with new experimental thinking.}
}
@article{BADIA2024101049,
title = {Analysing the radiation reliability, performance and energy consumption of low-power SoC through heterogeneous parallelism},
journal = {Sustainable Computing: Informatics and Systems},
volume = {44},
pages = {101049},
year = {2024},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2024.101049},
url = {https://www.sciencedirect.com/science/article/pii/S2210537924000945},
author = {Jose M. Badia and German Leon and Mario Garcia-Valderas and Jose A. Belloch and Almudena Lindoso and Luis Entrena},
keywords = {Heterogeneous parallelism, System-on-Chip, Fault tolerance, Energy consumption, Neutron irradiation},
abstract = {This study focuses on the low-power Tegra X1 System-on-Chip (SoC) from the Jetson Nano Developer Kit, which is increasingly used in various environments and tasks. As these SoCs grow in prevalence, it becomes crucial to analyse their computational performance, energy consumption, and reliability, especially for safety-critical applications. A key factor examined in this paper is the SoC’s neutron radiation tolerance. This is explored by subjecting a parallel version of matrix multiplication, which has been offloaded to various hardware components via OpenMP, to neutron irradiation. Through this approach, this researcher establishes a correlation between the SoC’s reliability and its computational and energy performance. The analysis enables the identification of an optimal workload distribution strategy, considering factors such as execution time, energy efficiency, and system reliability. Experimental results reveal that, while the GPU executes matrix multiplication tasks more rapidly and efficiently than the CPU, using both components only marginally reduces execution time. Interestingly, GPU usage significantly increases the SoC’s critical section, leading to an escalated error rate for both Detected Unrecoverable Errors (DUE) and Silent Data Corruptions (SDC), with the CPU showing a higher average number of affected elements per SDC.}
}
@article{M2023120604,
title = {Design of a Cognitive Knowledge Representation Model to Assess the Reasoning Levels of Primary School Children},
journal = {Expert Systems with Applications},
volume = {231},
pages = {120604},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120604},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423011065},
author = {Srivani M. and Abirami Murugappan},
keywords = {Customized AI based teaching, Cognitive performance test, Reasoning coefficient, Cognition level, Knowledge representation, Cognitive metrics},
abstract = {Background and aim:
In recent days, the research on student’s intelligence level modelling is a challenging Artificial Intelligence (AI) task, which gains more attraction because it provides actionable insights to the tutor by analysing the intelligence level of the learners. Each learner’s knowledge, comprehension, and intellectual capacities are unique. It is critical to identify these capacities and provide learners, particularly slow learners, with the necessary knowledge. Cognitive Performance Test (CPT) is an essential component for assessing the knowledge level of students. The reasoning level or coefficient deals with the analysis of the thinking capability in a logical way. It also reflects the child’s learning potential. The main aim of the proposed system is to design a Cognitive Knowledge Representation Model (CKRM), which fuses Cognitive Performance Metrics (CPM) calculation and Reasoning Coefficient Calculation (RCC) algorithms to assess the student’s intelligence level. The result of the proposed system is stratification of students to three different ranges of reasoning coefficient.
Methods:
The CKRM consists of the following phases: data collection, statistical Exploratory Data Analysis (EDA), model building and analysis, which involve the assessment of the knowledge level using CPT and calculation of reasoning coefficient using First Order Logic (FOL), and finally model evaluation using cognitive evaluation metrics. CPM and RCC algorithms have been proposed in this paper to calculate the student’s reasoning coefficient by using the forward chaining FOL inference engine. The dataset is a real time data which consists of the academic and cognitive performance details of school students from classes 1 to 6 for the year 2019 to 2020. The academic data are collected from the Educational Management Information System (EMIS) maintained by the school. The cognitive performance data are collected by conducting the tests for the students using the memory training application called Lumosity.
Results:
The proposed system’s performance is evaluated using ten Machine Learning (ML) algorithms in which the Quadratic Discriminant Analysis achieved an accuracy of 0.97 for classes 1, 2, and 3. For classes 4, 5, and 6, nearly twelve ML algorithms are evaluated in which Random Forest (RF) Classifier achieved an accuracy of 0.98. Six math expert committee teachers concluded that the reasoning coefficient value was acceptable with an average accuracy of 0.92 for classes 1, 2, 3 and 0.9 for classes 4, 5, 6. In comparison to the pre-existing models employed in the prior research, it was determined that the created CKRM (academic and cognitive) was superior. The cognitive metrics such as taskability, Response Time (RT), knowledge capacity and utilization has also been evaluated. The average values of taskability, RT, knowledge capacity and knowledge utilization are 0.85, 0.81, 0.55, and 0.44.
Conclusion:
The ultimate goal is to make customized teaching easier; hence, this article involves determining a student’s cognitive level by estimating their reasoning coefficient. The suggested approach analyses and categorizes students’ cognitive abilities, such as memory, reasoning, problem solving, thinking, and logical reasoning, using three different reasoning coefficients. This approach assists teachers in determining the degree of intelligence of their students.}
}
@article{PETERSON20223586,
title = {Physical computing for materials acceleration platforms},
journal = {Matter},
volume = {5},
number = {11},
pages = {3586-3596},
year = {2022},
issn = {2590-2385},
doi = {https://doi.org/10.1016/j.matt.2022.09.022},
url = {https://www.sciencedirect.com/science/article/pii/S2590238522005409},
author = {Erik Peterson and Alexander Lavin},
keywords = {materials acceleration platforms, AI-driven science, simulation intelligence, physical computing, self-driving labs, inverse design, computational metamaterials},
abstract = {Summary
A “technology lottery” describes a research idea or technology succeeding over others because it is suited to available software/hardware and not necessarily because it is superior. The nascent field of self-driving laboratories, particularly materials acceleration platforms (MAPs), is at risk: while it is logical and opportunistic to inject existing lab equipment and workflows with artificial intelligence (AI) and automation, such MAPs can constrain research by proliferating existing biases in science, mechatronics, and general-purpose computing. Rather than conformity, MAPs present opportunity to pursue new vectors of engineering physics with advances in cyber-physical learning and closed-loop, self-optimizing systems. We outline a simulation-based MAP program to design computers that use physics to solve optimization problems: the physical computing (PC)-MAP can mitigate hardware-software-substrate-user information losses present in all other MAP classes and eliminate lotteries by perfecting alignment between computing problems and media. We describe early PC advances and research pursuits toward optimal design of new materials and computing media.}
}
@article{LIU2024105391,
title = {Exploring three pillars of construction robotics via dual-track quantitative analysis},
journal = {Automation in Construction},
volume = {162},
pages = {105391},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105391},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524001274},
author = {Yuming Liu and Aidi Hizami Bin Alias and Nuzul Azam Haron and Nabilah Abu Bakar and Hao Wang},
keywords = {Construction robotics, BERTopic model, BIM, Human–robot collaboration, Deep reinforcement learning, Dual-track quantitative analysis},
abstract = {Construction robotics has emerged as a leading technology in the construction industry. This paper conducts an innovative dual-track quantitative comprehensive method to analyze the current literature and assess future trends. First, a bibliometric review of 955 journal articles published between 1974 and 2023 was performed, exploring keywords, journals, countries, and clusters. Furthermore, a neural topic model based on BERTopic addresses topic modeling repetition issues. The study identifies building information modeling (BIM), human–robot collaboration (HRC), and deep reinforcement learning (DRL) as “three pillars” in the field. Additionally, we systematically reviewed the relevant literature and nested symbiotic relationships. The outcome of this study is twofold: first, the findings provide quantitative and qualitative scientific guidance for future research on trends; second, the innovative dual-track quantitative analysis research methodology simultaneously stimulates critical thinking about the modeling of other similarly trending topics characterized to avoid high degree of homogeneity and corpus overlap.}
}
@article{LI2021104369,
title = {Elementary effects analysis of factors controlling COVID-19 infections in computational simulation reveals the importance of social distancing and mask usage},
journal = {Computers in Biology and Medicine},
volume = {134},
pages = {104369},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.104369},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521001633},
author = {Kelvin K.F. Li and Stephen A. Jarvis and Fayyaz Minhas},
keywords = {COVID-19, Agent-based modelling, Coronavirus, Simulation, SARS-COV-2, netlogo, Python, Epidemiology, Survival, Infectious diseases, VIRUS, Stochastic processes, Stochasticity, Social distancing, Masks, Isolation, Lockdown},
abstract = {COVID-19 was declared a pandemic by the World Health Organisation (WHO) on March 11th, 2020. With half of the world's countries in lockdown as of April due to this pandemic, monitoring and understanding the spread of the virus and infection rates and how these factors relate to behavioural and societal parameters is crucial for developing control strategies. This paper aims to investigate the effectiveness of masks, social distancing, lockdown and self-isolation for reducing the spread of SARS-CoV-2 infections. Our findings from an agent-based simulation modelling showed that whilst requiring a lockdown is widely believed to be the most efficient method to quickly reduce infection numbers, the practice of social distancing and the usage of surgical masks can potentially be more effective than requiring a lockdown. Our multivariate analysis of simulation results using the Morris Elementary Effects Method suggests that if a sufficient proportion of the population uses surgical masks and follows social distancing regulations, then SARS-CoV-2 infections can be controlled without requiring a lockdown.}
}
@article{CHARPIN2012613,
title = {A computational linear elastic fracture mechanics-based model for alkali–silica reaction},
journal = {Cement and Concrete Research},
volume = {42},
number = {4},
pages = {613-625},
year = {2012},
issn = {0008-8846},
doi = {https://doi.org/10.1016/j.cemconres.2012.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0008884612000051},
author = {Laurent Charpin and Alain Ehrlacher},
keywords = {Alkali-Aggregate Reaction (C), Expansion (C), Microcracking (B), Energy Criterion, Particle Size distribution (B)},
abstract = {A fracture mechanics model for alkali–silica reaction (ASR) is presented that deals with the case of a concrete made up of dense spherical aggregates. Chemistry and diffusion (of ions and gel) are not modelled. The focus is put on the mechanical consequences of the progressive replacement of the aggregates by a less dense gel. A ring-shaped crack then appears in the cement paste depending on the pressure build-up, according to an incremental energy criterion. The stored elastic energy and deformation of each configuration are determined assuming that each aggregate is embedded in an infinite cement paste matrix, through Finite Element Analysis. We note a very different behaviour of aggregates of different sizes. Adding the contributions of different aggregates leads to an estimate of the free expansion of a concrete of given aggregate size distribution. Parameters of the model are identified, providing a good fit to experiments taken from Multon's work.}
}
@incollection{ALISEDA2007431,
title = { - Logical, Historical and Computational Approaches},
editor = {Theo A.F. Kuipers},
booktitle = {General Philosophy of Science},
publisher = {North-Holland},
address = {Amsterdam},
pages = {431-513},
year = {2007},
series = {Handbook of the Philosophy of Science},
issn = {18789846},
doi = {https://doi.org/10.1016/B978-044451548-3/50010-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780444515483500100},
author = {Atocha Aliseda and Donald Gillies},
abstract = {Publisher Summary
This chapter focuses on the logical, historical and computational approaches to the philosophy of science. It discusses how the logical approach to philosophy of science was introduced by the Vienna Circle, and developed by them and their followers and associates. The logical approach to philosophy of science remained the dominant subject throughout the 1950s; but, from the early 1960s, it was challenged by a striking development of the historical approach. The historical approach was not introduced for the ﬁrst time in the 1960s. On the contrary, it had been developed by Mach and Duhem much earlier. Although, Mach and Duhem are cited by the Vienna Circle as important inﬂuences on their philosophy, the Vienna Circle did not adopt the historical features of these two thinkers. In the excitement generated by the new logic of Frege and Russell, history of science seems to have been temporarily forgotten. The general idea of the historical approach is not new in the 1960s, however, that decade saw striking developments in this approach. After Kuhn, the analysis of scientiﬁc revolutions became a major problem for philosophy of science, while Lakatos applied the historical approach to mathematics for the ﬁrst time.}
}
@article{CHEN20171,
title = {Heterogeneity in generalized reinforcement learning and its relation to cognitive ability},
journal = {Cognitive Systems Research},
volume = {42},
pages = {1-22},
year = {2017},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2016.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041716300559},
author = {Shu-Heng Chen and Ye-Rong Du},
keywords = {Generalized reinforcement learning, Experience-weighted attraction learning, Cognitive ability, Granularity},
abstract = {In this paper, we study the connections between working memory capacity (WMC) and learning in the context of economic guessing games. We apply a generalized version of reinforcement learning, popularly known as the experience-weighted attraction (EWA) learning model, which has a connection to specific cognitive constructs, such as memory decay, the depreciation of past experience, counterfactual thinking, and choice intensity. Through the estimates of the model, we examine behavioral differences among individuals due to different levels of WMC. In accordance with ‘Miller’s magic number’, which is the constraint of working memory capacity, we consider two different sizes (granularities) of strategy space: one is larger (finer) and one is smaller (coarser). We find that constraining the EWA models by using levels (granules) within the limits of working memory allows for a better characterization of the data based on individual differences in WMC. Using this level-reinforcement version of EWA learning, also referred to as the EWA rule learning model, we find that working memory capacity can significantly affect learning behavior. Our likelihood ratio test rejects the null that subjects with high WMC and subjects with low WMC follow the same EWA learning model. In addition, the parameter corresponding to ‘counterfactual thinking ability’ is found to be reduced when working memory capacity is low.}
}
@article{FATTAHITABASI20221151,
title = {Design and mechanism of building responsive skins: State-of-the-art and systematic analysis},
journal = {Frontiers of Architectural Research},
volume = {11},
number = {6},
pages = {1151-1176},
year = {2022},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2022.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S2095263522000565},
author = {Saba {Fattahi Tabasi} and Saeed Banihashemi},
keywords = {Responsive skin, Architectural design, Mechanism design},
abstract = {The demand to satisfy environmental and economic performance requirements of buildings highlights the application of the responsive skin facades in offering superior performance, as compared to conventional façades. With this respect, responsive skins have become a growing field of research during the recent decade while a thorough review of studies investigating their design and technology aspects is still missing. To fill the identified gap, this study aims to present a systematic literature review and state of the art in an untouched research area of the responsive skins, integrated with their geometric and mechanism design approaches. To this end, a total of 89 studies, collected from two major bibliographic databases of Scopus and Google Scholar from the first of 2010 to the mid of 2021, were reviewed and several classifications and analyses on the associated design thinking, skin systems and responsive mechanisms were presented. The gap analysis of the findings indicates that the lack of controllable substitution design for mechanical skins is one of the reasons preventing the application of responsive skins in construction industry. Furthermore, the gap between simulation and constructability and the relationship between the designed skin geometry with climatic analysis and performance provide basis for future studies.}
}
@article{GRIFFITHS2020873,
title = {Understanding Human Intelligence through Human Limitations},
journal = {Trends in Cognitive Sciences},
volume = {24},
number = {11},
pages = {873-883},
year = {2020},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2020.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1364661320302151},
author = {Thomas L. Griffiths},
keywords = {artificial intelligence, inductive bias, meta-learning, rational meta-reasoning, cultural evolution},
abstract = {Recent progress in artificial intelligence provides the opportunity to ask the question of what is unique about human intelligence, but with a new comparison class. I argue that we can understand human intelligence, and the ways in which it may differ from artificial intelligence, by considering the characteristics of the kind of computational problems that human minds have to solve. I claim that these problems acquire their structure from three fundamental limitations that apply to human beings: limited time, limited computation, and limited communication. From these limitations we can derive many of the properties we associate with human intelligence, such as rapid learning, the ability to break down problems into parts, and the capacity for cumulative cultural evolution.}
}
@incollection{KALET2014479,
title = {Chapter 5 - Computational Models and Methods},
editor = {Ira J. Kalet},
booktitle = {Principles of Biomedical Informatics (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {479-578},
year = {2014},
isbn = {978-0-12-416019-4},
doi = {https://doi.org/10.1016/B978-0-12-416019-4.00005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124160194000056},
author = {Ira J. Kalet},
keywords = {Computational models and methods, Computing with genes, Computing with proteins, Computing with cells, Natural language processing, State machines, Dynamic models, Stochastic processes},
abstract = {This chapter introduces additional methods for deriving useful results from data and for creating complex models of biological processes. These methods include: search through data suitably organized, as sequences, or as networks, natural language processing, and modeling with state machines.}
}
@article{MASHALEH20242245,
title = {IoT Smart Devices Risk Assessment Model Using Fuzzy Logic and PSO},
journal = {Computers, Materials and Continua},
volume = {78},
number = {2},
pages = {2245-2267},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2023.047323},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824001267},
author = {Ashraf S. Mashaleh and Noor Farizah Binti Ibrahim and Mohammad Alauthman and Mohammad Almseidin and Amjad Gawanmeh},
keywords = {IoT botnet detection, risk assessment, fuzzy logic, particle swarm optimization (PSO), cybersecurity, interconnected devices},
abstract = {Increasing Internet of Things (IoT) device connectivity makes botnet attacks more dangerous, carrying catastrophic hazards. As IoT botnets evolve, their dynamic and multifaceted nature hampers conventional detection methods. This paper proposes a risk assessment framework based on fuzzy logic and Particle Swarm Optimization (PSO) to address the risks associated with IoT botnets. Fuzzy logic addresses IoT threat uncertainties and ambiguities methodically. Fuzzy component settings are optimized using PSO to improve accuracy. The methodology allows for more complex thinking by transitioning from binary to continuous assessment. Instead of expert inputs, PSO data-driven tunes rules and membership functions. This study presents a complete IoT botnet risk assessment system. The methodology helps security teams allocate resources by categorizing threats as high, medium, or low severity. This study shows how CICIoT2023 can assess cyber risks. Our research has implications beyond detection, as it provides a proactive approach to risk management and promotes the development of more secure IoT environments.}
}
@article{LI2025e42756,
title = {Classifying breast intraductal proliferative lesions via a knowledge distillation framework using convolutional neural network-based nuclei-segmentation-assisted classification (KDCNN-NSAC)},
journal = {Heliyon},
pages = {e42756},
year = {2025},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2025.e42756},
url = {https://www.sciencedirect.com/science/article/pii/S2405844025011375},
author = {Xiangmin Li and Jiamei Chen and Bo Luo and Minyan Xia and Xu Zhang and Hangjia Zhu and Yutian Zhang-Cai and Yongshun Chen and Yang Yang and Yaofeng Wen},
keywords = {Breast intraductal proliferative lesions, Breast cancer, Knowledge distillation, Convolutional neural network, Nuclei segmentation, Classification},
abstract = {ABSTRACT
Background and objective
Diagnosis of breast intraductal proliferative lesions (BIDPLs) in hematoxylin-eosin (HE) images remains a time-consuming and intractable topic because of subjective processes and subtle morphological differences. Convolutional neural networks (CNNs) show great potential for providing objective analysis strategies for HE images. In this study, we proposed a novel knowledge distillation (KD) framework using CNN-based nuclei segmentation-assisted classification (KDCNN-NSAC).
Methods
The diagnosis of BIDPLs is treated as multiple class classification tasks in the BReAst Carcinoma Subtyping dataset. The KDCNN-NSAC fully leveraged the epithelial and stromal nuclei-level features in training phases and performed region-of-interest (ROI)-level classifications in predicting phases. Then, the whole slide image (WSI) was diagnosed based on the risk ratings of the ROIs within it, instead of processing a WSI.
Results
The principal results showed that in ROI-level classifications, KDCNN-NSAC outperformed the state-of-the-art methods for 7-class classification with an average F1 score of 63.26% and achieves F1 score of 98.36% and 94.21%, respectively, in distinguishing BIDPLs from invasive cancer and normal tissue. The WSI-level predictions obtained a high degree of consistency with the pathologists’ annotation (kappa value of 0.88). Ablation experiments showed that nuclei segmentation and classification components improve the performance of the baseline model in KDCNN-NSAC by 3%.
Conclusions
The KDCNN-NSAC makes the model focus on important cellular information and predicts the WSI in accordance with the pathologists’ diagnostic thinking, thus improving model explainability. Moreover, the introduce of KDCNN-NSAC will help achieve superior performance in diagnosing BIDPLs.}
}
@article{AIGBAVBOA20173003,
title = {Sustainable Construction Practices: “A Lazy View” of Construction Professionals in the South Africa Construction Industry},
journal = {Energy Procedia},
volume = {105},
pages = {3003-3010},
year = {2017},
note = {8th International Conference on Applied Energy, ICAE2016, 8-11 October 2016, Beijing, China},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2017.03.743},
url = {https://www.sciencedirect.com/science/article/pii/S1876610217308068},
author = {Clinton Aigbavboa and Ifije Ohiomah and Thulisile Zwane},
keywords = {Climate change, sustainable thinking, sustainable construction practices, South Africa},
abstract = {The construction industry has been found to cause damaging effects to the environment by means of waste generation, energy and water depletion and several other forms of damage to the environment. This damage has led to experts and environmentalist calling for a sustainable way of carrying out construction activities. Thus, this study addresses the challenges hindering the adoption of sustainable construction practices in the South Africa construction industry. The data used in this research were sourced from both primary and secondary sources. The primary data was collected through a questionnaire aimed at practicing construction professional in the South African construction industry. Indicative Findings from the questionnaire survey revealed that the foremost challenges faced by South African construction industry towards the adoption of sustainable construction practices is the assumption (a lazy view) of additional cost to building projects, followed by limited understanding of the benefits of sustainable construction amongst others. The study contributes to sustainability thinking in the South African construction industry; and it is recommended that strategies and actions should be pursued actively to speed up the process in creating a sustainable-oriented construction industry, which is paramount towards building a sustainable future.}
}
@article{MIASNIKOVA202126,
title = {Cross-frequency phase coupling of brain oscillations and relevance attribution as saliency detection in abstract reasoning},
journal = {Neuroscience Research},
volume = {166},
pages = {26-33},
year = {2021},
issn = {0168-0102},
doi = {https://doi.org/10.1016/j.neures.2020.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0168010219305772},
author = {Aleksandra Miasnikova and Gleb Perevoznyuk and Olga Martynova and Mikhail Baklushev},
keywords = {Abstract reasoning, Salience, Phase synchronization, Cross-frequency coupling, Phase-to-phase coupling, EEG},
abstract = {Abstract reasoning is associated with the ability to detect relations among objects, ideas, events. It underlies the understanding of other individuals’ thoughts and intentions. In natural settings, individuals have to infer relevant associations that have proven to be reliable or precise predictors. Salience theory suggests that the attribution of meaning to stimulus depends on their contingency, saliency, and relevance to adaptation. So far, subjective estimates of relevance have mostly been explored in motivation and implicit learning. Mechanisms underlying formation of associations in abstract thinking with regard to their subjective relevance, or salience, are not clear. Applying novel computational methods, we investigated relevance detection in categorization tasks in 17 healthy individuals. Two models of relevance detection were developed: a conventional one with nouns from the same semantic category, an aberrant one based on an insignificant common feature. Control condition introduced non-related words. The participants were to detect either a relevant principle or an insignificant feature to group presented words. In control condition they inferred that the stimuli were irrelevant to any grouping idea. Cross-frequency phase coupling analysis revealed statistically distinct patterns of synchronization representing search and decision in the models of normal and aberrant relevance detection. Significantly distinct frontotemporal functional networks with central and parietal components in the theta and alpha frequency bands may reflect differences in relevance detection.}
}
@incollection{PATTANAIK2025513,
title = {Chapter 26 - Challenges and limitations for cloud-based platforms and integration with AI algorithms for earth observation data analytics},
editor = {Vishakha Sood and Dileep Kumar Gupta and Sartajvir Singh and Biswajeet Pradhan},
booktitle = {Google Earth Engine and Artificial Intelligence for Earth Observation},
publisher = {Elsevier},
pages = {513-526},
year = {2025},
series = {Earth Observation},
isbn = {978-0-443-27372-8},
doi = {https://doi.org/10.1016/B978-0-443-27372-8.00022-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443273728000222},
author = {Srinibas Pattanaik and  Himanshi and Anshu Mehta and Alessandro Vinciarelli},
keywords = {AI inclusion, Analytics for predictive, Cloud computing, Data preprocessing, Data storage, Data visualization, Generative AI, Machine learning, Remote sensing, Systems within the service},
abstract = {Critical possibilities and leap forwards in checking the climate, debacle avoidance, and asset the executives emerge from the mix of computerized reasoning (artificial intelligence) methods with cloud-based stages for perception of the earth (EO) and information examination. Yet, there are a few snags and limitations related with this reconciliation. Information heterogeneity is one of the fundamental snags, requiring complex preprocessing and normalization procedures to ensure exactness and similarity across different datasets. The tremendous sum and speed of EO information led to adaptability challenges, requiring the utilization of dependable cloud framework and successful calculations for ongoing information handling. Simulated intelligence models additionally require superior execution cloud assets, which can be costly and asset serious because of their computational power. Because of the responsiveness of natural information and the reliance on external cloud suppliers, protection and security give additional surface. Moreover, coordinating numerous advances, guidelines, and conventions consistently can be a difficult technique all by itself during the joining system. Finally, the viability and accuracy of computer-based intelligence-driven investigation might be frustrated by the shortage of marked preparing information for regulated learning models.}
}
@article{GAN2024102786,
title = {Large models for intelligent transportation systems and autonomous vehicles: A survey},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102786},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102786},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624004348},
author = {Lu Gan and Wenbo Chu and Guofa Li and Xiaolin Tang and Keqiang Li},
keywords = {Intelligent transportation systems, Autonomous vehicles, Survey, Large models, Efficient deployment techniques},
abstract = {Large models are widely used in intelligent transportation systems (ITS) and autonomous vehicles (AV) due to their excellent new capabilities such as intelligence emergence, domain application adaptive, and multi-task learning. By integrating massive amounts of data, vehicles and transportation systems based on large models can understand the real-world environment, simulate the reasoning process of human drivers, optimize traffic flow, and improve driving safety and efficiency. However, in the practical implementation of large models, there are three key research questions: (1) How can model reasoning be consistent with the target task? (2) How to improve the redundancy of structures and weights caused by complex models? (3) How to solve the problems of supercomputing power, high latency, and large throughput of large models in practical deployment? These challenges have stimulated the development of deployment techniques for large models. Although existing review articles discuss large model technologies from singular or partial perspectives, there remains a lack of comprehensive systematic investigations into the application and deployment of large models for ITS and AV. Therefore, this paper conducted a quantitative analysis of scientific literature, demonstrating the necessity and significance of studying large models for ITS and AV. Subsequently, it outlined the concept and characteristics of large models and provided a detailed summary of the frontier progress of large models for ITS and AV. Moreover, to bridge the gap between large model inference and target tasks, address structural and weight redundancies, and tackle challenges in practical deployment such as high computational power, latency, and throughput, it explored efficient deployment techniques to accelerate the rapid deployment of large models. Finally, it discussed the challenges and future trends. This paper aims to provide researchers and engineers with an understanding of the forefront advancements and future trends of large models to facilitate the rapid implementation of large models and accelerate their development in ITS and AV.}
}
@article{GAO20241233,
title = {Hetero-Bäcklund transformation, bilinear forms and multi-solitons for a (2＋1)-dimensional generalized modified dispersive water-wave system for the shallow water},
journal = {Chinese Journal of Physics},
volume = {92},
pages = {1233-1239},
year = {2024},
issn = {0577-9073},
doi = {https://doi.org/10.1016/j.cjph.2024.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0577907324003940},
author = {Xin-Yi Gao},
keywords = {Shallow water, Nonlinear and dispersive long gravity waves, (2＋1)-dimensional generalized modified dispersive water-wave system, Hetero-Bäcklund transformation, Bilinear form, Soliton, Symbolic computation},
abstract = {This shallow-water-directed paper plans to consider a (2＋1)-dimensional generalized modified dispersive water-wave (2DGMDWW) system, which describes the nonlinear and dispersive long gravity waves travelling along two horizontal directions in the shallow water of uniform depth. With symbolic computation, (1) a hetero-Bäcklund transformation is constructed, coupling the solutions as for the 2DGMDWW system with the solutions as for a known (2＋1)-dimensional Boiti-Leon-Pempinelli system describing the water waves in an infinitely narrow channel of constant depth, with that hetero-Bäcklund transformation dependent on the shallow-water coefficients in the 2DGMDWW system, with the former solutions indicating certain shallow-water-wave patterns for the height of the water surface and the horizontal velocity of the water wave, while with the latter solutions related to the horizontal velocity and elevation of the water wave; (2) two sets of the bilinear forms are obtained, each set of which is shown to depend on the shallow-water coefficients in the 2DGMDWW system and to be linked to certain shallow-water-wave patterns for the height of the water surface and the horizontal velocity of the water wave; and (3) two sets of the N-soliton solutions are also worked out, each set of which is seen to rely on the shallow-water coefficients in the 2DGMDWW system and to represent the existence of N-solitonic shallow-water-wave patterns with respect to the height of the water surface and the horizontal velocity of the water wave, with N as a positive integer.}
}
@incollection{DORFLER202057,
title = {Artificial Intelligence},
editor = {Mark Runco and Steven Pritzker},
booktitle = {Encyclopedia of Creativity (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Oxford},
pages = {57-64},
year = {2020},
isbn = {978-0-12-815615-5},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.23863-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093245238637},
author = {Viktor Dörfler},
keywords = {AI, AI paradigms, Artificial intelligence, Artificial neural networks, Cognition, Creativity, Intuition, Learning, Machine learning, Mind, Mind and machine, Narrow AI, Thinking, Thinking machines, Wide AI},
abstract = {ARTIFICIAL INTELLIGENCE is a label coined to describe machines that can perform something humans would perform through thinking. In this chapter, I am looking at artificial intelligence (AI) specifically in the context of creativity. My view is inevitably a personal one, as for the time being, the answers to the tough questions on AI entail working with beliefs more so than with facts, opinions more so than hard evidence. What matters most in “AI Creativity” is how we define creativity, as this definition will determine whether we can consider AI to be creative, now or in the future. It is also important to explore what kind of impact AI has or may have on human creativity.}
}
@article{CASTELLO202354,
title = {Towards competency-based education in the chemical engineering undergraduate program in Uruguay: Three examples of integrating essential skills},
journal = {Education for Chemical Engineers},
volume = {44},
pages = {54-62},
year = {2023},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2023.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S1749772823000210},
author = {E. Castelló and C. Santiviago and J. Ferreira and R. Coniglio and E. Budelli and V. Larnaudie and M. Passeggi and I. López},
keywords = {Engineering education, Professional skills, Unconventional laboratory practice, Industrial Internships, Autonomous learning},
abstract = {In 2021, Universidad de la República in Uruguay approved a new Chemical Engineering undergraduate program that incorporates novel conceptual definitions such as competency-based education. This paper describes the process of defining the new curriculum plan and presents the program's structure, as well as specific and cross-disciplinary competencies. These competencies are then compared to the learning outcomes established in the guide for programs accreditation of the Institution of Chemical Engineers. To provide practical examples of how the competency-based approach was incorporated into the program, three specific cases are presented. The first case focuses on the implementation of the internship and industry project. The second case illustrates the incorporation of computational tools as an essential part of different courses throughout the degree program. Finally, the third case describes a new design for the fluid mechanics laboratory that emphasizes hands-on learning and helps students develop several competencies.}
}
@article{WORBOYS199885,
title = {Computation with imprecise geospatial data},
journal = {Computers, Environment and Urban Systems},
volume = {22},
number = {2},
pages = {85-106},
year = {1998},
issn = {0198-9715},
doi = {https://doi.org/10.1016/S0198-9715(98)00023-4},
url = {https://www.sciencedirect.com/science/article/pii/S0198971598000234},
author = {Michael Worboys},
abstract = {Imprecision in spatial data arises from the granularity or resolution at which observations of phenomena are made, and from the limitations imposed by computational representations, processing and presentational media. Precision is an important component of spatial data quality, and a key to appropriate integration of collections of data sets. Previous work of the author provides a theoretical foundation for imprecision of spatial data resulting from finite granularities, and gives the beginnings of an approach to reasoning with such data using methods similar to rough set theory. This paper develops the theory further, and extends the work to a model that includes both spatial and semantic components. Notions such as observation, schema, frame of discernment and vagueness are examined and formalised.}
}
@article{HO1993567,
title = {Recent Applications of Symbolic Computation in Control System Design},
journal = {IFAC Proceedings Volumes},
volume = {26},
number = {2, Part 2},
pages = {567-570},
year = {1993},
note = {12th Triennal Wold Congress of the International Federation of Automatic control. Volume 2 Robust Control, Design and Software, Sydney, Australia, 18-23 July},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)49006-9},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017490069},
author = {D.W.C. Ho and J. Lam and S.K. Tin and C.Y. Han},
keywords = {Symbolic Computation, Computer-aided Control System Design},
abstract = {In this paper we describe several recent applications of symbolic computation to control system design based on MACSYMA. These include routines to calculate the transfer function of a control system in block diagram representation and to compute and simplify state space realizations of multivariable control systems. The programs provide a quick way to formulate design problem and automate the calculation in the initial stage of a control system design process. An example on the application of these routines in the setting up of generalized plant state space realization for use in H∞/H2 optimial control is provided}
}
@article{CROSSLEY2024100865,
title = {A large-scale corpus for assessing written argumentation: PERSUADE 2.0},
journal = {Assessing Writing},
volume = {61},
pages = {100865},
year = {2024},
issn = {1075-2935},
doi = {https://doi.org/10.1016/j.asw.2024.100865},
url = {https://www.sciencedirect.com/science/article/pii/S1075293524000588},
author = {S.A. Crossley and Y. Tian and P. Baffour and A. Franklin and M. Benner and U. Boser},
keywords = {Corpus linguistics, Writing assessment, Argumentation, Individual differences},
abstract = {This research methods article introduces the open source PERSUADE 2.0 corpus. The PERSUADE 2.0 corpus comprises over 25,000 argumentative essays produced by 6th-12th grade students in the United States for 15 prompts on two writing tasks: independent and source-based writing. The PERSUADE 2.0 corpus also provides detailed individual and demographic information for each writer. The goal of the PERSUADE 2.0 corpus is to advance research into relationships between discourse elements, their effectiveness, writing quality, writing tasks and prompts, and demographic and individual differences.}
}
@article{ZHOU20241018,
title = {A 21st Century View of Allowed and Forbidden Electrocyclic Reactions},
journal = {The Journal of Organic Chemistry},
volume = {89},
number = {2},
pages = {1018-1034},
year = {2024},
issn = {0022-3263},
doi = {https://doi.org/10.1021/acs.joc.3c02103},
url = {https://www.sciencedirect.com/science/article/pii/S0022326324000720},
author = {Qingyang Zhou and Garrett Kukier and Igor Gordiy and Roald Hoffmann and Jeffrey I. Seeman and K. N. Houk},
abstract = {In 1965, Woodward and Hoffmann proposed a theory to predict the stereochemistry of electrocyclic reactions, which, after expansion and generalization, became known as the Woodward–Hoffmann Rules. Subsequently, Longuet-Higgins and Abrahamson used correlation diagrams to propose that the stereoselectivity of electrocyclizations could be explained by the correlation of reactant and product orbitals with the same symmetry. Immediately thereafter, Hoffmann and Woodward applied correlation diagrams to explain the mechanism of cycloadditions. We describe these discoveries and their evolution. We now report an investigation of various electrocyclic reactions using DFT and CASSCF. We track the frontier molecular orbitals along the intrinsic reaction coordinate and modeled trajectories and examine the correlation between HOMO and LUMO for thermally forbidden systems. We also investigate the electrocyclizations of several highly polarized systems for which the Houk group had predicted that donor–acceptor substitution can induce zwitterionic character, thereby providing low-energy pathways for formally forbidden reactions. We conclude with perspectives on the field of pericyclic reactions, including a refinement as the meaning of Woodward and Hoffmann’s “Violations. There are none!” Lastly, we comment on the burgeoning influence of computations on all fields of chemistry.}
}
@incollection{RUBIN2023125,
title = {Chapter 9 - Unlocking creative tensions with a paradox approach},
editor = {Roni Reiter-Palmon and Sam Hunter},
booktitle = {Handbook of Organizational Creativity (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {125-145},
year = {2023},
isbn = {978-0-323-91840-4},
doi = {https://doi.org/10.1016/B978-0-323-91840-4.00006-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323918404000062},
author = {Matthew Rubin and Ella Miron-Spektor and Joshua Keller},
keywords = {Creativity, Innovation, Paradox, Mindset, Paradoxical leadership, Culture},
abstract = {Leaders and their employees must navigate competing yet interrelated demands and processes when developing and implementing creative ideas. They have to engage in divergent and convergent thinking, challenge existing assumptions and accept them, plan and persist while remaining spontaneous and adaptive. We explore how, why, and when adopting a paradox approach to navigating such tensions enhances creativity and innovation. Rather than seeking to eliminate the discomfort associated with tensions by prioritizing one demand or process over the other, the paradox approach sees tensions as an opportunity for growth and learning. When adopting a paradox approach, people feel comfortable with the discomfort as tensions arise and recognize that by engaging in one process they enable the seemingly opposing process. We review research on paradoxical frames, mindset, and leadership, and offer a comprehensive theoretical model that delineates the related cognitive, affective, motivational, and social pathways, as well as contextual and cultural boundary conditions. We conclude by identifying promising future directions for research.}
}
@article{LOMBARDI2024e00322,
title = {Semantic modelling and HBIM: A new multidisciplinary workflow for archaeological heritage},
journal = {Digital Applications in Archaeology and Cultural Heritage},
volume = {32},
pages = {e00322},
year = {2024},
issn = {2212-0548},
doi = {https://doi.org/10.1016/j.daach.2024.e00322},
url = {https://www.sciencedirect.com/science/article/pii/S2212054824000079},
author = {Matteo Lombardi and Dario Rizzi},
keywords = {Digital archaeology, Semantic modelling, HBIM, Blender, BlenderBIM, Extended matrix},
abstract = {The aim of the study is to describe a methodological approach to represent, interpret, model and manage pluristratified archaeological contexts. The proposed methodology envisages a digital workflow, a BIM-thinking strategy, which integrates geometric and texture data obtained from laser and photogrammetric scans with information about construction techniques and materials, archaeological reports and documentation. The integration is based on a balanced combination of open-source and proprietary solutions, allowing professionals to work with their “comfort software” and assuring interoperability through the adoption of Open Standards. Experimentations are being conducted exploring the potential of connecting semantic 3D modelling and virtual reconstructions based on archaeological data made with Blender and the Extended Matrix Tool, with BIM software and capabilities thanks to the BlenderBIM addon. The proposed workflow, in combination with the described data-sharing-oriented process, adopts a new approach towards 3D models in order to promote a more sustainable mindset towards 3D dataset life-cycle by optimizing their usage and reducing waste on different levels, such as re-documenting the same structure twice. The expected overall result is the ability to generate semantic models that can enhance our understanding of the context as much as foster multidisciplinary BIM (Building Information Modelling) collaboration thus improving archaeological research, documentation and conservation practices.}
}
@article{ALKABI202368,
title = {Proposed artificial intelligence algorithm and deep learning techniques for development of higher education},
journal = {International Journal of Intelligent Networks},
volume = {4},
pages = {68-73},
year = {2023},
issn = {2666-6030},
doi = {https://doi.org/10.1016/j.ijin.2023.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S2666603023000039},
author = {Amin {Al Ka'bi}},
keywords = {Artificial intelligence (AI), Communication systems, Higher education, Neural networks, Attention process, Deep learning},
abstract = {Artificial intelligence (AI) has been increasingly impacting various aspects of our daily lives, including education. With the rise of digital technologies, higher education has also been experiencing a transformation, and AI has been playing a crucial role in this transformation. The application of AI in higher education has been rapidly increasing, with a focus on improving student engagement, increasing efficiency, and enhancing the learning experience. The use of AI in higher education is not without its challenges and ethical considerations. One of the biggest challenges is ensuring the accuracy and fairness of AI algorithms, as well as avoiding potential biases. In addition, there are concerns about the privacy of student data, as well as the potential for AI to replace human instructors and support staff. Another challenge is ensuring that AI is used in a way that supports the overall goals of higher education, such as promoting critical thinking and creativity, rather than just being used as a tool for automating tasks and increasing efficiency. In this article, we will discuss the various ways in which AI is being applied in higher education where a proposed model for improving the cognitive capability of students is proposed and compared to other existing algorithms. It will be shown that the proposed model shows better performance compared to other models.}
}
@article{YECKEL1998206,
title = {Three-dimensional computations of solution hydrodynamics during the growth of potassium dihydrogen phosphate: II. Spin down},
journal = {Journal of Crystal Growth},
volume = {191},
number = {1},
pages = {206-224},
year = {1998},
issn = {0022-0248},
doi = {https://doi.org/10.1016/S0022-0248(98)00102-X},
url = {https://www.sciencedirect.com/science/article/pii/S002202489800102X},
author = {Andrew Yeckel and Yuming Zhou and Michael Dennis and Jeffrey J. Derby},
keywords = {Fluid flow, Solution growth, Finite element model},
abstract = {Three-dimensional, time-dependent flows that occur in the Lawrence Livermore National Laboratory system for rapid growth of potassium dihydrogen phosphate (KDP) crystals from solution are studied using massively parallel finite element computations. The simulation reveals that excellent global mixing occurs during the spin-down phase of a time-dependent stirring cycle. The large scale fluid motions in the radial and axial directions that promote mixing are caused primarily by effects of platform geometry, but are augmented to some degree by the intrinsic tendency of a decelerating rotational flow to reverse direction within Ekman layers that form at the boundaries. Along with Part I of this work [Y. Zhou and J.J. Derby, J. Crystal Growth 180 (1997) 497], which emphasized spin up and steady rotation, significant advances have been made in our understanding of hydrodynamic phenomena in this system.}
}
@incollection{COXON2019179,
title = {Chapter 7 - Transforming Future Mobility},
editor = {Selby Coxon and Robbie Napper and Mark Richardson},
booktitle = {Urban Mobility Design},
publisher = {Elsevier},
pages = {179-214},
year = {2019},
isbn = {978-0-12-815038-2},
doi = {https://doi.org/10.1016/B978-0-12-815038-2.00007-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128150382000074},
author = {Selby Coxon and Robbie Napper and Mark Richardson},
keywords = {Innovation methodology, Design thinking, Future mobility},
abstract = {The book having built a picture of a driverless, accessible, positive experience and inventively built mobility landscape, now leverages the techniques of design thinking to demonstrate the tools of a future economy and how they might be applied to a range of future mobility speculations. This chapter demonstrates that a combination of technology developments and design thinking skills can generate inventive compelling solutions to mobility problems. The chapter is illustrated with examples of these research speculations.}
}
@article{KLIMUSOVA2016652,
title = {Psychometric Properties of the Learning Potential Test},
journal = {Procedia - Social and Behavioral Sciences},
volume = {217},
pages = {652-656},
year = {2016},
note = {Future Academy Multidisciplinary Conference “ICEEPSY & CPSYC & icPSIRS & BE-ci” 13–17 October 2015 Istanbul},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2016.02.089},
url = {https://www.sciencedirect.com/science/article/pii/S1877042816001142},
author = {Helena Klimusová and Petr Květon},
keywords = {psychometrics, admission test},
abstract = {The use of cognitive ability tests to help select highly performing students is becoming a standard in most major Czech universities. Such tests need to show good psychometric properties. To highlight the importance of these properties to the process of selection, this study explores the psychometric properties of the Learning Potential test, which is used as a selection criterion within the admission procedure at a major Czech university. This study's objective is to assess the psychometric properties of the Learning Potential Test and provide an insight into its structure. The Cronbach's alpha were computed to assess the internal consistency of the test. The structure of the items was explored by the factor analysis methods. Factor analysis indicated the anticipated structure of the test with two major factors - critical thinking/verbal reasoning abilities and numerical/spatial/analytical abilities. Since the role of admission tests in the process of selection new university students is crucial, it is essential to periodically reassess its psychometric characteristics to ensure that our test remain relevant and applicable.}
}
@article{CARLISLE1996248,
title = {Software Caching and Computation Migration in Olden},
journal = {Journal of Parallel and Distributed Computing},
volume = {38},
number = {2},
pages = {248-255},
year = {1996},
issn = {0743-7315},
doi = {https://doi.org/10.1006/jpdc.1996.0145},
url = {https://www.sciencedirect.com/science/article/pii/S0743731596901458},
author = {Martin C. Carlisle and Anne Rogers},
abstract = {Software caching and computation migration are mechanisms that satisfy remote references by either bringing a copy of the data to the computation or moving the computation to the data. We evaluate these mechanisms usingOlden, a system that, with minimal programmer annotations, provides parallelism for C programs that use recursively defined structures, such as trees, lists, and DAGs. We demonstrate that providing both software caching and computation migration can improve the performance of these programs, and provide a compile-time heuristic that selects between them for each pointer dereference. We have implemented the heuristic in Olden on the Thinking Machines CM-5. We describe our implementation and report on experiments with eleven benchmarks.}
}
@article{KING1980313,
title = {Thinking: Readings in cognitive science: P.N. Johnson-Laird and P.C. Wason Cambridge University Press, 1977},
journal = {Artificial Intelligence},
volume = {13},
number = {3},
pages = {313-322},
year = {1980},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(80)90005-3},
url = {https://www.sciencedirect.com/science/article/pii/0004370280900053},
author = {Margaret King}
}
@article{HAGSTROM1998385,
title = {Experiments with approximate radiation boundary conditions for computational aeroacoustics},
journal = {Applied Numerical Mathematics},
volume = {27},
number = {4},
pages = {385-402},
year = {1998},
note = {Special Issue on Absorbing Boundary Conditions},
issn = {0168-9274},
doi = {https://doi.org/10.1016/S0168-9274(98)00021-X},
url = {https://www.sciencedirect.com/science/article/pii/S016892749800021X},
author = {Thomas Hagstrom and John Goodrich},
keywords = {Nonreflecting Boundary Conditions, Aeroacoustics},
abstract = {We present a series of numerical experiments on the accuracy of approximate radiation boundary conditions for computational aeroacoustics based on Padé approximants. Our test problem is described by an infinite periodic array of pressure pulses, for which we can independently evaluate the exact solution by numerical quadrature. It is demonstrated that acceptable long time accuracy can be achieved, but only if conditions of high order are employed. As predicted by theory, the order required for a given accuracy is proportional to the time of the simulation.}
}
@article{CUI2024124662,
title = {Cooperative interference to achieve interval many-objective evolutionary algorithm for association privacy secure computing migration},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124662},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124662},
url = {https://www.sciencedirect.com/science/article/pii/S095741742401529X},
author = {Zhihua Cui and Zhenyu Shi and Qi Li and Tianhao Zhao and Wensheng Zhang and Jinjun Chen},
keywords = {Mobile edge computing, Computing migration, Physical layer security(PLS), Interval many-objective optimization, Evolutionary algorithm},
abstract = {In this paper, we study secure computing migration scenarios in uncertain environments with the presence of multiple malicious eavesdroppers (MEs). Specifically, when edge servers (ESs) execute tasks delivered by smart devices (SDs), SDs may move beyond the coverage of ESs, and computing migration (CM) of unfinished tasks is required to ensure service continuity. There is a risk of privacy leakage during task migration, and MEs use colluding eavesdropping to eavesdrop on the migrated tasks, and we consider eavesdropping on the associated tasks through data sharing among MEs to improve the eavesdropping efficiency. For eavesdropping in MEs, we achieve eavesdropping strikes using cooperative interference by jammers, which benefit by providing jamming services. In addition, uncertain computational scenarios directly affect the efficiency of task execution, and we consider the uncertainty factor in the malicious eavesdropping environment. To this end, this paper proposes the secure computational migration of associative privacy in uncertain environments (SCMAPUE) model, which transforms uncertainties into interval parameters, and optimizes the five objectives of migration delay, maximum completion time, energy consumption, load balancing and migration reliability to achieve efficient task execution and reliable migration. Aiming at the model characteristics, this paper designs an interval many-objective evolutionary algorithm for reliable migration (IMaOEA-RM), which employs a condition-based interval confidence strategy and a multi-access secure migration selection strategy to improve the convergence of the algorithm, and utilizes a dual-migration crossover strategy in order to adjust the jammer partners and improve the population diversity. Simulation results show that our proposed IMaOEA-RM algorithm can provide a more reliable and efficient migration scheme than existing algorithms.}
}
@article{CHAKRAVARTY2010606,
title = {The creative brain – Revisiting concepts},
journal = {Medical Hypotheses},
volume = {74},
number = {3},
pages = {606-612},
year = {2010},
issn = {0306-9877},
doi = {https://doi.org/10.1016/j.mehy.2009.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S0306987709006926},
author = {Ambar Chakravarty},
abstract = {Summary
Creativity is a complex neuro-psycho-philosophical phenomenon which is difficult to define literally. Fundamentally it involves the ability to understand and express novel orderly relationships. The creative process involves four stages – preparation, incubation, illumination and verification. A high level of general intelligence, domain specific knowledge and special skills are necessary pre-requisites. It is possible that in addition, some creative people might have architectural alternations of specific portions of the posterior neocortex. Associated with such pre-requisites, the process of creative innovation (incubation and illumination stages) necessitates the need for an ability of divergent thinking, a novelty seeking behavior, some degree of suppression of latent inhibition and a subtle degree of frontal dysfunction. The author hypothesizes that these features are often inter-linked and subtle frontally disinhibited behavior is conducive towards creativity by allowing uninterrupted flow of creative thought possessing and opening up new avenues towards problem solving. Perhaps the most essential feature of the creative brain is its degree of connectivity – both inter-hemispheric and intra-hemispheric. Connectivity correlates or binds together functions of apparently structurally isolated domains on brain modules sub-serving different functions. It is felt that creative cognition is a self rewarding process where divergent thinking would promote connectivity through development of new synapses. In addition, the phenomenon of synaesthesia has often been observed in creative visual artists. Creative innovation often occurs during low arousal states and creative people often manifests features of affective disorders. This suggests a role of neurotransmitters in creative innovation. Dopaminergic pathways are involved in the novelty seeking attitude of creative people while norepinephrine levels are depressed during discovery of novel orderly relationships. The relationship between mood and catecholamines and that of creative cognition is often in an inverted U-shaped form. It is hypothesized that that subtle frontal dysfunction is a pre-requisite for creative cognition but here again the relationship is also in an inverted U-form.}
}
@article{BOYDDAVIS2018185,
title = {‘A dialogue between the real-world and the operational model’ – The realities of design in Bruce Archer’s 1968 doctoral thesis},
journal = {Design Studies},
volume = {56},
pages = {185-204},
year = {2018},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2017.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X17300893},
author = {Stephen {Boyd Davis} and Simone Gristwood},
keywords = {design history, philosophy of design, science of design, design research, systematic method},
abstract = {The article centres on a single document, the 1968 doctoral thesis of L. Bruce Archer. It traces Archer’s earlier publications and the sources that informed and inspired his thinking as a way of understanding his influential work at the Royal College of Art from 1962. Analysis suggests that Archer’s ambition for a rigorous ‘science of design’ inspired by linear algorithmic approaches was increasingly threatened with disruption by his experience of large, complex design projects. Reflecting on Archer's engagement with other models of designing, the article ends with Archer’s retrospective view and an account of his significantly altered opinions. Archer is located as both a theorist and someone fascinated by the commercial and practical aspects of designing.}
}
@article{KUAI2020101103,
title = {The extensible Data-Brain model: Architecture, applications and directions},
journal = {Journal of Computational Science},
volume = {46},
pages = {101103},
year = {2020},
note = {20 years of computational science},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2020.101103},
url = {https://www.sciencedirect.com/science/article/pii/S1877750320300752},
author = {Hongzhi Kuai and Ning Zhong},
keywords = {Artificial intelligence (AI), Brain informatics, Brain computing, Data-Brain, Brain big data, Web intelligence (WI), Intelligence systems},
abstract = {One of the key ideas in realizing human-like intelligence is to understand information-processing mechanisms in the human brain. Brain Informatics is a rapidly expanding interdisciplinary field to systematically utilize brain-related data, information and knowledge coming from the entire research process for in-depth brain investigation. In the past few years, a data-centric conceptual brain model, namely Data-Brain, has been proposed, providing the foundation for the systematic Brain Informatics methodology. The Data-Brain model constitutes a conceptual framework and detailed guideline for managing and analyzing brain big data. The development of Data-Brain model also demands the support from advanced technologies. This paper presents an extensible version of the Data-Brain with advanced computing techniques in the connected world. It provides a global understanding of how multidisciplinary techniques work together to tackle brain computing challenges. Particularly, the integrated K-I-D (Knowledge-Information-Data) loop is proposed, constructing a cycle as the thinking space to help pursue the systematic brain investigation, by which the extensible Data-Brain model continuously iterates and evolves through the never-ending learning. Such synergistic evolvement will power future progress for building intelligence systems and applications connected with the study of complex human brain.}
}
@article{RAIKOV2018492,
title = {Cognitive Modelling Quality Rising by Applying Quantum and Optical Semantic Approaches},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {30},
pages = {492-497},
year = {2018},
note = {18th IFAC Conference on Technology, Culture and International Stability TECIS 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.11.309},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318329823},
author = {A. Raikov},
keywords = {big data, quantum semantic, cognitive modelling, deep learning, decision-making, optical computing},
abstract = {Advanced decision support systems require significant acceleration of decision-making under conditions when factors describing the situation are ill-defined and non-metric. As a rule, such conditions arise in the field of politics, culture and in the social sphere. Cognitive modelling technology is applied in these cases. The cognitive models are created by people, experts from different subjects’ fields. The modelling processes take a great deal of time. Furthermore, the result of the modelling has to be verified when the model’s creators cannot get complete information and have to understand the problem very quickly. The factors and their mutual relationships in cognitive models could be verified with Big Data analysis technology. But this approach takes into account only denotational semantics that are based on the mapping of the model on formalised logical constructions, words, objects, schemes. This paper addresses the issue of creating cognitive semantics that take into consideration thinking, feeling and transcendental factors. It is shown that the classical computer or quantum computer cannot ensure cognitive semantics because they are based on discrete representation of data. An optical computing and Optical Semantic approach could be applied. The architecture of the special optical processor is represented.}
}
@article{KING2023104028,
title = {Identifying risk controls for future advanced brain-computer interfaces: A prospective risk assessment approach using work domain analysis},
journal = {Applied Ergonomics},
volume = {111},
pages = {104028},
year = {2023},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2023.104028},
url = {https://www.sciencedirect.com/science/article/pii/S0003687023000662},
author = {Brandon J. King and Gemma J.M. Read and Paul M. Salmon},
keywords = {Brain-computer interfaces, Risk assessment, System modelling},
abstract = {Brain-computer interface (BCI) technologies are progressing rapidly and may eventually be implemented widely within society, yet their risks have arguably not yet been comprehensively identified, nor understood. This study analysed an anticipated invasive BCI system lifecycle to identify the individual, organisational, and societal risks associated with BCIs, and controls that could be used to mitigate or eliminate these risks. A BCI system lifecycle work domain analysis model was developed and validated with 10 subject matter experts. The model was subsequently used to undertake a systems thinking-based risk assessment approach to identify risks that could emerge when functions are either undertaken sub-optimally or not undertaken at all. Eighteen broad risk themes were identified that could negatively impact the BCI system lifecycle in a variety of unique ways, while a larger number of controls for these risks were also identified. The most concerning risks included inadequate regulation of BCI technologies and inadequate training of BCI stakeholders, such as users and clinicians. In addition to specifying a practical set of risk controls to inform BCI device design, manufacture, adoption, and utilisation, the results demonstrate the complexity involved in managing BCI risks and suggests that a system-wide coordinated response is required. Future research is required to evaluate the comprehensiveness of the identified risks and the practicality of implementing the risk controls.}
}
@article{MAKUMBI2025,
title = {Archaeal genomes linked to industrial wastewater and associated freshwater in South Africa},
journal = {Microbiology Resource Announcements},
volume = {14},
number = {4},
year = {2025},
issn = {2576-098X},
doi = {https://doi.org/10.1128/mra.01079-24},
url = {https://www.sciencedirect.com/science/article/pii/S2576098X25000957},
author = {J. P. Makumbi and O. K. Bezuidt and S. K. Leareng and T. P. Makhalanyane},
keywords = {archaea, wastewater},
abstract = {ABSTRACT

Archaea provide important ecosystem services including the degradation of contaminants. Here, we present archaeal genomes from understudied South African wastewater treatment plants (WWTPs) and associated rivers receiving industrial effluents. Functional analysis revealed key genes implicated in heavy metal degradation, offering a valuable resource for mechanistic studies on archaeal metabolism.}
}
@article{CHOI199617,
title = {Computation and semiotic practice as compositional process},
journal = {Computers & Mathematics with Applications},
volume = {32},
number = {1},
pages = {17-35},
year = {1996},
issn = {0898-1221},
doi = {https://doi.org/10.1016/0898-1221(96)00084-3},
url = {https://www.sciencedirect.com/science/article/pii/0898122196000843},
author = {I. Choi},
keywords = {Dynamical systems, Semiotics, Computer music, Synergetics, Cognitive systems, Music composition, Chaos, Music synthesis},
abstract = {In sound computation, computational processes are brought into the acoustic domain by a set of formalized instructions for controlling parameters in synthesis engines and compositional algorithms. From the acoustic events, listeners often extract patterns or “musical objects” in their perception to the extent that certain associations are made external to the computational process. Perceived, musical objects rapidly become immutable, and that immutability may be considered a compositional problem. The problem is how to approach a compositional project for bringing new insights into play while, on one side, using the existing representational system such as symbolic language in computation, and on the other side, facing listeners' perceptual tendency to make external associations. For composers, the problem requires technical solutions as well as an ability to articulate the philosophical issues. This problem also exists in semiotics, a general study of signs, when one has to borrow language from existing linguistic systems in order to express a new thought without being trapped within the immutability of given linguistic sources. Semiotic practice is a discipline which emphasizes the function of semiotics to generate necessary discourse to examine the linguistic system in use and its logocentric tendency—the tendency towards known signs. We define semiotic practice as a signifying process in which meaning may be generated during that particular process under study; thus, meaning in semiotic practice is temporal context-dependent as a function of signifiers. The compositional problems involving sound computation for generating cases to support semiotic practice inquires about two tasks: 1.(1) how to design software which enables acoustic events to be observed as processes rather than observing sounds only as familiar objects or transformations featuring the recognition of objects, and2.(2) how to compose a piece of music so mutability of signs can be observed. To meet these problems, this paper examines perspectives on systems and cognition from multiple views such as semiotics, computation theories, and synergetics. We also discuss software designed for composition in terms of semiotic practice.}
}
@article{ALTAY20111111,
title = {Fuzzy cognitive mapping in factor elimination: A case study for innovative power and risks},
journal = {Procedia Computer Science},
volume = {3},
pages = {1111-1119},
year = {2011},
note = {World Conference on Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2010.12.181},
url = {https://www.sciencedirect.com/science/article/pii/S1877050910005569},
author = {Ayca Altay and Gülgün Kayakutlu},
keywords = {Fuzzy cognitive maps, Innovation, Factor prioritization, Factor elimination},
abstract = {Factor or criteria prioritization is essential for decision making and planning. In most areas in decision making, integrating the related literature yields an exuberance of criteria which leads a robust decision. Yet, an excess number of criteria may handicap decision making or evaluations in terms of computational time and complexity. In these circumstances, decreasing the number of factors in exchange for a negligible amount of knowledge can emancipate the decision maker yet does not affect the quality of the decision. This elimination can be conducted through qualitative methods such as interviews or quantitative methods. However, quantitative methods are more trustworthy since qualitative methods can be deceptive due to the perceptions of the interviewee. Furthermore, working with larger groups is more prone to neutrality in terms of group thinking. On the subject of innovative power and risks, the literature offers 48 criteria depending on the industry, size or demographics of related companies. Prioritizing and working with these criteria for their decision making applications becomes computationally expensive, especially when embedded in more complex algorithms. In this study, 48 criteria will be reduced using Fuzzy Cognitive Maps and it is believed to provide a sufficient number of criteria with a negligible loss of information and comparisons will be conducted.}
}
@article{PELAEZ2025109363,
title = {Universally Adaptable Multiscale Molecular Dynamics (UAMMD). A native-GPU software ecosystem for complex fluids, soft matter, and beyond},
journal = {Computer Physics Communications},
volume = {306},
pages = {109363},
year = {2025},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2024.109363},
url = {https://www.sciencedirect.com/science/article/pii/S0010465524002868},
author = {Raúl P. Peláez and Pablo Ibáñez-Freire and Pablo Palacios-Alonso and Aleksandar Donev and Rafael Delgado-Buscalioni},
keywords = {Molecular dynamics, Hydrodynamics, C++, CUDA, Soft matter},
abstract = {We introduce UAMMD (Universally Adaptable Multiscale Molecular Dynamics), a novel software infrastructure tailored for mesoscale complex fluid simulations on GPUs. The UAMMD library encompasses a comprehensive range of computational schemes optimized for the GPU, spanning from molecular dynamics to immersed boundary fluctuating-hydrodynamics. Developed in CUDA/C++14, this header-only open-source software serves both as a simulation engine and as a library with a modular architecture, offering a vast array of independent modules, categorized as interactors (neighbor search, bonded, non-bonded and electrostatic interactions, etc.) and integrators (molecular dynamics, dissipative particle dynamics, smooth particle hydrodynamics, Brownian hydrodynamics and a rather complete array of Immersed Boundary -IB- schemes). UAMMD excels in schemes that couple particle-based elastic structures with continuum fields in different regions of the mesoscale. To that end, thermal fluctuations can be added in physically consistent ways, and fast modes can be eliminated to adapt UAMMD to different regimes (compressible or incompressible flow, inertial or Stokesian dynamics, etc.). Thus, UAMMD is extremely useful for coarse-grained simulations of nanoparticles, and soft and biological matter (from proteins to viruses and micro-swimmers). Importantly, all UAMMD developments are hand-to-hand validated against experimental techniques, and it has proven to quantitatively reproduce experimental signals from quartz-crystal microbalance, atomic force microscopy, magnetic sensors, optic-matter interaction and ultrasound.
Program summary
Program Title: UAMMD CPC Library link to program files: https://doi.org/10.17632/srrt2y5s4m.1 Developer's repository link: https://github.com/RaulPPelaez/UAMMD/ Licensing provisions: GPLv3 Programming language: C++/CUDA Nature of problem: The key problem addressed in computational physics is simulating the behavior of matter at various scales, encompassing both discrete (particle-based) and continuum (field-based) approaches. The challenge lies in accurately and efficiently modeling interactions at different spatio-temporal scales, ranging from atomic (microscopic) to fluid dynamics (macroscopic). This complexity is further amplified in mesoscale regions, where different physics domains intersect, necessitating advanced computational techniques to capture the nuanced dynamics of systems such as colloids, polymers, and biological structures. Solution method: The present solution consists in the creation of UAMMD (Universally Adaptable Multiscale Molecular Dynamics), a CUDA/C++14 library designed for GPU-accelerated complex fluid simulations. UAMMD offers a flexible platform that integrates discrete particle dynamics with continuum fluid dynamics. It supports a variety of computational schemes, each tailored for specific spatio-temporal regimes. The library's modular architecture allows for the seamless introduction of new algorithms and easy integration into existing codebases. Additional comments including restrictions and unusual features: UAMMD's design emphasizes modularity and GPU-native architecture, optimizing computational efficiency and flexibility. However, its focus on GPU acceleration and low level nature means it requires compatible hardware and familiarity with CUDA programming. While UAMMD is versatile in handling various physical regimes, it currently lacks certain standard force field potentials and multi-GPU support. Nonetheless, its ongoing development and open-source nature promise continual enhancements.}
}
@article{HULME2017345,
title = {From control to causation: Validating a ‘complex systems model’ of running-related injury development and prevention},
journal = {Applied Ergonomics},
volume = {65},
pages = {345-354},
year = {2017},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2017.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S000368701730159X},
author = {A. Hulme and P.M. Salmon and R.O. Nielsen and G.J.M. Read and C.F. Finch},
keywords = {Systems ergonomics, STAMP, Sports injury prevention, Running injury},
abstract = {Introduction
There is a need for an ecological and complex systems approach for better understanding the development and prevention of running-related injury (RRI). In a previous article, we proposed a prototype model of the Australian recreational distance running system which was based on the Systems Theoretic Accident Mapping and Processes (STAMP) method. That model included the influence of political, organisational, managerial, and sociocultural determinants alongside individual-level factors in relation to RRI development. The purpose of this study was to validate that prototype model by drawing on the expertise of both systems thinking and distance running experts.
Materials and methods
This study used a modified Delphi technique involving a series of online surveys (December 2016- March 2017). The initial survey was divided into four sections containing a total of seven questions pertaining to different features associated with the prototype model. Consensus in opinion about the validity of the prototype model was reached when the number of experts who agreed or disagreed with survey statement was ≥75% of the total number of respondents.
Results
A total of two Delphi rounds was needed to validate the prototype model. Out of a total of 51 experts who were initially contacted, 50.9% (n = 26) completed the first round of the Delphi, and 92.3% (n = 24) of those in the first round participated in the second. Most of the 24 full participants considered themselves to be a running expert (66.7%), and approximately a third indicated their expertise as a systems thinker (33.3%). After the second round, 91.7% of the experts agreed that the prototype model was a valid description of the Australian distance running system.
Conclusion
This is the first study to formally examine the development and prevention of RRI from an ecological and complex systems perspective. The validated model of the Australian distance running system facilitates theoretical advancement in terms of identifying practical system-wide opportunities for the implementation of sustainable RRI prevention interventions. This ‘big picture’ perspective represents the first step required when thinking about the range of contributory causal factors that affect other system elements, as well as runners' behaviours in relation to RRI risk.}
}
@article{JONES2001325,
title = {NMR quantum computation},
journal = {Progress in Nuclear Magnetic Resonance Spectroscopy},
volume = {38},
number = {4},
pages = {325-360},
year = {2001},
issn = {0079-6565},
doi = {https://doi.org/10.1016/S0079-6565(00)00033-9},
url = {https://www.sciencedirect.com/science/article/pii/S0079656500000339},
author = {J.A. Jones}
}
@incollection{1998361,
title = {Appendix B - Suggestions for further reading in computational biology},
editor = {Steven L. Salzberg and David B. Searls and Simon Kasif},
series = {New Comprehensive Biochemistry},
publisher = {Elsevier},
volume = {32},
pages = {361-365},
year = {1998},
booktitle = {Computational Methods in Molecular Biology},
issn = {0167-7306},
doi = {https://doi.org/10.1016/S0167-7306(08)60474-3},
url = {https://www.sciencedirect.com/science/article/pii/S0167730608604743}
}
@article{SAWLEY1994363,
title = {A comparative study of the use of the data-parallel approach for compressible flow calculations},
journal = {Parallel Computing},
volume = {20},
number = {3},
pages = {363-373},
year = {1994},
issn = {0167-8191},
doi = {https://doi.org/10.1016/S0167-8191(06)80019-0},
url = {https://www.sciencedirect.com/science/article/pii/S0167819106800190},
author = {M.L. Sawley and C.M. Bergman},
keywords = {Computational fluid dynamics, Euler equations, Data-parallel programming, Portability, Performance results},
abstract = {The results are presented of an investigation into the use of the data-parallel programming approach on four different massively-parallel computers: the MasPar MP-1 and MP-2 and the Thinking Machines CM-200 and CM-5. A code to calculate inviscid compressible flow, originally written in FORTRAN 77 for a traditional vector computer, has been re-written entirely in Fortran 90 to take advantage of the compilers available on the massively-parallel computers. It is shown that the discretization of the governing equations on a regular mesh is well adapted to data parallelism. For a typical test problem of supersonic flow through a ramped duct, computational speeds have been achieved using these massively-parallel computers that are superior to those obtained using a single processor of a Cray Y-MP. In addition, this study has enabled the question of code portability between the different computers to be assessed.}
}
@article{PERCHTOLDSTEFAN202398,
title = {Functional EEG Alpha Activation Patterns During Malevolent Creativity},
journal = {Neuroscience},
volume = {522},
pages = {98-108},
year = {2023},
issn = {0306-4522},
doi = {https://doi.org/10.1016/j.neuroscience.2023.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0306452223002178},
author = {Corinna M. Perchtold-Stefan and Christian Rominger and Ilona Papousek and Andreas Fink},
keywords = {malevolent creativity, EEG, alpha power, time-course, divergent thinking},
abstract = {On the dark side of creativity, creative ideation is intentionally used to damage others. This first electroencephalogram (EEG) study on malevolent creativity investigated task-related power (TRP) changes in the alpha band while n = 89 participants (52 women, 37 men) generated original ideas for revenge in the psychometric Malevolent Creativity Test. TRP changes were assessed for different stages of the idea generation process and linked to performance indicators of malevolent creativity. This study revealed three crucial findings: 1) Malevolent creativity yielded topographically distinct alpha power increases similar to conventional creative ideation. 2) Time-related activity changes during malevolent creative ideation were reflected in early prefrontal and mid-stage temporal alpha power increases in individuals with higher malevolent creativity performance. This performance-related, time-sensitive pattern of TRP changes during malevolent creativity may reflect early conceptual expansion from prosocial to antisocial perspectives, and subsequent inhibition of dominant semantic associations in favor of novel revenge ideas. 3) The observed, right-lateralized alpha power increases over the entire ideation phase may denote an additional emotional load of creative ideation. Our study highlights the seminal role of EEG alpha oscillations as a biomarker for creativity, also when creative processes operate in a malevolent context.}
}
@article{NIRMALADEVI2025126553,
title = {DCNN-SBiL: EEG signal based mild cognitive impairment classification using compact convolutional network},
journal = {Expert Systems with Applications},
volume = {273},
pages = {126553},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126553},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425001757},
author = {A. {Nirmala Devi} and M. Latha},
keywords = {Mild cognitive impairment, Deep learning, Compact convolutional neural network, EEG signal, Dual attention, Alzheimer’s disease, Improved tuneable Q wavelet transform},
abstract = {Mild cognitive impairment (MCI) is a state that falls between the more severe decline of dementia and the typical aging-related loss of memory and thinking. MCI must be diagnosed earlier to avoid complete memory loss. Several Machine Learning (ML) and Deep Learning (DL) models employ standard feature extraction approaches to achieve effective MCI categorization. However, it has some drawbacks, including lower accuracy, longer time consumption, less feature learning, and increased model complexity. The proposed method introduces a novel deep learning model to address the limitations of existing MCI classification approaches. Initially, the Electroencephalography (EEG) signal is pre-processed using the Sequential Savitzky-Golay filtering model (SEQ-SG), which improves the signal’s quality and removes unnecessary noise. The Improved Tuneable Q Wavelet Transform (ITQWT) feature extraction model is used to extract relevant features. The Coati Stochastic Optimization (CSO) algorithm selects the most optimal channel features from the EEG signal. Finally, the proposed deep learning model, Dual Attention Assisted Compact Convolutional Network with Stacked Bi-LSTM (DCCN-SBiL), is used to classify EEG signals into three categories: Alzheimer’s disease, MCI, and normal. The proposed model is optimized using the Gazelle Optimization Algorithm (GOA), which tunes the classification model’s hyperparameters. The proposed classification model is evaluated using the Mendeley Dataset, which contains EEG signals from Alzheimer's disease, MCI and Normal. The proposed model has shown great performance in many performance parameters, including 97.25% accuracy, 95.94% recall, 96.03% precision, and 94.65% specificity in MCI classification.}
}
@article{DELLACQUA20241727,
title = {Empathy-Aware Behavior Trees for Social Care Decision Systems},
journal = {Procedia Computer Science},
volume = {239},
pages = {1727-1735},
year = {2024},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2023},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.06.351},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924015953},
author = {Pierangelo Dell’Acqua and Stefania Costantini and Abeer Dyoub and Giovanni De Gasperis and Andrea Monaldini and Andrea Rafanelli},
keywords = {Behavior Trees, Affective Computing, Decision Making},
abstract = {There is growing attention on the importance of building intelligent systems where humans and Artificial Intelligence-based systems (AIs) form teams exploiting the potentially synergistic relationships between humans and automation. In the last decade, the computational modeling of empathy has gained increasing attention. Empowering interactive agents with empathic capabilities leads, on the human’s side, to more trust, increases engagement, and thus interaction length, helps cope with stress. These findings suggest that agents endowed with empathy may enhance social interaction in educational applications, artificial companions, medical assistants, and gaming applications. This article focuses on modeling the empathic behavior of virtual agents interacting with humans. We propose a formal model that enables virtual agents to exhibit empathic, emotional behavior. Specifically, we extend the modeling of empathy via behavior trees with a new type of node allowing the specification of various kinds of empathy. Using the proposed extension, we show how different agents’ reactive behavior can be modeled.}
}
@article{KARIMI199745,
title = {A Parallel Algorithm for Routing: Best Solutions at Low Computational Costs},
journal = {Geomatica},
volume = {51},
number = {1},
pages = {45-51},
year = {1997},
issn = {1195-1036},
doi = {https://doi.org/10.5623/geomat-1997-0006},
url = {https://www.sciencedirect.com/science/article/pii/S1195103624002969},
author = {Hassan A. Karimi and Dongming Hwang},
abstract = {Routing is a common activity in network analysis. Of the many routing algorithms available, Dijkstra’s algorithm is one of the most widely used for computing best paths in networks. Dijkstra’s algorithm guarantees best solutions with a time complexity ofO(N2) in the worst case. This time complexity results in very long processing times for computing best paths in large networks, which can be a serious problem for real-time applications. Alternative approaches that give a better time complexity have been suggested, most of which are heuristics. However, these heuristics do not guarantee best solutions. To achieve best solutions at low computational costs, a parallel algorithm for parallel machines is suggested.
L’établissement de parcours s’emploie fréquemment dans l’analyse de réseau. De tous les algorithmes d’établissement de parcours disponibles, l’algorithme de Dijkstra est parmi ceux qu’on emploie le plus souvent afin de calculer les meilleurs itinéraires de réseaux. Cet algorithme assure les meilleures solutions avec une complexité temporelle de 0(N2) dans la pire éventualité. Cette complexité temporelle entraîne des temps de traitement très longs lors du calcul des meilleurs itinéraires dans les grands réseaux, ce qui peut constituer un problème sérieux pour les applications en temps réel. On a suggéré d’autres méthodes qui offrent une meilleure complexité temporelle, la plupart étant des heuristiques. Toutefois, ces heuristiques ne garantissent pas les meilleures solutions. Afin d’en arriver aux meilleurs solutions tout en réduisant les coûts de calcul, on suggère un algorithme parallèle pour des machines parallèles.}
}
@incollection{DRYGAS20201,
title = {1 - Introduction to computational methods and theory of composites},
editor = {Piotr Drygaś and Simon Gluzman and Vladimir Mityushev and Wojciech Nawalaniec},
booktitle = {Applied Analysis of Composite Media},
publisher = {Woodhead Publishing},
pages = {1-56},
year = {2020},
series = {Woodhead Publishing Series in Composites Science and Engineering},
isbn = {978-0-08-102670-0},
doi = {https://doi.org/10.1016/B978-0-08-102670-0.00010-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008102670000010X},
author = {Piotr Drygaś and Simon Gluzman and Vladimir Mityushev and Wojciech Nawalaniec},
keywords = {Self-consistent approximation, structural sum, statistical mechanics methods, self-Similarity and renormalization-group},
abstract = {Overview of traditional approaches based on self-consistent approximations in composite materials is presented. Their restrictions are underlined. Neoclassical approach previously introduced in the Preface, is illustrated and compared to methods applied in statistical mechanics. The structural sums, the key construction of the neoclassical approach, are outlined. Method of series and asymptotic method of approximants, Padé approximants, DLog Padé approximants, Factor, Root, Additive approximants are briefly discussed. Notion of Self-Similarity and renormalization-group is introduced. Minimal difference and minimal derivative methods of calculation for short series are discussed in detail. Critical Index is calculated from various short series. DLog root approximants are introduced and illustrate by several examples, where the DLog Padé approximants fail. DLog additive approximants are introduced and presented iteratively. Multiple examples are presented in the chapter and in the appendix. Method of Log Padé approximants is suggested.}
}
@article{WANG2025102570,
title = {ST-TNet: An spatio-temporal joint transformer network for CSI feedback in FDD-MIMO systems},
journal = {Physical Communication},
volume = {68},
pages = {102570},
year = {2025},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2024.102570},
url = {https://www.sciencedirect.com/science/article/pii/S187449072400288X},
author = {Linyu Wang and Yize Cao and Jianhong Xiang},
keywords = {Massive MIMO, CSI feedback, Deep learning, Lightweighting, Transformer, Attention},
abstract = {In recent years, deep learning methods have been shown to have strong potential and superiority in reducing channel state information (CSI) feedback overhead and further improving feedback accuracy to maximize the performance benefits of massive Multiple-Input Multiple-Output (MIMO) in frequency division duplex (FDD) mode. As the CSI matrices are transformed into sequences for input to the Transformer model, the rearrangement leads to the loss of the original physical location relationships. Based on this problem, this paper proposes a transformer decoder based on spatio-temporal joint (ST-T). We employ a spatial attention mechanism to compensate for this information loss and focus on key spatial features more accurately, further exploiting the potential of single- and two-layer transformers in reconstructing CSI matrices. The results are validated by simulations based on DCRNet and CLNet encoders, which show that higher performance can be achieved with lower computational load compared to other lightweight models.}
}
@incollection{NIEVERGELT1993167,
title = {Experiments in Computational Heuristics and Their Lessons for Software and Knowledge Engineering},
editor = {Marshall C. Yovits},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {37},
pages = {167-205},
year = {1993},
issn = {0065-2458},
doi = {https://doi.org/10.1016/S0065-2458(08)60405-2},
url = {https://www.sciencedirect.com/science/article/pii/S0065245808604052},
author = {Jurg Nievergelt},
abstract = {Publisher Summary
This chapter presents examples that illustrate a type of programming project increasingly prominent in the field of knowledge engineering. The examples are chosen on grounds of familiarity, without any claim to represent the field of heuristic programming at large. The chapter begins by describing some software projects in computational heuristics. These projects are presented as case studies of the interaction between software engineering and knowledge engineering that illustrate the decisive importance of a powerful software environment. The chapter presents the case of the smart game board and describes the main software tool for rapid prototyping of game implementations, needed to conduct experiments. It also describes a project involving heuristics and knowledge engineering that has been evolving without interruption for the past five years. It presents paradigms of software development favored by system designers and implementers of exploratory development projects, and explains why these are often diametrically opposed to the conventional software engineering lore.}
}
@article{ESCOLAGASCON2023111893,
title = {Who falls for fake news? Psychological and clinical profiling evidence of fake news consumers},
journal = {Personality and Individual Differences},
volume = {200},
pages = {111893},
year = {2023},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2022.111893},
url = {https://www.sciencedirect.com/science/article/pii/S0191886922003981},
author = {Álex Escolà-Gascón and Neil Dagnall and Andrew Denovan and Kenneth Drinkwater and Miriam Diez-Bosch},
keywords = {Fake news, Pseudoscientific information, Cognitive biases, Individual differences, Clinical prevention},
abstract = {Awareness of the potential psychological significance of false news increased during the coronavirus pandemic, however, its impact on psychopathology and individual differences remains unclear. Acknowledging this, the authors investigated the psychological and psychopathological profiles that characterize fake news consumption. A total of 1452 volunteers from the general population with no previous psychiatric history participated. They responded to clinical psychopathology assessment tests. Respondents solved a fake news screening test, which allowed them to be allocated to a quasi-experimental condition: group 1 (non-fake news consumers) or group 2 (fake news consumers). Mean comparison, Bayesian inference, and multiple regression analyses were applied. Participants with a schizotypal, paranoid, and histrionic personality were ineffective at detecting fake news. They were also more vulnerable to suffer its negative effects. Specifically, they displayed higher levels of anxiety and committed more cognitive biases based on suggestibility and the Barnum Effect. No significant effects on psychotic symptomatology or affective mood states were observed. Corresponding to these outcomes, two clinical and therapeutic recommendations related to the reduction of the Barnum Effect and the reinterpretation of digital media sensationalism were made. The impact of fake news and possible ways of prevention are discussed.}
}
@article{HROBARIK20066,
title = {Computational study of bonding trends in the metalloactinyl series EThM and MThM′ (E=N−, O, F+; M, M′=Ir−, Pt, Au+)},
journal = {Chemical Physics Letters},
volume = {431},
number = {1},
pages = {6-12},
year = {2006},
issn = {0009-2614},
doi = {https://doi.org/10.1016/j.cplett.2006.08.144},
url = {https://www.sciencedirect.com/science/article/pii/S0009261406013741},
author = {Peter Hrobárik and Michal Straka and Pekka Pyykkö},
abstract = {The title systems, including EThE′, are treated at DFT level using a B3LYP functional and small-core quasirelativistic pseudopotentials. Most of the studied systems are bent, like their isoelectronic ThO2 analogue, except for some anionic systems containing Ir. The bond lengths vary considerably and can lie above or below the sum of triple-bond covalent radii. Among the studied systems, the iridium-containing species show the strongest back-donation to Th. The bonding can be simply understood and could theoretically go up to a ‘24-electron principle’ limit at the actinide.}
}
@article{WOODSIDE2011153,
title = {Responding to the severe limitations of cross-sectional surveys: Commenting on Rong and Wilkinson’s perspectives},
journal = {Australasian Marketing Journal (AMJ)},
volume = {19},
number = {3},
pages = {153-156},
year = {2011},
note = {Special Section: Marketing and Public Policy},
issn = {1441-3582},
doi = {https://doi.org/10.1016/j.ausmj.2011.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S1441358211000231},
author = {Arch G. Woodside},
keywords = {Direct research, Executives, Folk theory-of-mind, fs/QCA.com, Sensemaking, Surveys, Thinking},
abstract = {While a meta-analysis is necessary to test the claim that the logic dominates the majority of studies, most studies by academic scholars on thinking and actions by executives appear to rely on cross-sectional surveys that use self-reports by executives via scaled (e.g., strongly disagree to strongly agree) instruments whereby one executive per firm completes the instrument and data are collected for 50–500 firms. Useable response rates in these studies are almost always below 30% of the distributions of the surveys. While these studies are sometimes worthwhile for learning how respondents assess concepts and relationships among concepts, Rong and Wilkinson’s perspective on the severe limits to the value of such studies rings true: such surveys reveal more about executives’ sensemaking processes than the actual processes. The limitations of using one-shot, one-person-per-firm, self-reports as valid indicators of causal relationships of actual processes are so severe that academics should do more than think twice before using such surveys as the main method for collecting data – if scholars seek to understand and describe actual processes additional methods are necessary for data collection. The relevant literature includes several gems of exceptionally high quality, validity, and usefulness in the study of actual processes; identifying these studies is a useful step toward reducing the reliance on one-shot self-report surveys.}
}
@article{EGOROV2007293,
title = {Neural logic molecular, counter-intuitive},
journal = {Biomolecular Engineering},
volume = {24},
number = {3},
pages = {293-299},
year = {2007},
note = {6th Atlantic Symposium on Computational Biology},
issn = {1389-0344},
doi = {https://doi.org/10.1016/j.bioeng.2007.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S1389034407000342},
author = {Igor K. Egorov},
keywords = {Creative thinking, Boolean logic gates, Molecular mechanism, Transcription regulation, Somatic hypermutation, Neuron},
abstract = {A hypothesis is proposed that multiple “LOGIC” genes control Boolean logic in a neuron. Each hypothetical LOGIC gene encodes a transcription factor that regulates another LOGIC gene(s). Through transcription regulation, LOGIC genes connect into a complex circuit, such as a XOR logic gate or a two-input flip–flop logic circuit capable of retaining information. LOGIC gene duplication, mutation and recombination may result in the diversification of Boolean logic gates. Creative thinking may sometimes require counter-intuitive reasoning, rather than common sense. Such reasoning is likely to engage novel logic circuits produced by LOGIC somatic mutations. An individual's logic maturates by a mechanism of somatic hypermutation, gene conversion and recombination of LOGIC genes in precursor cells followed by selection of neurons in the brain for functional competence. In this model, a single neuron among billions in the brain may contain a unique logic circuit being the key to a hard intellectual problem. The output of a logic neuron is likely to be a neurotransmitter. This neuron is connected to other neurons in the spiking neural network. The LOGIC gene hypothesis is testable by molecular techniques. Understanding mechanisms of authentic human ingenuity may help to invent digital systems capable of creative thinking.}
}
@incollection{MURMAN2012323,
title = {15 - Innovation in aeronautics through Lean Engineering},
editor = {Trevor M. Young and Mike Hirst},
booktitle = {Innovation in Aeronautics},
publisher = {Woodhead Publishing},
pages = {323-360},
year = {2012},
isbn = {978-1-84569-550-7},
doi = {https://doi.org/10.1533/9780857096098.3.323},
url = {https://www.sciencedirect.com/science/article/pii/B978184569550750015X},
author = {E.M. Murman},
keywords = {Lean Engineering, Lean Product Development},
abstract = {Abstract:
The dynamics of innovation theory indicate that, for products as mature as aircraft, process innovation is an important contributor to product success and innovation. Many aerospace companies have adopted Lean Thinking as an enterprise-wide continuous improvement strategy. This chapter extends Lean Thinking to the engineering domain with a Lean Engineering framework based upon observational findings from a decade of research in the aerospace domain, published works on Toyota and Southwest Airlines, and practitioner input. Examples illustrate how the framework maybe be applied. Lean Engineering is not totally new to aerospace, and it continues to evolve. Future challenges are briefly summarized.}
}
@article{SAVIN202110,
title = {Main topics in EIST during its first decade: A computational-linguistic analysis},
journal = {Environmental Innovation and Societal Transitions},
volume = {41},
pages = {10-17},
year = {2021},
note = {Celebrating a decade of EIST: What’s next for transition studies?},
issn = {2210-4224},
doi = {https://doi.org/10.1016/j.eist.2021.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S221042242100037X},
author = {Ivan Savin and Jeroen {van den Bergh}},
keywords = {Machine learning, Topic modelling, Literature review},
abstract = {We analyse 465 articles published in EIST from June 2011 until June 2021 to identify topics addressed in the journal. We find eight main topics and assess how their shares changed over time as well as how many citations they received. The topics with the largest shares in all publications are “Theory of socio-technical transitions” and “Urban regimes and niches”. The two most cited topics, “Theory of socio-technical transitions” and “Geography and diffusion of eco-innovations”, showed a rising share over time, while the share of topic “Finance, investment and growth” declined. We further assess the geographical coverage of topics, through affiliations of the corresponding authors. The resulting map indicates dominant topics for the 34 countries that contributed to publications in EIST.}
}
@article{SUN201859,
title = {An ecosystemic framework for business sustainability},
journal = {Business Horizons},
volume = {61},
number = {1},
pages = {59-72},
year = {2018},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2017.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0007681317301271},
author = {Jiazhe Sun and Shunan Wu and Kaizhong Yang},
keywords = {Business sustainability, Systems theory, Ecosystemic theory, Complexity science, Adaptive management, Corporate sustainability},
abstract = {This article introduces an ecosystemic framework to foster innovation for business sustainability. We emphasize the idea of systemic thinking in which the business operates as a system similar to a living organism. In this framework, businesses impact the environment in which they operate in a fluid, dynamic, and interdependent way. This approach contrasts with the linear approach commonly used in business and other disciplines, which tries to explain what might cause an action or reaction but ignores any feedback effect between the subsequent action and its cause. This article offers practical solutions and guidance for business leaders to incorporate complexity science into creating sustainable businesses.}
}
@article{RODRIGUEZ2022104446,
title = {Using scaffolded feedforward and peer feedback to improve problem-based learning in large classes},
journal = {Computers & Education},
volume = {182},
pages = {104446},
year = {2022},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2022.104446},
url = {https://www.sciencedirect.com/science/article/pii/S0360131522000173},
author = {María Fernanda Rodríguez and Miguel Nussbaum and Leyla Yunis and Tomás Reyes and Danilo Alvares and Jean Joublan and Patricio Navarrete},
abstract = {The growing demand for access to higher education has seen institutions turn increasingly towards large classes. Implementing active, problem-based learning in this context can be difficult as it requires the lecturer to attend to every student's individual needs. Given the lack of tools for providing personalized feedback, this represents a significant challenge. The aim of this study is to see how best to support lecturers in giving timely feedback to students in a large class during problem-based learning. To meet this goal, we propose a model that combines feedforward, scaffolded using an automated summarization tool, with peer feedback. In this sense, the lecturer first provides feedforward through a series of general comments before an anonymous peer gives personalized feedback. The results show that, despite not giving personalized feedback, the lecturer is able to provide enriched formative feedforward thanks to the summary generated by the automated system. Furthermore, in more qualitative terms, the students show that they appreciate the opportunity to both give and receive feedback. Finally, the students' critical thinking skills are also shown to improve progressively from one activity to the next. Given the research gap regarding how lecturers use the reports generated by automated summarization tools, our study contributes to the literature by proposing a strategy for lecturers to use such reports to provide feedforward. Additionally, this study also contributes to the literature by proposing a model that can be fully integrated in both synchronous and asynchronous online learning.}
}
@article{BAYRAKTARSARI2024110835,
title = {Architectural spatial layout design for hospitals: A review},
journal = {Journal of Building Engineering},
volume = {97},
pages = {110835},
year = {2024},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2024.110835},
url = {https://www.sciencedirect.com/science/article/pii/S2352710224024033},
author = {Aysegul Ozlem {Bayraktar Sari} and Wassim Jabi},
keywords = {Architectural spatial layout design, Hospital spatial layout design, Computational design, Facility layout planning, Machine learning (ML) driven layout design, Systematic review},
abstract = {The design of hospital spatial layouts is a critical aspect of healthcare architecture, directly influencing patient outcomes, staff efficiency, and the overall quality of care. A well-designed hospital layout is essential for ensuring smooth operations, minimizing errors, and improving both patient and staff experiences. This paper reviews the significant advances in the field, particularly focusing on the transition from traditional design methods to the integration of computational techniques and machine learning (ML) in hospital layout planning. Despite these technological advancements, there remains a notable gap in the full adoption and optimization of these methods to effectively address the inherent complexities of healthcare environments. This review identifies that while computational methods and machine learning-driven approaches have brought precision and innovation to hospital design, the challenge lies in balancing these technologies with the expertise and insights of human designers. Moreover, the need for interdisciplinary collaboration between architects, healthcare professionals, and engineers is emphasized as crucial for the successful implementation of advanced design strategies. Insights from this review highlight the potential of future research to bridge the existing gaps, proposing directions for the continuous integration of technology in hospital layout design.}
}
@article{EGIDI2020155,
title = {Desertification risk, economic resilience and social issues: From theory to practice},
journal = {Chinese Journal of Population, Resources and Environment},
volume = {18},
number = {2},
pages = {155-163},
year = {2020},
issn = {2325-4262},
doi = {https://doi.org/10.1016/j.cjpre.2021.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S2325426221000310},
author = {Gianluca Egidi and Luca Salvati},
keywords = {Population dynamics, Ecosystem functioning, Socio-ecological resilience, Complex adaptive systems, Interpretative framework},
abstract = {Land degradation and early forms of desertification in both advanced economies and emerging countries reflect complex socio-environmental processes driven by multiple interactions between biophysical and socioeconomic forces across different spatial scales. The present study investigates desertification risk, land degradation, and socio-demographic dynamics through the lens of “resilience,” adopting complex adaptive systems (CAS) thinking. The resilience of socio-environmental systems exposed to land degradation is defined as the capacity of a regional economy to respond to crises and reorganize by making changes to preserve functions, structure, and feedback, and to promote future development options. By reviewing the socioeconomic resilience of local socio-ecological systems exposed to land degradation, this study achieves a better comprehension of the multifaceted processes that lead to a higher risk of desertification and the intimate relationship with underlying population trends and demographic dynamics. A comprehensive approach based on resilience thinking was formulated to review both environmental and socio-demographic issues at the landscape scale, and provide a suitable foundation for sustainability science and regional development policies.}
}
@article{SAVIN2024108324,
title = {Reviewing studies of degrowth: Are claims matched by data, methods and policy analysis?},
journal = {Ecological Economics},
volume = {226},
pages = {108324},
year = {2024},
issn = {0921-8009},
doi = {https://doi.org/10.1016/j.ecolecon.2024.108324},
url = {https://www.sciencedirect.com/science/article/pii/S0921800924002210},
author = {Ivan Savin and Jeroen {van den Bergh}},
keywords = {Economic growth, Environmental policy, GDP, Political feasibility, Post-growth},
abstract = {In the last decade many publications have appeared on degrowth as a strategy to confront environmental and social problems. We undertake a systematic review of their content, data and methods. This involves the use of computational linguistics to identify main topics investigated. Based on a sample of 561 studies we conclude that: (1) content covers 11 main topics; (2) the large majority (almost 90%) of studies are opinions rather than analysis; (3) few studies use quantitative or qualitative data, and even fewer ones use formal modelling; (4) the first and second type tend to include small samples or focus on non-representative cases; (5) most studies offer ad hoc and subjective policy advice, lacking policy evaluation and integration with insights from the literature on environmental/climate policies; (6) of the few studies on public support, a majority concludes that degrowth strategies and policies are socially-politically infeasible; (7) various studies represent a “reverse causality” confusion, i.e. use the term degrowth not for a deliberate strategy but to denote economic decline (in GDP terms) resulting from exogenous factors or public policies; (8) few studies adopt a system-wide perspective – instead most focus on small, local cases without a clear implication for the economy as a whole. We illustrate each of these findings for concrete studies.}
}
@article{WELLEK1961715,
title = {The contribution of the perception-typological approaches to the typology of character, and the role of sensation, imagination, and thinking in the organizational concept of personality},
journal = {Acta Psychologica},
volume = {19},
pages = {715-723},
year = {1961},
issn = {0001-6918},
doi = {https://doi.org/10.1016/S0001-6918(61)80321-1},
url = {https://www.sciencedirect.com/science/article/pii/S0001691861803211},
author = {Albert Wellek}
}
@article{HARTWIGSEN20212075,
title = {How does hemispheric specialization contribute to human-defining cognition?},
journal = {Neuron},
volume = {109},
number = {13},
pages = {2075-2090},
year = {2021},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2021.04.024},
url = {https://www.sciencedirect.com/science/article/pii/S0896627321002907},
author = {Gesa Hartwigsen and Yoshua Bengio and Danilo Bzdok},
keywords = {human intelligence, artificial general intelligence, computational design principles, deep learning, language, global workspace theory},
abstract = {Summary
Uniquely human cognitive faculties arise from flexible interplay between specific local neural modules, with hemispheric asymmetries in functional specialization. Here, we discuss how these computational design principles provide a scaffold that enables some of the most advanced cognitive operations, such as semantic understanding of world structure, logical reasoning, and communication via language. We draw parallels to dual-processing theories of cognition by placing a focus on Kahneman’s System 1 and System 2. We propose integration of these ideas with the global workspace theory to explain dynamic relay of information products between both systems. Deepening the current understanding of how neurocognitive asymmetry makes humans special can ignite the next wave of neuroscience-inspired artificial intelligence.}
}
@article{KEARNS2024543,
title = {The Application of Knowledge Engineering via the Use of a Biomimetic Digital Twin Ecosystem, Phenotype-Driven Variant Analysis, and Exome Sequencing to Understand the Molecular Mechanisms of Disease},
journal = {The Journal of Molecular Diagnostics},
volume = {26},
number = {7},
pages = {543-551},
year = {2024},
issn = {1525-1578},
doi = {https://doi.org/10.1016/j.jmoldx.2024.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S152515782400062X},
author = {William G. Kearns and Georgios Stamoulis and Joseph Glick and Lawrence Baisch and Andrew Benner and Dalton Brough and Luke Du and Bradford Wilson and Laura Kearns and Nicholas Ng and Maya Seshan and Raymond Anchan},
abstract = {Applied artificial intelligence, particularly large language models, in biomedical research is accelerating, but effective discovery and validation requires a toolset without limitations or bias. On January 30, 2023, the National Academies of Sciences, Engineering, and Medicine (NAS) appointed an ad hoc committee to identify the needs and opportunities to advance the mathematical, statistical, and computational foundations of digital twins in applications across science, medicine, engineering, and society. On December 15, 2023, the NAS released a 164-page report, “Foundational Research Gaps and Future Directions for Digital Twins.” This report described the importance of using digital twins in biomedical research. The current study was designed to develop an innovative method that incorporated phenotype-ranking algorithms with knowledge engineering via a biomimetic digital twin ecosystem. This ecosystem applied real-world reasoning principles to nonnormalized, raw data to identify hidden or "dark" data. Clinical exome sequencing study on patients with endometriosis indicated four variants of unknown clinical significance potentially associated with endometriosis-related disorders in nearly all patients analyzed. One variant of unknown clinical significance was identified in all patient samples and could be a biomarker for diagnostics. To the best of our knowledge, this is the first study to incorporate the recommendations of the NAS to biomedical research. This method can be used to understand the mechanisms of any disease, for virtual clinical trials, and to identify effective new therapies.}
}
@article{KULIK20242338,
title = {Reaction: The challenge of open-shell transition metal catalysis in “systems chemistry”},
journal = {Chem},
volume = {10},
number = {8},
pages = {2338-2339},
year = {2024},
issn = {2451-9294},
doi = {https://doi.org/10.1016/j.chempr.2024.06.026},
url = {https://www.sciencedirect.com/science/article/pii/S245192942400305X},
author = {Heather J. Kulik},
abstract = {Professor Heather J. Kulik is a professor in chemical engineering and chemistry at MIT. She received her BE in chemical engineering from the Cooper Union in 2004 and her PhD from the Department of Materials Science and Engineering at MIT in 2009. She completed postdocs at Lawrence Livermore and Stanford prior to joining MIT as a faculty member in 2013. Her research in computational inorganic chemistry has been recognized by an ONR YIP, a DARPA Director’s fellowship, an NSF CAREER Award, a Sloan Fellowship, an AIChE CoMSEF Impact Award, and a Hans Fischer Senior Fellowship from TU Munich, among others.}
}
@article{SHUKLA2024117388,
title = {Association of road traffic noise exposure and school childrens’ cognition: A structural equation model approach},
journal = {Environmental Research},
volume = {240},
pages = {117388},
year = {2024},
issn = {0013-9351},
doi = {https://doi.org/10.1016/j.envres.2023.117388},
url = {https://www.sciencedirect.com/science/article/pii/S0013935123021928},
author = {Avnish Shukla and Bhaven N. Tandel},
keywords = {School children, Cognition, Traffic noise index (TNI), Exploratory factor analysis (EFA), Structural equation modeling (SEM)},
abstract = {This study explores the complex relationship between traffic noise and school children's cognition, acknowledging existing empirical inconsistencies and aiming to contribute to a richer understanding of this pivotal issue. Schools adjacent to noisy roads were selected, and outdoor noise levels were measured employing a Kimo dB300 sound level meter, focusing on noise level indices LAeq, L10, and L90. Subsequent calculations were performed to determine the noise pollution level (Lnp), noise climate (NC), and traffic noise index (TNI), revealing a severe noise exposure when compared to standard guidelines. A perception questionnaire for various noise and acoustic factors influencing cognition was developed, and 1524 student responses were collected. Data analysis incorporated Principal Component Analysis (PCA) and Exploratory Factor Analysis (EFA) for dimension reduction, revealing three latent factors labelled 'annoyance,' 'behaviour,' and 'cognition'. Further, Structural Equation Modeling (SEM) was utilized to explore multivariate relationships between variables and latent factors. Resultant path coefficients were obtained as 0.12, 0.98, and 0.10 for the impact of 'behaviour' and 'annoyance' on 'cognition' and the correlation between 'annoyance' and 'behaviour', respectively. Findings underscore a potent positive impact of annoyance, stemming from acute ambient noise exposure, on the deterioration of children's cognition. While suggesting that ambient noise may be correlated with adverse health impacts due to its influence on cognition, this study emphasizes the pressing necessity for noise mitigation in roadside schools and stringent enforcement of noise pollution guidelines in academic zones.}
}
@article{FLINT2025100948,
title = {Expansion of analytical methods in auditing education},
journal = {Journal of Accounting Education},
volume = {70},
pages = {100948},
year = {2025},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2024.100948},
url = {https://www.sciencedirect.com/science/article/pii/S0748575124000642},
author = {Michele S. Flint},
keywords = {Auditing education, Analytical procedures, Data analytics, Beneish M−score, Altman Z-score, Sloan Accrual},
abstract = {Data analytics is changing the audit environment and carries significant implications for auditing education. Both international auditing education (International Accounting Education Standards Board (IAESB), 2019a; IAESB, 2019b) and U.S.-based regulatory bodies (American Institute of Certified Public Accountants (AICPA), 2021c; AICPA & National Association of State Boards of Accountancy (NASBA), 2021) have made efforts to address the growing expectations for auditing education, citing fraud risk and going concern risk. While auditing courses have progressed to include some computerized audit software for case studies, the study of analytical procedures has been limited to the application of basic financial ratios, trend analyses and common-size financial statements. Demands for advanced analytics place most emphasis on computerized query and computational methods; however, several advanced analytical models, namely the Altman Z-score, Beneish M−score and the Sloan Accrual formula provide opportunities for greater insight on specific audit risks and do not require advanced computer-based skills. The ability to link audit procedures, specifically analytical procedures to the audit objectives of financial risk and going concern risk strengthens the rationale for introduction of these advanced models within the context of auditing education. This paper discusses the inherent value in these analytical models, links them to audit objectives, proposes the inclusion of these three analytical models as a component of auditing education, and suggests that future study be undertaken to assess implementation and student learning. In addition, we recommend future study of other analytical models that may provide further insight for auditing students.}
}
@article{ZHAO2025128946,
title = {The effect of the head number for multi-head self-attention in remaining useful life prediction of rolling bearing and interpretability},
journal = {Neurocomputing},
volume = {616},
pages = {128946},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128946},
url = {https://www.sciencedirect.com/science/article/pii/S092523122401717X},
author = {Qiwu Zhao and Xiaoli Zhang and Fangzhen Wang and Panfeng Fan and Erick Mbeka},
keywords = {Remaining useful life prediction, Machine learning, Multi-head self-attention mechanism, Interpretability, Graph theory, Functional networks},
abstract = {As one of the machine learning (ML) models, the multi-head self-attention mechanism (MSM) is competent in encoding high-level feature representations, providing computing superiorities, and systematically processing sequences bypassing the recurrent neural networks (RNN) models. However, the model performance and computational results are affected by head number, and the lack of impact interpretability has become a primary obstacle due to the complex internal working mechanisms. Therefore, the effects of the head number of the MSM on the accuracy of the result, the robustness of the model, and computation efficiency are investigated in the remaining useful life (RUL) prediction of rolling bearings. The results show that the accuracy of prediction results will be reduced caused by large or few head numbers. In addition, the more heads are selected, the more robust and higher the predictive efficiency of the model is achieved. The above effects are explained relying on the visualization of the attention weight distribution and functional networks, which are constructed and solved by the equivalent fully connected layer and graph theory analysis, respectively. The model's attention coefficient distribution during training and prediction shows that the representative information will be captured inadequately if fewer heads are selected, which causes MSM to neglect to assign large attention coefficients to degraded information. On the contrary, representational degradation information and redundant information will be acquired by models with too many heads. MSM will be disturbed by this redundant information in the attention weight distribution, resulting in incorrect allocation of attention. Both of these cases will reduce the accuracy of the prediction results. In addition, the selection rules of the head number are established based on the feature complexity that is measured by the sample entropy (SamEn). The local range for head selection is also found based on the relationship between head number and feature complexity; The effects of the head number of the MSM on the robustness of the model and computation efficiency are explained by the changes in the three parameters (average of the clustering coefficients, global efficiency, and of the average shortest path length) of the graph, which is constructed after solving the function network. The research provides a reference for rolling bearing prediction with high computational accuracy, calculation efficiency, and strong robustness using MSM.}
}
@article{TWORZYDLO1995759,
title = {Knowledge-based methods and smart algorithms in computational mechanics},
journal = {Engineering Fracture Mechanics},
volume = {50},
number = {5},
pages = {759-800},
year = {1995},
issn = {0013-7944},
doi = {https://doi.org/10.1016/0013-7944(94)E0060-T},
url = {https://www.sciencedirect.com/science/article/pii/0013794494E0060T},
author = {W.W. Tworzydlo and J.T. Oden},
abstract = {Effective methods leading to automated, computer-based solution of complex engineering design problems are studied in this paper. In particular, methods of automation of the finite element analyses are of primary interest here. These include algorithmic approaches, based on error estimation, adaptivity and smart algorithms, as well as heuristic approaches based on methods of knowledge engineering. A computational environment, which interactively couples h-p adaptive finite element methods with object-oriented programming and expert system tools, is presented. Several examples illustrate the merit and potential of the approaches studied here.}
}
@article{MATSUDA2009970,
title = {Multiple cognitive deficits in patients during the mild cognitive impairment stage of Alzheimer's disease: how are cognitive domains other than episodic memory impaired?},
journal = {International Psychogeriatrics},
volume = {21},
number = {5},
pages = {970-976},
year = {2009},
issn = {1041-6102},
doi = {https://doi.org/10.1017/S1041610209990330},
url = {https://www.sciencedirect.com/science/article/pii/S1041610224025894},
author = {Osamu Matsuda and Masahiko Saito},
keywords = {Alzheimer's disease, COGNISTAT, mild cognitive impairment},
abstract = {ABSTRACT
Background: Little is known about how cognitive domains other than episodic memory are affected during the mild cognitive impairment (MCI) stage of Alzheimer's disease (AD). We attempted to clarify this issue in this study. Methods: Fifty-seven Japanese subjects were divided into two groups: one comprising people in the MCI stage of AD (MCI group, n = 28) and the other of normal controls (NC group, n = 29). Cognitive functions were assessed using the Japanese version of the neurobehavioral cognitive status examination (J-COGNISTAT). Results: The MCI group performed significantly worse than the NC group on subtests that assessed orientation, confrontational naming, constructive ability, episodic memory, and abstract thinking. Three-quarters of the MCI group had deficits in memory and other non-mnemonic domains, particularly constructive ability and abstract thinking. However, within-subject comparisons showed that the MCI group performed significantly worse on the memory subtest compared to any other subtest. Conclusions: Besides episodic memory, multiple non-mnemonic cognitive domains, such as constructive ability and abstract thinking, are also impaired during the MCI stage of AD; however, these non-mnemonic deficits are smaller than episodic memory impairment.}
}
@article{TSUTAKAWA2020102972,
title = {Envisioning how the prototypic molecular machine TFIIH functions in transcription initiation and DNA repair},
journal = {DNA Repair},
volume = {96},
pages = {102972},
year = {2020},
issn = {1568-7864},
doi = {https://doi.org/10.1016/j.dnarep.2020.102972},
url = {https://www.sciencedirect.com/science/article/pii/S1568786420302214},
author = {Susan E. Tsutakawa and Chi-Lin Tsai and Chunli Yan and Amer Bralić and Walter J. Chazin and Samir M. Hamdan and Orlando D. Schärer and Ivaylo Ivanov and John A. Tainer},
keywords = {TFIIH, Helicase, Transcription initiation, Transcription-coupled repair, Nucleotide excision repair, XPB, XPD, Translocase, DNA damage, DNA repair},
abstract = {Critical for transcription initiation and bulky lesion DNA repair, TFIIH provides an exemplary system to connect molecular mechanisms to biological outcomes due to its strong genetic links to different specific human diseases. Recent advances in structural and computational biology provide a unique opportunity to re-examine biologically relevant molecular structures and develop possible mechanistic insights for the large dynamic TFIIH complex. TFIIH presents many puzzles involving how its two SF2 helicase family enzymes, XPB and XPD, function in transcription initiation and repair: how do they initiate transcription, detect and verify DNA damage, select the damaged strand for incision, coordinate repair with transcription and cell cycle through Cdk-activating-kinase (CAK) signaling, and result in very different specific human diseases associated with cancer, aging, and development from single missense mutations? By joining analyses of breakthrough cryo-electron microscopy (cryo-EM) structures and advanced computation with data from biochemistry and human genetics, we develop unified concepts and molecular level understanding for TFIIH functions with a focus on structural mechanisms. We provocatively consider that TFIIH may have first evolved from evolutionary pressure for TCR to resolve arrested transcription blocks to DNA replication and later added its key roles in transcription initiation and global DNA repair. We anticipate that this level of mechanistic information will have significant impact on thinking about TFIIH, laying a robust foundation suitable to develop new paradigms for DNA transcription initiation and repair along with insights into disease prevention, susceptibility, diagnosis and interventions.}
}
@article{ARASTEH2025107403,
title = {A data-driven prediction method for multi-period portfolio optimization using the real options approach},
journal = {Finance Research Letters},
volume = {80},
pages = {107403},
year = {2025},
issn = {1544-6123},
doi = {https://doi.org/10.1016/j.frl.2025.107403},
url = {https://www.sciencedirect.com/science/article/pii/S1544612325006634},
author = {Abdollah Arasteh},
keywords = {Multi-period portfolio optimization, Probabilistic risk measure, Piecewise linear interpolation, ARIMA-GARCH models, Real options theory},
abstract = {Financial portfolio optimization balances risk and returns. Traditional multi-period models ignore financial time series dynamics and volatility by assuming normally distributed returns and static predictions. Many models ignore unequal estimation penalties, making them difficult. Different distribution models and uncertainty management in finance are sought to fill this gap. We test t-distributions and kernel estimators and add probabilistic risk criteria to the multi-period capital portfolio selection algorithm. Real options manage uncertainty in complex environments and provide accurate forecasts with strong decision-making tools despite volatile financial data. Modern theory applied to empirical applications improves dynamic financial system portfolio optimization and adaptive approaches.}
}
@article{IVANOV2023108938,
title = {Intelligent digital twin (iDT) for supply chain stress-testing, resilience, and viability},
journal = {International Journal of Production Economics},
volume = {263},
pages = {108938},
year = {2023},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2023.108938},
url = {https://www.sciencedirect.com/science/article/pii/S0925527323001706},
author = {Dmitry Ivanov},
keywords = {Supply chain resilience, Intelligent digital twin, Data analytics, Stress-test, Ripple effect, anyLogistix},
abstract = {A large variety of models have been developed in the last two decades aiming at supply chain (SC) stress-testing and resilience. New digital and artificial intelligence (AI) technologies allow to develop novel approaches and tools in this area for the transition from standalone models to intelligent decision-support systems (DSSs). However, the literature lacks concepts and guidelines for the design of such systems. In this paper, we offer a generalized decision-making framework for using digital twins in SC stress-testing and resilience analysis as well as delineate how digital twins can contribute to theory development in SC resilience and viability. We position our proposed approach as an intelligent digital twin (iDT) – a human–AI system which visualizes physical SCs in digital form, collects and processes data for modelling using analytics methods, mimics human decision-making rules, and creates new knowledge and decision-making algorithms through human–AI collaboration. We conclude that the iDT supports monitoring, disruption prediction (early signals), event-driven responses, learning, and proactive thinking, integrating proactive and reactive approaches to SC resilience. The iDT helps to make the unknown known and so contributes to the development of a proactive, adaptation-based view on SC resilience and viability. This research can be used to solve existing problems in the industry, and it develops new methods and infrastructures for solutions to future problems.}
}
@article{YANG2018182,
title = {A Geodesign Method of Human-Energy-Water Interactive Systems for Urban Infrastructure Design: 10KM2 Near-Zero District Project in Shanghai},
journal = {Engineering},
volume = {4},
number = {2},
pages = {182-189},
year = {2018},
note = {Sustainable Infrastructure},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2018.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S2095809918301978},
author = {Perry Pei-Ju Yang and Cheryl Shu-Fang Chi and Yihan Wu and Steven Jige Quan},
keywords = {Geodesign, Urban design, Urban infrastructure, Energy performance, Iterative process, Multi-objective optimization},
abstract = {The grand challenges of climate change demand a new paradigm of urban design that takes the performance of urban systems into account, such as energy and water efficiency. Traditional urban design methods focus on the form-making process and lack performance dimensions. Geodesign is an emerging approach that emphasizes the links between systems thinking, digital technology, and geographic context. This paper presents the research results of the first phase of a larger research collaboration and proposes an extended geodesign method for a district-scale urban design to integrate systems of renewable energy production, energy consumption, and storm water management, as well as a measurement of human experiences in cities. The method incorporates geographic information system (GIS), parametric modeling techniques, and multidisciplinary design optimization (MDO) tools that enable collaborative design decision-making. The method is tested and refined in a test case with the objective of designing a near-zero-energy urban district. Our final method has three characteristics. ① Integrated geodesign and parametric design: It uses a parametric design approach to generate focal-scale district prototypes by means of a custom procedural algorithm, and applies geodesign to evaluate the performances of design proposals. ② A focus on design flow: It elaborates how to define problems, what information is selected, and what criteria are used in making design decisions. ③ Multi-objective optimization: The test case produces indicators from performance modeling and derives principles through a multi-objective computational experiment to inform how the design can be improved. This paper concludes with issues and next steps in modeling urban design and infrastructure systems based on MDO tools.}
}
@article{LUO202571,
title = {HybProm: An attention-assisted hybrid CNN-BiLSTM model for the interpretable prediction of DNA promoter},
journal = {Methods},
volume = {235},
pages = {71-80},
year = {2025},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2025.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1046202325000349},
author = {Rentao Luo and Jiawei Liu and Lixin Guan and Mengshan Li},
keywords = {Promoter, Deep learning, Attention, Gene sequences, Bioinformatics},
abstract = {Promoter prediction is essential for analyzing gene structures, understanding regulatory networks, transcription mechanisms, and precisely controlling gene expression. Recently, computational and deep learning methods for promoter prediction have gained attention. However, there is still room to improve their accuracy. To address this, we propose the HybProm model, which uses DNA2Vec to transform DNA sequences into low-dimensional vectors, followed by a CNN-BiLSTM-Attention architecture to extract features and predict promoters across species, including E. coli, humans, mice, and plants. Experiments show that HybProm consistently achieves high accuracy (90%-99%) and offers good interpretability by identifying key sequence patterns and positions that drive predictions.}
}
@article{FALBEN2023105386,
title = {The power of the unexpected: Prediction errors enhance stereotype-based learning},
journal = {Cognition},
volume = {235},
pages = {105386},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2023.105386},
url = {https://www.sciencedirect.com/science/article/pii/S0010027723000203},
author = {Johanna K. Falbén and Marius Golubickis and Dimitra Tsamadi and Linn M. Persson and C. Neil Macrae},
keywords = {Stereotyping, Person perception, Reinforcement learning, Prediction errors, Drift diffusion model},
abstract = {Stereotyping is a ubiquitous feature of social cognition, yet surprisingly little is known about how group-related beliefs influence the acquisition of person knowledge. Accordingly, in combination with computational modeling (i.e., Reinforcement Learning Drift Diffusion Model analysis), here we used a probabilistic selection task to explore the extent to which gender stereotypes impact instrumental learning. Several theoretically interesting effects were observed. First, reflecting the impact of cultural socialization on person construal, an expectancy-based preference for stereotype-consistent (vs. stereotype-inconsistent) responses was observed. Second, underscoring the potency of unexpected information, learning rates were faster for counter-stereotypic compared to stereotypic individuals, both for negative and positive prediction errors. Collectively, these findings are consistent with predictive accounts of social perception and have implications for the conditions under which stereotyping can potentially be reduced.}
}
@article{DAVIS20111046,
title = {Homogeneous steady deformation: A review of computational techniques},
journal = {Journal of Structural Geology},
volume = {33},
number = {6},
pages = {1046-1062},
year = {2011},
issn = {0191-8141},
doi = {https://doi.org/10.1016/j.jsg.2011.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0191814111000447},
author = {Joshua R. Davis and Sarah J. Titus},
keywords = {Kinematic model, Homogeneous deformation, Velocity gradient, Transpression, Vorticity},
abstract = {Homogeneous steady models are frequently used in the structural geology community to describe rock deformation. We review the literature on these models in a streamlined, coordinate-free framework based on matrix exponentials and logarithms. These mathematical tools allow us to compute progressive and simultaneous deformations easily. As an application, we develop transpression with triclinic symmetry in two ways. The tools let us integrate field data related to position and velocity in computing best-fit models with many degrees of freedom. As an application, we reanalyze a published study to demonstrate the extent to which kinematic vorticity is sensitive to modeling assumptions. The tools also open the door to an increased role for the mathematics of Lie groups (spaces of deformations) in structural geology. We suggest two topics for further study: numerical methods for non-steady deformations, and statistics of deformation tensors.}
}
@article{NOVOTOTSKYVLASOV1995S114,
title = {PS-12-13 Event-related brain activity analysis by mean wave halfperiod duration computation method},
journal = {Electroencephalography and Clinical Neurophysiology/Electromyography and Motor Control},
volume = {97},
number = {4},
pages = {S114},
year = {1995},
issn = {0924-980X},
doi = {https://doi.org/10.1016/0924-980X(95)92838-D},
url = {https://www.sciencedirect.com/science/article/pii/0924980X9592838D},
author = {V.Y. Novototsky-Vlasov}
}
@article{STOLOWY2022101334,
title = {Competing for narrative authority in capital markets: Activist short sellers vs. financial analysts},
journal = {Accounting, Organizations and Society},
volume = {100},
pages = {101334},
year = {2022},
issn = {0361-3682},
doi = {https://doi.org/10.1016/j.aos.2022.101334},
url = {https://www.sciencedirect.com/science/article/pii/S0361368222000010},
author = {Hervé Stolowy and Luc Paugam and Yves Gendron},
keywords = {Activist short sellers, Expertise, Financial analysts, Framing, Narrative authority},
abstract = {Activist short sellers (AShSs) and financial analysts are information intermediaries who analyze firm disclosures as well as produce and disseminate influential investment narratives. This study aims to better understand narrative challenges surrounding the legitimate expertise of financial analysts. Specifically, we examine how AShSs challenge sell-side financial analysts' narrative authority (i.e., the perception that they produce expert knowledge) in interpreting firms' performance and future prospects. We investigate how analysts respond (or do not respond) to this challenge. We use 442 AShS reports, 12 interviews with AShSs and analysts, and analysts' stock recommendations and target prices. In their criticisms of analysts (found in one-third of reports), AShSs frequently frame analysts as lacking market expertise and critical thinking – two core dimensions of analysts' narrative authority. Sixty-six percent of analysts, although explicitly criticized in AShS reports, do not engage in written responses in their equity research reports because they reportedly either adopt a renunciation attitude to the challenge or they engage in off-the-record discussions with certain market participants. However, 34% of analysts respond overtly by counter-framing AShSs as lacking market expertise and objectivity. After the dissemination of AShS reports, analysts, on average, do not revise their highly visible stock recommendations but they revise target prices downward. Theoretically, this study extends our understanding of the construction of narrative authority in capital markets as we examine a challenge to the expertise of influential information intermediaries.}
}
@article{WEISSMAN2011516,
title = {A computational framework for authoring and searching product design specifications},
journal = {Advanced Engineering Informatics},
volume = {25},
number = {3},
pages = {516-534},
year = {2011},
note = {Special Section: Engineering informatics in port operations and logistics},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2011.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1474034611000061},
author = {Alexander Weissman and Martin Petrov and Satyandra K. Gupta},
keywords = {Product design specifications, Engineering design, Requirements engineering},
abstract = {The development of product design specifications (PDS) is an important part of the product development process. Incompleteness, ambiguity, or inconsistency in the PDS can lead to problems during the design process and may require unnecessary design iterations. This generally results in increased design time and cost. Currently, in many organizations, PDS are written using word processors. Since documents written by different authors can be inconsistent in style and word choice, it is difficult to automatically search for specific requirements. Moreover, this approach does not allow the possibility of automated design verification and validation against the design requirements and specifications. In this paper, we present a computational framework and a software tool based on this framework for writing, annotating, and searching computer-interpretable PDS. Our approach allows authors to write requirement statements in natural language to be consistent with the existing authoring practice. However, using mathematical expressions, keywords from predefined taxonomies, and other metadata the author of PDS can then annotate different parts of the requirement statements. This approach provides unambiguous meaning to the information contained in PDS, and helps to eliminate mistakes later in the process when designers must interpret requirements. Our approach also enables users to construct a new PDS document from the results of the search for requirements of similar devices and in similar contexts. This capability speeds up the process of creating PDS and helps authors write more detailed documents by utilizing previous, well written PDS documents. Our approach also enables checking for internal inconsistencies in the requirement statements.}
}
@article{NOWACK2024459,
title = {Science and reflections: With some thoughts to young applied scientists and engineers},
journal = {Earthquake Science},
volume = {37},
number = {5},
pages = {459-493},
year = {2024},
issn = {1674-4519},
doi = {https://doi.org/10.1016/j.eqs.2024.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1674451924000648},
author = {Robert L. Nowack},
keywords = {geophysics, computational and data science, applied science and engineering},
abstract = {I provide some science and reflections from my experiences working in geophysics, along with connections to computational and data sciences, including recent developments in machine learning. I highlight several individuals and groups who have influenced me, both through direct collaborations as well as from ideas and insights that I have learned from. While my reflections are rooted in geophysics, they should also be relevant to other computational scientific and engineering fields. I also provide some thoughts for young, applied scientists and engineers.}
}
@article{KLOOSTER2024110771,
title = {A systematic review on eHealth technology personalization approaches},
journal = {iScience},
volume = {27},
number = {9},
pages = {110771},
year = {2024},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2024.110771},
url = {https://www.sciencedirect.com/science/article/pii/S2589004224019965},
author = {Iris ten Klooster and Hanneke Kip and Lisette {van Gemert-Pijnen} and Rik Crutzen and Saskia Kelders},
keywords = {Health sciences, Health technology},
abstract = {Summary
Despite the widespread use of personalization of eHealth technologies, there is a lack of comprehensive understanding regarding its application. This systematic review aims to bridge this gap by identifying and clustering different personalization approaches based on the type of variables used for user segmentation and the adaptations to the eHealth technology and examining the role of computational methods in the literature. From the 412 included reports, we identified 13 clusters of personalization approaches, such as behavior + channeling and environment + recommendations. Within these clusters, 10 computational methods were utilized to match segments with technology adaptations, such as classification-based methods and reinforcement learning. Several gaps were identified in the literature, such as the limited exploration of technology-related variables, the limited focus on user interaction reminders, and a frequent reliance on a single type of variable for personalization. Future research should explore leveraging technology-specific features to attain individualistic segmentation approaches.}
}
@article{ROUX2025115353,
title = {Turning private possessions into assets: A calculative-based approach to platform versus proximity rentals},
journal = {Journal of Business Research},
volume = {193},
pages = {115353},
year = {2025},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2025.115353},
url = {https://www.sciencedirect.com/science/article/pii/S0148296325001766},
author = {Dominique Roux and Russell Belk},
keywords = {Assetization, Calculation, Rentiership, Collaborative consumption, Platform rentals, Proximity rentals},
abstract = {While previous research has examined economic motivations for renting personal possessions, the calculative processes owners use remain understudied. We explored how consumers calculate and repurpose their possessions as assets. Through a qualitative analysis of interviews with seven Airbnb hosts and 37 owners renting various possessions, plus 154 Facebook posts from ‘Airbnb propriétaires France,’ we identified four key calculative operations: extraction, qualification, computation, and protection. Our findings reveal a dual pathway—platforms and closed circles—through which mundane possessions become profitable, despite owners’ suboptimal calculations. By including the protection and selective exclusion of possessions, we extend the sociological theory of calculation and improve our understanding of proximity rentals. Overall, calculation generates three benefits: economically, as owners always consider rental revenues significant; ecologically, as even small compensation motivates owners to circulate private goods; and socially, as mutual exchanges between strangers and acquaintances escape gift economy constraints and debt obligations.}
}
@article{SEEMAN202211461,
title = {Understanding chemistry: from “heuristic (soft) explanations and reasoning by analogy” to “quantum chemistry”††Dedicated to Dudley Herschbach in celebration of his 90th year who, when asked whether he was a theoretician or an experimentalist, responded, “The molecules don't know and don't care.”},
journal = {Chemical Science},
volume = {13},
number = {39},
pages = {11461-11486},
year = {2022},
issn = {2041-6520},
doi = {https://doi.org/10.1039/d2sc02535c},
url = {https://www.sciencedirect.com/science/article/pii/S2041652023014347},
author = {Jeffrey I. Seeman and Dean J. Tantillo},
abstract = {ABSTRACT
“Soft theories,” i.e., “heuristic models based on reasoning by analogy” largely drove chemistry understanding for 150 years or more. But soft theories have their limitations and with the expansion of chemistry in the mid-20th century, more and more inexplicable (by soft theory) experimental results were being obtained. In the past 50 years, quantum chemistry, most often in the guise of applied theoretical chemistry including computational chemistry, has provided (a) the underlying “hard evidence” for many soft theories and (b) the explanations for chemical phenomena that were unavailable by soft theories. In this publication, we define “hard theories” as “theories derived from quantum chemistry.” Both soft and hard theories can be qualitative and quantitative, and the “Houk quadrant” is proposed as a helpful categorization tool. Furthermore, the language of soft theories is often used appropriately to describe quantum chemical results. A valid and useful way of doing science is the appropriate use and application of both soft and hard theories along with the best nomenclature available for successful communication of results and ideas.}
}
@article{XIA2025108857,
title = {LSDNet: Lightweight strip-steel surface defect detection networks for edge device environment},
journal = {Optics and Lasers in Engineering},
volume = {186},
pages = {108857},
year = {2025},
issn = {0143-8166},
doi = {https://doi.org/10.1016/j.optlaseng.2025.108857},
url = {https://www.sciencedirect.com/science/article/pii/S0143816625000442},
author = {Xuhui Xia and Jiale Guo and Zelin Zhang and Lei Wang and Yuyao Guo},
keywords = {Cold-rolled strip steel, Defect classification, Lightweight network, Feature extraction},
abstract = {Online recognizing defects of the strip-steel surface on resource-constrained embedded devices is a difficult problem. The traditional deep learning model with deep network layers and large parameter counts cannot balance the efficiency and the accuracy. This paper proposes a specialized lightweight deep learning detection model (LSDNet) for strip-steel surface defects. Moreover, LSDNet effectively classifies and recognizes these defects with fewer model parameters. LSDNet adopts Mobilenetv2 as the basic framework and constructs a new feature extraction module. The SPD-Conv module enhances the feature learning capacity for small targets and reduces model redundancy, while the ECANet module improves feature extraction capabilities. Additionally, the parameter-free attention mechanism (SimAM) is incorporated after the initial and final convolutional layers to boost recognition accuracy. Computational efficiency is achieved by substituting fully connected layers with a spatially invariant global average pooling layer, thereby preserving essential depth information. Dropout layers are deployed to enhance generalization, and dynamic learning rate adjustments optimize the training process. Experimental results demonstrate that the proposed LSDNet achieves a classification accuracy of 98.60 %, an F1−score of 98.57 %, with only 0.76 million parameters and 0.095 billion FLOPs for strip-steel surface defects. Compared to Mobilenetv2, LSDNet reduces the parameter count by 2.749 million and improves the classification accuracy by 1.69 %. This method performs better than other classification models in balancing recognition efficiency and accuracy.}
}
@article{FADLALLA1995987,
title = {Improving the performance of enumerative search methods—part II: Computational experiments},
journal = {Computers & Operations Research},
volume = {22},
number = {10},
pages = {987-994},
year = {1995},
issn = {0305-0548},
doi = {https://doi.org/10.1016/0305-0548(95)00016-F},
url = {https://www.sciencedirect.com/science/article/pii/030505489500016F},
author = {Adam Fadlalla and James R. Evans and Martin S. Levy},
abstract = {Generally, branch and bound algorithms typically use mechanistic search strategies and generally do not fully exploit “local” information inherent in problem structures; that is, specific problem-domain knowledge. Some exceptions are found in [2–5]. Incorporatiing intelligence in branch and bound algorithms has been suggested by Glover [1], but not studied in a rigorous experimental framework. We use the mean tardiness job sequencing problem to explore these issues. This paper is divided into two Parts. In Part I [9], we provided the intuitive motivation for this investigation and an experimental framework. In Part II, we present detailed computational results and statistical analysis. The results indicate that branch and bound algorithms can be enhanced significantly by exploiting local knowledge of problem structure and more judicious search strategies.}
}
@article{AGRAWAL2022101673,
title = {Spectrum sensing in cognitive radio networks and metacognition for dynamic spectrum sharing between radar and communication system: A review},
journal = {Physical Communication},
volume = {52},
pages = {101673},
year = {2022},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2022.101673},
url = {https://www.sciencedirect.com/science/article/pii/S187449072200043X},
author = {Sumit Kumar Agrawal and Abhay Samant and Sandeep Kumar Yadav},
keywords = {Cognitive radio, Spectrum sensing, Spectrum sharing, Cognitive radar, Metacognition, Metacognitive radar},
abstract = {The massive growth in mobile users and wireless technologies has resulted in increased data traffic and created demand for additional radio spectrum. This growing demand for radio spectrum has resulted in spectrum congestion and mandated the need for coexistence between radar and interfering communication emitters. To address the aforementioned issues, it is critical to review existing policies and evaluate new technologies that can utilize spectrum in an efficient and intelligent manner. Cognitive radio and cognitive radar are two promising technologies that exploit spectrum using dynamic spectrum access techniques. Additionally, introducing the bio-inspired concept ‘metacognition’ in a cognitive process has shown to increase the effectiveness and robustness of the cognitive radio and cognitive radar system. Metacognition is a high-order thinking agent that monitors and regulates the cognition process through a feedback and control process called the perception–action cycle. Extensive research has been done in the field of spectrum sensing in cognitive radio and spectral coexistence between radar and communication systems. This paper provides a detailed classification of spectrum sensing schemes and explains how dynamic spectrum access strategies share the spectrum between radar and communication systems. In addition to this, the fundamentals of cognitive radio, its architecture, spectrum management framework, and metacognition concept in radar are discussed. Furthermore, this paper presents various research issues, challenges, and future research directions associated with spectrum sensing in cognitive radar and dynamic spectrum access strategies in cognitive radar.}
}
@incollection{MOHAN2025541,
title = {Chapter 51 - Exploring the exciting potential and challenges of brain computer interfaces},
editor = {M.A. Ansari and R.S. Anand and Pragati Tripathi and Rajat Mehrotra and Md Belal Bin Heyat},
booktitle = {Artificial Intelligence in Biomedical and Modern Healthcare Informatics},
publisher = {Academic Press},
pages = {541-550},
year = {2025},
isbn = {978-0-443-21870-5},
doi = {https://doi.org/10.1016/B978-0-443-21870-5.00051-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443218705000510},
author = {Anand Mohan and R.S. Anand},
keywords = {Brain–computer interface (BCI), EEG, Machine learning, Motor imagery, PSD},
abstract = {Electroencephalogram (EEG) signals contain various information about the cognitive thinking, emotion, and thoughts of a person. Verbal communication is the normal form of interaction method used, but various kinds of physically disabled people who are not in the condition to express themselves can be assisted using the EEG signal rehabilitation technique. EEG signals can be used effectively in rehabilitation by using brain–computer interfaces (BCIs). BCI is a technology that allows interaction between the brain and a computer. This kind of technique can be used to treat patients with paralyzed muscles and locked in syndromes by helping them interact with others using their EEG signals. The application of BCI can be in medical field, education, and security. In this chapter, all aspects of BCIs are discussed in great detail and also have worked on motor imaginary-based dataset and have used linear discriminant analysis (LDA) algorithm as the classifier, which showed 91% accuracy.}
}