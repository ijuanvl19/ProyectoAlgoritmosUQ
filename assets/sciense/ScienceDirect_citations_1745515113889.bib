@article{ZHOU2019244,
title = {Lightweight IoT-based authentication scheme in cloud computing circumstance},
journal = {Future Generation Computer Systems},
volume = {91},
pages = {244-251},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.08.038},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18307878},
author = {Lu Zhou and Xiong Li and Kuo-Hui Yeh and Chunhua Su and Wayne Chiu},
keywords = {Internet-of-things (IoT), Cloud computing, Authentication, Proverif, User tracking},
abstract = {Recently, authentication technologies integrated with the Internet of Things (IoT) and cloud computing have been promptly investigated for secure data retrieval and robust access control on large-scale IoT networks. However, it does not have a best practice for simultaneously deploying IoT and cloud computing with robust security. In this study, we present a novel authentication scheme for IoT-based architectures combined with cloud servers. To pursue the best efficiency, lightweight crypto-modules, such as one-way hash function and exclusive-or operation, are adopted in our authentication scheme. It not only removes the computation burden but also makes our proposed scheme suitable for resource-limited objects, such as sensors or IoT devices. Through the formal verification delivered by Proverif, the security robustness of the proposed authentication scheme is guaranteed. Furthermore, the performance evaluation presents the practicability of our proposed scheme in which a user-acceptable computation cost is achieved.}
}
@article{BOLER201875,
title = {The affective politics of the “post-truth” era: Feeling rules and networked subjectivity},
journal = {Emotion, Space and Society},
volume = {27},
pages = {75-85},
year = {2018},
issn = {1755-4586},
doi = {https://doi.org/10.1016/j.emospa.2018.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S1755458617301585},
author = {Megan Boler and Elizabeth Davis},
keywords = {Affect, Emotion, Post-truth, Feeling rules, Truthiness, Digital media, Algorithmic governance, Computational propaganda},
abstract = {This essay maps interdisciplinary lines of inquiry to assess current research on affect and emotion in relation to digital and social media, in the context of the fractured news media landscape and increasingly visible emotionality in political life. The essay sketches the context of polarized emotionality and the crisis of truth characterizing current U.S. politics, centrally engaging Arlie Hochschild's concept of “feeling rules”. We explore the limitations of “affect theory” for researching mediatized politics, contending that the stark differentiation of “affect” from “emotion” reifies the rational, autonomous, liberal conception of the subject, and is of limited value for political communications research. Instead, we emphasize the relational nature of affect and emotion, and the value of feminist politics of emotion research. Our analysis evaluates the limitations of contemporary scholarship on affect, social media, and politics in the context of the grave challenges posed by algorithmic governance and computational propaganda. We conclude by suggesting the concept of “networked subjectivity” for understanding mediatized politics, and the importance of the “affective feedback loop” within the context of the social media “culture of likes.”}
}
@article{BACK202423,
title = {Accelerated chemical science with AI},
journal = {Digital Discovery},
volume = {3},
number = {1},
pages = {23-33},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d3dd00213f},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X24000858},
author = {Seoin Back and Alán Aspuru-Guzik and Michele Ceriotti and Ganna Gryn'ova and Bartosz Grzybowski and Geun Ho Gu and Jason Hein and Kedar Hippalgaonkar and Rodrigo Hormázabal and Yousung Jung and Seonah Kim and Woo Youn Kim and Seyed Mohamad Moosavi and Juhwan Noh and Changyoung Park and Joshua Schrier and Philippe Schwaller and Koji Tsuda and Tejs Vegge and O. Anatole {von Lilienfeld} and Aron Walsh},
abstract = {In light of the pressing need for practical materials and molecular solutions to renewable energy and health problems, to name just two examples, one wonders how to accelerate research and development in the chemical sciences, so as to address the time it takes to bring materials from initial discovery to commercialization. Artificial intelligence (AI)-based techniques, in particular, are having a transformative and accelerating impact on many if not most, technological domains. To shed light on these questions, the authors and participants gathered in person for the ASLLA Symposium on the theme of ‘Accelerated Chemical Science with AI’ at Gangneung, Republic of Korea. We present the findings, ideas, comments, and often contentious opinions expressed during four panel discussions related to the respective general topics: ‘Data’, ‘New applications’, ‘Machine learning algorithms’, and ‘Education’. All discussions were recorded, transcribed into text using Open AI's Whisper, and summarized using LG AI Research's EXAONE LLM, followed by revision by all authors. For the broader benefit of current researchers, educators in higher education, and academic bodies such as associations, publishers, librarians, and companies, we provide chemistry-specific recommendations and summarize the resulting conclusions.}
}
@article{STAMMITTI2013e58,
title = {Spreadsheets for assisting Transport Phenomena Laboratory experiences},
journal = {Education for Chemical Engineers},
volume = {8},
number = {2},
pages = {e58-e71},
year = {2013},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2013.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S1749772813000067},
author = {Aurelio Stammitti},
keywords = {Educational spreadsheets, Transport Phenomena Laboratory, Laboratory experience quality, Data processing task, Hands-on learning, Student analytical thinking},
abstract = {Academic laboratories have been traditionally used for complementing and reinforcing in a practical way the theoretical instruction received in classroom lectures. However, data processing and model evaluation tasks are time consuming and do not add much value to the student's learning experience as they reduce available time for result analysis, critical thinking and report writing skills development. Therefore, this project addressed this issue by selecting three experiences of the Transport Phenomena Laboratory, namely: metallic bar temperature profiles, transient heat conduction and fixed and fluidised bed behaviour, and developed a spreadsheet for each one of them. These spreadsheets, without demanding programming skills, easily process experimental data sets, evaluate complex analytical and numerical models and correlations, not formerly considered and, convey results in tables and plots. Chemical engineering students that tested the spreadsheets were surveyed and expressed the added value of the sheets, being user-friendly, helped them to fulfil lab objectives by reducing their workload and, allowed them to complete deeper analyses that instructors could not request before, as they were able to quickly evaluate, compare and validate different model assumptions and correlations. Students also provided valuable suggestions for improving the spreadsheet experience. Through these sheets, students’ lab learning experience was updated.}
}
@article{POOBALAN2025101667,
title = {A novel and secured email classification using deep neural network with bidirectional long short-term memory},
journal = {Computer Speech & Language},
volume = {89},
pages = {101667},
year = {2025},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2024.101667},
url = {https://www.sciencedirect.com/science/article/pii/S0885230824000500},
author = {A. Poobalan and K. Ganapriya and K. Kalaivani and K. Parthiban},
keywords = {Email classification, DNN-BiLSTM, AES algorithm, Rabit algorithm, Random forests (RF)},
abstract = {Email data has some characteristics that are different from other social media data, such as a large range of answers, formal language, notable length variations, high degrees of anomalies, and indirect relationships. The main goal in this research is to develop a robust and computationally efficient classifier that can distinguish between spam and regular email content. The benchmark Enron dataset, which is accessible to the public, was used for the tests. The six distinct Enron data sets we acquired were combined to generate the final seven Enron data sets. The dataset undergoes early preprocessing to remove superfluous sentences. The proposed model Bidirectional Long Short-Term Memory (BiLSTM) apply spam labels and to examine email documents for spam. On seven Enron datasets, DNN-BiLSTM performs better than other classifiers in the performance comparison in terms of accuracy. DNN-BiLSTM and convolutional neural networks demonstrated that they can classify spam with 96.39 % and 98.69 % accuracy, respectively, in comparison to other machine learning classifiers. The risks associated with cloud data management and potential security flaws are also covered in the paper. This research presents hybrid encryption as a means of protecting cloud data while preserving privacy by using the hybrid AES-Rabit encryption algorithm which is based on symmetric session key exchange.}
}
@article{LILI20171611,
title = {An Inverse Optimization Model for Human Resource Allocation Problem Considering Competency Disadvantage Structure},
journal = {Procedia Computer Science},
volume = {112},
pages = {1611-1622},
year = {2017},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 21st International Conference, KES-20176-8 September 2017, Marseille, France},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.08.248},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917316575},
author = {Zhang Lili},
keywords = {inverse optimization, linear programming, human resource allocation, competency, evaluation according to disadvantage structure},
abstract = {Most of serious and major accidents that happened during the production procedure of process industry are caused by improper equipment operations, which is further owing to inappropriate human resources allocation and ignorance of individual competencies differences. In order to take both of competency disadvantage and adjustment requirement into consideration, we use an inverse optimization method to solve a human resource allocation problem, and furthermore, adjust equipment operating parameters to make the per-defined settings optimized, such as the total number of jobs, security-related parameters and so on.In the solving process, firstly a standard competence hierarchy system is conducted; secondly we propose an assessment method according to disadvantage structure; thirdly we use inverse optimization method to solve the problem and optimize the predefined allocation plan. Lastly, we give an example to prove its feasibility and effectiveness. In this paper a novel formulation of human resource allocation problem is proposed, in which some of main individual characteristics are considered and described mathematically, including psychology, behaviour and characteristics diged from them such as weakness. The other contribution of this paper is using inverse optimization to adjust parameters based on the given ideal allocation plan. Both of these propositions have a positive significance on promoting development and security construction for process industries.This research incorporates the academic thinking of inverse optimization, it not only puts psychology and behavior into optimization model, but also data mines weakness characteristics under the psychology and behavior data, and find a new way to introducing the weakness characteristics into decision making model. It provides a new thought for the following decision making problem, that is the ideal decision plan is known, and optimization parameters are changeable. It promotes the combining of psychology, behavior and operations research, it is good for process industries to develop in a safety and efficiency way.}
}
@article{ASAHIRO202016,
title = {Graph orientation with splits},
journal = {Theoretical Computer Science},
volume = {844},
pages = {16-25},
year = {2020},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2020.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S030439752030387X},
author = {Yuichi Asahiro and Jesper Jansson and Eiji Miyano and Hesam Nikpey and Hirotaka Ono},
keywords = {Graph orientation, Maximum flow, Vertex cover, Partition, Algorithm, Computational complexity},
abstract = {The Minimum Maximum Outdegree Problem (MMO) is to assign a direction to every edge in an input undirected, edge-weighted graph so that the maximum weighted outdegree taken over all vertices becomes as small as possible. In this paper, we introduce a new variant of MMO called the p-Split Minimum Maximum Outdegree Problem (p-Split-MMO) in which one is allowed to perform a sequence of p split operations on the vertices before orienting the edges, for some specified non-negative integer p, and study its computational complexity.}
}
@article{CHOI2025,
title = {Association Between Shift Working and Brain Morphometric Changes in Workers: A Voxel-wise Comparison},
journal = {Safety and Health at Work},
year = {2025},
issn = {2093-7911},
doi = {https://doi.org/10.1016/j.shaw.2025.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S2093791125000113},
author = {Joon Yul Choi and Sungmin Kim and Yongho Lee and Dohyeon Kim and Wanhyung Lee},
keywords = {Brain MRI, Neuroplasticity, Shift work, Voxel-wise comparison},
abstract = {Objective
There is abundant evidence from observational studies linking various health problems to shift work, but there is a lack of brain-based neurological evidence. Therefore, we examined morphometric changes on brain magnetic resonance imaging (MRI) between shift and non-shift workers.
Methods
A total 111 healthy workers participated in this study and underwent brain MRI, with the analysis incorporating merged workers' health surveillance data from regional hospital workers. Voxel-based morphometry analysis was used to investigate regional changes in the gray matter volume. To investigate the association of structural changes between shift workers and non-shift workers, a general linear model and threshold-free cluster enhancement were used with covariates, including total intracranial volume, age, and sex.
Results
After family-wise error correction, non-shift workers exhibited a significantly larger cerebellar region (p < 0.05) than shift workers. Conversely, the inferior parietal gyrus was found to be significantly larger in shift workers than in non-shift workers with family-wise error correction.
Conclusions
We observed increased clusters in the brains of both shift and non-shift workers, suggesting that the acquired occupational environment, including the shift work schedule, could influence brain neuroplasticity, which is an important consideration for occupational health.}
}
@incollection{FARMER2008228,
title = {Chapter 11 - Fragment-Based Drug Discovery},
editor = {Camille Georges Wermuth},
booktitle = {The Practice of Medicinal Chemistry (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {New York},
pages = {228-243},
year = {2008},
isbn = {978-0-12-374194-3},
doi = {https://doi.org/10.1016/B978-0-12-374194-3.00011-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780123741943000111},
author = {Bennett T. Farmer and Allen B. Reitz},
abstract = {Publisher Summary
Although target proteins are flexible and can adopt one or more of a manifold of induced conformations, binding sites on proteins have evolved to recognize a limited number of endogenous modulators and substrates and to exclude others. This chapter reviews fragment-based drug discovery (FBDD) on the historical and operational level and explains how it can be applied on a case-by-case basis. FBBD determines which molecular substructures or fragments interact with targets of interest and how they bind, and then uses that information to obtain drugs for therapy. It represents a paradigm shift in thinking of how to approach the lead generation process in drug discovery, and is an attempt to get more information rapidly while doing the same amount of work overall. The study draws comparison between the FBDD and HTS/HTL approaches. In the FBDD approach, the medicinal chemist plays the role of a combined synthetic and structural chemist. The emphasis on informatics is greatly reduced because there is less data overall and most of it, such as from NMR or X-ray crystal structures, is visually analyzed, typically being complemented only by functional assay data on just the target itself. Several different computational methods have been developed to prescreen fragment libraries as a way to select members for further study and consideration.}
}
@article{KLEEBARILLAS2015455,
title = {A comparative study and validation of state estimation algorithms for Li-ion batteries in battery management systems},
journal = {Applied Energy},
volume = {155},
pages = {455-462},
year = {2015},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2015.05.102},
url = {https://www.sciencedirect.com/science/article/pii/S0306261915007357},
author = {Joaquín {Klee Barillas} and Jiahao Li and Clemens Günther and Michael A. Danzer},
keywords = {Lithium-ion battery, Battery management system, State of charge estimation, Robustness analysis, Sliding-mode observer, Kalman-based SOC estimation},
abstract = {To increase lifetime, safety, and energy usage battery management systems (BMS) for Li-ion batteries have to be capable of estimating the state of charge (SOC) of the battery cells with a very low estimation error. The accurate SOC estimation and the real time reliability are critical issues for a BMS. In general an increasing complexity of the estimation methods leads to higher accuracy. On the other hand it also leads to a higher computational load and may exceed the BMS limitations or increase its costs. An approach to evaluate and verify estimation algorithms is presented as a requisite prior the release of the battery system. The approach consists of an analysis concerning the SOC estimation accuracy, the code properties, complexity, the computation time, and the memory usage. Furthermore, a study for estimation methods is proposed for their evaluation and validation with respect to convergence behavior, parameter sensitivity, initialization error, and performance. In this work, the introduced analysis is demonstrated with four of the most published model-based estimation algorithms including Luenberger observer, sliding-mode observer, Extended Kalman Filter and Sigma-point Kalman Filter. The experiments under dynamic current conditions are used to verify the real time functionality of the BMS. The results show that a simple estimation method like the sliding-mode observer can compete with the Kalman-based methods presenting less computational time and memory usage. Depending on the battery system’s application the estimation algorithm has to be selected to fulfill the specific requirements of the BMS.}
}
@article{CHERNYSHOV2004535,
title = {A System Identification Approach to Assessing Airline Pilot Skills},
journal = {IFAC Proceedings Volumes},
volume = {37},
number = {6},
pages = {535-540},
year = {2004},
note = {16th IFAC Symposium on Automatic Control in Aerospace 2004, Saint-Petersburg, Russia, 14-18 June 2004},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)32230-9},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017322309},
author = {Kirill Chernyshov},
keywords = {Aircraft control, Skill, Human factors, Performance monitoring, Identification, Stochastic systems, Coupling coefficients, Cross correlation functions, Estimation algorithms, Sampled data},
abstract = {The paper presents a new approach to eliciting information on current professional airline pilot skills and pilotage experience as a decision making person (DMP). Such an approach is regarded to the heuristic regularities of the DMP thinking process. In turn, the regularities are revealed on basis of recording the motions of the pilot eyes over the information field of the flight deck and processing the experimental data obtained. For the data mining, a probability theoretical approach is involved. Such an approach is based on applying the notion of consistency of measures of stochastic dependence of random variables; and a method of deriving almost sure converging estimate of such a measure by sample data is proposed.}
}
@incollection{HEILMAN201319,
title = {Chapter 2 - Visual artistic creativity and the brain},
editor = {Stanley Finger and Dahlia W. Zaidel and François Boller and Julien Bogousslavsky},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {204},
pages = {19-43},
year = {2013},
booktitle = {The Fine Arts, Neurology, and Neuroscience},
issn = {0079-6123},
doi = {https://doi.org/10.1016/B978-0-444-63287-6.00002-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780444632876000026},
author = {Kenneth M. Heilman and Lealani Mae Acosta},
keywords = {artistic creativity, hemispheric functions, visuospatial skills, creative innovation, divergent thinking, imagery, global and focal attention},
abstract = {Creativity is the development of a new or novel understanding—insight that leads to the expression of orderly relationships (e.g., finding and revealing the thread that unites). Visual artistic creativity plays an important role in the quality of human lives, and the goal of this chapter is to describe some of the brain mechanisms that may be important in visual artistic creativity. The initial major means of learning how the brain mediates any activity is to understand the anatomy and physiology that may support these processes. A further understanding of specific cognitive activities and behaviors may be gained by studying patients who have diseases of the brain and how these diseases influence these functions. Physiological recording such as electroencephalography and brain imaging techniques such as PET and fMRI have also allowed us to gain a better understanding of the brain mechanisms important in visual creativity. In this chapter, we discuss anatomic and physiological studies, as well as neuropsychological studies of healthy artists and patients with neurological disease that have helped us gain some insight into the brain mechanisms that mediate artistic creativity.}
}
@article{CALDEIRA2025102657,
title = {Model compression techniques in biometrics applications: A survey},
journal = {Information Fusion},
volume = {114},
pages = {102657},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102657},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524004354},
author = {Eduarda Caldeira and Pedro C. Neto and Marco Huber and Naser Damer and Ana F. Sequeira},
keywords = {Compression, Knowledge distillation, Quantization, Pruning, Biometrics, Bias},
abstract = {The development of deep learning algorithms has extensively empowered humanity’s task automatization capacity. However, the huge improvement in the performance of these models is highly correlated with their increasing level of complexity, limiting their usefulness in human-oriented applications, which are usually deployed in resource-constrained devices. This led to the development of compression techniques that drastically reduce the computational and memory costs of deep learning models without significant performance degradation. These compressed models are especially essential when implementing multi-model fusion solutions where multiple models are required to operate simultaneously. This paper aims to systematize the current literature on this topic by presenting a comprehensive survey of model compression techniques in biometrics applications, namely quantization, knowledge distillation and pruning. We conduct a critical analysis of the comparative value of these techniques, focusing on their advantages and disadvantages and presenting suggestions for future work directions that can potentially improve the current methods. Additionally, we discuss and analyze the link between model bias and model compression, highlighting the need to direct compression research toward model fairness in future works.}
}
@article{SHANAHAN2005157,
title = {Applying global workspace theory to the frame problem},
journal = {Cognition},
volume = {98},
number = {2},
pages = {157-176},
year = {2005},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2004.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0010027704002288},
author = {Murray Shanahan and Bernard Baars},
keywords = {Frame problem, Relevance problem, Global workspace theory, Consciousness, Analogical reasoning},
abstract = {The subject of this article is the frame problem, as conceived by certain cognitive scientists and philosophers of mind, notably Fodor for whom it stands as a fundamental obstacle to progress in cognitive science. The challenge is to explain the capacity of so-called informationally unencapsulated cognitive processes to deal effectively with information from potentially any cognitive domain without the burden of having to explicitly sift the relevant from the irrelevant. The paper advocates a global workspace architecture, with its ability to manage massively parallel resources in the context of a serial thread of computation, as an answer to this challenge. Analogical reasoning is given particular attention, since it exemplifies informational unencapsulation in its most extreme form. Because global workspace theory also purports to account for the distinction between conscious and unconscious information processing, the paper advances the tentative conclusion that consciousness may go hand-in-hand with a solution to the frame problem in the biological brain.}
}
@incollection{HADAP2023319,
title = {Chapter 16 - Theories methods and the parameters of quantitative structure–activity relationships and artificial neural network},
editor = {Dakeshwar Kumar Verma and Chandrabhan Verma and Jeenat Aslam},
booktitle = {Computational Modelling and Simulations for Designing of Corrosion Inhibitors},
publisher = {Elsevier},
pages = {319-335},
year = {2023},
isbn = {978-0-323-95161-6},
doi = {https://doi.org/10.1016/B978-0-323-95161-6.00019-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323951616000199},
author = {Arti Hadap and Ashutosh Pandey and Bhawana Jain and Reena Rawat},
keywords = {Corrosion, QSAR (quantitative structure and activity relationship), ANN (artificial neural network), inhibitor adsorption, metal surface},
abstract = {Quantitative Structure–Activity Relationships (QSAR) is a computational model used to describe and anticipate the interaction and surface interactions of substances. In addition, Artificial Neural Network (ANN) has been used for the development of linear and sigmoidal functionals, aiming to predict low-carbon steel, copper, and aluminum corrosion rates corresponding to environmental parameters. Thus this chapter explains the theory behind the QSAR/ANN and demonstrates its effectiveness related to corrosion. QSAR aims to draw an attention to the link between the effectiveness of prevention (any function) and the structural features (adjectives). It involves finding one or more items that, by mathematical equation, link these definitions to their blocking function. ANN (artificial neural network) shows excellent performance in the prediction (output) for various complex characteristic data (input). The obtained results give deep insight into the corrosion systems by analyzing the point of surface corrosion attack, the more stable site of the inhibitor adsorption, and the binding power of the adsorbed coating.}
}
@article{LAMB2023101031,
title = {Flexibility across and flexibility within: The domain of integer addition and subtraction},
journal = {The Journal of Mathematical Behavior},
volume = {70},
pages = {101031},
year = {2023},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2023.101031},
url = {https://www.sciencedirect.com/science/article/pii/S0732312323000019},
author = {Lisa Lamb and Jessica Bishop and Ian Whitacre and Randolph Philipp},
keywords = {Number concepts and operations, Cognition, Flexibility, Adaptive expertise, Strategy variability, Integers},
abstract = {To better understand the role that flexibility plays in students’ success on integer addition and subtraction problems, we examined students’ flexibility when solving open number sentences. We define flexibility as the degree to which a learner uses more than one strategy to solve a single task when prompted, as well as the degree to which a learner changes strategies when solving a range of tasks to accommodate task differences. We introduce the categorizations of flexibility within and flexibility across to distinguish these two ways of operationalizing flexibility. We examined flexibility and performance within and among three groups of students — 2nd and 4th graders who had negative numbers in their numerical domains, 7th graders, and college-track 11th graders. Profiles of five students are shared to provide insight in relation to the quantitative findings.}
}
@article{HE2025103404,
title = {Thermal imaging monitoring based on image texture feature analysis for simulating gymnastics teaching images in universities},
journal = {Thermal Science and Engineering Progress},
volume = {60},
pages = {103404},
year = {2025},
issn = {2451-9049},
doi = {https://doi.org/10.1016/j.tsep.2025.103404},
url = {https://www.sciencedirect.com/science/article/pii/S2451904925001945},
author = {Yuxin He and Dongcheng Ma},
keywords = {Image texture feature analysis, Thermal imaging monitoring, College gymnastics teaching, Image simulation},
abstract = {With the development of science and technology, thermal imaging technology is used as a non-contact monitoring means. This study designed and developed a gymnastics teaching image simulation system based on thermal imaging monitoring technology. The system aims to extract key image texture features by analyzing the thermal imaging images of gymnasts in the training process, so as to realize real-time monitoring and evaluation of athletes’ physical conditions. In this paper, image processing technology is used to extract key texture features from thermal imaging images, and the quantitative index reflecting athletes’ physical condition is extracted from thermal imaging images, and it is applied to the simulation of gymnastics teaching images. Through the analysis of image texture features, researchers successfully extracted the key indicators reflecting the athlete’s physical condition, and applied these indicators to the simulation of gymnastics teaching images. The experimental results show that the image simulation system based on thermal imaging monitoring can provide more accurate and objective evaluation results, which is helpful for coaches to better understand the training status of athletes and adjust the training plan in time.}
}
@article{ABDELAZIZ2024100615,
title = {A scoping review of artificial intelligence within pharmacy education},
journal = {American Journal of Pharmaceutical Education},
volume = {88},
number = {1},
pages = {100615},
year = {2024},
issn = {0002-9459},
doi = {https://doi.org/10.1016/j.ajpe.2023.100615},
url = {https://www.sciencedirect.com/science/article/pii/S0002945923045539},
author = {May H. {Abdel Aziz} and Casey Rowe and Robin Southwood and Anna Nogid and Sarah Berman and Kyle Gustafson},
keywords = {Pharmacy education, Artificial intelligence, Deep learning, Machine learning},
abstract = {Objectives
This scoping review aimed to summarize the available literature on the use of artificial intelligence (AI) in pharmacy education and identify gaps where additional research is needed.
Findings
Seven studies specifically addressing the use of AI in pharmacy education were identified. Of these 7 studies, 5 focused on AI use in the context of teaching and learning, 1 on the prediction of academic performance for admissions, and the final study focused on using AI text generation to elucidate the benefits and limitations of ChatGPT use in pharmacy education.
Summary
There are currently a limited number of available publications that describe AI use in pharmacy education. Several challenges exist regarding the use of AI in pharmacy education, including the need for faculty expertise and time, limited generalizability of tools, limited outcomes data, and several legal and ethical concerns. As AI use increases and implementation becomes more standardized, opportunities will be created for the inclusion of AI in pharmacy education.}
}
@article{HUNTER198763,
title = {What is fundamental in an information age? A focus on curriculum},
journal = {Education and Computing},
volume = {3},
number = {1},
pages = {63-73},
year = {1987},
note = {Special Issue on Educational Computer Policy Alternatives in the United States},
issn = {0167-9287},
doi = {https://doi.org/10.1016/S0167-9287(87)80513-7},
url = {https://www.sciencedirect.com/science/article/pii/S0167928787805137},
author = {Beverly Hunter},
keywords = {Curriculum change, Knowledge-creative Learning, Problem Solving Tools, Information Handling, Algorithmic Thinking, Critical Thinking Skills, Higher-order Thinking Skills, Information Age, Computer Literacy, Problem Solving, Decision Making, Inquiry, Reasoning, Valuing},
abstract = {Systematic reassessment of both overt and covert curriculum content and methods is needed in response to broader social change involved in the information revolution. The educational system in the United States is moving (unevenly) through three overlapping stages of curriculum change: (1) focus on technology, (2) integration of technology into curriculum, and (3) focus on fundamental change in curriculum. Indicators of the current state of change in elementary and secondary schools include state education agency mandates, teacher-oriented publications, past and present surveys of computer use in schools, teacher attitudes, private industry initiatives, and recommendations of national study groups and commissions.}
}
@article{GAO2023101631,
title = {Key nodes identification in complex networks based on subnetwork feature extraction},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {35},
number = {7},
pages = {101631},
year = {2023},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2023.101631},
url = {https://www.sciencedirect.com/science/article/pii/S1319157823001854},
author = {Luyuan Gao and Xiaoyang Liu and Chao Liu and Yihao Zhang and Giacomo Fiumara and Pasquale De Meo},
keywords = {Key nodes identification, Complex network, Subnetwork feature extraction, Graph convolutional networks},
abstract = {The problem of detecting key nodes in a network (i.e. nodes with the greatest ability to spread an infection) has been studied extensively in the past. Some approaches to key node detection compute node centrality, but there is no formal proof that central nodes also have the greatest spreading capacity. Other methods use epidemiological models (e.g., the SIR model) to describe the spread of an infection and rely on numerical simulations to find out key nodes; these methods are highly accurate but computationally expensive. To efficiently but accurately detect key nodes, we propose a novel deep learning method called Rank by Graph Convolutional Network, RGCN. Our method constructs a subnetwork around each node to estimate its spreading power; then RGCN applies a graph convolutional network to each subnetwork and the adjacency matrix of the network to learn node embeddings. Finally, a neural network is applied to the node embeddings to detect key nodes. Our RGCN method outperforms state-of-the-art approaches such as RCNN and MRCNN by 11.84% and 13.99%, respectively, when we compare the Kendall’s τ coefficient between the node ranking produced by each method with the true ranking obtained by SIR simulations.}
}
@article{OSWALD2025100552,
title = {Understanding individual differences in non-ordinary state of consciousness: Relationship between phenomenological experiences and autonomic nervous system},
journal = {International Journal of Clinical and Health Psychology},
volume = {25},
number = {1},
pages = {100552},
year = {2025},
issn = {1697-2600},
doi = {https://doi.org/10.1016/j.ijchp.2025.100552},
url = {https://www.sciencedirect.com/science/article/pii/S1697260025000109},
author = {Victor Oswald and Karim Jerbi and Corine Sombrun and Annen Jitka and Charlotte Martial and Olivia Gosseries and Audrey Vanhaudenhuyse},
keywords = {Non-ordinary states of consciousness, Auto-induced cognitive trance, Heart rate variability, Phenomenological experiences, Machine learning, Inter-individual differences},
abstract = {Non-ordinary states of consciousness offer a unique opportunity to explore the interplay between phenomenological experiences and physiological processes. This study investigated individual differences in phenomenological and autonomic nervous system changes between a resting state condition and a non-ordinary state of consciousness (auto-induced cognitive trance, AICT). Specifically, it examined the relationship between self-reported experiences (e.g., absorption, visual representations) and heart rate variability (HRV). Twenty-seven participants underwent electrocardiography recordings and completed self-report questionnaires during rest and AICT. A machine learning framework distinguished the rest and AICT states based on self-reported measures and HRV metrics. A linear mixed-effects model assessed inter-individual differences in HRV and self-reported phenomenology between the two states. Finally, the relationship between relative change in HRV and self-reported experiences was explored. Results showed changes in self-reported phenomenology (accuracy=86 %; p<.001) and HRV (accuracy=73 %; p<.001) characterizing the AICT state compared to rest. The baseline level in phenomenology or HRV was associated with change amplitude during AICT. Moreover, relative change in HRV was associated with change in phenomenology. The findings suggest that inter-individual differences at rest revealed a functional mechanism between phenomenology and the autonomic nervous system during non-ordinary states of consciousness, offering a novel perspective on how physiological mechanisms shape subjective experiences.}
}
@article{MARGINEANU20161,
title = {Neuropharmacology beyond reductionism – A likely prospect},
journal = {Biosystems},
volume = {141},
pages = {1-9},
year = {2016},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2015.11.010},
url = {https://www.sciencedirect.com/science/article/pii/S0303264715002026},
author = {Doru Georg Margineanu},
keywords = {Neuropharmacology, Systems pharmacology, Reductionism, Multi-target drug, Phenotypic screening, Emergent properties, Serendipity},
abstract = {Neuropharmacology had several major past successes, but the last few decades did not witness any leap forward in the drug treatment of brain disorders. Moreover, current drugs used in neurology and psychiatry alleviate the symptoms, while hardly curing any cause of disease, basically because the etiology of most neuro-psychic syndromes is but poorly known. This review argues that this largely derives from the unbalanced prevalence in neuroscience of the analytic reductionist approach, focused on the cellular and molecular level, while the understanding of integrated brain activities remains flimsier. The decline of drug discovery output in the last decades, quite obvious in neuropharmacology, coincided with the advent of the single target-focused search of potent ligands selective for a well-defined protein, deemed critical in a given pathology. However, all the widespread neuro-psychic troubles are multi-mechanistic and polygenic, their complex etiology making unsuited the single-target drug discovery. An evolving approach, based on systems biology considers that a disease expresses a disturbance of the network of interactions underlying organismic functions, rather than alteration of single molecular components. Accordingly, systems pharmacology seeks to restore a disturbed network via multi-targeted drugs. This review notices that neuropharmacology in fact relies on drugs which are multi-target, this feature having occurred just because those drugs were selected by phenotypic screening in vivo, or emerged from serendipitous clinical observations. The novel systems pharmacology aims, however, to devise ab initio multi-target drugs that will appropriately act on multiple molecular entities. Though this is a task much more complex than the single-target strategy, major informatics resources and computational tools for the systemic approach of drug discovery are already set forth and their rapid progress forecasts promising outcomes for neuropharmacology.}
}
@article{BRAITHWAITE201640,
title = {Non-formal mechanisms in mathematical cognitive development: The case of arithmetic},
journal = {Cognition},
volume = {149},
pages = {40-55},
year = {2016},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2016.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S001002771630004X},
author = {David W. Braithwaite and Robert L. Goldstone and Han L.J. {van der Maas} and David H. Landy},
keywords = {Mathematical cognitive development, Concrete to abstract shift, Arithmetic, Syntax, Perception, Mathematics education},
abstract = {The idea that cognitive development involves a shift towards abstraction has a long history in psychology. One incarnation of this idea holds that development in the domain of mathematics involves a shift from non-formal mechanisms to formal rules and axioms. Contrary to this view, the present study provides evidence that reliance on non-formal mechanisms may actually increase with age. Participants – Dutch primary school children – evaluated three-term arithmetic expressions in which violation of formally correct order of evaluation led to errors, termed foil errors. Participants solved the problems as part of their regular mathematics practice through an online study platform, and data were collected from over 50,000 children representing approximately 10% of all primary schools in the Netherlands, suggesting that the results have high external validity. Foil errors were more common for problems in which formally lower-priority sub-expressions were spaced close together, and also for problems in which such sub-expressions were relatively easy to calculate. We interpret these effects as resulting from reliance on two non-formal mechanisms, perceptual grouping and opportunistic selection, to determine order of evaluation. Critically, these effects reliably increased with participants’ grade level, suggesting that these mechanisms are not phased out but actually become more important over development, even when they cause systematic violations of formal rules. This conclusion presents a challenge for the shift towards abstraction view as a description of cognitive development in arithmetic. Implications of this result for educational practice are discussed.}
}
@article{ASTLE2024105539,
title = {Understanding divergence: Placing developmental neuroscience in its dynamic context},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {157},
pages = {105539},
year = {2024},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2024.105539},
url = {https://www.sciencedirect.com/science/article/pii/S0149763424000071},
author = {Duncan E. Astle and Dani S. Bassett and Essi Viding},
keywords = {Development, Systems neuroscience, Neurodevelopmental condition, Mental health, Computational neuroscience},
abstract = {Neurodevelopment is not merely a process of brain maturation, but an adaptation to constraints unique to each individual and to the environments we co-create. However, our theoretical and methodological toolkits often ignore this reality. There is growing awareness that a shift is needed that allows us to study divergence of brain and behaviour across conventional categorical boundaries. However, we argue that in future our study of divergence must also incorporate the developmental dynamics that capture the emergence of those neurodevelopmental differences. This crucial step will require adjustments in study design and methodology. If our ultimate aim is to incorporate the developmental dynamics that capture how, and ultimately when, divergence takes place then we will need an analytic toolkit equal to these ambitions. We argue that the over reliance on group averages has been a conceptual dead-end with regard to the neurodevelopmental differences. This is in part because any individual differences and developmental dynamics are inevitably lost within the group average. Instead, analytic approaches which are themselves new, or simply newly applied within this context, may allow us to shift our theoretical and methodological frameworks from groups to individuals. Likewise, methods capable of modelling complex dynamic systems may allow us to understand the emergent dynamics only possible at the level of an interacting neural system.}
}
@article{DRABECK202591,
title = {Disability in ecology and evolution},
journal = {Trends in Ecology & Evolution},
volume = {40},
number = {2},
pages = {91-95},
year = {2025},
issn = {0169-5347},
doi = {https://doi.org/10.1016/j.tree.2024.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S0169534724003173},
author = {Danielle Drabeck and Chris Rensing and Kat {Van der Poorten}}
}
@article{COSTABILE2021126306,
title = {A 2D-SWEs framework for efficient catchment-scale simulations: Hydrodynamic scaling properties of river networks and implications for non-uniform grids generation},
journal = {Journal of Hydrology},
volume = {599},
pages = {126306},
year = {2021},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2021.126306},
url = {https://www.sciencedirect.com/science/article/pii/S002216942100353X},
author = {Pierfranco Costabile and Carmelina Costanzo},
keywords = {2D shallow water equations, Surface runoff, River networks, Non-uniform grids, Channel heads, Scaling laws},
abstract = {The application of two-dimensional shallow-water equations models (2D-SWEs) for the description of hydrodynamic-based surface runoff computations is becoming a reference approach in rainfall-runoff simulations at the catchment scale. Due to their ability in generation of flow patterns throughout the basin, they can be used not only as an advanced method for flood mapping studies and hazard assessment but also as an innovative tool for the analysis of river drainage networks, opening new perspectives for several environmental processes. In particular, in this work we put the river networks in a 2D-SWEs framework, meaning that the traditional tree-like fluvial structure, represented by a skeleton composed of a set of lines, is replaced by a collection of points discretizing the 2-D geometry of the river structure itself, for which the values of the hydrodynamic values are provided by the numerical simulations. This approach is used here to derive a new scaling property that relates the specific discharge threshold, used to identify the river network cells, to the total areas of the network cells themselves. The hydrodynamic and geomorphological interpretation of this power law function and the influence of grid resolution, on some relevant parameters of this curve, have inspired the development of a heuristic procedure for non-uniform grid generation, able to detect the most hydrodynamically active areas of the basins for which the grid refinement process makes sense. Moreover, information related to how much grid refinement is needed is provided as well. The performances of this procedure are very promising in terms of accuracy of simulated discharges, hydrodynamic behaviour of the river network and flooded areas, reducing significantly the computational times in respect to the use of fine uniform grids.}
}
@article{IZMALKOV2011121,
title = {Perfect implementation},
journal = {Games and Economic Behavior},
volume = {71},
number = {1},
pages = {121-140},
year = {2011},
note = {Special Issue In Honor of John Nash},
issn = {0899-8256},
doi = {https://doi.org/10.1016/j.geb.2010.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0899825610000758},
author = {Sergei Izmalkov and Matt Lepinski and Silvio Micali},
keywords = {Mechanism design, Trust, Privacy},
abstract = {Privacy and trust affect our strategic thinking, yet have not been precisely modeled in mechanism design. In settings of incomplete information, traditional implementations of a normal-form mechanism—by disregarding the players' privacy, or assuming trust in a mediator—may fail to reach the mechanism's objectives. We thus investigate implementations of a new type. We put forward the notion of a perfect implementation of a normal-form mechanism M: in essence, a concrete extensive-form mechanism exactly preserving all strategic properties of M, without relying on trusted mediators or violating the players' privacy. We prove that any normal-form mechanism can be perfectly implemented by a verifiable mediator using envelopes and an envelope-randomizing device. Differently from a trusted mediator, a verifiable one only performs prescribed public actions, so that everyone can verify that he is acting properly, and that he never learns any information that should remain private.}
}
@article{SAEED2022122012,
title = {A simple approach for short-term wind speed interval prediction based on independently recurrent neural networks and error probability distribution},
journal = {Energy},
volume = {238},
pages = {122012},
year = {2022},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.122012},
url = {https://www.sciencedirect.com/science/article/pii/S036054422102260X},
author = {Adnan Saeed and Chaoshun Li and Zhenhao Gan and Yuying Xie and Fangjie Liu},
keywords = {Wind speed interval prediction, Independently recurrent neural networks, Quantile regression, Error prediction, Distribution estimation},
abstract = {Improving the quality of Wind Speed Interval prediction is important to maximize the usage of integrated wind energy as well as to reduce the adverse effects of the uncertainties, introduced by the random fluctuations of wind, to the power systems. This paper utilizes independently recurrent neural network to propose two new interval prediction frameworks. This network possesses the ability to retain memory at different lengths, which is helpful in capturing temporal features, especially for multi-horizon forecasts where the local dynamics get quite involved. In the first approach, we integrated a quantile regression loss function into this network to generate the intervals. This framework however, require to train different regressors to generate the conditional quantiles. Removing this limitation, a new simple and intuitive approach, is proposed which estimates the prediction intervals using a Gaussian function centered on the prediction and estimated error by a point prediction model and an error prediction model respectively. In our computational experiments, which involve two different wind fields contributing to eight different cases, an improvement of 43% and 12%, in average coverage width criterion index, over traditional models and LSTM based model respectively is remarkable. Thus, the proposed framework is able to produce high quality PIs while simultaneously reducing the computational cost.}
}
@article{ZHU2024100138,
title = {Exploring the impact of ChatGPT on art creation and collaboration: Benefits, challenges and ethical implications},
journal = {Telematics and Informatics Reports},
volume = {14},
pages = {100138},
year = {2024},
issn = {2772-5030},
doi = {https://doi.org/10.1016/j.teler.2024.100138},
url = {https://www.sciencedirect.com/science/article/pii/S2772503024000240},
author = {Sijin Zhu and Zheng Wang and Yuan Zhuang and Yuyang Jiang and Mengyao Guo and Xiaolin Zhang and Ze Gao},
keywords = {Creative AI, HumanAI collaboration, Language models, Interactive AI literacy},
abstract = {This paper examines the chaos caused by introducing advanced language models, specifically ChatGPT, to art. Our focus is on the potential impact of ChatGPT on art creation and collaboration. We explore how it has been utilized to generate art and assist in creative writing and how it facilitates collaboration between artists. This exploration includes an investigation into the use of AI in creating art, music, and literature, emphasizing ChatGPT’s role in generating poetry and prose and its ability to provide valuable suggestions for sentence structure and word choice in creative writing. We conduct case studies and interviews with diverse artists and AI experts to understand the benefits and challenges of using ChatGPT in the creative process. Our findings reveal that artists find ChatGPT helpful in generating new ideas, overcoming creative blocks, and improving the quality of their work. It enables remote collaboration between artists by providing a real-time communication and idea-sharing platform. However, ethical concerns relating to authorship ownership and authenticity have emerged. Artists fear using ChatGPT may lead to losing their artistic identity and ownership of their work. While our data suggests that ChatGPT holds the potential to transform the art world, careful consideration must be given to the ethical implications of AI in art. We recommend future research to focus on developing guidelines for the responsible use of AI in art, safeguarding artists’ rights, and preserving artistic authenticity.}
}
@article{WANDELL2017298,
title = {Diagnosing the Neural Circuitry of Reading},
journal = {Neuron},
volume = {96},
number = {2},
pages = {298-311},
year = {2017},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2017.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0896627317306980},
author = {Brian A. Wandell and Rosemary K. Le},
keywords = {reading, diffusion imaging, development, fMRI, computational modeling},
abstract = {We summarize the current state of knowledge of the brain’s reading circuits, and then we describe opportunities to use quantitative and reproducible methods for diagnosing these circuits. Neural circuit diagnostics—by which we mean identifying the locations and responses in an individual that differ significantly from measurements in good readers—can help parents and educators select the best remediation strategy. A sustained effort to develop and share diagnostic methods can support the societal goal of improving literacy.}
}
@article{AGUIRRE2011305,
title = {Geovisual evaluation of public participation in decision making: The grapevine},
journal = {Journal of Visual Languages & Computing},
volume = {22},
number = {4},
pages = {305-321},
year = {2011},
note = {Part Special Issue on Challenging Problems in Geovisual Analytics},
issn = {1045-926X},
doi = {https://doi.org/10.1016/j.jvlc.2010.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S1045926X10000790},
author = {Robert Aguirre and Timothy Nyerges},
keywords = {Grapevine, Geovisual analytics, Public participation, Decision making, Spatio-temporal events, Human–computer–human interaction},
abstract = {This article reports on a three-dimensional (time–space) geovisual analytic called a “grapevine.” People often use metaphors to describe the temporal and spatial structure of online discussions, e.g., “threads” growing as a result of message exchanges. We created a visualization to evaluate the temporal and spatial structure of online message exchanges based on the shape of a grapevine naturally cultivated in a vineyard. Our grapevine visualization extends up through time with features like buds, nodes, tendrils, and leaves produced as a result of message posting, replying, and voting. Using a rotatable and fully interactive three-dimensional GIS (Geographic Information System) environment, a geovisual analyst can evaluate the quality of deliberation in the grapevine visualization by looking for productive patterns in fine-grained human–computer–human interaction (HCHI) data and then sub-sampling the productive parts for content analysis. We present an example of how we used the technique in a study of participatory interactions during an online field experiment about improving transportation in the central Puget Sound region of Washington called the Let's Improve Transportation (LIT) Challenge. We conclude with insights about how our grapevine could be applied as a general purpose technique for evaluation of any participatory learning, thinking, or decision making situation.}
}
@article{BOULGAKOV2020154,
title = {Bringing Microscopy-By-Sequencing into View},
journal = {Trends in Biotechnology},
volume = {38},
number = {2},
pages = {154-162},
year = {2020},
issn = {0167-7799},
doi = {https://doi.org/10.1016/j.tibtech.2019.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167779919301349},
author = {Alexander A. Boulgakov and Andrew D. Ellington and Edward M. Marcotte},
keywords = {DNA microscopy, next-generation sequencing, barcoding, localization, oligonucleotides},
abstract = {The spatial distribution of molecules and cells is fundamental to understanding biological systems. Traditionally, microscopies based on electromagnetic waves such as visible light have been used to localize cellular components by direct visualization. However, these techniques suffer from limitations of transmissibility and throughput. Complementary to optical approaches, biochemical techniques such as crosslinking can colocalize molecules without suffering the same limitations. However, biochemical approaches are often unable to combine individual colocalizations into a map across entire cells or tissues. Microscopy-by-sequencing techniques aim to biochemically colocalize DNA-barcoded molecules and, by tracking their thus unique identities, reconcile all colocalizations into a global spatial map. Here, we review this new field and discuss its enormous potential to answer a broad spectrum of questions.}
}
@article{COOKE2020138,
title = {Diverse perspectives on interdisciplinarity from Members of the College of the Royal Society of Canada},
journal = {FACETS},
volume = {5},
number = {1},
pages = {138-165},
year = {2020},
issn = {2371-1671},
doi = {https://doi.org/10.1139/facets-2019-0044},
url = {https://www.sciencedirect.com/science/article/pii/S2371167120000551},
author = {Steven J. Cooke and Vivian M. Nguyen and Dimitry Anastakis and Shannon D. Scott and Merritt R. Turetsky and Alidad Amirfazli and Alison Hearn and Cynthia E. Milton and Laura Loewen and Eric E. Smith and D. Ryan Norris and Kim L. Lavoie and Alice Aiken and Daniel Ansari and Alissa N. Antle and Molly Babel and Jane Bailey and Daniel M. Bernstein and Rachel Birnbaum and Carrie Bourassa and Antonio Calcagno and Aurélie Campana and Bing Chen and Karen Collins and Catherine E. Connelly and Myriam Denov and Benoît Dupont and Eric George and Irene Gregory-Eaves and Steven High and Josephine M. Hill and Philip L. Jackson and Nathalie Jette and Mark Jurdjevic and Anita Kothari and Paul Khairy and Sylvie A. Lamoureux and Kiera Ladner and Christian R. Landry and François Légaré and Nadia Lehoux and Christian Leuprecht and Angela R. Lieverse and Artur Luczak and Mark L. Mallory and Erin Manning and Ali Mazalek and Stuart J. Murray and Lenore L. Newman and Valerie Oosterveld and Patrice Potvin and Sheryl Reimer-Kirkham and Jennifer Rowsell and Dawn Stacey and Susan L. Tighe and David J. Vocadlo and Anne E. Wilson and Andrew Woolford and Jules M. Blais},
keywords = {interdisciplinarity, academic institutions, universities, funding, scholarly activity, boundary crossing, barriers},
abstract = {Various multiple-disciplinary terms and concepts (although most commonly “interdisciplinarity,” which is used herein) are used to frame education, scholarship, research, and interactions within and outside academia. In principle, the premise of interdisciplinarity may appear to have many strengths; yet, the extent to which interdisciplinarity is embraced by the current generation of academics, the benefits and risks for doing so, and the barriers and facilitators to achieving interdisciplinarity, represent inherent challenges. Much has been written on the topic of interdisciplinarity, but to our knowledge there have been few attempts to consider and present diverse perspectives from scholars, artists, and scientists in a cohesive manner. As a team of 57 members from the Canadian College of New Scholars, Artists, and Scientists of the Royal Society of Canada (the College) who self-identify as being engaged or interested in interdisciplinarity, we provide diverse intellectual, cultural, and social perspectives. The goal of this paper is to share our collective wisdom on this topic with the broader community and to stimulate discourse and debate on the merits and challenges associated with interdisciplinarity. Perhaps the clearest message emerging from this exercise is that working across established boundaries of scholarly communities is rewarding, necessary, and is more likely to result in impact. However, there are barriers that limit the ease with which this can occur (e.g., lack of institutional structures and funding to facilitate cross-disciplinary exploration). Occasionally, there can be significant risk associated with doing interdisciplinary work (e.g., lack of adequate measurement or recognition of work by disciplinary peers). Solving many of the world’s complex and pressing problems (e.g., climate change, sustainable agriculture, the burden of chronic disease, and aging populations) demands thinking and working across long-standing, but in some ways restrictive, academic boundaries. Academic institutions and key support structures, especially funding bodies, will play an important role in helping to realize what is readily apparent to all who contributed to this paper—that interdisciplinarity is essential for solving complex problems; it is the new norm. Failure to empower and encourage those doing this research will serve as a great impediment to training, knowledge, and addressing societal issues.}
}
@incollection{OLSON200116640,
title = {Writing Systems, Psychology of},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {16640-16643},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/01563-1},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767015631},
author = {D.R. Olson},
abstract = {The writing systems of the world differ importantly in how they relate to spoken language. Tokening and pictographic scripts relate to meanings or intentions directly. So-called full writing systems represent properties of the spoken language but in completely different ways. Morphophonemic (logographic) scripts represent the words or morphemes of the language, syllabic scripts represent the syllables of the language whether or not they also represent word boundaries. Alphabetic scripts represent, with varying degrees of success, the phonemes of the language, but also by means of spaces, the words of the language. Not only do these differences have an effect on learning to read, they also have an important effect on the ways in which one thinks about language and consequently about the world and the mind. Writing systems provide models for thinking about speech.}
}
@article{JANG2022103225,
title = {Generative Design by Reinforcement Learning: Enhancing the Diversity of Topology Optimization Designs},
journal = {Computer-Aided Design},
volume = {146},
pages = {103225},
year = {2022},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2022.103225},
url = {https://www.sciencedirect.com/science/article/pii/S0010448522000239},
author = {Seowoo Jang and Soyoung Yoo and Namwoo Kang},
keywords = {Generative design, Topology optimization, Deep learning, Reinforcement learning, Design diversity},
abstract = {Generative design refers to computational design methods that can automatically conduct design exploration under constraints defined by designers. Among many approaches, topology optimization-based generative designs aim to explore diverse topology designs, which cannot be represented by conventional parametric design approaches. Recently, data-driven topology optimization research has started to exploit artificial intelligence, such as deep learning or machine learning, to improve the capability of design exploration. This study proposes a reinforcement learning (RL) based generative design process, with reward functions maximizing the diversity of topology designs. We formulate generative design as a sequential problem of finding optimal design parameter combinations in accordance with a given reference design. Proximal Policy Optimization is used as the learning framework, which is demonstrated in the case study of an automotive wheel design problem. To reduce the heavy computational burden of the wheel topology optimization process required by our RL formulation, we approximate the optimization process with neural networks. With efficient data preprocessing/augmentation and neural architecture, the neural networks achieve a generalized performance and symmetricity-reserving characteristics. We show that RL-based generative design produces a large number of diverse designs within a short inference time by exploiting GPU in a fully automated manner. It is different from the previous approach using CPU which takes much more processing time and involving human intervention.}
}
@article{NAWAZ2024121481,
title = {CoffeeNet: A deep learning approach for coffee plant leaves diseases recognition},
journal = {Expert Systems with Applications},
volume = {237},
pages = {121481},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121481},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423019838},
author = {Marriam Nawaz and Tahira Nazir and Ali Javed and Sherif {Tawfik Amin} and Fathe Jeribi and Ali Tahir},
keywords = {CenterNet, Coffee plant disease, Classification, Deep learning, ResNet},
abstract = {Coffee is regarded as the highest consumed drink around the globe and has accounted as a major source of income in the regions where it is cultivated. To meet the coffee marketplace's requirements around the globe, cultivators must boost and analyze its cultivation and quality. Several factors like environmental changes and plant diseases are the major hindrance to increasing the yield of coffee. The development in the field of computer vision has facilitated the earliest diagnostic of diseased plant samples, however, the incidence of various image distortions i.e., color, light, size, orientation changes, and similarity in the healthy and diseased portions of examined samples are the major challenges in the effective recognition of various coffee plant leaf infections. The proposed work is focused to overwhelm the mentioned limitations by proposing a novel and effective DL model called the CoffeeNet. Explicitly, an improved CenterNet approach is proposed by introducing spatial-channel attention strategy-based ResNet-50 model for the computation of deep and disease-specific sample characteristics which are then classified by the 1-step detector of the CenterNet framework. We investigated the localization and cataloging outcomes of the suggested method on the Arabica coffee leaf repository which contains the images captured in the more realistic and complicated environmental constraints. The CoffeeNet model acquires a classification accuracy number of 98.54%, along with an mAP of 0.97 that is presenting the usefulness of our technique in localizing and categorizing various sorts of coffee plant leaf disorders.}
}
@article{LI2023106560,
title = {High energy capacity or high power rating: Which is the more important performance metric for battery energy storage systems at different penetrations of variable renewables?},
journal = {Journal of Energy Storage},
volume = {59},
pages = {106560},
year = {2023},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2022.106560},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X2202549X},
author = {Mingquan Li and Rui Shan and Ahmed Abdulla and Jialin Tian and Shuo Gao},
keywords = {Energy storage, Energy-to-power ratio (EPR), Decarbonization, Carbon emissions, Renewable integration, Low-carbon transition},
abstract = {Studies exploring the role and value of energy storage in deep decarbonization often overlook the balance between the energy capacity and the power rating of storage systems—a key performance parameter that can affect every part of storage operation. Here, we quantitatively evaluate the system-wide impacts of battery storage systems with various energy-to-power ratios (EPRs) and at different levels of renewable penetration. We take Jiangsu province in China as our case study, due to its high electricity consumption and aggressive renewable energy targets. Our results show the evolving role of storage: as renewable penetration increases, higher EPRs are favored, as they lead to system-wide cost reductions, lower GHG emissions, and higher power system reliability. Whereas existing studies make exogenous assumptions about the lifetime of storage, we show that lifetimes across EPRs and renewable scenarios span 10 to 20 years. Existing research can thus send false signals to investors and grid planners, delaying the deployment of storage and retarding the energy transition. By showing how different EPRs yield different benefits at different stages of the energy transition, our results help investors, policy makers, and system planners design forward-thinking and dynamic policies that encourage prudent storage uptake.}
}
@incollection{POULTON20013,
title = {Chapter 1 A brief history},
editor = {Mary M. Poulton},
series = {Handbook of Geophysical Exploration: Seismic Exploration},
publisher = {Pergamon},
volume = {30},
pages = {3-18},
year = {2001},
booktitle = {Computational neural networks for geophysical data processing},
issn = {0950-1401},
doi = {https://doi.org/10.1016/S0950-1401(01)80015-X},
url = {https://www.sciencedirect.com/science/article/pii/S095014010180015X},
author = {Mary M. Poulton},
abstract = {Publisher Summary
Computational neural networks are not just the grist of science fiction writers anymore nor are they a temporary success that will soon fade from use. The field of computational neural networks has matured in the last decade and found so many industrial applications that the notion of using a neural network to solve a particular problem no longer needs a “sales pitch” to management in many companies. Neural networks are now being routinely used in process control, manufacturing, quality control, product design, financial analysis, fraud detection, loan approval, voice and handwriting recognition, and data mining to name just a few application areas. The resurgence of neural network research is often attributed to the publication of a nonlinear network algorithm that overcame many of the limitations of the Perceptron and ADALINE. Many industrial applications of neural networks can claim significant increases in productivity, reduced costs, improved quality, or new products. This chapter present neural networks to the geophysicists as a serious computational tool—a tool with great potential and great limitations.}
}
@article{BARTH201937,
title = {Progressive Circuit Changes during Learning and Disease},
journal = {Neuron},
volume = {104},
number = {1},
pages = {37-46},
year = {2019},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2019.09.032},
url = {https://www.sciencedirect.com/science/article/pii/S0896627319308347},
author = {Alison L. Barth and Ajit Ray},
abstract = {A critical step toward understanding cognition, learning, and brain dysfunction will be identification of the underlying cellular computations that occur in and across discrete brain areas, as well as how they are progressively altered by experience or disease. These computations will be revealed by targeted analyses of the neurons that perform these calculations, defined not only by their firing properties but also by their molecular identity and how they are wired within the local and broad-scale network of the brain. New studies that take advantage of sophisticated genetic tools for cell-type-specific identification and control are revealing how learning and neurological disorders initiate and successively change the properties of defined neural circuits. Understanding the temporal sequence of adaptive or pathological synaptic changes across multiple synapses within a network will shed light into how small-scale neural circuits contribute to higher cognitive functions during learning and disease.}
}
@article{DIESTER20242265,
title = {Internal world models in humans, animals, and AI},
journal = {Neuron},
volume = {112},
number = {14},
pages = {2265-2268},
year = {2024},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2024.06.019},
url = {https://www.sciencedirect.com/science/article/pii/S0896627324004549},
author = {Ilka Diester and Marlene Bartos and Joschka Bödecker and Adam Kortylewski and Christian Leibold and Johannes Letzkus and Matthew M. Nour and Monika Schönauer and Andrew Straw and Abhinav Valada and Andreas Vlachos and Thomas Brox},
abstract = {Summary
How do brains—biological or artificial—respond and adapt to an ever-changing environment? In a recent meeting, experts from various fields of neuroscience and artificial intelligence met to discuss internal world models in brains and machines, arguing for an interdisciplinary approach to gain deeper insights into the underlying mechanisms.}
}
@article{STENNING1988143,
title = {Knowledge-rich solutions to the binding problem: a simulation of some human computational mechanisms},
journal = {Knowledge-Based Systems},
volume = {1},
number = {3},
pages = {143-152},
year = {1988},
issn = {0950-7051},
doi = {https://doi.org/10.1016/0950-7051(88)90072-X},
url = {https://www.sciencedirect.com/science/article/pii/095070518890072X},
author = {Keith Stenning and Joe Levy},
keywords = {binding, memory, PDP system, knowledge-rich, human memory, representations},
abstract = {The binding problem, how properties are represented as belonging to individuals, is identified as a severe problem for human memory, for which the memory adopts knowledge-rich solutions. It is argued that it is the nature of these solutions that endows human memory with many of its positive properties, particularly rapid retrieval on the basis of unreliable search clues. Parallel Distributed Processing (PDP) systems offer some insight into how human memory systems may work, as they also have to solve the binding problem by knowledge-rich methods. Experimental analysis and statistical models of Memory for Individuals Task (MIT) are presented, which provide evidence that the memory representations underlying human performance consist of sets of existential facts containing no referential terms. It is shown that the proposed representations can be incorporated directly into a PDP simulation of the inference from representation to response, and that the resulting system produces human-like errors when subjected to noisy input. The PDP simulation captures some of the asymmetries between stimulus and response which the statistical model cannot.}
}
@article{MIENYE2025181,
title = {ChatGPT in Education: A Review of Ethical Challenges and Approaches to Enhancing Transparency and Privacy},
journal = {Procedia Computer Science},
volume = {254},
pages = {181-190},
year = {2025},
note = {International Conference on Digital Sovereignty (ICDS)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.02.077},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925004272},
author = {Ibomoiye Domor Mienye and Theo G. Swart},
keywords = {ChatGPT, Education, Ethics, LLMs, Privacy, Transparency},
abstract = {The integration of ChatGPT and large language models (LLMs) into education has created new possibilities for personalized learning, tutoring, and automation of administrative tasks. However, these advancements also present ethical challenges. This paper critically examines the ethical implications of deploying ChatGPT in educational settings, with a focus on data privacy, the opaque nature of AI decision-making, and the risks of biased outputs. To address these issues, we outline actionable approaches, including Explainable AI (XAI) techniques and privacy-preserving strategies, aimed at enabling transparency and protecting student data. We also outline frameworks that support human oversight and governance to maintain trust and accountability in Al-driven educational tools.}
}
@article{WANG2020223,
title = {Anonymous data collection scheme for cloud-aided mobile edge networks},
journal = {Digital Communications and Networks},
volume = {6},
number = {2},
pages = {223-228},
year = {2020},
issn = {2352-8648},
doi = {https://doi.org/10.1016/j.dcan.2019.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352864819300574},
author = {Anxi Wang and Jian Shen and Chen Wang and Huijie Yang and Dengzhi Liu},
keywords = {Cloud-aided mobile edge networks, Anonymous data collection, Communication model, Path selection},
abstract = {With the rapid spread of smart sensors, data collection is becoming more and more important in Mobile Edge Networks (MENs). The collected data can be used in many applications based on the analysis results of these data by cloud computing. Nowadays, data collection schemes have been widely studied by researchers. However, most of the researches take the amount of collected data into consideration without thinking about the problem of privacy leakage of the collected data. In this paper, we propose an energy-efficient and anonymous data collection scheme for MENs to keep a balance between energy consumption and data privacy, in which the privacy information of senors is hidden during data communication. In addition, the residual energy of nodes is taken into consideration in this scheme in particular when it comes to the selection of the relay node. The security analysis shows that no privacy information of the source node and relay node is leaked to attackers. Moreover, the simulation results demonstrate that the proposed scheme is better than other schemes in aspects of lifetime and energy consumption. At the end of the simulation part, we present a qualitative analysis for the proposed scheme and some conventional protocols. It is noteworthy that the proposed scheme outperforms the existing protocols in terms of the above indicators.}
}
@article{LI2023103984,
title = {Improving short-term bike sharing demand forecast through an irregular convolutional neural network},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {147},
pages = {103984},
year = {2023},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2022.103984},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X22003977},
author = {Xinyu Li and Yang Xu and Xiaohu Zhang and Wenzhong Shi and Yang Yue and Qingquan Li},
keywords = {Bike sharing, Deep learning, Travel demand forecast, Spatial–temporal analysis, Irregular convolution},
abstract = {As an important task for the management of bike sharing systems, accurate forecast of travel demand could facilitate dispatch and relocation of bicycles to improve user satisfaction. In recent years, many deep learning algorithms have been introduced to improve bicycle usage forecast. A typical practice is to integrate convolutional (CNN) and recurrent neural network (RNN) to capture spatial–temporal dependency in historical travel demand. For typical CNN, the convolution operation is conducted through a kernel that moves across a “matrix-format” city to extract features over spatially adjacent urban areas. This practice assumes that areas close to each other could provide useful information that improves prediction accuracy. However, bicycle usage in neighboring areas might not always be similar, given spatial variations in built environment characteristics and travel behavior that affect cycling activities. Yet, areas that are far apart can be relatively more similar in temporal usage patterns. To utilize the hidden linkage among these distant urban areas, the study proposes an irregular convolutional Long-Short Term Memory model (IrConv+LSTM) to improve short-term bike sharing demand forecast. The model modifies traditional CNN with irregular convolutional architecture to leverage the hidden linkage among “semantic neighbors”. The proposed model is evaluated with a set of benchmark models in five study sites, which include one dockless bike sharing system in Singapore, and four station-based systems in Chicago, Washington, D.C., New York, and London. We find that IrConv+LSTM outperforms other benchmark models in the five cities. The model also achieves superior performance in areas with varying levels of bicycle usage and during peak periods. The findings suggest that “thinking beyond spatial neighbors” can further improve short-term travel demand prediction of urban bike sharing systems.}
}
@article{DEBRUIJNSMOLDERS2024e39439,
title = {Effective student engagement with blended learning: A systematic review},
journal = {Heliyon},
volume = {10},
number = {23},
pages = {e39439},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e39439},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024154709},
author = {M. {De Bruijn-Smolders} and F.R. Prinsen},
keywords = {Blended learning, Student engagement, Learning outcomes, Systematic review},
abstract = {Although student engagement is known to promote learning outcomes in higher education, what elements of blended learning designs impact effective student engagement and hereby learning outcomes, has not been clarified yet. Hence, it is unknown how to engage students with blended learning in an effective manner. The current study breaks down student engagement into four dimensions (academic, behavioral, cognitive, and affective), and reviews the evidence regarding blended learning that engages students effectively, whether this is academically, personally, socially, or with regard to citizenship. The studies reviewed (k = 15, N = 1,428) overall asserted that all blended learning interventions investigated had a moderate to high impact on student engagement and on learning outcomes. This review, a summary and insight into the evidence, is important for the field's understanding as well as for professionals in higher education: for lecturers and policy makers who want to introduce and monitor blended learning as a means to promote both student engagement and their learning outcomes in higher education. Further research is required to increase our knowledge of how blended learning impacts both multi-dimensional constructs: student engagement and learning outcomes.}
}
@article{MORISHITA2023102079,
title = {Data assimilation and control system for adaptive model predictive control},
journal = {Journal of Computational Science},
volume = {72},
pages = {102079},
year = {2023},
issn = {1877-7503},
doi = {https://doi.org/10.1016/j.jocs.2023.102079},
url = {https://www.sciencedirect.com/science/article/pii/S1877750323001394},
author = {Y. Morishita and S. Murakami and M. Yokoyama and G. Ueno},
keywords = {Data assimilation, Model-based control, Fusion plasma, ASTI},
abstract = {Model-based control of complex systems is a challenging task, particularly when the system model involves many uncertain elements. To achieve model predictive control of complex systems, we require a method that sequentially reduces uncertainties in the system model using observations and estimates control inputs under the model uncertainties. In this work, we propose an extended data assimilation framework, named data assimilation and control system (DACS), to integrate data assimilation and optimal control-input estimation. The DACS framework comprises a prediction step and three filtering steps and provides adaptive model predictive control algorithms. Since the DACS framework does not require additional prediction steps, the framework can even be applied to a large system in which iterative model prediction is prohibitive due to computational burden. Through numerical experiments in controlling virtual (numerically created) fusion plasma, we demonstrate the effectiveness of DACS and reveal the characteristics of the control performance related to the choice of hyper parameters and the discrepancies between the system model and the real system.}
}
@article{ZU2023107200,
title = {Random walk numerical scheme for the steady-state of stochastic differential equations},
journal = {Communications in Nonlinear Science and Numerical Simulation},
volume = {121},
pages = {107200},
year = {2023},
issn = {1007-5704},
doi = {https://doi.org/10.1016/j.cnsns.2023.107200},
url = {https://www.sciencedirect.com/science/article/pii/S1007570423001181},
author = {Jian Zu},
keywords = {Continuous-time random walk, Stochastic differential equation, Steady state, Invariant distribution},
abstract = {The continuous-time random walk (CTRW) scheme is a time-continuous and space-discretization method to obtain the numerical solution of stochastic differential equations (SDEs). Compared with the traditional time-discretization scheme, it has the advantages of numerical stability and can alleviate the curse of dimensionality. This paper proposes an improved version of the CTRW scheme for the numerical solution of SDEs. By compensating the artificial diffusion caused by the Poisson approximation of the drift term of the SDE, the improved CTRW scheme has significantly better performance in the weak noise case, especially in approximating the invariant probability measure. Numerical studies show that the improved CTRW scheme has more accuracy than the existing one but takes less computation time. In addition, it has better accuracy of the mean holding time. We also modify the hybrid Fokker–Planck solver proposed for the CTRW scheme to compute the invariant probability measure.}
}
@article{OVERLAN2017320,
title = {Learning abstract visual concepts via probabilistic program induction in a Language of Thought},
journal = {Cognition},
volume = {168},
pages = {320-334},
year = {2017},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2017.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0010027717302020},
author = {Matthew C. Overlan and Robert A. Jacobs and Steven T. Piantadosi},
keywords = {Concept learning, Visual learning, Language of Thought, Computational modeling, Behavioral experiment},
abstract = {The ability to learn abstract concepts is a powerful component of human cognition. It has been argued that variable binding is the key element enabling this ability, but the computational aspects of variable binding remain poorly understood. Here, we address this shortcoming by formalizing the Hierarchical Language of Thought (HLOT) model of rule learning. Given a set of data items, the model uses Bayesian inference to infer a probability distribution over stochastic programs that implement variable binding. Because the model makes use of symbolic variables as well as Bayesian inference and programs with stochastic primitives, it combines many of the advantages of both symbolic and statistical approaches to cognitive modeling. To evaluate the model, we conducted an experiment in which human subjects viewed training items and then judged which test items belong to the same concept as the training items. We found that the HLOT model provides a close match to human generalization patterns, significantly outperforming two variants of the Generalized Context Model, one variant based on string similarity and the other based on visual similarity using features from a deep convolutional neural network. Additional results suggest that variable binding happens automatically, implying that binding operations do not add complexity to peoples’ hypothesized rules. Overall, this work demonstrates that a cognitive model combining symbolic variables with Bayesian inference and stochastic program primitives provides a new perspective for understanding people’s patterns of generalization.}
}
@article{GAO2019146333,
title = {Coactivations of barrel and piriform cortices induce their mutual synapse innervations and recruit associative memory cells},
journal = {Brain Research},
volume = {1721},
pages = {146333},
year = {2019},
issn = {0006-8993},
doi = {https://doi.org/10.1016/j.brainres.2019.146333},
url = {https://www.sciencedirect.com/science/article/pii/S0006899319303877},
author = {Zilong Gao and Ruixiang Wu and Changfeng Chen and Bo Wen and Yahui Liu and Wei Lu and Na Chen and Jing Feng and Ruichen Fan and Dangui Wang and Shan Cui and Jin-Hui Wang},
keywords = {Associative learning, Memory cell, Neural circuit, Barrel cortex, Piriform cortex},
abstract = {After associative learning, a signal induces the recall of its associated signal, or the other way around. This reciprocal retrieval of associated signals is essential for associative thinking and logical reasoning. For the cellular mechanism underlying this associative memory, we hypothesized that the formation of synapse innervations among coactivated sensory cortices and the recruitment of associative memory cells were involved in the integrative storage and reciprocal retrieval of associated signals. Our study indicated that the paired whisker and olfaction stimulations led to an odorant-induced whisker motion and a whisker-induced olfaction response, a reciprocal form of associative memory retrieval. In mice that showed the reciprocal retrieval of associated signals, their barrel and piriform cortical neurons became mutually innervated through their axon projection and new synapse formation. These piriform and barrel cortical neurons gained the ability to encode both whisker and olfaction signals based on synapse innervations from the innate input and the newly formed input. Therefore, the associated activation of sensory cortices by pairing input signals initiates their mutual synapse innervations, and the neurons innervated by new and innate synapses are recruited to be associative memory cells that encode these associated signals. Mutual synapse innervations among sensory cortices to recruit associative memory cells may compose the primary foundation for the integrative storage and reciprocal retrieval of associated signals. Our study also reveals that new synapses onto the neurons enable these neurons to encode memories to new specific signals.}
}
@article{GILBOA202196,
title = {The complexity of the consumer problem},
journal = {Research in Economics},
volume = {75},
number = {1},
pages = {96-103},
year = {2021},
issn = {1090-9443},
doi = {https://doi.org/10.1016/j.rie.2021.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S1090944321000016},
author = {Itzhak Gilboa and Andrew Postlewaite and David Schmeidler},
keywords = {Consumer theory, Computational complexity, Mental accounting},
abstract = {A literal interpretation of neo-classical consumer theory suggests that the consumer solves a very complex problem. In the presence of indivisible goods, the consumer problem is NP-Hard, and it appears unlikely that it can be optimally solved by a human. Two implications of this observation are that (i) households may imitate each other’s choices; (ii) households may adopt heuristics that give rise to the phenomenon of mental accounting.}
}
@article{XING2024103704,
title = {Financial risk tolerance profiling from text},
journal = {Information Processing & Management},
volume = {61},
number = {4},
pages = {103704},
year = {2024},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.103704},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324000645},
author = {Frank Xing},
keywords = {Artificial intelligence in finance, Risk tolerance, Risk profiling, Text mining, Convolutional neural network},
abstract = {Traditionally, individual financial risk tolerance information is gathered via questionnaires or similar structured psychometric tools. Our abundant digital footprint, as an unstructured alternative, is less investigated. Leveraging such information can potentially support large-scale and cost-efficient financial services. Therefore, I explore the possibility of building a computational model that distills risk tolerance information from user texts in this study, and discuss the design principles discovered from empirical results and their implications. Specifically, a new quaternary classification task is defined for text mining-based risk profiling. Experiments show that pre-trained large language models set a baseline micro-F1 of circa 0.34. Using a convolutional neural network (CNN), the reported system achieves a micro-F1 of circa 0.51, which significantly outperforms the baselines, and is a circa 4% further improvement over the standard CNN configurations (micro-F1 of circa 0.47). Textual feature richness and supervised learning are found to be the key contributors to model performances, while other machine learning strategies suggested by previous research (data augmentation and multi-tasking) are less effective. The findings confirm user texts to be a useful risk profiling resource and provide several insights on this task.}
}
@incollection{DOLIVEIRACOELHO2020259,
title = {Chapter 5.1 - Osteomics: Decision support systems for forensic anthropologists},
editor = {Zuzana Obertová and Alistair Stewart and Cristina Cattaneo},
booktitle = {Statistics and Probability in Forensic Anthropology},
publisher = {Academic Press},
pages = {259-273},
year = {2020},
isbn = {978-0-12-815764-0},
doi = {https://doi.org/10.1016/B978-0-12-815764-0.00005-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128157640000058},
author = {João {d’Oliveira Coelho} and Francisco Curate and David Navega},
keywords = {Biological profile, Machine learning, Population data, Web-based applications, Age at death, Sex diagnosis, Biogeographic origins, Body parameters, Medicolegal contexts, Cross validation},
abstract = {The popularity of web-based analytical tools with an emphasis on improved statistical analyses within the landscape of forensic anthropology is increasing. Osteomics is a web-based platform composed of a suite of forensic decision support systems designed to contend with the challenges posed by the estimation of the biological profile of human skeletal remains and particularly the estimation of age at death, the diagnosis of sex, the calculation of body parameters, and the prediction of biogeographic origin. The web applications designed at Osteomics intend to make innovative and reliable statistical models freely available. The suggested models are grounded around traditional and advanced statistical thinking, data visualization and processing, and predictive modeling under the machine learning paradigm. This paper aims to introduce the potential of the web platforms as forensic decision support systems and to give a detailed description of the statistical techniques used in the web-based applications available at Osteomics.}
}
@incollection{GALLISTEL2017141,
title = {1.08 - Learning and Representation☆},
editor = {John H. Byrne},
booktitle = {Learning and Memory: A Comprehensive Reference (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {141-154},
year = {2017},
isbn = {978-0-12-805291-4},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.21009-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093245210092},
author = {Randy Gallistel},
keywords = {Associations, Cognitive map, Episodic memory, Information, Memory molecules, Path integration, Read–write memory, Signals, Sun compass, Symbols},
abstract = {Behavioral and electrophysiological evidence implies that brains compute representations of aspects of the experienced world. For example, they compute the animal's position in the world by integrating its velocity with respect to time. Other examples are the learning of the solar ephemeris, the construction of a cognitive map, and episodic memory in food caching. Representations require a symbolic read–write memory that carries information extracted from experience forward in time in a computationally accessible form. The analogy between the architecture of computer memory and the genetic architecture suggests the sort of memory structure to be looked for in the nervous system.}
}
@article{WANG2022103414,
title = {Cross-layer progressive attention bilinear fusion method for fine-grained visual classification},
journal = {Journal of Visual Communication and Image Representation},
volume = {82},
pages = {103414},
year = {2022},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2021.103414},
url = {https://www.sciencedirect.com/science/article/pii/S1047320321002789},
author = {Chaoqing Wang and Yurong Qian and Weijun Gong and Junjong Cheng and Yongqiang Wang and Yuefei Wang},
keywords = {Fine-grained visual classification, Feature fusion, Attention, Progressive},
abstract = {Fine-grained visual classification (FGVC) is a critical task in the field of computer vision. However, FGVC is full of challenges due to the large intra-class variation and small inter-class variation of the classes to be classified on an image. The key in dealing with the problem is to capture subtle visual differences from the image and effectively represent the discriminative features. Existing methods are often limited by insufficient localization accuracy and insufficient feature representation capabilities. In this paper, we propose a cross-layer progressive attention bilinear fusion (CPABF in short) method, which can efficiently express the characteristics of discriminative regions. The CPABF method involves three components: 1) Cross-Layer Attention (CLA) locates and reinforces the discriminative region with low computational costs; 2) The Cross-Layer Bilinear Fusion Module (CBFM) effectively integrates the semantic information from the low-level to the high-level 3) Progressive Training optimizes the parameters in the network to the best state in a delicate way. The CPABF shows excellent performance on the four FGVC datasets and outperforms some state-of-the-art methods.}
}
@article{ZHU2021118730,
title = {From gratitude to injustice: Neurocomputational mechanisms of gratitude-induced injustice},
journal = {NeuroImage},
volume = {245},
pages = {118730},
year = {2021},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2021.118730},
url = {https://www.sciencedirect.com/science/article/pii/S1053811921010028},
author = {Ruida Zhu and Zhenhua Xu and Song Su and Chunliang Feng and Yi Luo and Honghong Tang and Shen Zhang and Xiaoyan Wu and Xiaoqin Mai and Chao Liu},
keywords = {Gratitude, Protection tendency, Injustice, Mentalizing, Reward processing},
abstract = {Gratitude shapes individuals’ behaviours and impacts the harmony of society. Many previous studies focused on its association with prosocial behaviours. A possibility that gratitude can lead to moral violation has been overlooked until recently. Nevertheless, the neurocognitive mechanisms of gratitude-induced moral violation are still unclear. On the other hand, though neural correlates of the gratitude's formation have been examined, the neural underpinnings of gratitude-induced behaviour remain unknown. For addressing these two overlapped research gaps, we developed novel tasks to investigate how participants who had received voluntary (Gratitude group) or involuntary help (Control group) punished their benefactors’ unfairness with functional magnetic resonance imaging (fMRI). The Gratitude group punished their benefactors less than the Control group. The self-report and computational modelling results demonstrated a crucial role of the boosted protection tendency on behalf of benefactors in the gratitude-induced injustice. The fMRI results showed that activities in the regions associated with mentalizing (temporoparietal junction) and reward processing (ventral medial prefrontal cortex) differed between the groups and were related to the gratitude-induced injustice. They suggest that grateful individuals concern for benefactors’ benefits, value chances to interact with benefactors, and refrain from action that perturbs relationship-building (i.e., exert less punishment on benefactors’ unfairness), which reveal a dark side of gratitude and enrich the gratitude theory (i.e., the find-bind-remind theory). Our findings provide psychological, computational, and neural accounts of the gratitude-induced behaviour and further the understanding of the nature of gratitude.}
}
@article{REN2025100774,
title = {Immersive E-learning mode application in Chinese language teaching system based on big data recommendation algorithm},
journal = {Entertainment Computing},
volume = {52},
pages = {100774},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100774},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124001423},
author = {Chunjiao Ren},
keywords = {Big data, Interest recommendation algorithm, Immersive, E-Learning mode, Chinese teaching system},
abstract = {With the rapid development of information technology, E-Learning has become an innovative teaching method. However, in the field of Chinese teaching, how to provide effective learning resources and recommendation algorithms in E-Learning mode is still a challenge. This study aims to improve the effectiveness of Chinese teaching system and students’ learning outcomes through an immersive E-Learning model based on big data interest recommendation algorithm. This paper adopts an immersive E-Learning model based on big data interest recommendation algorithm, and constructs a Chinese teaching system. The web crawler is used to fully collect the experimental data and collate it in a targeted manner, and the required data is screened out by using a more efficient separation method. Adding big data recommendation algorithm to the system of this paper can not only record and analyze historical behaviors of users, but also recommend data information according to users’ interests, so that users can clarify their real information needs. By testing the system’s professional ability and recording the experimental data, this paper finds that the overall performance of this Chinese language teaching system is very good, and can achieve the original expected design purpose. In addition, the system largely solves the problem that the traditional system based on data recommendation algorithm is difficult to carry out effective recommendation smoothly when the total amount of data is too large.}
}
@article{VEZOLI2021117479,
title = {Cortical hierarchy, dual counterstream architecture and the importance of top-down generative networks},
journal = {NeuroImage},
volume = {225},
pages = {117479},
year = {2021},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2020.117479},
url = {https://www.sciencedirect.com/science/article/pii/S1053811920309642},
author = {Julien Vezoli and Loïc Magrou and Rainer Goebel and Xiao-Jing Wang and Kenneth Knoblauch and Martin Vinck and Henry Kennedy},
keywords = {Non-human primate, Human, Brain, Electrophysiology, Anatomy, Modeling, Connectivity, Predictive coding, Perception, Consciousness},
abstract = {Hierarchy is a major organizational principle of the cortex and underscores modern computational theories of cortical function. The local microcircuit amplifies long-distance inter-areal input, which show distance-dependent changes in their laminar profiles. Statistical modeling of these changes in laminar profiles demonstrates that inputs from multiple hierarchical levels to their target areas show remarkable consistency, allowing the construction of a cortical hierarchy based on a principle of hierarchical distance. The statistical modeling that is applied to structure can also be applied to laminar differences in the oscillatory coherence between areas thereby determining a functional hierarchy of the cortex. Close examination of the anatomy of inter-areal connectivity reveals a dual counterstream architecture with well-defined distance-dependent feedback and feedforward pathways in both the supra- and infragranular layers, suggesting a multiplicity of feedback pathways with well-defined functional properties. These findings are consistent with feedback connections providing a generative network involved in a wide range of cognitive functions. A dynamical model constrained by connectivity data sheds insight into the experimentally observed signatures of frequency-dependent Granger causality for feedforward versus feedback signaling. Concerted experiments capitalizing on recent technical advances and combining tract-tracing, high-resolution fMRI, optogenetics and mathematical modeling hold the promise of a much improved understanding of lamina-constrained mechanisms of neural computation and cognition. However, because inter-areal interactions involve cortical layers that have been the target of important evolutionary changes in the primate lineage, these investigations will need to include human and non-human primate comparisons.}
}
@article{WIECHA2024101129,
title = {Deep learning for nano-photonic materials – The solution to everything!?},
journal = {Current Opinion in Solid State and Materials Science},
volume = {28},
pages = {101129},
year = {2024},
issn = {1359-0286},
doi = {https://doi.org/10.1016/j.cossms.2023.101129},
url = {https://www.sciencedirect.com/science/article/pii/S1359028623000748},
author = {Peter R. Wiecha},
abstract = {Deep learning is currently being hyped as an almost magical tool for solving all kinds of difficult problems that computers have not been able to solve in the past. Particularly in the fields of computer vision and natural language processing, spectacular results have been achieved. The hype has now infiltrated several scientific communities. In (nano-) photonics, researchers are trying to apply deep learning to all kinds of forward and inverse problems. A particularly challenging problem is for instance the rational design of nanophotonic materials and devices. In this opinion article, I will first discuss the public expectations of deep learning and give an overview of the quite different scales at which actors from industry and research are operating their deep learning models. I then examine the weaknesses and dangers associated with deep learning. Finally, I’ll discuss the key strengths that make this new set of statistical methods so attractive, and review a personal selection of opportunities that shouldn’t be missed in the current developments.}
}
@article{IGELSTROM201770,
title = {The inferior parietal lobule and temporoparietal junction: A network perspective},
journal = {Neuropsychologia},
volume = {105},
pages = {70-83},
year = {2017},
note = {Special Issue: Concepts, Actions and Objects: Functional and Neural Perspectives},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2017.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0028393217300015},
author = {Kajsa M. Igelström and Michael S.A. Graziano},
keywords = {Angular gyrus, Supramarginal gyrus, Ventral parietal cortex, Posterior superior temporal sulcus, Internal cognition, Frontoparietal executive control network},
abstract = {Information processing in specialized, spatially distributed brain networks underlies the diversity and complexity of our cognitive and behavioral repertoire. Networks converge at a small number of hubs – highly connected regions that are central for multimodal integration and higher-order cognition. We review one major network hub of the human brain: the inferior parietal lobule and the overlapping temporoparietal junction (IPL/TPJ). The IPL is greatly expanded in humans compared to other primates and matures late in human development, consistent with its importance in higher-order functions. Evidence from neuroimaging studies suggests that the IPL/TPJ participates in a broad range of behaviors and functions, from bottom-up perception to cognitive capacities that are uniquely human. The organization of the IPL/TPJ is challenging to study due to the complex anatomy and high inter-individual variability of this cortical region. In this review we aimed to synthesize findings from anatomical and functional studies of the IPL/TPJ that used neuroimaging at rest and during a wide range of tasks. The first half of the review describes subdivisions of the IPL/TPJ identified using cytoarchitectonics, resting-state functional connectivity analysis and structural connectivity methods. The second half of the article reviews IPL/TPJ activations and network participation in bottom-up attention, lower-order self-perception, undirected thinking, episodic memory and social cognition. The central theme of this review is to discuss how network nodes within the IPL/TPJ are organized and how they participate in human perception and cognition.}
}
@article{THACKER2023101782,
title = {Climate change by the numbers: Leveraging mathematical skills for science learning online},
journal = {Learning and Instruction},
volume = {86},
pages = {101782},
year = {2023},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2023.101782},
url = {https://www.sciencedirect.com/science/article/pii/S0959475223000518},
author = {Ian Thacker},
keywords = {Climate change, Conceptual change, Epistemic dispositions, Numerical estimation, Plausibility judgments, Learning technology},
abstract = {The purpose of this preregistered study was to test an online intervention that presents participants with novel numbers about climate change after they estimate those numbers. An experimental study design was used to investigate the impact of the intervention on undergraduate students’ climate change understanding and perceptions that human caused climate change is plausible. Findings revealed that posttest climate change knowledge and plausibility perceptions were higher among those randomly assigned to use the intervention compared with those assigned to a control condition, and that supplementing this experience with numeracy instruction was linked with the use of more explicit estimation strategies and greater learning gains for people with adaptive epistemic dispositions. Findings from this study replicate and extend prior research, support the idea that novel data can support knowledge revision, identify estimation strategies used in this context, and offer an open-source online intervention for sharing surprising data with students and teachers.}
}
@article{RINGE2023101268,
title = {Cation effects on electrocatalytic reduction processes at the example of the hydrogen evolution reaction},
journal = {Current Opinion in Electrochemistry},
volume = {39},
pages = {101268},
year = {2023},
issn = {2451-9103},
doi = {https://doi.org/10.1016/j.coelec.2023.101268},
url = {https://www.sciencedirect.com/science/article/pii/S2451910323000613},
author = {Stefan Ringe},
keywords = {Cation effects, Hydrogen evolution reaction, Hydrogen underpotential deposition, CO reduction, Electric double layer, Solid-liquid interface},
abstract = {Cation effects provide invaluable insights into electrochemistry. In this review, I discuss them with a main focus on the hydrogen evolution reaction and a summary of recent in situ spectroscopic and electrochemical measurements as well as advanced computational simulation results conducted at varying cation identities, concentrations, and pH. According to these works, the interfacial cation concentration is the main descriptor to explain cation and pH effects. The detailed mechanism (such as e.g. water polarization, water structure changes, field-stabilization of intermediates) depends strongly on potential, pH, oxophilicity of the electrode, or the nature of the rate-limiting step and proton donor. With growing convergence in this field, cation effects remain a highly challenging and promising topic for research.}
}
@article{GUPTA2005267,
title = {Power-law distribution in a learning process: competition, learning and natural selection},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {345},
number = {1},
pages = {267-274},
year = {2005},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2004.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0378437104009860},
author = {Hari M. Gupta and José R. Campanha},
keywords = {Power-law, Learning, Natural selection},
abstract = {In the present work, we propose a model for the statistical distribution of people versus number of steps acquired by them in a learning process, based on competition, learning and natural selection. We consider that learning ability is normally distributed. We found that the number of people versus step acquired by them in a learning process is given through a power law. As competition, learning and selection is also at the core of all economical and social systems, we consider that power-law scaling is a quantitative description of this process in social systems. This gives an alternative thinking in holistic properties of complex systems.}
}
@article{HOU2025105329,
title = {Measuring undergraduate students' reliance on Generative AI during problem-solving: Scale development and validation},
journal = {Computers & Education},
volume = {234},
pages = {105329},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2025.105329},
url = {https://www.sciencedirect.com/science/article/pii/S0360131525000971},
author = {Chenyu Hou and Gaoxia Zhu and Vidya Sudarshan and Fun Siong Lim and Yew Soon Ong},
keywords = {Human-AI collaboration, Problem-solving, Generative AI, Higher education, Reliance on AI, Scale development},
abstract = {Reliance on AI describes the behavioral patterns of when and how individuals depend on AI suggestions, and appropriate reliance patterns are necessary to achieve effective human-AI collaboration. Traditional measures often link reliance to decision-making outcomes, which may not be suitable for complex problem-solving tasks where outcomes are not binary (i.e., correct or incorrect) or immediately clear. Therefore, this study aims to develop a scale to measure undergraduate students' behaviors of using Generative AI during problem-solving tasks without directly linking them to specific outcomes. We conducted an exploratory factor analysis on 800 responses collected after students finished one problem-solving activity, which revealed four distinct factors: reflective use, cautious use, thoughtless use, and collaborative use. The overall scale has reached sufficient internal reliability (Cronbach's alpha = .84). Two confirmatory factor analyses (CFAs) were conducted to validate the factors using the remaining 730 responses from this activity and 1173 responses from another problem-solving activity. CFA indices showed adequate model fit for data from both problem-solving tasks, suggesting that the scale can be applied to various human-AI problem-solving tasks. This study offers a validated scale to measure students' reliance behaviors in different human-AI problem-solving activities and provides implications for educators to responsively integrate Generative AI in higher education.}
}
@article{MARZANO20231028,
title = {Manufacturing Ergonomics Improvements in Distillery Industry Using Digital Tools},
journal = {Procedia CIRP},
volume = {118},
pages = {1028-1032},
year = {2023},
note = {16th CIRP Conference on Intelligent Computation in Manufacturing Engineering},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.06.176},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123004031},
author = {Adelaide Marzano},
keywords = {Digital manufacturing system, ergonomics, design},
abstract = {This paper presents the steps taken by distilleries to uphold years old traditions and how new design tools can streamlined the current manufacturing processes. Different methods for bung removal are explored and how they are used today within warehouses and distilleries worldwide. The aim is to test new designs to replace the current tools used in distillery process to perform heavily manual tasks. Models of the current and new design are produced, and both are tested in a digital environment for ergonomics and time efficiency purposes.}
}
@article{RAVISHANKAR20211,
title = {Time dependent network resource optimization in cyber–physical systems using game theory},
journal = {Computer Communications},
volume = {176},
pages = {1-12},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.04.034},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421001857},
author = {Monica Ravishankar and Thompson Stephan and Thinagaran Perumal},
keywords = {Critical infrastructures, Cyber–physical systems, Game theory, Reinforcement learning technique, Linguistic fuzzy variables},
abstract = {The social and economic stability of a country is dependent on critical infrastructures (CIs) whose services range from financial to healthcare and power to transportation and communications. Most of these CIs are cyber–physical systems (CPSs), which integrate the network’s computational and communication capabilities to facilitate the monitoring and controlling of physical processes. Such systems are vulnerable to damage due to natural disasters, physical incidents, or cyber-attacks impacting the CPS organizations managing complex industrial control systems and data acquisition systems. When these CPSs are exposed to systemic cyber risks and cascaded network failures, network administrators need to recover from the compromise under limited resources. This is formulated as an attacker-defender game model to emulate the decision-making process in choosing an appropriate attack/defence mechanism in response to cybersecurity incidents using game theory. To further improve the assumptions made in the pure game-theoretic model, we relax the constraints on the rationality of the players, monetary payoff, and completeness of information by incorporating learning in games using reinforcement learning technique and compute the expected payoff using linguistic fuzzy variables.}
}
@article{WERNER200782,
title = {Perspectives on the Neuroscience of Cognition and Consciousness},
journal = {Biosystems},
volume = {87},
number = {1},
pages = {82-95},
year = {2007},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2006.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0303264706000608},
author = {Gerhard Werner},
keywords = {Cognition, Consciousness, Metastability, Phase space, Coordination dynamics, Computation, Representation, Information},
abstract = {The origin and current use of the concepts of computation, representation and information in Neuroscience are examined and conceptual flaws are identified which vitiate their usefulness for addressing the problem of the neural basis of Cognition and Consciousness. In contrast, a convergence of views is presented to support the characterization of the Nervous System as a complex dynamical system operating in a metastable regime, and capable of evolving to configurations and transitions in phase space with potential relevance for Cognition and Consciousness.}
}
@article{RODRIGUES2021406,
title = {Convolutional Neural Network for Respiratory Mechanics Estimation during Pressure Support Ventilation},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {15},
pages = {406-411},
year = {2021},
note = {11th IFAC Symposium on Biological and Medical Systems BMS 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.10.290},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321016955},
author = {Adriano S. Rodrigues and Marcos R.O.A. Maximo and Marcus H. Victor},
keywords = {Mechanical Ventilation, Respiratory Mechanics, Respiratory Effort, Deep Learning, Convolutional Neural Networks},
abstract = {In mechanically ventilated patients, some lung injuries can be reduced or avoided with therapy individualization, while the lung function is evaluated continuously, breath by breath. However, obtaining information on respiratory mechanics (respiratory system resistance and compliance) in the presence of respiratory effort is challenging, even if using invasive and complex procedures. The contribution of this work is to predict both respiratory system resistance and compliance over time using a convolutional neural network (CNN) and estimate the respiratory effort profile using the respiratory dynamics. Therefore, the approach used in this work was to generate a large amount of simulated data to feed a CNN so it could learn how to predict the correct values of the respiratory system resistance and compliance. Then, the respiratory effort was estimated by solving a first-order linear model. The main results showed a normalized mean squared error of 5.7% for the respiratory system resistance and 11.56% for compliance from Bland-Altman plots derived from the computational simulator. Finally, the method was validated using real data from an active lung simulator within which respiratory mechanics varied, and some ventilator settings were adjusted to mimic actual patient situations. The active lung simulator effort profile was obtained with a normalized mean squared error of 8.31% considering the use of an active lung simulator. The results have shown that the simulated data were valuable for the CNN training, while the performance over the real data suggested that the network was generalized accordingly for estimating respiratory parameters and effort profile.}
}
@article{RAY2020106679,
title = {A framework for probabilistic model-based engineering and data synthesis},
journal = {Reliability Engineering & System Safety},
volume = {193},
pages = {106679},
year = {2020},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2019.106679},
url = {https://www.sciencedirect.com/science/article/pii/S0951832018312754},
author = {Douglas Ray and Jose Ramirez-Marquez},
keywords = {Modeling and Simulation (M&S), Design of experiments (DOE), Deterministic computer experiments, Space filling designs, Uncertainty Quantification (UQ), Probabilistic optimization, Verification, Validation, Calibration, Trade space, Sensitivity analysis, Statistical engineering},
abstract = {Modern computing resources provide scientists, engineers, and system design teams the ability to study phenomena, such as system behavior, in a virtual setting. Computational modeling and simulation (M&S) enables engineers to avoid many of the challenges encountered in traditional design engineering, including the design, manufacture, and testing of expensive prototypes prior to having an optimized design. However, the use of M&S carries its own challenges, such as the computational time and resources required to execute effective studies, and uncertainties arising from simplifying assumptions inherent to computer models, which are intended to be an approximate representation of reality. In recent year advances have been made in a number of areas related to the efficient and reliable use of M&S for system evaluations, including design & analysis of computer experiments, uncertainty quantification, probabilistic analysis, response optimization, and data synthesis techniques. In this review paper, a general framework for systematically executing efficient M&S studies at the component-level, product-level, system-level, and system-of-systems-level is described. A case study is used to demonstrate how statistical and probabilistic techniques can be integrated with M&S to address those challenges inherent to model-based engineering, and how this aligns with the proposed workflow. The example is a gun-launch dynamics model of an artillery projectile developed by US Army engineers, and illustrates the application of this workflow in the study of subsystem system reliability, performance, and end-to-end system-level characterization.}
}
@article{JANG2020107524,
title = {Hemispheric asymmetries in processing numerical meaning in arithmetic},
journal = {Neuropsychologia},
volume = {146},
pages = {107524},
year = {2020},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2020.107524},
url = {https://www.sciencedirect.com/science/article/pii/S0028393220301974},
author = {Selim Jang and Daniel C. Hyde},
keywords = {Arithmetic, Numerical cognition, Cerebral hemispheres, Late positivity, Distance effect},
abstract = {Hemispheric asymmetries in arithmetic have been hypothesized based on neuropsychological, developmental, and neuroimaging work. However, it has been challenging to separate asymmetries related to arithmetic specifically, from those associated general cognitive or linguistic processes. Here we attempt to experimentally isolate the processing of numerical meaning in arithmetic problems from language and memory retrieval by employing novel non-symbolic addition problems, where participants estimated the sum of two dot arrays and judged whether a probe dot array was the correct sum of the first two arrays. Furthermore, we experimentally manipulated which hemisphere receive the probe array first using a visual half-field paradigm while recording event-related potentials (ERP). We find that neural sensitivity to numerical meaning in arithmetic arises under left but not right visual field presentation during early and middle portions of the late positive complex (LPC, 400-800 ms). Furthermore, we find that subsequent accuracy for judgements of whether the probe is the correct sum is better under right visual field presentation than left, suggesting a left hemisphere advantage for integrating information for categorization or decision making related to arithmetic. Finally, neural signatures of operational momentum, or differential sensitivity to whether the probe was greater or less than the sum, occurred at a later portion of the LPC (800-1000 ms) and regardless of visual field of presentation, suggesting a temporal and functional dissociation between magnitude and ordinal processing in arithmetic. Together these results provide novel evidence for differences in timing and hemispheric lateralization for several cognitive processes involved in arithmetic thinking.}
}
@article{RAYAMORENO2024125385,
title = {Degradation of the ZT thermoelectric figure of merit in silicon when nanostructuring: From bulk to nanowires},
journal = {International Journal of Heat and Mass Transfer},
volume = {225},
pages = {125385},
year = {2024},
issn = {0017-9310},
doi = {https://doi.org/10.1016/j.ijheatmasstransfer.2024.125385},
url = {https://www.sciencedirect.com/science/article/pii/S0017931024002163},
author = {Martí Raya-Moreno and Riccardo Rurali and Xavier Cartoixà},
keywords = {Thermoelectrics, Nanowires, Phonon drag, , Coupled e-ph Boltzmann transport equation},
abstract = {Since the landmark paper by Hicks and Dresselhaus [Phys. Rev. B 47, 16631(R) (1993)], there has been a general consensus that one-dimensional nanoscale conductors, i.e. nanowires, provide the long sought paradigm to implement the so-called phonon-glass electron-crystal material, which results in large improvements in the thermoelectric figure of merit ZT. Despite some encouraging—though isolated—experimental results, this idea has never been subjected to a rigorous scrutiny and the effect of the coupled dynamics of electrons and phonons has usually been oversimplified. To bypass these limitations, we have calculated the effective thermoelectric parameters for silicon nanowires (SiNWs) by iteratively solving the coupled electron-phonon Boltzmann transport equation (EPBTE) supplied with first-principles data. This allows for an unprecedented precision in determining the correct dependence of the thermoelectric parameters with system size; including, but not limited to, the figure of merit and its enhancement or degradation due to nanostructuring. Indeed, we demonstrate that the commonly used relaxation time approximation (RTA), or the uncoupled beyond the RTA (iterative) solution fail to describe the correct effect of nanostructuring on the thermoelectric properties and efficiency in SiNWs due to the strong contribution of phonon drag to the Seebeck coefficient, so that the use of fully coupled solution of the EPBTE is essential to obtain the correct effect of nanostructuring. Most importantly, we show that, contrarily to what commonly argued, resorting to NWs is not necessarily beneficial for ZT. Indeed, in a wide range of diameters nanostructuring diminishes the Seebeck coefficient faster than the decrease in thermal conductivity, due to the suppression of very long wavelength phonons responsible for the largest contribution to the phonon drag component of the Seebeck coefficient. This penalty to ZT can be mitigated if the NWs have a very rough surface, providing additional reduction to the thermal conductivity. Additionally, we demonstrate that our methodology provides improved data sets for an accurate determination of doping concentration in NWs through electrical-based inference and excellent agreement with the available experimental data.}
}
@article{RENDONCASTRILLON2023104,
title = {Training strategies from the undergraduate degree in chemical engineering focused on bioprocesses using PBL in the last decade},
journal = {Education for Chemical Engineers},
volume = {44},
pages = {104-116},
year = {2023},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2023.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S1749772823000258},
author = {Leidy Rendón-Castrillón and Margarita Ramírez-Carmona and Carlos Ocampo-López},
keywords = {Research hotbed, Biotechnology, Green chemistry, Circular economy, Sustainability, ABET, Engineering education},
abstract = {Global engineering education addresses the development of professional competencies in undergraduates to prepare professionals capable of solving complex technical problems under social, environmental, and economic challenges. In this work, training was carried out to incorporate the bioprocess research of the chemical engineering students at Universidad Pontificia Bolivariana in Medellin, Colombia, using a project-based learning methodology (PBL). An open call was made to the students, and they were challenged to build a prototype which they had to support together with a written report as evidence for their admission to the research hotbed and assign them research projects in bioprocesses. In the last decade, 276 students participated in the hotbed generating 21 conference presentations, four software, 14 research articles, and 16 academic awards. In parallel, a survey was conducted to analyze the perception of graduates participating in the hotbed according to a list of 17 competency criteria relevant to the chemical engineering program. It was found that the average perception is at the highest levels (4−5), which indicates that most of the graduates value the significant contribution made by the CIBIOT hotbed to the development of a professional in experimentation, communication, and acquisition of new knowledge.}
}
@article{BANIK2022106232,
title = {Geometric systems of unbiased representatives},
journal = {Information Processing Letters},
volume = {176},
pages = {106232},
year = {2022},
issn = {0020-0190},
doi = {https://doi.org/10.1016/j.ipl.2021.106232},
url = {https://www.sciencedirect.com/science/article/pii/S0020019021001472},
author = {Aritra Banik and Bhaswar B. Bhattacharya and Sujoy Bhore and Leonardo Martínez-Sandoval},
keywords = {Computational geometry, Systems of unbiased representatives, Bicolorings, Np-Hard problems, Geometric ranges},
abstract = {Let P be a finite point set in Rd, B be a bicoloring of P and O be a family of geometric objects (that is, intervals, boxes, balls, etc). An object from O is called balanced with respect to B if it contains the same number of points from each color of B. For a collection B of bicolorings of P, a geometric system of unbiased representatives (G-SUR) is a subset O′⊆O such that for any bicoloring B of B there is an object in O′ that is balanced with respect to B. We pose and study problems on finding G-SURs. We obtain general bounds on the size of G-SURs consisting of intervals, size-restricted intervals, axis-parallel boxes and Euclidean balls. We show that the G-SUR problem is NP-Hard even in the simple case of points on a line and interval ranges. Furthermore, we study a related problem on determining the size of the largest and smallest balanced intervals for points on the real line with a random distribution and coloring. Our results are a natural extension to a geometric context of the work initiated by Balachandran et al. (Discrete Mathematics, 2018) on arbitrary systems of unbiased representatives.}
}
@article{CIMBUROVA2023127839,
title = {Making trees visible: A GIS method and tool for modelling visibility in the valuation of urban trees},
journal = {Urban Forestry & Urban Greening},
volume = {81},
pages = {127839},
year = {2023},
issn = {1618-8667},
doi = {https://doi.org/10.1016/j.ufug.2023.127839},
url = {https://www.sciencedirect.com/science/article/pii/S1618866723000109},
author = {Zofie Cimburova and Stefan Blumentrath and David N. Barton},
keywords = {Cultural ecosystem services, GIS, Tree valuation, Urban trees, Visibility analysis},
abstract = {Tree visibility is a key determinant of cultural ecosystem services of urban trees. This paper develops a flexible, efficient and easy-to-use GIS method for modelling individual tree visibility to support tree valuation. The method is implemented as a GRASS GIS AddOn tool called v.viewshed.impact, making it available to a broad spectrum of users and purposes. Thanks to empirically validated underlying algorithms and parallel processing, the method is accurate and fast in analysing high-resolution datasets and large numbers of trees. We demonstrate the method in two use cases in Oslo, Norway, showing that it provides an alternative to field-based assessment of visibility indicators in tree valuation methods and facilitates the inclusion of complex visibility indicators not possible to assess in the field. We argue that the method could also be used for tree management and planning, urban ecosystem accounting and neighbour conflict resolution related to trees.}
}
@article{XIONG2020180,
title = {Construction of approximate reasoning model for dynamic CPS network and system parameter identification},
journal = {Computer Communications},
volume = {154},
pages = {180-187},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.02.073},
url = {https://www.sciencedirect.com/science/article/pii/S0140366420301225},
author = {Juxia Xiong and Jinzhao Wu},
keywords = {Cyber physical system, Network, Event message modeling, Interactive multi-model algorithm},
abstract = {CPS (Cyber Physical System) is a large and complex real-time feedback system that integrates computing processes, physical processes, communication networks, sensor networks, and control systems. It has the powerful function of sensing and controlling the physical environment, which is a big wave following the Internet technology. Because the forms of communication, interaction, and collaboration between heterogeneous units within the CPS are intricate and complex, a comprehensive model needs to be established to describe and analyze the CPS. This paper analyzes the CPS architecture and proposes a new and more complete CPS architecture, decomposes according to this architecture, and classifies the physical entities in the CPS. At the same time, event-based modeling thinking is used to define, classify and formalize event messages. Considering the higher real-time requirements of CPS, an event weighting algorithm was designed according to the different priorities of real-time events. In order to reduce the congestion caused by the limited network bandwidth in the CPS system, improve the ability to identify abnormal data with great uncertainty, and fully guarantee the response rate of the CPS system to emergencies, this paper analyzes the complexity of the CPS system from the perspective of information theory. The average dynamic complexity of the CPS system is set as a threshold to determine the level of information entropy of the sensor data in a certain period of time. The CPS system selects high information entropy data to send first. The effectiveness is analyzed through experiments.}
}
@article{LIBERATORE2024103456,
title = {The ghosts of forgotten things: A study on size after forgetting},
journal = {Annals of Pure and Applied Logic},
volume = {175},
number = {8},
pages = {103456},
year = {2024},
issn = {0168-0072},
doi = {https://doi.org/10.1016/j.apal.2024.103456},
url = {https://www.sciencedirect.com/science/article/pii/S016800722400054X},
author = {Paolo Liberatore},
keywords = {Logical forgetting, Boolean minimization},
abstract = {Forgetting is removing variables from a logical formula while preserving the constraints on the other variables. In spite of reducing information, it does not always decrease the size of the formula and may sometimes increase it. This article discusses the implications of such an increase and analyzes the computational properties of the phenomenon. Given a propositional Horn formula, a set of variables and a maximum allowed size, deciding whether forgetting the variables from the formula can be expressed in that size is Dp-hard in Σ2p. The same problem for unrestricted CNF propositional formulae is D2p-hard in Σ3p.}
}
@article{STABLER1984155,
title = {Berwick and Weinberg on linguistics and computational psychology},
journal = {Cognition},
volume = {17},
number = {2},
pages = {155-179},
year = {1984},
issn = {0010-0277},
doi = {https://doi.org/10.1016/0010-0277(84)90017-9},
url = {https://www.sciencedirect.com/science/article/pii/0010027784900179},
author = {Edward P. Stabler}
}
@article{DEBOER2010502,
title = {Frame-based guide to situated decision-making on climate change},
journal = {Global Environmental Change},
volume = {20},
number = {3},
pages = {502-510},
year = {2010},
note = {Governance, Complexity and Resilience},
issn = {0959-3780},
doi = {https://doi.org/10.1016/j.gloenvcha.2010.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0959378010000245},
author = {Joop {de Boer} and J. Arjan Wardekker and Jeroen P. {van der Sluijs}},
keywords = {Climate change, Adaptation, Decision-making, Frames},
abstract = {The present paper describes a frame-based approach to situated-decision-making on climate change. Building on the multidisciplinary literature on the relationship between frames and decision-making, it argues that decision-makers may gain from making frames more explicit and using them for generating different visions about the central issues. Frames act as organizing principles that shape in a “hidden” and taken-for-granted way how people conceptualize an issue. Science-related issues, such as climate change, are often linked to only a few frames, which consistently appear across different policy areas. Indeed, it appears that there are some very contrasting ways in which climate change may be framed. These frames can be characterized in terms of a simple framework that highlights specific interpretations of climate issues. A second framework clarifies the built-in frames of decision tools. Using Thompson's two basic dimensions of decision, it identifies the main uncertainties that should be considered in developing a decision strategy. The paper characterizes four types of decision strategy, focusing on (1) computation, (2) compromise, (3) judgment, or (4) inspiration, and links each strategy to the appropriate methods and tools, as well as the appropriate social structures. Our experiences show that the frame-based guide can work as an eye-opener for decision-makers, particularly where it demonstrates how to add more perspectives to the decision.}
}
@article{DWYER20111021,
title = {An approach to quantitatively measuring collaborative performance in online conversations},
journal = {Computers in Human Behavior},
volume = {27},
number = {2},
pages = {1021-1032},
year = {2011},
note = {Web 2.0 in Travel and Tourism: Empowering and Changing the Role of Travelers},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2010.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0747563210003730},
author = {Paul Dwyer},
keywords = {Collaboration, Cognitive modeling, Collective thinking},
abstract = {Interpersonal dynamics often hinder people from optimizing collaboration. Researchers who monitor the intellectual activity of people as they converse online receive less value when such collaboration is impaired. How can they detect suboptimal collaboration? This study builds on a new metric for measuring collaborative value from the information content of participant contributions to propose a measure of collaborative efficiency, and demonstrates its utility by assessing collaboration around a sample of weblogs. The new collaborative value metric can augment qualitative research by highlighting for deeper investigation conversational themes that triggered elevated collaborative production. Identifying these themes may also define the cognitive box people have built within a collaborative venue. Challenging people to consider fresh ideas by deliberately introducing them into collaborative venues is recommended as the key to overcoming collaborative dysfunction.}
}
@article{BARLOW1983107,
title = {Vision: A computational investigation into the human representation and processing of visual information: David Marr. San Francisco: W. H. Freeman, 1982. pp. xvi + 397},
journal = {Journal of Mathematical Psychology},
volume = {27},
number = {1},
pages = {107-110},
year = {1983},
issn = {0022-2496},
doi = {https://doi.org/10.1016/0022-2496(83)90030-5},
url = {https://www.sciencedirect.com/science/article/pii/0022249683900305},
author = {H.B. Barlow}
}
@article{BONSIGNORE2017298,
title = {Present and future approaches to lifetime prediction of superelastic nitinol},
journal = {Theoretical and Applied Fracture Mechanics},
volume = {92},
pages = {298-305},
year = {2017},
issn = {0167-8442},
doi = {https://doi.org/10.1016/j.tafmec.2017.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0167844217300587},
author = {Craig Bonsignore}
}
@article{MCDONALD201955,
title = {Cognitive bots and algorithmic humans: toward a shared understanding of social intelligence},
journal = {Current Opinion in Behavioral Sciences},
volume = {29},
pages = {55-62},
year = {2019},
note = {Artificial Intelligence},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2019.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S2352154618301979},
author = {Kelsey R McDonald and John M Pearson},
abstract = {Questions of social behavior are simultaneously among the most fundamental in neuroscience and the most challenging in artificial intelligence. Yet despite decades of work, a unified perspective from the cognitive and computational approaches to the problem has yet to emerge. Recently, however, excitement around the challenges posed to reinforcement learning by multiplayer video games, coupled with the adoption of more complex modeling strategies in social neuroscience, has broadened the interface between the two fields. Here, we review recent progress from both directions, arguing that advances in artificial intelligence provide neuroscientists with valuable tools for modeling social interactions. At the same time, the study of humans as efficient social learners can inform the design of new algorithms for multi-agent systems. We conclude by encouraging a joint approach that incorporates the best of both domains to advance a shared picture of social intelligence.}
}
@incollection{CLAUSER20231,
title = {Past, present, and future of educational measurement},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {1-14},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.10001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305100016},
author = {Brian E. Clauser and Melissa J. Margolis},
keywords = {Karl Pearson, Francis Galton, Classical test theory, Item response theory, Generalizability theory, Frederic Lord, Lee Cronbach, Coefficient alpha, Validity theory, Charles Spearman, Eugenics movement, Spearman-Brown formula, Alfred Binet, Army Alpha test, Georg Rasch, Intelligence testing},
abstract = {This article provides an overview of the past, present, and future of educational measurement. We begin by examining the historical events in the 1800s that led to the development of a coherent mathematical theory of test scores in the first half of the 20th century. In this section we describe the contributions of Francis Galton, Karl Pearson, Charles Spearman, Truman Kelley, and Lee Cronbach. In addition to outlining the theoretical contributions of these researchers, we describe the rise of large-scale testing beginning with the Army Alpha test in 1917 and the administration of IQ tests to millions of school children in the decade that followed. We continue by discussing the current state of educational measurement theory and practice including the development and widespread use of item response theory, generalizability theory, validity theory, and large-scale national and international achievement testing to evaluate educational systems. Finally, we consider directions and developments that are likely to define the future of the field. These directions include increased use of computational power in assessment, the use of new sources of data (referred to as process data), automated systems to create test materials, and an increased emphasis on fairness.}
}
@article{YETISEN2015724,
title = {Bioart},
journal = {Trends in Biotechnology},
volume = {33},
number = {12},
pages = {724-734},
year = {2015},
issn = {0167-7799},
doi = {https://doi.org/10.1016/j.tibtech.2015.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S016777991500205X},
author = {Ali K. Yetisen and Joe Davis and Ahmet F. Coskun and George M. Church and Seok Hyun Yun},
keywords = {genetics, transgenic art, tissue engineering, ethics, aesthetics},
abstract = {Bioart is a creative practice that adapts scientific methods and draws inspiration from the philosophical, societal, and environmental implications of recombinant genetics, molecular biology, and biotechnology. Some bioartists foster interdisciplinary relationships that blur distinctions between art and science. Others emphasize critical responses to emerging trends in the life sciences. Since bioart can be combined with realistic views of scientific developments, it may help inform the public about science. Artistic responses to biotechnology also integrate cultural commentary resembling political activism. Art is not only about ‘responses’, however. Bioart can also initiate new science and engineering concepts, foster openness to collaboration and increasing scientific literacy, and help to form the basis of artists’ future relationships with the communities of biology and the life sciences.}
}
@incollection{ZHANG2022363,
title = {Chapter 18 - KPF: A retrospective view on urban planning AI for 2020},
editor = {Imdat As and Prithwish Basu and Pratap Talwar},
booktitle = {Artificial Intelligence in Urban Planning and Design},
publisher = {Elsevier},
pages = {363-380},
year = {2022},
isbn = {978-0-12-823941-4},
doi = {https://doi.org/10.1016/B978-0-12-823941-4.00004-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128239414000044},
author = {Snoweria Zhang and Kate Ringo and Richard Chou and Brandon Pachuca and Eric Pietraszkiewicz and Luc Wilson},
keywords = {Computational design, Digital twin, Urban design, Future history, City planning},
abstract = {Architectural historians have been fascinated by the year 1000, as the expectation of an impending apocalypse drove the sharp contrast between a dearth of construction before and a booming market after. One thousand years later, residents of 2020 found themselves at the crossroads again with the effects of climate change looming as a global threat. We constructed this chapter as a piece of a future, speculative, and historical document that examines the use of AI in urban planning and design in 2020. As historians from 2120, we study the evolution of tools at this critical junction with the backdrop of a confluence of crises. From explorative visual interfaces, open data initiatives, and computational design to AI that augments and collaborates with humans in the design and development of the city, we present case studies of both the technology and the projects that demonstrate some of the first applications of AI in negotiating the threat of climate change. Through these first examples, we trace the development of tools and corresponding trends in urban AI to the present year of 2120. The speculative narrative frame allows for an explication of the current urban design workflow using AI alongside an opportunity to conjecture where we believe AI development in design and planning ought to be. City makers in 2020 were not involved in the development of AI technologies. This work can act to inspire technologists who are envisioning the future of the city.}
}
@article{YAMAKAWA2021478,
title = {The whole brain architecture approach: Accelerating the development of artificial general intelligence by referring to the brain},
journal = {Neural Networks},
volume = {144},
pages = {478-495},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608021003543},
author = {Hiroshi Yamakawa},
keywords = {Brain reference architecture, Structure-constrained interface decomposition method, Brain information flow, Hypothetical component diagram, Brain-inspired artificial general intelligence, Whole-brain architecture},
abstract = {The vastness of the design space that is created by the combination of numerous computational mechanisms, including machine learning, is an obstacle to creating artificial general intelligence (AGI). Brain-inspired AGI development; that is, the reduction of the design space to resemble a biological brain more closely, is a promising approach for solving this problem. However, it is difficult for an individual to design a software program that corresponds to the entire brain as the neuroscientific data that are required to understand the architecture of the brain are extensive and complicated. The whole-brain architecture approach divides the brain-inspired AGI development process into the task of designing the brain reference architecture (BRA), which provides the flow of information and a diagram of the corresponding components, and the task of developing each component using the BRA. This is known as BRA-driven development. Another difficulty lies in the extraction of the operating principles that are necessary for reproducing the cognitive–behavioral function of the brain from neuroscience data. Therefore, this study proposes structure-constrained interface decomposition (SCID), which is a hypothesis-building method for creating a hypothetical component diagram that is consistent with neuroscientific findings. The application of this approach has been initiated for constructing various regions of the brain. In the future, we will examine methods for evaluating the biological plausibility of brain-inspired software. This evaluation will also be used to prioritize different computational mechanisms, which should be integrated and associated with the same regions of the brain.}
}
@article{WILKINS202440,
title = {We need to think differently about artificial intelligence},
journal = {New Scientist},
volume = {263},
number = {3509},
pages = {40-43},
year = {2024},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(24)01696-8},
url = {https://www.sciencedirect.com/science/article/pii/S0262407924016968},
author = {Alex Wilkins},
abstract = {Will AI ever emulate human intelligence? Professor of machine intelligence Neil Lawrence tells Alex Wilkins it is misleading to compare the two}
}
@article{LIU2024100744,
title = {Application of entertainment E-learning mode based on Apriori algorithm in intelligent English reading assistance mode},
journal = {Entertainment Computing},
volume = {51},
pages = {100744},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100744},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124001125},
author = {Shanshan Liu},
keywords = {Apriori algorithm, Entertainment E-learning model, Intelligent English teaching, Auxiliary reading mode},
abstract = {With the assistance of digital media, entertainment oriented E-learning models can effectively enhance students’ learning enthusiasm. This article analyzes the application of entertainment E-learning mode based on Apriori algorithm in intelligent English reading assistance mode. At present, English reading teaching faces some problems, such as outdated teaching methods, passive learning among students, excessive emphasis on imparting grammar knowledge while neglecting the improvement of students’ reading skills and strategies, and so on. Therefore, this article conducts research on intelligent English reading comprehension tools based on semantic analysis and Apriori algorithm. This paper proposes a recreational E-learning model based on Apriori algorithm. Based on Apriori algorithm, students’ interests and preferences on different learning resources and entertainment elements are mined and incorporated into the learning model design. Then, a set of entertaining English reading assistant model is designed, which uses a variety of entertainment elements, such as gamified learning, interactive activities and reward mechanism, to increase students’ learning participation and enthusiasm. This article adopts the idea of LSA algorithm to construct a BERT semantic analysis model. We treat nodes in the network as word items and then use singular value decomposition algorithm to decompose the word document matrix. Secondly, the original association rule Apriori algorithm was optimized, and the optimized association rule Apriori algorithm effectively solved the problem of excessive computation in traditional algorithms. Finally, based on semantic analysis and Apriori algorithm, this article designs an intelligent English reading comprehension tool, mainly analyzing the practical application of the system and greatly improving the efficiency of English reading teaching.}
}
@article{BRAMSON2023105397,
title = {Emotion regulation from an action-control perspective},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {153},
pages = {105397},
year = {2023},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2023.105397},
url = {https://www.sciencedirect.com/science/article/pii/S0149763423003664},
author = {Bob Bramson and Ivan Toni and Karin Roelofs},
keywords = {Emotion control, Emotion regulation, Emotional-action selection, Forward modelling},
abstract = {Despite increasing interest in emotional processes in cognitive science, theories on emotion regulation have remained rather isolated, predominantly focused on cognitive regulation strategies such as reappraisal. However, recent neurocognitive evidence suggests that early emotion regulation may involve sensorimotor control in addition to other emotion-regulation processes. We propose an action-oriented view of emotion regulation, in which feedforward predictions develop from action-selection mechanisms. Those can account for acute emotional-action control as well as more abstract instances of emotion regulation such as cognitive reappraisal. We argue the latter occurs in absence of overt motor output, yet in the presence of full-blown autonomic, visceral, and subjective changes. This provides an integrated framework with testable neuro-computational predictions and concrete starting points for intervention to improve emotion control in affective disorders.}
}
@article{GARAVAGLIA2010258,
title = {Modelling industrial dynamics with “History-friendly” simulations},
journal = {Structural Change and Economic Dynamics},
volume = {21},
number = {4},
pages = {258-275},
year = {2010},
issn = {0954-349X},
doi = {https://doi.org/10.1016/j.strueco.2010.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0954349X10000573},
author = {Christian Garavaglia},
keywords = {Simulation, Industrial dynamics, Evolutionary economics, “History-Friendly” models, Complexity},
abstract = {The use of simulation techniques has increased greatly in recent years. In economics the industrial dynamics approach makes use of simulation techniques to understand the complexity of the industrial process of continuous change. Among these models, a new branch of studies known as “History-friendly” models aims at establishing a close link between formal theory, developing stand-alone theoretical simulation models, and empirical evidence. In this paper, we study “History-friendly” analyses and counterfactuals. Some examples of “History-friendly” models are widely examined. Finally, the paper makes a critical contribution to “History-friendly” methodology and defines the role of “History-friendly” models in the debate on the empirical validation of simulations.}
}
@article{SMOLENTSEV2020111671,
title = {On the role of integrated computer modelling in fusion technology},
journal = {Fusion Engineering and Design},
volume = {157},
pages = {111671},
year = {2020},
issn = {0920-3796},
doi = {https://doi.org/10.1016/j.fusengdes.2020.111671},
url = {https://www.sciencedirect.com/science/article/pii/S0920379620302192},
author = {Sergey Smolentsev and Gandolfo Alessandro Spagnuolo and Arkady Serikov and Jens Juul Rasmussen and Anders H. Nielsen and Volker Naulin and Jaime Marian and Matti Coleman and Lorenzo Malerba},
keywords = {Fusion technology, Computer modelling, Neutronics, Materials, Plasma, MHD thermofluids, Model integration},
abstract = {Computer modelling is expected to play an increasingly important role in fusion design and technology, where the complexity of the physical processes involved (plasma, materials, engineering), and the highly interconnected nature of systems and components (“system of systems” design), call for support from sophisticated and integrated computer simulation tools. In this paper, we review the contribution of coupled computer modelling to the design of the reactor, breeding blanket and integrated first wall in terms of neutronics, materials behaviour (including plasma-materials interaction, radiation effects and compatibility with fluids), magnetohydrodynamics thermofluid issues and thermo-hydraulic aspects, as well as simulations of plasma transport out of the confinement region to determine heat and particle loads on plasma facing components. The current capabilities and levels of maturity of existing simulation tools are critically analysed, having in mind the possibility of integrating several tools in a single computational suite in the future and highlighting the perspectives and difficulties of such an endeavour.}
}
@article{NAKHALAKEL20252288,
title = {System-theoretic analysis for the identification of emerging risks in the storage of dangerous substances},
journal = {Procedia Computer Science},
volume = {253},
pages = {2288-2295},
year = {2025},
note = {6th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.01.289},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925002972},
author = {Antonio Javier {Nakhal Akel} and Francesco Simone and Elena Stefana and Lorenzo Fedele and Riccardo Patriarca},
keywords = {STAMP, cyber attacks, socio-technical systems, operations management},
abstract = {The energy transition process lets novel risks emerge, impacting safety of modern industrial settings. The introduction of automation and digitalization fosters the collaboration and the interconnection between system agents (both humans and technologies) to comply with new sustainability objectives. Cyber-physical systems are increasingly present in industries, stressing the need to consider safety and security jointly. Systemic approaches, such as System-Theoretic Process Analysis (STPA), have been shown to be effective tools for dealing with such problems. This paper employs STPA to identify and analyse emergent risks within an energy transition scenario. Performing STPA permitted to identify control flaws and unsafe interactions when integrating renewable energy technologies. Results highlight critical agents and actions that may lead accidents. Specifically, a case study related to the storage of dangerous substances is presented in this paper, showing how tank’s automated controls may be susceptible to disruptions.}
}
@article{HAN2025101699,
title = {Understanding the role of virtual mobility on how and what people create in virtual reality},
journal = {Thinking Skills and Creativity},
volume = {56},
pages = {101699},
year = {2025},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2024.101699},
url = {https://www.sciencedirect.com/science/article/pii/S1871187124002372},
author = {Eugy Han and Portia Wang and Cyan DeVeaux and Gabriella M. Harari and Jeremy N. Bailenson},
keywords = {Virtual reality, Creativity, Virtual mobility, Design creations},
abstract = {Virtual reality (VR) is considered a compelling tool to foster creativity by allowing its users to create in 3D space. However, the challenge lies in understanding how people use these tools and what they create, hindering the drawing of meaningful conclusions about VR as a viable tool for creativity. Furthermore, past research has shown that contextual factors shape how people create within VR, suggesting the existence of other factors. Here, we analyze the 3D creations of 137 participants responding to different creativity activities across seven sessions on a social VR platform. Specifically, we evaluate the role of virtual mobility, the capacity to move freely or have restricted movement in virtual space. We additionally present a VR-specific creativity coding scheme that follows recommendations from previous literature. Using dimensions derived from this coding scheme, we examine how these dimensions relate to behaviors and features of the creations in the context of virtual mobility. Results showed the significant role of virtual mobility on the design process, such that participants iterated and revised more by deleting more when their avatars were allowed to teleport and translate freely, compared to when their avatar’s movements were restricted to sitting down in virtual chairs. Furthermore, participants built shorter creations and took up less projection space with restricted virtual mobility. Results also showed that participants created more practical, unique, and well-implemented creations the more 3D models they used. Similarly, the more participants deleted, the more well-implemented the creations were. We discuss implications for designers of creation-oriented VR platforms and pedagogy for instructors facilitating activities in educational contexts.}
}
@article{GIERISCH200972,
title = {Factors associated with annual-interval mammography for women in their 40s},
journal = {Cancer Epidemiology},
volume = {33},
number = {1},
pages = {72-78},
year = {2009},
issn = {1877-7821},
doi = {https://doi.org/10.1016/j.cdp.2009.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0361090X0900021X},
author = {Jennifer M. Gierisch and Suzanne C. O’Neill and Barbara K. Rimer and Jessica T. DeFrank and J. Michael Bowling and Celette Sugg Skinner},
keywords = {Breast neoplasms, Guideline adherence, Health behavior, Middle aged, Attitude to health, Patient compliance, Mass screening, Female, Risk factor, Health knowledge},
abstract = {Background: Evidence is mounting that annual mammography for women in their 40s may be the optimal schedule to reduce morbidity and mortality from breast cancer. Few studies have assessed predictors of repeat mammography on an annual interval among these women. Methods: We assessed mammography screening status among 596 insured Black and Non-Hispanic white women ages 43–49. Adherence was defined as having a second mammogram 10–14 months after a previous mammogram. We examined socio-demographic, medical and healthcare-related variables on receipt of annual-interval repeat mammograms. We also assessed barriers associated with screening. Results: 44.8% of the sample were adherent to annual-interval mammography. A history of self-reported abnormal mammograms, family history of breast cancer and never having smoked were associated with adherence. Saying they had not received mammography reminders and reporting barriers to mammography were associated with non-adherence. Four barrier categories were associated with women's non-adherence: lack of knowledge/not thinking mammograms are needed, cost, being too busy, and forgetting to make/keep appointments. Conclusions: Barriers we identified are similar to those found in other studies. Health professionals may need to take extra care in discussing mammography screening risk and benefits due to ambiguity about screening guidelines for women in their 40s, especially for women without family histories of breast cancer or histories of abnormal mammograms. Reminders are important in promoting mammography and should be coupled with other strategies to help women maintain adherence to regular mammography.}
}
@article{BARON2022113861,
title = {Might pain be experienced in the brainstem rather than in the cerebral cortex?},
journal = {Behavioural Brain Research},
volume = {427},
pages = {113861},
year = {2022},
issn = {0166-4328},
doi = {https://doi.org/10.1016/j.bbr.2022.113861},
url = {https://www.sciencedirect.com/science/article/pii/S0166432822001292},
author = {Mark Baron and Marshall Devor},
keywords = {Anesthesia, Brain evolution, Consciousness, Coma, Mesopontine tegmentum, MPTA},
abstract = {It is nearly axiomatic that pain, among other examples of conscious experience, is an outcome of still-uncertain forms of neural processing that occur in the cerebral cortex, and specifically within thalamo-cortical networks. This belief rests largely on the dramatic relative expansion of the cortex in the course of primate evolution, in humans in particular, and on the fact that direct activation of sensory representations in the cortex evokes a corresponding conscious percept. Here we assemble evidence, drawn from a number of sources, suggesting that pain experience is unlike the other senses and may not, in fact, be an expression of cortical processing. These include the virtual inability to evoke pain by cortical stimulation, the rarity of painful auras in epileptic patients and outcomes of cortical lesions. And yet, pain perception is clearly a function of a conscious brain. Indeed, it is perhaps the most archetypical example of conscious experience. This draws us to conclude that conscious experience, at least as realized in the pain system, is seated subcortically, perhaps even in the “primitive” brainstem. Our conjecture is that the massive expansion of the cortex over the course of evolution was not driven by the adaptive value of implementing consciousness. Rather, the cortex evolved because of the adaptive value of providing an already existing subcortical generator of consciousness with a feed of critical information that requires the computationally intensive capability of the cerebral cortex.}
}
@article{SHAWKY2023100547,
title = {Adaptive chaotic map-based key extraction for efficient cross-layer authentication in VANETs},
journal = {Vehicular Communications},
volume = {39},
pages = {100547},
year = {2023},
issn = {2214-2096},
doi = {https://doi.org/10.1016/j.vehcom.2022.100547},
url = {https://www.sciencedirect.com/science/article/pii/S2214209622000948},
author = {Mahmoud A. Shawky and Muhammad Usman and Muhammad Ali Imran and Qammer H. Abbasi and Shuja Ansari and Ahmad Taha},
keywords = {Chebyshev chaotic mapping, Cross-layer authentication, Doppler emulation, Physical-layer signatures, Secret key extraction, Vehicular ad-hoc networks},
abstract = {Vehicle-to-everything (V2X) communication is expected to offer users available and ultra-reliable transmission, particularly for critical applications related to safety and autonomy. In this context, establishing a secure and resilient authentication process with low latency and high functionality may not be achieved using conventional cryptographic methodologies due to their significant computation costs. Recent research has focused on employing the physical (PHY) characteristics of wireless channels to develop efficient discrimination techniques to overcome the shortcomings of crypto-based authentication. This paper presents a cross-layer authentication scheme for multicarrier communication, leveraging the spatially/temporally correlated wireless channel features to facilitate key verification without exposing its secrecy. By mapping the time-stamped hashed key and masking it with channel phase responses, we create a PHY-layer signature, allowing for verifying the sender's identity while employing the correlated channel responses between subcarriers to verify messages' integrity. Furthermore, we developed a Diffie-Hellman secret key extraction algorithm that employs the computationally intractable problems of the Chebyshev chaotic mapping for channel probing. Thus, terminals can extract high entropy shared keys that can be used to create dynamic PHY-layer signatures, supporting forward and backward secrecy. We evaluated the scheme's security strength against active/passive attacks. Besides theoretical analysis, we designed a 3-Dimensional (3D) scattering Doppler emulator to investigate the scheme's performance at different speeds of a moving vehicle and signal-to-noise ratios (SNRs) for a realistic vehicular channel. Theoretical and hardware implementation analyses proved the capability of the proposed scheme to support high detection probability at SNR ≥ 0 dB and speed ≤ 45 m/s.}
}
@article{OBRIEN2021104184,
title = {Misplaced trust: When trust in science fosters belief in pseudoscience and the benefits of critical evaluation},
journal = {Journal of Experimental Social Psychology},
volume = {96},
pages = {104184},
year = {2021},
issn = {0022-1031},
doi = {https://doi.org/10.1016/j.jesp.2021.104184},
url = {https://www.sciencedirect.com/science/article/pii/S0022103121000871},
author = {Thomas C. O'Brien and Ryan Palmer and Dolores Albarracin},
keywords = {Misinformation, Trust in science, Critical thinking, Methodological literacy},
abstract = {At a time when pseudoscience threatens the survival of communities, understanding this vulnerability, and how to reduce it, is paramount. Four preregistered experiments (N = 532, N = 472, N = 605, N = 382) with online U.S. samples introduced false claims concerning a (fictional) virus created as a bioweapon, mirroring conspiracy theories about COVID-19, and carcinogenic effects of GMOs (Genetically Modified Organisms). We identify two critical determinants of vulnerability to pseudoscience. First, participants who trust science are more likely to believe and disseminate false claims that contain scientific references than false claims that do not. Second, reminding participants of the value of critical evaluation reduces belief in false claims, whereas reminders of the value of trusting science do not. We conclude that trust in science, although desirable in many ways, makes people vulnerable to pseudoscience. These findings have implications for science broadly and the application of psychological science to curbing misinformation during the COVID-19 pandemic.}
}
@article{TRAUSANMATU20231052,
title = {Identification of creativity in collaborative conversations based on the polyphonic model},
journal = {Procedia Computer Science},
volume = {221},
pages = {1052-1057},
year = {2023},
note = {Tenth International Conference on Information Technology and Quantitative Management (ITQM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.08.087},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923008451},
author = {Stefan Trausan-Matu},
keywords = {polyphonic model, creativity, brainstorming, collaboration, computer-supported collaborative learning, natural language processing, deep learning},
abstract = {The paper presents a theoretical approach and a set of experiments that operationalize it for the identification of creative moments in conversations. State-of-the-art artificial intelligence technology is used for the operationalization: natural language processing, machine learning, and deep neural networks The approach is based on the polyphonic model introduced by Trausan-Matu, which starts from Mikhail Bakhtin's analogy of discourse building in texts with polyphonic music. The divergent and convergent steps of creativity are related to the inter-animation of voices through dissonances and consonances in polyphonic, contrapuntal music.}
}
@article{SCHWAB2018500,
title = {A Robust Fault Detection Method using a Zonotopic Kaucher Set-membership Approach},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {24},
pages = {500-507},
year = {2018},
note = {10th IFAC Symposium on Fault Detection, Supervision and Safety for Technical Processes SAFEPROCESS 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.09.623},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318323358},
author = {Stefan Schwab and Vicenç Puig and Soeren Hohmann},
keywords = {Robust fault detection, set-membership approach, Kaucher arithmetic},
abstract = {This paper presents a robust fault detection method using a zonotopic Kaucher set-membership method. The fault detection approach is based on checking the consistency between the model and the data. Consistency is given if there is an intersection between the feasible parameter set and the nominal parameter set. To allow efficient computation the feasible set is approximated by a zonotope. Due to the usage of Kaucher interval arithmetic the results are mathematically guaranteed. The proposed approach is assessed using an illustrative application based on a well-known four-tank case study. The study shows that it is possible to detect even small errors in a noisy setting.}
}
@article{LEWTON202538,
title = {“Now is the time to realise useful autonomous quantum machines”},
journal = {New Scientist},
volume = {265},
number = {3534},
pages = {38-41},
year = {2025},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(25)00433-6},
url = {https://www.sciencedirect.com/science/article/pii/S0262407925004336},
author = {Thomas Lewton},
abstract = {Quantum technology is still in its infancy, says Nicole Yunger Halpern. But, she tells Thomas Lewton, she intends to change that}
}
@article{BIENVENU202049,
title = {On low for speed oracles},
journal = {Journal of Computer and System Sciences},
volume = {108},
pages = {49-63},
year = {2020},
issn = {0022-0000},
doi = {https://doi.org/10.1016/j.jcss.2019.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0022000018305828},
author = {Laurent Bienvenu and Rod Downey},
keywords = {Oracle computations, Lowness for speed},
abstract = {Relativizing computations of Turing machines to an oracle is a central concept in the theory of computation, both in complexity theory and in computability theory(!). Inspired by lowness notions from computability theory, Allender introduced the concept of “low for speed” oracles. An oracle A is low for speed if relativizing to A has essentially no effect on computational complexity, meaning that if a decidable language can be decided in time f(n) with access to oracle A, then it can be decided in time poly(f(n)) without any oracle. The existence of non-computable such A's was later proven by Bayer and Slaman, who even constructed a computably enumerable one, and exhibited a number of properties of these oracles. In this paper, we pursue this line of research, answering the questions left by Bayer and Slaman and give further evidence that the class of low for speed oracles is a very rich one.}
}