@article{FISCHER199721,
title = {Computational environments supporting creativity in the context of lifelong learning and design},
journal = {Knowledge-Based Systems},
volume = {10},
number = {1},
pages = {21-28},
year = {1997},
note = {Information Technology Support for Creativity},
issn = {0950-7051},
doi = {https://doi.org/10.1016/S0950-7051(97)00010-5},
url = {https://www.sciencedirect.com/science/article/pii/S0950705197000105},
author = {Gerhard Fischer and Kumiyo Nakakoji},
keywords = {Creativity support, Domain-oriented design environments (DODEs), Lifelong-learning},
abstract = {Much of our intelligence and creativity results from the collective memory of communities of practice and of the artifacts and technology surrounding them. Rather than studying individual creativity in isolation, we have developed a conceptual framework of creativity in the context of everyday practice — where design activities prevail and learning is constantly required. The conceptual framework explores new role distributions between people and computers based on theories that view design as reflection-in-action and breakdowns as opportunities for learning and creativity. We use an example from the domain of multimedia information design to illustrate how creativity is supported by domain-oriented design environments. The paper describes the mechanisms, architectures and processes underlying these environments.}
}
@incollection{JACOBLOPES202177,
title = {Chapter 5 - Assistant’s tools toward life cycle assessment},
editor = {Eduardo Jacob-Lopes and Leila Queiroz Zepka and Mariany Costa Deprá},
booktitle = {Sustainability Metrics and Indicators of Environmental Impact},
publisher = {Elsevier},
pages = {77-90},
year = {2021},
isbn = {978-0-12-823411-2},
doi = {https://doi.org/10.1016/B978-0-12-823411-2.00006-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234112000062},
author = {Eduardo Jacob-Lopes and Leila Queiroz Zepka and Mariany Costa Deprá},
keywords = {Theoretical approach, Sustainability metrics, Life cycle thinking, Social life cycle assessment, Life cycle costing, Life cycle sustainability assessment},
abstract = {This chapter aims to elucidate the main assistant’s tools created to assist in a global assessment of sustainability metrics and indicators. To this end, the chapter will provide a general review of the main assistant’s tools, considering the Life cycle thinking, social life cycle assessment, life cycle cost, and life cycle sustainability assessment tool. In addition, it guides some necessary criteria to be followed to apply each of these tools. Finally, this compilation of information strongly suggests, at the end of the chapter, the application of sensitivity analyses at the end of the process evaluations.}
}
@article{LI2023110701,
title = {Graph neural network architecture search for rotating machinery fault diagnosis based on reinforcement learning},
journal = {Mechanical Systems and Signal Processing},
volume = {202},
pages = {110701},
year = {2023},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2023.110701},
url = {https://www.sciencedirect.com/science/article/pii/S088832702300609X},
author = {Jialin Li and Xuan Cao and Renxiang Chen and Xia Zhang and Xianzhen Huang and Yongzhi Qu},
keywords = {Rotating machinery, Fault diagnosis, Graph neural network, Neural architecture search, Reinforcement learning},
abstract = {In order to improve the accuracy of fault diagnosis, researchers are constantly trying to develop new diagnostic models. However, limited by the inherent thinking of human beings, it has always been difficult to build a pioneering architecture for rotating machinery fault diagnosis. In order to solve this problem, this paper uses reinforcement learning algorithm based on adjacency matrix to carry out network architecture search (NAS) of rotating machinery fault diagnosis model. A reinforcement learning agent for deep deterministic policy gradient (DDPG) is developed based on actor–critic neural networks. The observation state of reinforcement learning is used to develop the graph neural network (GNN) diagnosis model, and the diagnosis accuracy is fed back to the agent as a reward for updating the reinforcement learning parameters. The MFPT bearing fault datasets and the developed gear pitting fault experimental data are used to validate the proposed network architecture search method based on reinforcement learning (RL-NAS). The proposed method is proved to be practical and effective in various aspects such as fault diagnosis ability, search space, search efficiency and multi-working condition performance.}
}
@article{HIPOLITO2023103510,
title = {Breaking boundaries: The Bayesian Brain Hypothesis for perception and prediction},
journal = {Consciousness and Cognition},
volume = {111},
pages = {103510},
year = {2023},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2023.103510},
url = {https://www.sciencedirect.com/science/article/pii/S1053810023000478},
author = {Inês Hipólito and Michael Kirchhoff},
keywords = {Bayesian Brain Hypothesis, Modularity of the Mind, Cognitive processes, Informational boundaries},
abstract = {This special issue aims to provide a comprehensive overview of the current state of the Bayesian Brain Hypothesis and its standing across neuroscience, cognitive science and the philosophy of cognitive science. By gathering cutting-edge research from leading experts, this issue seeks to showcase the latest advancements in our understanding of the Bayesian brain, as well as its potential implications for future research in perception, cognition, and motor control. A special focus to achieve this aim is adopted in this special issue, as it seeks to explore the relation between two seemingly incompatible frameworks for the understanding of cognitive structure and function: the Bayesian Brain Hypothesis and the Modularity Theory of the Mind. In assessing the compatibility between these theories, the contributors to this special issue open up new pathways of thinking and advance our understanding of cognitive processes.}
}
@article{STROMMER2022134322,
title = {Forward-looking impact assessment – An interdisciplinary systematic review and research agenda},
journal = {Journal of Cleaner Production},
volume = {377},
pages = {134322},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.134322},
url = {https://www.sciencedirect.com/science/article/pii/S095965262203894X},
author = {Kiia Strömmer and Jarrod Ormiston},
keywords = {Impact assessment, Forward-looking, Temporality, Futures thinking},
abstract = {New and established ventures are under increasing pressure to consider how their current actions impact our future world. Whilst many practitioners are paying greater attention to their future impact, most impact assessment research focuses on the retrospective measurement of impact. Limited studies have explored how impact assessment is used as a tool to forecast or predict the intended impact of organisational action. This study aims to overcome this gap by exploring forward-looking approaches to impact assessment. An interdisciplinary systematic review of the impact assessment literature was conducted to answer the question: “How and why do organisations utilise forward-looking, future-oriented approaches to impact assessment?“. The findings elaborate on the common research themes, challenges, and gaps in understanding forward-looking impact assessment. An integrated process model is developed to show the relationships between various antecedents, methods, and effects of forward-looking impact assessment. Based on the review, the paper puts forward a research agenda to provoke further inquiry on forward-looking, future-oriented approaches to impact assessments related to four research themes: uncertainty, values and assumptions, stakeholder cooperation, and learning. The study contributes to the impact assessment literature by providing an overview of how the current literature comprehends forward-looking approaches and insights into how a more holistic view of temporality in impact assessment can be developed.}
}
@article{GABRIEL2008330,
title = {A friend is a present you give to your “Self”: Avoidance of intimacy moderates the effects of friends on self-liking},
journal = {Journal of Experimental Social Psychology},
volume = {44},
number = {2},
pages = {330-343},
year = {2008},
issn = {0022-1031},
doi = {https://doi.org/10.1016/j.jesp.2007.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0022103107001126},
author = {Shira Gabriel and Mauricio Carvallo and Lisa M. Jaremka and Brooke Tippin},
keywords = {The self, Social comparison, Friendship, Avoidance of intimacy, Attachment style},
abstract = {The current research proposes that thinking about friends improves feelings about the self and does so differentially depending on avoidance of intimacy. Based on previous findings that individuals who avoid intimacy in relationships (avoidant individuals) contrast their self-concepts with primed friends whereas those who pursue intimacy in relationships (non-avoidant individuals) assimilate their self-concepts to primed friends [Gabriel, S., Carvallo, M., Dean, K., Tippin, B. D., & Renaud, J. (2005). How I see “Me” depends on how I see “We”: The role of avoidance of intimacy in social comparison. Personality and Social Psychology Bulletin, 31, 156–157], we predicted that friends who embody negative aspects of self would lead avoidant individuals to like themselves more, whereas friends who embody positive aspects of self would lead non-avoidant individuals to like themselves more. A pretest determined that good friends were seen as more similar to positive and ideal aspects of the self, whereas friends about whom participants had more mixed feelings (ambivalent friends) were seen as more similar to disliked and feared aspects of the self. Four experiments supported the main hypotheses. In Experiment 1, non-avoidant individuals like themselves more when good friends were primed. In Experiment 2, avoidant individuals like themselves more when ambivalent friends were primed. In Experiment 3, non-avoidant individuals liked themselves better after thinking about a friend’s positive traits, whereas avoidant individuals liked themselves better after thinking about a friend’s negative traits. In Experiment 4, all individuals under self-esteem threat strategically brought friends to mind who would help them like themselves more.}
}
@article{ZHAO2023106750,
title = {A cooperative population-based iterated greedy algorithm for distributed permutation flowshop group scheduling problem},
journal = {Engineering Applications of Artificial Intelligence},
volume = {125},
pages = {106750},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106750},
url = {https://www.sciencedirect.com/science/article/pii/S095219762300934X},
author = {Hui Zhao and Quan-Ke Pan and Kai-Zhou Gao},
keywords = {Distributed permutation flowshop, Group scheduling, Total flowtime, Iterated greedy algorithm, Co-evolutionary},
abstract = {This paper studies the distributed permutation flowshop group scheduling problem (DPFGSP) with the consideration of minimizing total flowtime (TF), which has important applications in the modern manufacturing process. Based on the characteristics of the problem, a cooperative population-based iterated greedy (CPIG) algorithm is proposed by combining the advantages of the divide-and-rule policy, population-based evolution and iterated greedy algorithm. The CPIG divides the DPFGSP into two coupled sub-problems of group scheduling sub-problem and job scheduling sub-problem, and starts with a single population for simplicity. Unlike in the traditional cooperative co-evolutionary algorithms, the two-coupled sub-problems are addressed with a certain probability that can be determined in favor of solving the whole scheduling problem. Some advanced technologies are used, including the constructive heuristics based initialization, the critical factories based destruction and construction, the new best solution based population updating mechanism. The comprehensive experimental evaluation of 810 instances shows that the CPIG algorithm performs much better than the five state-of-the-art metaheuristics in the literature which are closely related to the considered scheduling problem.}
}
@article{BRYANSMITH2023105405,
title = {Real-time social media sentiment analysis for rapid impact assessment of floods},
journal = {Computers & Geosciences},
volume = {178},
pages = {105405},
year = {2023},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2023.105405},
url = {https://www.sciencedirect.com/science/article/pii/S0098300423001097},
author = {Lydia Bryan-Smith and Jake Godsall and Franky George and Kelly Egode and Nina Dethlefs and Dan Parsons},
keywords = {Social media, Sentiment analysis, Flooding, Artificial Intelligence},
abstract = {Traditional approaches to flood modelling mostly rely on hydrodynamic physical simulations. While these simulations can be accurate, they are computationally expensive and prohibitively so when thinking about real-time prediction based on dynamic environmental conditions. Alternatively, social media platforms such as Twitter are often used by people to communicate during a flooding event, but discovering which tweets hold useful information is the key challenge in extracting information from posts in real time. In this article, we present a novel model for flood forecasting and monitoring that makes use of a transformer network that assesses the severity of a flooding situation based on sentiment analysis of the multimodal inputs (text and images). We also present an experimental comparison of a range of state-of-the-art deep learning methods for image processing and natural language processing. Finally, we demonstrate that information induced from tweets can be used effectively to visualise fine-grained geographical flood-related information dynamically and in real-time.}
}
@incollection{KUMAR2024147,
title = {Chapter Eight - Machine learning model for teaching and emotional intelligence},
editor = {Muskan Garg and Deepika Koundal},
booktitle = {Emotional AI and Human-AI Interactions in Social Networking},
publisher = {Academic Press},
pages = {147-168},
year = {2024},
isbn = {978-0-443-19096-4},
doi = {https://doi.org/10.1016/B978-0-443-19096-4.00014-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443190964000146},
author = {Mohit Kumar and Syam Machinathu Parambil Gangadharan and Nabanita Choudhury},
keywords = {Cognitive thinking, Design thinking, E-Learning, Emotional intelligence, Intelligent quotient, Social neuroscience},
abstract = {Education that is ongoing and permanent for the purpose of adaptable up-skilling and retraining has been identified as a contributory factor, along with a relentless race against time, fast scientific progress, and unanticipated challenges. Students in postsecondary learning require mechanisms that can enable long-term, dependable knowledge production and storage, and this is particularly true in the age after a pandemic that occurred during the development of new technologies. There has been an explosion of e-learning platforms and methodologies that have been developed to remedy this issue; however, not all of them have been as successful as would be ideal. This type of new knowledge is very difficult to execute properly; it needs complex, careful educational and interface design to perform as well as it does and keep learners interested. This framework was designed and developed in this study. This technology was employed to support a cutting-edge pedagogic study based on neuroscience that was offered to faculty at universities of higher education. This framework was designed and developed in this study. This technology was employed to support a cutting-edge pedagogic study based on neuroscience that was offered to faculty at universities of higher education. Having a high intelligence quotient does not guarantee a successful and happy life. Success also necessitates self-awareness and emotional control. Our idea was to create a computational paradigm that would educate students in both programming and emotional intelligence. This experiment was successful in addressing the problem of excessive screen use by providing a student interface without displays.}
}
@article{SUN2025e01027,
title = {First-principles calculations of electronic and mechanical properties of magnesium indium intermetallic compounds},
journal = {Computational Condensed Matter},
volume = {43},
pages = {e01027},
year = {2025},
issn = {2352-2143},
doi = {https://doi.org/10.1016/j.cocom.2025.e01027},
url = {https://www.sciencedirect.com/science/article/pii/S2352214325000267},
author = {Liang Sun and Yidan Huang and Kaifeng Zhao and Zuoming Chen and Xiongtao Shang and Wenzhen Xu and wenyan Zhai and Pengyue Han and Jin Jia and Jianhong Peng},
keywords = {Mg-In intermetallic compounds, First-principles calculations, Phonon spectra, Anisotropy, Mechanical properties, Electronic properties},
abstract = {In the search for innovative alternatives to aluminum-magnesium alloys, this study takes a unique approach by focusing on magnesium-indium binary alloys, with an emphasis on the intermetallic compounds Mg2In, MgIn3, Mg5In2, and Mg3In. With the help of cutting-edge first-principles computational techniques, the four compounds are comprehensively and thoroughly analyzed in terms of crystal structure, anisotropy, phonon spectra, electronic properties, and mechanical properties. The charge transfer phenomenon from magnesium to indium is found for the first time, and the s-orbital density of indium is at its peak in the Mg-In phase. In terms of mechanical properties, Mg2In, Mg5In2, and MgIn3 exhibit similar bulk moduli, while the shear modulus, Young's modulus, and hardness of MgIn3 are significantly lower than those of the other phases, emphasizing its unique deformability. Taking the results together, MgIn3 shows great potential for application in cutting-edge fields such as biomedical materials due to its compact size, corrosion resistance, low hardness, and high plasticity, which opens up a new way of thinking for the development of Mg-In alloy-based advanced materials.}
}
@article{BIDERMAN2020542,
title = {What Are Memories For? The Hippocampus Bridges Past Experience with Future Decisions},
journal = {Trends in Cognitive Sciences},
volume = {24},
number = {7},
pages = {542-556},
year = {2020},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2020.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S1364661320301066},
author = {Natalie Biderman and Akram Bakkour and Daphna Shohamy},
keywords = {memory, decision-making, amnesia, hippocampus, value},
abstract = {Many decisions require flexible reasoning that depends on inference, generalization, and deliberation. Here, we review emerging findings indicating that the hippocampus, known for its role in long-term memory, contributes to these flexible aspects of value-based decision-making. This work offers new insights into the role of memory in decision-making and suggests that memory may shape decisions even in situations that do not appear, at first glance, to depend on memory at all. Uncovering the pervasive role of memory in decision-making challenges the way we define what memory is and what it does, suggesting that memory’s primary purpose may be to guide future behavior and that storing a record of the past is just one way to do so.}
}
@article{BANERJEE2015143,
title = {Z*-numbers: Augmented Z-numbers for machine-subjectivity representation},
journal = {Information Sciences},
volume = {323},
pages = {143-178},
year = {2015},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2015.06.026},
url = {https://www.sciencedirect.com/science/article/pii/S0020025515004582},
author = {Romi Banerjee and Sankar K. Pal},
keywords = {Artificial-mindfulness, Machine-consciousness, Machine-qualia, Machine-self, Perception-operators, Thinking machine},
abstract = {Envisaging a futuristic environment of man–machine and machine–machine synergy, this article documents our research on the augmented Z-numbers, the Z*-numbers, for machine-perception encapsulation. The Z*-numbers have been envisioned as operands of endogenous machine-mind processes underlying bespoke comprehension of the real world. Besides information-certainty, as in a Z-number, a Z*-number incorporates context, time and affects as essential factors of subjectivity representation. We have proposed: (a) definitions for certainty and affect parameters—arising out of socio-cultural influences on machine-knowledge, (b) a Z*-number based rudimentary procedure for natural-language comprehension emulation, and (c) primitive perception-operators for ‘machine-mentalese’ simulation using Z*-number information-equivalents. Our work draws from non-symbolic theories of cognition and ‘mindfulness’, human-mind processes—studied through behavioral experiments, and theories of the ‘self’ and ‘qualia’. The article includes detailed discussions of these experiments and consequent insights, analysis of a theoretical run-through of the defined procedure, and correspondence-studies between the Z*-number paradigm and philosophies of the self. Our research raises questions on cognitive biases and autogenous mind-processes that highlight crucial practical challenges in the current realization of a synthetic-mind. All ideas herein aim to contribute to studies on the ‘self’ and its machine-embodiment for the synthesis of an empathetic machine-mind.}
}
@article{GARCIACAIRASCO2021107930,
title = {Searching for a paradigm shift in the research on the epilepsies and associated neuropsychiatric comorbidities. From ancient historical knowledge to the challenge of contemporary systems complexity and emergent functions},
journal = {Epilepsy & Behavior},
volume = {121},
pages = {107930},
year = {2021},
note = {NEWroscience 2018},
issn = {1525-5050},
doi = {https://doi.org/10.1016/j.yebeh.2021.107930},
url = {https://www.sciencedirect.com/science/article/pii/S1525505021001645},
author = {Norberto Garcia-Cairasco and Guilherme Podolsky-Gondim and Julian Tejada},
keywords = {Ancestral knowledge, Superstitious versus scientific knowledge, Epilepsies and neuropsychiatric comorbidities, Clinical semiology and neurosurgery methods, experimental and computational modeling, Complexity and emergent properties},
abstract = {In this review, we will discuss in four scenarios our challenges to offer possible solutions for the puzzle associated with the epilepsies and neuropsychiatric comorbidities. We need to recognize that (1) since quite old times, human wisdom was linked to the plural (distinct global places/cultures) perception of the Universe we are in, with deep respect for earth and nature. Plural ancestral knowledge was added with the scientific methods; however, their joint efforts are the ideal scenario; (2) human behavior is not different than animal behavior, in essence the product of Darwinian natural selection; knowledge of animal and human behavior are complementary; (3) the expression of human behavior follows the same rules that complex systems with emergent properties, therefore, we can measure events in human, clinical, neurobiological situations with complexity systems’ tools; (4) we can use the semiology of epilepsies and comorbidities, their neural substrates, and potential treatments (including experimental/computational modeling, neurosurgical interventions), as a source and collection of integrated big data to predict with them (e.g.: machine/deep learning) diagnosis/prognosis, individualized solutions (precision medicine), basic underlying mechanisms and molecular targets. Once the group of symptoms/signals (with a myriad of changing definitions and interpretations over time) and their specific sequences are determined, in epileptology research and clinical settings, the use of modern and contemporary techniques such as neuroanatomical maps, surface electroencephalogram and stereoelectroencephalography (SEEG) and imaging (MRI, BOLD, DTI, SPECT/PET), neuropsychological testing, among others, are auxiliary in the determination of the best electroclinical hypothesis, and help design a specific treatment, usually as the first attempt, with available pharmacological resources. On top of ancient knowledge, currently known and potentially new antiepileptic drugs, alternative treatments and mechanisms are usually produced as a consequence of the hard, multidisciplinary, and integrated studies of clinicians, surgeons, and basic scientists, all over the world. The existence of pharmacoresistant patients, calls for search of other solutions, being along the decades the surgeries the most common interventions, such as resective procedures (i.e., selective or standard lobectomy, lesionectomy), callosotomy, hemispherectomy and hemispherotomy, added by vagus nerve stimulation (VNS), deep brain stimulation (DBS), neuromodulation, and more recently focal minimal or noninvasive ablation. What is critical when we consider the pharmacoresistance aspect with the potential solution through surgery, is still the pursuit of localization-dependent regions (e.g.: epileptogenic zone (EZ)), in order to decide, no matter how sophisticated are the brain mapping tools (EEG and MRI), the size and location of the tissue to be removed. Mimicking the semiology and studying potential neural mechanisms and molecular targets – by means of experimental and computational modeling – are fundamental steps of the whole process. Concluding, with the conjunction of ancient knowledge, coupled to critical and creative contemporary, scientific (not dogmatic) clinical/surgical, and experimental/computational contributions, a better world and of improved quality of life can be offered to the people with epilepsy and neuropsychiatric comorbidities, who are still waiting (as well as the scientists) for a paradigm shift in epileptology, both in the Basic Science, Computational, Clinical, and Neurosurgical Arenas. This article is part of the Special Issue “NEWroscience 2018”.}
}
@incollection{MUBAYI2017249,
title = {Chapter 10 - Computational Modeling Approaches Linking Health and Social Sciences: Sensitivity of Social Determinants on the Patterns of Health Risk Behaviors and Diseases},
editor = {Arni S.R. {Srinivasa Rao} and Saumyadipta Pyne and C.R. Rao},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {36},
pages = {249-304},
year = {2017},
booktitle = {Disease Modelling and Public Health, Part A},
issn = {0169-7161},
doi = {https://doi.org/10.1016/bs.host.2017.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0169716117300172},
author = {Anuj Mubayi},
keywords = {Health risk behaviors, Dynamic models, Data mining, Sensitivity and uncertainty analysis, Ecological models, Social influences},
abstract = {Developing health promotion programs that support healthy lifestyle behaviors require comprehensive understanding of mechanisms that drive such complex social systems. Policy makers can use models and theories to guide this process at the individuals, groups, and communities levels. Individuals can have multiple risky health behaviors including physical inactivity, unhealthy diets, smoking, and alcohol drinking that are often shaped by social and ecological factors. Collective understanding of these factors can provide ability to design and evaluate intervention programs that can change unhealthy or risky behaviors over long period of time. However, it is overwhelming task to optimize intervention based on only empirical and/or cross-sectional studies. Effective long lasting intervention needs a thorough understanding of the role of social and environmental mechanisms at multiple scales on the dynamics of health behaviors. Recent mathematical and computational methods developed in other fields, such as epidemiology and finance, can provide systematic and in-depth understanding of mechanisms. However, the use of such methods in social and behaviors sciences have been limited. In this chapter, some real life working examples of social health behaviors problems are provided which uses some cutting edge methods from dynamical systems and data mining to uncertainty quantification.}
}
@article{GOBERT201581,
title = {Using educational data mining to assess students’ skills at designing and conducting experiments within a complex systems microworld},
journal = {Thinking Skills and Creativity},
volume = {18},
pages = {81-90},
year = {2015},
note = {21st Century Skills: International Advancements and Recent Developments},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2015.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S1871187115300067},
author = {Janice D. Gobert and Yoon Jeon Kim and Michael A. {Sao Pedro} and Michael Kennedy and Cameron G. Betts},
keywords = {Complex systems, Inquiry assessment, Performance assessment, Educational data mining, 21st century skills},
abstract = {Many national policy documents underscore the importance of 21st century skills, including critical thinking. In parallel, recent American frameworks for K-12 science education call for the development of critical thinking skills in science, also referred to as science inquiry skills/practices. Assessment of these skills is necessary, as indicated in policy documents; however, this has posed a great challenge for assessment researchers. Recently, some science learning environments seek to assess these science skills. These systems log all students’ interactions within the given system, and if fully leveraged, these logs provide rich assessments of inquiry skills. Here, we describe our environment Inq-ITS (inquiry intelligent tutoring system), that uses educational data mining to assess science inquiry skills, as described as 21st century skills. Additionally, here, we describe how we measure students’ skills at designing controlled experiments, a lynchpin skill of inquiry, in the context of complex systems. In doing so, our work addresses 21st century skill assessment in two ways, namely of inquiry (designing and conducting experiments), and in the context of complex systems, a key topic area of 21st century skills. We use educational data mining to develop our assessment of this skill for complex systems.}
}
@article{WARD202154,
title = {On value-laden science},
journal = {Studies in History and Philosophy of Science Part A},
volume = {85},
pages = {54-62},
year = {2021},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2020.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0039368120301783},
author = {Zina B. Ward},
keywords = {Values, Values in science, Argument from inductive risk, Motivating reasons, Justifying reasons},
abstract = {Philosophical work on values in science is held back by widespread ambiguity about how values bear on scientific choices. Here, I disambiguate several ways in which a choice can be value-laden and show that this disambiguation has the potential to solve and dissolve philosophical problems about values in science. First, I characterize four ways in which values relate to choices: values can motivate, justify, cause, or be impacted by the choices we make. Next, I put my proposed taxonomy to work, using it to clarify one version of the argument from inductive risk. The claim that non-epistemic values must play a role in scientific choices that run inductive risk makes most sense as a claim about values being needed to justify such choices. The argument from inductive risk is not unique: many philosophical arguments about values in science can be more clearly understood and assessed by paying close attention to how values and choices are related.}
}
@article{KATTERFELDT201872,
title = {Physical computing with plug-and-play toolkits:Key recommendations for collaborative learning implementations},
journal = {International Journal of Child-Computer Interaction},
volume = {17},
pages = {72-82},
year = {2018},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2018.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212868917300351},
author = {Eva-Sophie Katterfeldt and Mutlu Cukurova and Daniel Spikol and David Cuartielles},
keywords = {Collaborative learning, Education, Motivation, Physical computing, Programming, Toolkit},
abstract = {Physical computing toolkits have long been used in educational contexts to learn about computational concepts by engaging in the making of interactive projects. This paper presents a comprehensive toolkit that can help educators teach programming with an emphasis on collaboration, and provides suggestions for its effective pedagogical implementation. The toolkit comprises the Talkoo kit with physical computing plug-and-play modules and a visual programming environment. The key suggestions are inspired by the results of the evaluation studies which show that children (aged 14–18 in a sample group of 34 students) are well motivated when working with the toolkit but lack confidence in the kit’s support for collaborative learning. If the intention is to move beyond tools and code in computer education to community and context, thus encouraging computational participation, collaboration should be considered as a key aspect of physical computing activities. Our approach expands the field of programming with physical computing for teenage children with a focus on empowering teachers and students with not only a kit but also its appropriate classroom implementation for collaborative learning.}
}
@article{SHAFFER199795,
title = {Learning mathematics through design: The anatomy of Escher's world},
journal = {The Journal of Mathematical Behavior},
volume = {16},
number = {2},
pages = {95-112},
year = {1997},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(97)90019-5},
url = {https://www.sciencedirect.com/science/article/pii/S0732312397900195},
author = {David Williamson Shaffer},
abstract = {This article explores one example of an open learning environment created by combining mathematics and design activities in a “mathematics studio”. Two iterations of the mathematics studio experiment in a project at the MIT Media Laboratory known as Escher's World suggest that: (a) students can learn about the mathematical concept of symmetry in a studio learning environment, (b) students learn to use visual thinking to solve mathematical problems in a studio learning environment, and (c) students develop a more positive attitude towards mathematics as a result of working in a studio learning environment. This article uses a qualitative research model to explore the specific characteristics of the mathematics studio that were influential in creating a successful learning environment—in particular, how expressive mathematics activities and expressive computational media give students a sense of control over their learning.}
}
@article{KASNECI2023102274,
title = {ChatGPT for good? On opportunities and challenges of large language models for education},
journal = {Learning and Individual Differences},
volume = {103},
pages = {102274},
year = {2023},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2023.102274},
url = {https://www.sciencedirect.com/science/article/pii/S1041608023000195},
author = {Enkelejda Kasneci and Kathrin Sessler and Stefan Küchemann and Maria Bannert and Daryna Dementieva and Frank Fischer and Urs Gasser and Georg Groh and Stephan Günnemann and Eyke Hüllermeier and Stephan Krusche and Gitta Kutyniok and Tilman Michaeli and Claudia Nerdel and Jürgen Pfeffer and Oleksandra Poquet and Michael Sailer and Albrecht Schmidt and Tina Seidel and Matthias Stadler and Jochen Weller and Jochen Kuhn and Gjergji Kasneci},
keywords = {Large language models, Artificial intelligence, Education, Educational technologies},
abstract = {Large language models represent a significant advancement in the field of AI. The underlying technology is key to further innovations and, despite critical views and even bans within communities and regions, large language models are here to stay. This commentary presents the potential benefits and challenges of educational applications of large language models, from student and teacher perspectives. We briefly discuss the current state of large language models and their applications. We then highlight how these models can be used to create educational content, improve student engagement and interaction, and personalize learning experiences. With regard to challenges, we argue that large language models in education require teachers and learners to develop sets of competencies and literacies necessary to both understand the technology as well as their limitations and unexpected brittleness of such systems. In addition, a clear strategy within educational systems and a clear pedagogical approach with a strong focus on critical thinking and strategies for fact checking are required to integrate and take full advantage of large language models in learning settings and teaching curricula. Other challenges such as the potential bias in the output, the need for continuous human oversight, and the potential for misuse are not unique to the application of AI in education. But we believe that, if handled sensibly, these challenges can offer insights and opportunities in education scenarios to acquaint students early on with potential societal biases, criticalities, and risks of AI applications. We conclude with recommendations for how to address these challenges and ensure that such models are used in a responsible and ethical manner in education.}
}
@article{HUANG202233634,
title = {Transition from synaptic simulation to nonvolatile resistive switching behavior based on an Ag/Ag:ZnO/Pt memristor},
journal = {RSC Advances},
volume = {12},
number = {52},
pages = {33634-33640},
year = {2022},
issn = {2046-2069},
doi = {https://doi.org/10.1039/d2ra05483c},
url = {https://www.sciencedirect.com/science/article/pii/S2046206922032296},
author = {Yong Huang and Jiahao Yu and Yu Kong and Xiaoqiu Wang},
abstract = {ABSTRACT
The advent of memristors and the continuing research and development in the field of brain-inspired computing could allow realization of a veritable “thinking machine”. In this study, ZnO-based memristors were fabricated using a radio frequency magnetron sputtering method. The ZnO oxide layer was prepared by incorporating silver nanocrystals (NCs). Several synaptic functions, i.e. nonlinear transmission characteristics, short-term potentiation, long-term potentiation/depression, and pair-pulse facilitation, were imitated in the memristor successfully. Furthermore, the transition from synaptic behaviors to bipolar resistive switching behaviors of the device was also observed under repeated stimulus. It is speculated that the switching mechanism is due to the formation and rupture of the conductive Ag filaments and the corresponding electrochemical metallization. The experimental results demonstrate that the Ag/Ag:ZnO/Pt memristor with resistive switching and several synaptic behaviors has a potential application in neuromorphic computing and data storage systems.}
}
@article{LOPEZSILVA202446,
title = {‘Are these my thoughts?’: A 20-year prospective study of thought insertion, thought withdrawal, thought broadcasting, and their relationship to auditory verbal hallucinations},
journal = {Schizophrenia Research},
volume = {265},
pages = {46-57},
year = {2024},
note = {Hallucinations: Neurobiology and Patient Experience},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2022.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0920996422002778},
author = {Pablo López-Silva and Martin Harrow and Thomas H. Jobe and Michele Tufano and Helen Harrow and Cherise Rosen},
keywords = {Schizophrenia, Psychosis, Thought insertion, Thought withdrawal, Thought broadcasting, Auditory-verbal hallucinations},
abstract = {The co-occurrence of delusions and other symptoms at the onset of psychosis is a challenge for theories about the aetiology of psychosis. This paper explores the relatedness of delusions about the experience of thinking (thought insertion, thought withdrawal, and thought broadcasting) and auditory verbal hallucinations by describing their trajectories over a 20-year period in individuals diagnosed with schizophrenia, affective and other psychosis, and unipolar depression nonpsychosis. The sample consisted of 407 participants who were recruited at index hospitalization and evaluated over six follow-ups over 20 years. The symptom structure associated with thought insertion included auditory verbal hallucinations, somatic hallucinations, other hallucinations, delusions of thought-dissemination, delusions of control, delusion of self-depreciation, depersonalization and anxiety. The symptom constellation of thought withdrawal included somatic hallucinations, other hallucinations, delusions of thought dissemination, delusions of control, sexual delusions, depersonalization, negative symptoms, depression, and anxiety. The symptom constellation of thought broadcasting included auditory verbal hallucinations, somatic hallucinations, delusions of thought-dissemination, delusion of self-depreciation, fantastic delusions, sexual delusions, and depersonalization. Auditory verbal hallucinations and delusions of self-depreciation were significantly associated with both thought insertion and thought broadcasting. Thought insertion and thought withdrawal were significantly associated with other hallucinations, delusions of control, and anxiety; thought withdrawal and thought broadcasting were significantly related to sexual delusions. We hypothesize that specific symptom constellations over time might be explained as the product of pseudo-coherent realities created to give meaning to the experience of the world and the self of individuals in psychosis based on both prior top-down and ongoing bottom-up elements.}
}
@article{BEDNORZ2024101169,
title = {Effects of domain-specific linguistic factors on the difficulty of mathematics tasks},
journal = {The Journal of Mathematical Behavior},
volume = {75},
pages = {101169},
year = {2024},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2024.101169},
url = {https://www.sciencedirect.com/science/article/pii/S0732312324000464},
author = {David Bednorz and Michael Kleine and Rudolf {vom Hofe}},
keywords = {Mathematical tasks, Task features, linguistic complexity, Task difficulty},
abstract = {Linguistic features as a task-related feature influence the difficulty of mathematical tasks. To reduce this influence (e.g., in testing situations), studies on linguistic simplification focus on modifying linguistic features. These studies show little or no effect on increasing test performance. An open question is whether a quantitative–exploratory approach with texts from a specific domain can be an additional model for reducing the linguistic influence on mathematical tasks. To answer this question, generalized linear mixed models were used to determine the effects of linguistic factors, the requirements of the items, and the effects of linguistic factors when differentiating the requirements of the items, while controlling for further person- and item-related effects. The results show that linguistic factors can have either a negative or positive influence on test performance. The findings indicate that for mathematics assessments and teaching, it might be essential to consider the influence of language factors and task requirements.}
}
@article{BAKER2021101933,
title = {Who is marginalized in energy justice? Amplifying community leader perspectives of energy transitions in Ghana},
journal = {Energy Research & Social Science},
volume = {73},
pages = {101933},
year = {2021},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2021.101933},
url = {https://www.sciencedirect.com/science/article/pii/S2214629621000268},
author = {Erin Baker and Destenie Nock and Todd Levin and Samuel A. Atarah and Anthony Afful-Dadzie and David Dodoo-Arhin and Léonce Ndikumana and Ekundayo Shittu and Edwin Muchapondwa and Charles Van-Hein Sackey},
abstract = {There is a divide in energy access studies, between technologically-focused modeling papers in engineering and economics, and energy justice frameworks and principles grounded in social sciences. Quantitative computational models are necessary when analyzing energy, and more specifically electricity, systems, as they are technologically-complex systems that can diverge from intuitive patterns. To assure energy justice, these models must be reflective of, and informative to, a wide range of stakeholders, including households and communities alongside utilities, governments, and others. Yet, moving from a qualitative understanding of preferences to quantitative modeling is challenging. In this perspective piece, we pilot the use of the value-focused thinking framework to inform stakeholder engagement. The result is a strategic objective hierarchy that highlights the tradeoffs and the social, economic and technological factors that need to be measured in models. We apply the process in Ghana, using a survey, stakeholder workshops, and follow-up interviews to uncover key tradeoffs and stakeholder-derived objectives. We discuss three key areas that have been rarely, if ever, well-represented in energy models: (1) the relationship between the dynamics of electricity end-use and the technology and economic structure of the system; (2) explicit tradeoffs between electricity access, cost, and reliability as defined by stakeholders; and (3) the definition of new objectives, such as minimizing hazards related to theft. We conclude that this model of engagement provides an opportunity to tie together rigorous qualitative analysis and stakeholder engagement with crucial quantitative models of the electricity system.}
}
@article{MOLINARO20231150,
title = {A goal-centric outlook on learning},
journal = {Trends in Cognitive Sciences},
volume = {27},
number = {12},
pages = {1150-1164},
year = {2023},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2023.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S1364661323002073},
author = {Gaia Molinaro and Anne G.E. Collins},
keywords = {goals, learning, decision-making, reinforcement learning, rewards, abstraction, motivation, computational modeling},
abstract = {Goals play a central role in human cognition. However, computational theories of learning and decision-making often take goals as given. Here, we review key empirical findings showing that goals shape the representations of inputs, responses, and outcomes, such that setting a goal crucially influences the central aspects of any learning process: states, actions, and rewards. We thus argue that studying goal selection is essential to advance our understanding of learning. By following existing literature in framing goal selection within a hierarchy of decision-making problems, we synthesize important findings on the principles underlying goal value attribution and exploration strategies. Ultimately, we propose that a goal-centric perspective will help develop more complete accounts of learning in both biological and artificial agents.}
}
@incollection{ZHUGE2016149,
title = {15 - Limitations and challenges},
editor = {Hai Zhuge},
booktitle = {Multi-Dimensional Summarization in Cyber-Physical Society},
publisher = {Morgan Kaufmann},
pages = {149-151},
year = {2016},
series = {Computer Science Reviews and Trends},
isbn = {978-0-12-803455-2},
doi = {https://doi.org/10.1016/B978-0-12-803455-2.00015-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128034552000159},
author = {Hai Zhuge},
keywords = {Summarization, limitations, challenges, representations, computing},
abstract = {The limitation of summarisation lies in the natural differences between human and machine, between languages, and between the ways of observation and thinking of authors and those of readers. The significant evolution of documents in form and function in cyber-physical society challenges the paradigm of summarization research.}
}
@article{ALANO20221,
title = {Professor Richard Carter (1945–2021)},
journal = {Trends in Parasitology},
volume = {38},
number = {1},
pages = {1-3},
year = {2022},
issn = {1471-4922},
doi = {https://doi.org/10.1016/j.pt.2021.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S1471492221002609},
author = {Pietro Alano and Richard Culleton and Christian Doerig and Louis Miller},
abstract = {The malaria research community lost a pioneer when Professor Richard Carter passed away at the age of 76 on 4 September 2021. Richard was an exceptionally brilliant malariologist, always inquisitive and gifted with an unorthodox way of thinking.}
}
@article{MAHOWALD2024517,
title = {Dissociating language and thought in large language models},
journal = {Trends in Cognitive Sciences},
volume = {28},
number = {6},
pages = {517-540},
year = {2024},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2024.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S1364661324000275},
author = {Kyle Mahowald and Anna A. Ivanova and Idan A. Blank and Nancy Kanwisher and Joshua B. Tenenbaum and Evelina Fedorenko},
keywords = {large language models, language and thought, cognitive neuroscience, linguistic competence, computational modeling},
abstract = {Large language models (LLMs) have come closest among all models to date to mastering human language, yet opinions about their linguistic and cognitive capabilities remain split. Here, we evaluate LLMs using a distinction between formal linguistic competence (knowledge of linguistic rules and patterns) and functional linguistic competence (understanding and using language in the world). We ground this distinction in human neuroscience, which has shown that formal and functional competence rely on different neural mechanisms. Although LLMs are surprisingly good at formal competence, their performance on functional competence tasks remains spotty and often requires specialized fine-tuning and/or coupling with external modules. We posit that models that use language in human-like ways would need to master both of these competence types, which, in turn, could require the emergence of separate mechanisms specialized for formal versus functional linguistic competence.}
}
@article{WANG2022e09982,
title = {Applying the post-digital strategy of anexact architecture to non-standard design practices within the challenging construction contexts},
journal = {Heliyon},
volume = {8},
number = {8},
pages = {e09982},
year = {2022},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2022.e09982},
url = {https://www.sciencedirect.com/science/article/pii/S2405844022012701},
author = {Sining Wang and Dandan Lin},
keywords = {Design practice strategy, Post-digital architecture, Parametric design, Developing region, Non-standard architecture},
abstract = {New architectural forms offered by digital design approaches often appear incompatible with the prescribed precision and control in construction, especially in developing regions where advanced implementation means are limited. In response, this paper suggests working with design practice indeterminacy. Named ‘anexact architecture’, the post-digital design practice strategy presents a convergent diagram of seeking the feasible design solution space. It relies on the procedural parametric modelling to constantly integrate computation and humanisation, so that a rigorous built outcome is capable of accommodating project-specific idiosyncrasies and constraints. The demonstrator projects are discussed based on the combination of the Participatory Action Research method and the idea of anexact architecture. This paper aims to illustrate the peculiarity of anexact architecture and its ideology of treating design delivery uncertainties as essentials rather than negatives when practicing in a volatile construction context.}
}
@article{SURYARAJ2024124407,
title = {Block based motion estimation model using CNN with representative point matching algorithm for object tracking in videos},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124407},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124407},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424012739},
author = {C.K. Suryaraj and M.R. Geetha},
keywords = {Motion Estimation, Object Tracking, CNN, RPM, SSIM, Video Sequence, Computation Time},
abstract = {Motion estimation is considered significant for tracking the movement of an object in video sequences, and it is widely used in various video processing applications. Traditionally, many researchers focus on pixel-based motion estimation for object tracking, but it experienced increased computation time and cost. To reduce computation time, the utilization of a block-based motion estimation approach for object tracking is a recent trend. The existing block-based approach faces difficulty in finding representative points within the intensity domain. Therefore, this current research merged the deep learning approach with a block-matching algorithm for achieving efficient object tracking. In this proposed work, initially, video sequences are collected from a benchmark video dataset. Then, the acquired video sequences are segmented into frames. From the segmented frames, current and previous frames are considered for motion estimation. Frames are sent for the data augmentation process in which the process of flipping, cropping, and rotation is carried out. Then, the augmented frames are sent into Convolutional Neural Network (CNN) for feature extraction. Representative Point Matching (RPM) is used to estimate the motion vector based on the extracted features. After estimating the motion vector, the similarity between two consecutive frames is found using Structural Similarity Index (SSIM) technique. Finally, based on the similarity score, the movement of an object in the video is tracked effectively. Simulation analysis of the proposed block-based motion estimation model is done by evaluating some performance metrics. RMSE, PSNR, Execution Time, SSIM, and accuracy obtained for the proposed model are 27.5, 26.5 db, 31 sec, 0.91, and 94 %. This analysis suggested that the proposed CNN-RPM motion estimation model performs better in tracking the movement of the object.}
}
@article{SHEFFIELD2024100333,
title = {Understanding Cognitive Behavioral Therapy for Psychosis Through the Predictive Coding Framework},
journal = {Biological Psychiatry Global Open Science},
volume = {4},
number = {4},
pages = {100333},
year = {2024},
issn = {2667-1743},
doi = {https://doi.org/10.1016/j.bpsgos.2024.100333},
url = {https://www.sciencedirect.com/science/article/pii/S2667174324000466},
author = {Julia M. Sheffield and Aaron P. Brinen and Brandee Feola and Stephan Heckers and Philip R. Corlett},
keywords = {Belief updating, CBTp, Persecutory delusions, Predictive coding, Psychotherapy, Volatility},
abstract = {Psychological treatments for persecutory delusions, particularly cognitive behavioral therapy for psychosis, are efficacious; however, mechanistic theories explaining why they work rarely bridge to the level of cognitive neuroscience. Predictive coding, a general brain processing theory rooted in cognitive and computational neuroscience, has increasing experimental support for explaining symptoms of psychosis, including the formation and maintenance of delusions. Here, we describe recent advances in cognitive behavioral therapy for psychosis–based psychotherapy for persecutory delusions, which targets specific psychological processes at the computational level of information processing. We outline how Bayesian learning models employed in predictive coding are superior to simple associative learning models for understanding the impact of cognitive behavioral interventions at the algorithmic level. We review hierarchical predictive coding as an account of belief updating rooted in prediction error signaling. We examine how this process is abnormal in psychotic disorders, garnering noisy sensory data that is made sense of through the development of overly strong delusional priors. We argue that effective cognitive behavioral therapy for psychosis systematically targets the way sensory data are selected, experienced, and interpreted, thus allowing for the strengthening of alternative beliefs. Finally, future directions based on these arguments are discussed.}
}
@incollection{MARWALA202185,
title = {Chapter 7 - Bounded rational counterfactuals},
editor = {Tshilidzi Marwala},
booktitle = {Rational Machines and Artificial Intelligence},
publisher = {Academic Press},
pages = {85-96},
year = {2021},
isbn = {978-0-12-820676-8},
doi = {https://doi.org/10.1016/B978-0-12-820676-8.00012-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128206768000120},
author = {Tshilidzi Marwala},
keywords = {Rational counterfactuals, Bounded rationality, Optimization, Artificial intelligence},
abstract = {The rational counterfactual is identified from the factual and the knowledge of the laws that govern the relationships between the antecedent and the consequent of the factual, which maximizes the attainment of the desired consequent. However, the attainment of the desired consequent is not perfect and is, in fact, limited, which makes these counterfactuals bounded rational counterfactuals. In counterfactual thinking, factual statements such as “The COVID-19 afflicted the world, and the world economy contracted by 3%,” has the counterfactual “The COVID-19 did not afflict the world, and the world economy grew by 3%.” In this chapter, we use intelligent machines that use AI to build bounded rational counterfactuals. It is observed that intelligent machines can achieve bounded rational counterfactual better than human agents. In general, quantifiable factual easily has bounded rational counterfactuals when compared to qualitative counterfactuals.}
}
@article{SCHREIBER20142544,
title = {A few bad ideas on the way to the triumph of parallel computing},
journal = {Journal of Parallel and Distributed Computing},
volume = {74},
number = {7},
pages = {2544-2547},
year = {2014},
note = {Special Issue on Perspectives on Parallel and Distributed Processing},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2013.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0743731513002177},
author = {Robert Schreiber},
keywords = {Parallelism, Amdahl, Automatic parallelization, Accelerators, Exascale},
abstract = {Parallelism has become mainstream, in the multicore chip, the GPU, and the internet datacenter running MapReduce. In my field, large-scale scientific computing, parallelism now reigns triumphant. It was no simple, direct route that led to this triumph. Along the way, we were confused by ideas that, in retrospect, turned out to be distractions and errors. The thinking behind them was reasonable, but wrong. One can learn from a dissection of mistakes, so I will retell part of the story here.}
}
@article{NISHANT2020102104,
title = {Artificial intelligence for sustainability: Challenges, opportunities, and a research agenda},
journal = {International Journal of Information Management},
volume = {53},
pages = {102104},
year = {2020},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2020.102104},
url = {https://www.sciencedirect.com/science/article/pii/S0268401220300967},
author = {Rohit Nishant and Mike Kennedy and Jacqueline Corbett},
keywords = {Agenda for practice, AI, Artificial intelligence, Climate change, Environmental governance, Natural environment, Research agenda, Sustainability},
abstract = {Artificial intelligence (AI) will transform business practices and industries and has the potential to address major societal problems, including sustainability. Degradation of the natural environment and the climate crisis are exceedingly complex phenomena requiring the most advanced and innovative solutions. Aiming to spur groundbreaking research and practical solutions of AI for environmental sustainability, we argue that AI can support the derivation of culturally appropriate organizational processes and individual practices to reduce the natural resource and energy intensity of human activities. The true value of AI will not be in how it enables society to reduce its energy, water, and land use intensities, but rather, at a higher level, how it facilitates and fosters environmental governance. A comprehensive review of the literature indicates that research regarding AI for sustainability is challenged by (1) overreliance on historical data in machine learning models, (2) uncertain human behavioral responses to AI-based interventions, (3) increased cybersecurity risks, (4) adverse impacts of AI applications, and (5) difficulties in measuring effects of intervention strategies. The review indicates that future studies of AI for sustainability should incorporate (1) multilevel views, (2) systems dynamics approaches, (3) design thinking, (4) psychological and sociological considerations, and (5) economic value considerations to show how AI can deliver immediate solutions without introducing long-term threats to environmental sustainability.}
}
@article{BEATY2017189,
title = {Creative constraints: Brain activity and network dynamics underlying semantic interference during idea production},
journal = {NeuroImage},
volume = {148},
pages = {189-196},
year = {2017},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2017.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S1053811917300125},
author = {Roger E. Beaty and Alexander P. Christensen and Mathias Benedek and Paul J. Silvia and Daniel L. Schacter},
keywords = {Creativity, Divergent thinking, Cognitive control, Functional connectivity, Default network, Executive control network},
abstract = {Functional neuroimaging research has recently revealed brain network interactions during performance on creative thinking tasks—particularly among regions of the default and executive control networks—but the cognitive mechanisms related to these interactions remain poorly understood. Here we test the hypothesis that the executive control network can interact with the default network to inhibit salient conceptual knowledge (i.e., pre-potent responses) elicited from memory during creative idea production. Participants studied common noun-verb pairs and were given a cued-recall test with corrective feedback to strengthen the paired association in memory. They then completed a verb generation task that presented either a previously studied noun (high-constraint) or an unstudied noun (low-constraint), and were asked to “think creatively” while searching for a novel verb to relate to the presented noun. Latent Semantic Analysis of verbal responses showed decreased semantic distance values in the high-constraint (i.e., interference) condition, which corresponded to increased neural activity within regions of the default (posterior cingulate cortex and bilateral angular gyri), salience (right anterior insula), and executive control (left dorsolateral prefrontal cortex) networks. Independent component analysis of intrinsic functional connectivity networks extended this finding by revealing differential interactions among these large-scale networks across the task conditions. The results suggest that interactions between the default and executive control networks underlie response inhibition during constrained idea production, providing insight into specific neurocognitive mechanisms supporting creative cognition.}
}
@article{JACKSON2012370,
title = {Information technology use and creativity: Findings from the Children and Technology Project},
journal = {Computers in Human Behavior},
volume = {28},
number = {2},
pages = {370-376},
year = {2012},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2011.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0747563211002147},
author = {Linda A. Jackson and Edward A. Witt and Alexander Ivan Games and Hiram E. Fitzgerald and Alexander {von Eye} and Yong Zhao},
keywords = {Videogames, Creativity, Children, Technology use},
abstract = {This research examined relationships between children’s information technology (IT) use and their creativity. Four types of information technology were considered: computer use, Internet use, videogame playing and cell phone use. A multidimensional measure of creativity was developed based on Sternberg and Lubart, 1999, Subrahmanyam et al., 2006 test of creative thinking. Participants were 491 12-year olds; 53% were female, 34% were African American and 66% were Caucasian American. Results indicated that videogame playing predicted of all measures of creativity. Regardless of gender or race, greater videogame playing was associated with greater creativity. Type of videogame (e.g., violent, interpersonal) was unrelated to videogame effects on creativity. Gender but not race differences were obtained in the amount and type of videogame playing, but not in creativity. Implications of the findings for future research to test the causal relationship between videogame playing and creativity and to identify mediator and moderator variables are discussed.}
}
@article{GUO2017677,
title = {Research on Element Importance of Shafting Installation Based on QFD and FMEA},
journal = {Procedia Engineering},
volume = {174},
pages = {677-685},
year = {2017},
note = {13th Global Congress on Manufacturing and Management Zhengzhou, China 28-30 November, 2016},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.01.205},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817302059},
author = {Qi Guo and Kuangjie Sheng and Zheng Wang and Xilin Zhang and hengyi Yang and Rui Miao},
keywords = {Quality Function Deployment, Failure Mode and Effects Analysis, Marine Shafting, Comprehensive Importance},
abstract = {Development in today's shipbuilding economy is transforming from the quantitative growth to the quality growth. Quality function deployment (QFD) and failure mode and effects analysis (FMEA) adopt different ways of thinking, they remedy their respective limitations for each other, that can effectively guide the quality control. This paper is combined of HuDong ZhongHua Shipbuilding (group) co. LTD.’s shafting installation process, starting from the QFD customer requirements for finding the importance of production process elements and correction by FMEA, ultimately acquire the comprehensive importance of shafting installation process elements.}
}
@article{AGARWAL1992251,
title = {Computational fluid dynamics on parallel processors},
journal = {Computing Systems in Engineering},
volume = {3},
number = {1},
pages = {251-259},
year = {1992},
note = {High-Performance Computing for Flight Vehicles},
issn = {0956-0521},
doi = {https://doi.org/10.1016/0956-0521(92)90110-5},
url = {https://www.sciencedirect.com/science/article/pii/0956052192901105},
author = {R.K. Agarwal and J.C. Lewis},
abstract = {Greater computational power is needed for solving computational fluid dynamics problems of interest in engineering design. Parallel computers offer the promise of providing orders of magnitude increases in computational power compared with current uniprocessor vector supercomputers. This paper is mainly concerned with the implementation of a three-dimensional Navier-Stokes code MDNS3D on concurrent computers with grain sizes ranging from fine to coarse. An overview of commercially available parallel machines and the current state of the art in parallel algorithms is presented. The implementation of MDNS3D on machines such as the CRAY Y-MP/8, IBM 3090S, BBN Butterfly II, Intel iPSC/2, Symult 2010, MASPAR, and the Connection Machine CM-2, is described. Particular attention is paid to differences in implementation on SIMD and MIMD architectures. Factors affecting the performance of the code on different architectures are addressed. In addition, user interface and software portability issues are considered for various machines. Finally, future trends in parallel hardware and software development are assessed, and the factors important in determining the most suitable architecture for performing very large scale calculations are discussed.}
}
@article{JANKOVMASIREVIC2025129569,
title = {Observations on the McKay Iν Bessel distribution II},
journal = {Journal of Mathematical Analysis and Applications},
volume = {550},
number = {2},
pages = {129569},
year = {2025},
issn = {0022-247X},
doi = {https://doi.org/10.1016/j.jmaa.2025.129569},
url = {https://www.sciencedirect.com/science/article/pii/S0022247X25003506},
author = {Dragana {Jankov Maširević} and Tibor K. Pogány and Nataša Ujić},
keywords = {McKay  Bessel distribution, Mean–value theorem for definite integrals, Modified Bessel function of the first kind, Lambert  function, –Lambert function, Count data},
abstract = {Motivated by a wide spectrum of possible applications of the McKay Iν Bessel distribution we aim to present two new formulae for the appropriate distribution function, using the mean–value theorems for integrals: one of Bonnet type and another relying on the stronger version of the Okamura's variant of the second integral mean–value theorem. In both of those results, a point, characteristic for the mean–value theorems, is explicitly presented in terms of the Lambert W function. In addition, the computational efficiency of the newly derived formulae versus initial definition of the mentioned cumulative distribution function is established and the count data problem is resolved for this probability law.}
}
@article{ELAZZAOUI2023119595,
title = {A digital twin-based edge intelligence framework for decentralized decision in IoV system},
journal = {Information Sciences},
volume = {649},
pages = {119595},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.119595},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523011805},
author = {Abir {El Azzaoui} and Sekione Reward Jeremiah and Neal N. Xiong and Jong Hyuk Park},
keywords = {IoV, Digital twin, Smart contracts, Smart transportation},
abstract = {The Internet of Vehicle (IoV) is an emerging technology for the development of future smart cities. With the fast and exponential growing rate of Internet of Things (IoT), the smart transportation field is ushering in a revolutionary advancement. Smart transportation systems facilitate better informed, more coordinated, and smarter use of transport networks, with the use of advanced information and communication technologies applied to vehicles to help improve traffic management, minimize congestion, improve safety, and ultimately provide a more intelligent use of transport networks. Smart transportation is an integral part of modern-day smart city projects. However, the world climate institution has reported that carbon emissions from the overall transportation system accounts for one-fifth of global carbon dioxide with a sum of around 24% of energy. Electric vehicles (EV) represent a solution for this issue, yet, it is not sustainable. The communication between EVs and a roadside unit (RSU), and the continuous computational power required to support an IoV system also requires a reliance on energy harvesting. With this in mind, in this paper, we propose a decentralized trust management solution for IoV systems to reduce both carbon footprint and offload the computation power required. Our solution resides in developing the digital twin of vehicles on an intelligent edge environment to simulate the physical vehicle and handle the required data processing. Also, we implement a smart contract model for fast, secure, and sustainable on-road battery recharging between EVs.}
}
@article{TRAENKLE1994197,
title = {Solving microstructure electrostatistics with MIMD parallel supercomputers and Split-C},
journal = {Journal of Non-Newtonian Fluid Mechanics},
volume = {53},
pages = {197-213},
year = {1994},
issn = {0377-0257},
doi = {https://doi.org/10.1016/0377-0257(94)85049-6},
url = {https://www.sciencedirect.com/science/article/pii/0377025794850496},
author = {Frank Traenkle and Matthew I. Frank and Mary K. Vernon and Sangtae Kirn},
keywords = {Microstructure electrostatics, Multiple Instruction Multiple Data (MIMD) model, Parallel computational algorithms, Split-C},
abstract = {We consider parallel computational algorithms for the boundary integral solutions of the Laplace equation for use in the simulation of electrorheological fluids and as a model study of a class of elliptic partial differential equations that appear in basic microscopic descriptions of heterogeneous structured continua. The viewpoint is that of constructing large scale simulations that bridge micro- and macro-length and time scales on state-of-the-art parallel supercomputers. Because of long range interactions, fast communications are the key to scalable N-Body algorithms. The communication scheduling strategies of Fuentes and Kim are examined in two contexts on the Thinking Machines CM-5 parallel computer. First, an N-Body simulation implementation in C using the standard send-receive message passing primitives in the CMMD 2.0 library shows that communication scheduling leads to dramatic improvements in computational performance. Second, an implementation in Split-C, which uses highly efficient activemessages to implement shared memory communication, reduces communication overhead by an order of magnitude. Taken together, these two developments portend great promise for the development of efficient large scale simulations using portable, high level parallel programming languages.}
}
@article{GAO2022509,
title = {An integrated simulation method for PVSS parametric design using multi-objective optimization},
journal = {Frontiers of Architectural Research},
volume = {11},
number = {3},
pages = {509-526},
year = {2022},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2021.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S209526352100087X},
author = {Qing Gao and Ying Yang and Qian Wang},
keywords = {Integrated simulation, PV shading System, Parametric design, Multi-objective optimization, Thermal-daylighting balance},
abstract = {An adequate strategy for achieving energy efficiency when designing a photovoltaic shading system (PVSS) shall find an equilibrium between sunlight heat gain and daylight transmittances through effective analysis tools in a building's early design phases. However, traditional simulation methods are either time-consuming or lacking architectonical thinking. This paper proposes a new method for architects to integrate thermal and daylighting performance by using parametric script modelling and optimize their balance with multi-objective optimization (MOO) algorithm in PVSS design. A case study was conducted to demonstrate the workflow of proposed integrated simulation method in PVSS design, and further compared the results with that of three single-objective optimizations under the same design requirement. The findings show that the integrated framework is a feasible method for PVSS design and can be extended into the design of other advance shading system or building integrated photovoltaic.}
}
@incollection{TEZDUYAR199521,
title = { - Massively parallel finite element computation of 3d flows - mesh update strategies in computation of moving boundaries and interfaces°†},
editor = {A. Ecer and J. Hauser and P. Leca and J. Periaux},
booktitle = {Parallel Computational Fluid Dynamics 1993},
publisher = {North-Holland},
address = {Amsterdam},
pages = {21-30},
year = {1995},
isbn = {978-0-444-81999-4},
doi = {https://doi.org/10.1016/B978-044481999-4/50131-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780444819994501316},
author = {T. Tezduyar and S. Aliabadi and M. Behr and A. Johnson and S. Mittal},
abstract = {Publisher Summary
This chapter describes the parallel implicit finite element computations of compressible and incompressible flows with the Connection Machine (CM)—CM-200 and CM-5. The parallel implementations are based on the assumption that the mesh is unstructured. The computations of flow problems involving moving boundaries and interfaces are achieved by using the deformable-spatial-domain or stabilized space-time method. In this method, with special mesh update strategies, the frequency of remeshing is minimized. This avoids the projection errors generated by remeshing and also avoids the cost associated with repeated mesh generation and parallelization setup. This method and its implementation on the massively parallel supercomputers provide a new capability to solve a large class of practical problems involving free surfaces, two-liquid interfaces, and fluid-structure interactions. Now 3D incompressible flow computations can be carried out at sustained speeds of up to 7.0 GigaFLOPS on the CM-5. The 3D compressible flow computations are carried out at sustained speeds of up to 12.2 GigaFLOPS on the CM-5. This parallel performance is significant in the sense that now there is a new level of computational capability in finite element solution of 3D flow problems. Several 3D flow problems are solved using these parallel and update mesh strategies. The chapter discusses the computation of incompressible flow occurring between two concentric cylinders, sloshing in a liquid-filled container subjected to vertical vibrations, and supersonic flow past a delta-wing.}
}
@article{FORCAEL2025100667,
title = {Enhanced robotic cross-laminated timber panel assembly process},
journal = {Developments in the Built Environment},
volume = {22},
pages = {100667},
year = {2025},
issn = {2666-1659},
doi = {https://doi.org/10.1016/j.dibe.2025.100667},
url = {https://www.sciencedirect.com/science/article/pii/S2666165925000675},
author = {Eric Forcael and Ramón Mata and Bryan González and Alexander Opazo-Vega and Rodrigo García-Alvarado and Marcelo González and Eduardo Núñez and Javiera Padilla},
keywords = {Built environment construction, Robotic timber construction, Robotic fabrication, Non-standard timber structures, Computational design},
abstract = {The use of Cross-Laminated Timber (CLT) panels in construction is often constrained by their weight, making handling and installation challenging. These limitations frequently result in on-site planning and manual assembly, increasing risks and inefficiencies. This study proposes an integrated framework that combines Building Information Modeling (BIM), discrete event simulation, and robotic assembly to optimize the installation of CLT structures within the built environment. By leveraging these technologies, the methodology addresses material handling challenges while enhancing construction efficiency and adaptability to urban and prefabricated settings. Numerical simulations and robotic assembly experimental tests were conducted to evaluate the framework’s performance. Results demonstrate an improvement in assembly efficiency, reducing both accident risks and installation time compared to manual methods. Strong agreement between numerical and experimental findings underscores the potential of computational tools in advancing automated construction practices. This research provides actionable recommendations to promote the broader adoption of automated processes in CLT construction, contributing to safer, more efficient, and sustainable building practices within the evolving built environment.}
}
@article{CIULLO2021100349,
title = {A framework for building climate storylines based on downward counterfactuals: The case of the European Union Solidarity fund},
journal = {Climate Risk Management},
volume = {33},
pages = {100349},
year = {2021},
issn = {2212-0963},
doi = {https://doi.org/10.1016/j.crm.2021.100349},
url = {https://www.sciencedirect.com/science/article/pii/S2212096321000784},
author = {Alessio Ciullo and Olivia Martius and Eric Strobl and David N. Bresch},
keywords = {Climate storylines, Downward counterfactuals, European Union Solidarity Fund},
abstract = {Recent research introduced the concept of climate storylines as an alternative approach to estimate climate impact and better deal with uncertainties. A climate storyline is an event-based approach which aims at building “physically self-consistent unfolding of past events, or of plausible future events or pathways”. As such, climate storylines may profit from downward counterfactual thinking, which aims at analyzing how past events could have been worse. Notwithstanding the various applications of downward counterfactual thinking in the natural risk management literature, no study relates this with the climate storyline approach. The main goal of this paper is thus to introduce a framework that supports the development of climate storylines from downward counterfactuals. The framework is event-oriented, it focuses on impact, and it is designed to be applied in a participatory fashion. As a proof-of-concept application, we study the impact of tropical cyclones on the European Union Solidarity Fund (EUSF) without conducting a participatory analysis. Tropical cyclones represent a serious threat for the European outermost regions, and their impact to the EUSF capital availability has never been studied. We find that payouts due to tropical cyclones can hamper a recovery of the fund if large payouts concurrently occur in mainland Europe. To avoid this also considering future changes, an increase in capitalization up to 90 % percent may be required.}
}
@article{KUAI2021150,
title = {Multi-source brain computing with systematic fusion for smart health},
journal = {Information Fusion},
volume = {75},
pages = {150-167},
year = {2021},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2021.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S1566253521000646},
author = {Hongzhi Kuai and Ning Zhong and Jianhui Chen and Yang Yang and Xiaofei Zhang and Peipeng Liang and Kazuyuki Imamura and Lianfang Ma and Haiyuan Wang},
keywords = {Brain computing, Brain informatics, Data-Brain, Smart health, Systematic fusion, Intelligence system},
abstract = {With the progress of artificial intelligence, big data and functional neuroimaging technologies, brain computing has rapidly advanced our understanding of brain intelligence and brain disorders. We argue that existing data analytical methods have become insufficient for brain computing when dealing with multiple brain big data sources, because such methods mainly focus on flattening strategies and fail to work well for systematic understanding of the constituent elements of cognition, emotion and disease, as well as the intra- and inter-relations within and among themselves. To address this problem, we present in this paper a novel multi-source brain computing platform by Data-Brain driven systematic fusion. First, we formalize a series of behaviors surrounding the Brain Informatics-based investigation process, and present a conceptual model to systematically represent content and context of functional neuroimaging data. Then, we propose the systematic brain computing framework with multi-aspect fusion and inference to understand brain specificity and give uncertainty quantification, as well as its inspiration and applications for translational studies on brain health. In particular, a graph matching-based task search algorithm is introduced to help systematic experimental design and data sampling with multiple cognitive tasks. The study increases the interpretability and transparency of brain computing findings by inferring and testing multiple hypotheses taking into consideration the effect of evidence combination. Finally, multiple sources of knowledge (K), information (I) and data (D) are driven by a KID loop as the thinking space to inspire never-ending learning and multi-dimensional interactions in the connected social–cyber–physical spaces. Experimental results have demonstrated the efficacy of the proposed brain computing method with systematic fusion.}
}
@article{SCHUELLER1997197,
title = {A state-of-the-art report on computational stochastic mechanics},
journal = {Probabilistic Engineering Mechanics},
volume = {12},
number = {4},
pages = {197-321},
year = {1997},
note = {A State-of-the-Art Report on Computational Stochastic Mechanics},
issn = {0266-8920},
doi = {https://doi.org/10.1016/S0266-8920(97)00003-9},
url = {https://www.sciencedirect.com/science/article/pii/S0266892097000039},
author = {G.I. Schuëller}
}
@article{SUYOTO2015328,
title = {Parametric Approach as a Tool for Decision-making in Planning and Design Process. Case study: Office Tower in Kebayoran Lama},
journal = {Procedia - Social and Behavioral Sciences},
volume = {184},
pages = {328-337},
year = {2015},
note = {REFLECTIONS ON CREATIVITY: PUBLIC ENGAGEMENT AND THE MAKING OF PLACE},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2015.05.098},
url = {https://www.sciencedirect.com/science/article/pii/S1877042815033479},
author = {William Suyoto and Aswin Indraprastha and Heru W. Purbo},
keywords = {parametric design, discrete method, office tower, building modeling},
abstract = {This study offers discrete method in parametric design to solve problems during design process (programming, site planning, massing, structure planning, and facade planning). This study is applied in the design of office tower in Kebayoran Lama, Jakarta. The objective of the study is to explore the uses of parametric design method, yet, maintains its time feasibility. The result of the study is a method for planning and design that is more advantageous than the conventional ones in terms of simultaneous, coordinated and accountable. This method enables designer to do many iterations and monitor changes during the design process. However, the method needs a higher skill in logical thinking during the process, which demands time.}
}
@article{GROUT2014680,
title = {Taking Computer Science and Programming into Schools: The Glyndŵr/BCS Turing Project},
journal = {Procedia - Social and Behavioral Sciences},
volume = {141},
pages = {680-685},
year = {2014},
note = {4th World Conference on Learning Teaching and Educational Leadership (WCLTA-2013)},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2014.05.119},
url = {https://www.sciencedirect.com/science/article/pii/S1877042814035435},
author = {Vic Grout and Nigel Houlden},
keywords = {Computer science, programming, computing curriculum, teacher training, British Computer Society (BCS) Academy, Computing At School (CAS), Council of Professors and Heads of Computing (CPHC), Lego NXT Mindstorm, Raspberry Pi, Robot C, Scratch, Picoboards ;},
abstract = {2012 and 2013 have been challenging years for Computer Science (CS) education in the UK. After decades of national neglect, there has been a sudden impetus to reintroduce CS into the 11-16 age school curriculums. Immediate obstacles include a generation of children with no CS background and an estimated need for 20,000 new CS teachers - existing UK IT teachers being insufficiently qualified and experienced. The Computing at School (CAS) movement has been instrumental in this quantum transition from an IT to Computing syllabus, as have the British Computer Society (BCS), leading UK universities and a number of major international technology companies, including Microsoft, Google, IBM, British Telecom and Facebook.This paper discusses the background to this position and the progress being made to address these challenges. It describes, in particular, the work of the BCS-funded Glyndwr University ‘Turing Project’ in introducing Welsh high-school students and staff to high-level programming and ‘computational thinking’. The Turing Project uses an innovative combination of Lego NXT Mindstorm robots, Raspberry Pi computers and PicoBoard hardware together with the Robot C and Scratch programming platforms. The paper discusses initial objectives and the general approach, describes focused delivery across different age groups and ability ranges and presents results and analysis demonstrating the effectiveness of the programme. Lessons learnt and future directions are considered in conclusion.}
}
@incollection{CAPPAI2024804,
title = {Molecular Dynamics Simulations of Thermal Transport in Solid State Systems},
editor = {Manuel Yáñez and Russell J. Boyd},
booktitle = {Comprehensive Computational Chemistry (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {804-820},
year = {2024},
isbn = {978-0-12-823256-9},
doi = {https://doi.org/10.1016/B978-0-12-821978-2.00095-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128219782000957},
author = {Antonio Cappai and Claudio Melis and Luciano Colombo and Riccardo Dettori},
keywords = {Atomistic simulations, Classical molecular dynamics, Computational methods, Harmonic and anharmonic vibrational properties, Lattice thermal conductivity, Nanoscale thermal transport, Non-equilibrium thermodynamics, Thermal management, Thermal sciences, Thermoelectricity},
abstract = {In this chapter, we provide a synoptic review of the theoretical/computational approaches currently used to characterize thermal transport at the nanoscale, a topic of paramount importance for several applications and technological thermal management requirements. We focus in particular on the description of the atomistic techniques based on equilibrium (EMD), non-equilibrium (NEMD), and approach to equilibrium (AEMD) molecular dynamics (MD), which allow to efficiently describe relatively large and structurally complex systems with a reduced computational cost as compared to fully "ab-initio" techniques. We describe the theoretical background for each simulation strategy, as well as their implementation in state-of-the-art MD codes by underlying their intrinsic limitations and providing strategies to control some of them. We finally perform a series of benchmark calculations on bulk crystalline silicon by showing that the estimated thermal conductivity is weakly dependent on the specific strategy actually employed, while the overall computational cost is largely dependent on it.}
}
@article{GUNNING2021169,
title = {Brain-based mechanisms of late-life depression: Implications for novel interventions},
journal = {Seminars in Cell & Developmental Biology},
volume = {116},
pages = {169-179},
year = {2021},
note = {Special Issue: Myelin edited by Gonçalo Castelo-Branco and Roman Chrast / Special issue: Aging in the nervous system edited by Mara Mather},
issn = {1084-9521},
doi = {https://doi.org/10.1016/j.semcdb.2021.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1084952121001117},
author = {Faith M. Gunning and Lauren E. Oberlin and Maddy Schier and Lindsay W. Victoria},
keywords = {Aging, Depression, Functional connectivity, White matter, Apathy, Executive function},
abstract = {Late-life depression (LLD) is a particularly debilitating illness. Older adults suffering from depression commonly experience poor outcomes in response to antidepressant treatments, medical comorbidities, and declines in daily functioning. This review aims to further our understanding of the brain network dysfunctions underlying LLD that contribute to disrupted cognitive and affective processes and corresponding clinical manifestations. We provide an overview of a network model of LLD that integrates the salience network, the default mode network (DMN) and the executive control network (ECN). We discuss the brain-based structural and functional mechanisms of LLD with an emphasis on their link to clinical subtypes that often fail to respond to available treatments. Understanding the brain networks that underlie these disrupted processes can inform the development of targeted interventions for LLD. We propose behavioral, cognitive, or computational approaches to identifying novel, personalized interventions that may more effectively target the key cognitive and affective symptoms of LLD.}
}
@article{TWORZYDLO199387,
title = {Towards an automated environment in computational mechanics},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {104},
number = {1},
pages = {87-143},
year = {1993},
issn = {0045-7825},
doi = {https://doi.org/10.1016/0045-7825(93)90208-F},
url = {https://www.sciencedirect.com/science/article/pii/004578259390208F},
author = {W.W. Tworzydlo and J.T. Oden},
abstract = {Effective methods leading to an automated, computer-based solution of complex engineering design problems are studied in this paper. In particular, methods of automation of the finite element analyses are of primary interest here. This includes algorithmic approaches, based on error estimation. adaptivity and smart algorithms, as well as heuristic approaches based on methods of knowledge engineering. A computational environment, which interactively couples h-p adaptive finite element methods with object oriented programming and expert system tools, is presented. Several examples illustrate the merit and potential of the approaches studied here and confirm the feasibility of developing fully automated design environments.}
}
@incollection{LI2014249,
title = {Chapter 8 - Image Processing at Your Fingertips: The New Horizon of Mobile Imaging},
editor = {Joel Trussell and Anuj Srivastava and Amit K. Roy-Chowdhury and Ankur Srivastava and Patrick A. Naylor and Rama Chellappa and Sergios Theodoridis},
series = {Academic Press Library in Signal Processing},
publisher = {Elsevier},
volume = {4},
pages = {249-264},
year = {2014},
booktitle = {Academic Press Library in Signal Processing: Volume 4},
issn = {2351-9819},
doi = {https://doi.org/10.1016/B978-0-12-396501-1.00008-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012396501100008X},
author = {Xin Li},
keywords = {Mobile imaging, Mobile computing, Interactive image processing, Human network interaction},
abstract = {In this chapter, we briefly review the history of mobile imaging and current trend of mobile computing—namely interacting with a computer without an interface. Specifically, we highlight a list of image processing problems at fingertips: intelligent image acquisition, interactive image matting, dynamic image mosaicing and supervised image restoration. The unifying theme is how human interaction can reshape our thinking of conventional image processing algorithms. Several promising applications related to fingertip image processing are discussed at the end.}
}
@article{DUENASDIEZ2019514,
title = {How Chemistry Computes: Language Recognition by Non-Biochemical Chemical Automata. From Finite Automata to Turing Machines},
journal = {iScience},
volume = {19},
pages = {514-526},
year = {2019},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2019.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S2589004219302858},
author = {Marta Dueñas-Díez and Juan Pérez-Mercader},
keywords = {Chemistry, Chemical Reaction, Computer Science, Theory of Computation},
abstract = {Summary
Every problem in computing can be cast as decision problems of whether strings are in a language or not. Computations and language recognition are carried out by three classes of automata, the most complex of which is the Turing machine. Living systems compute using biochemistry; in the artificial, computation today is mostly electronic. Thinking of chemical reactions as molecular recognition machines, and without using biochemistry, we realize one automaton in each class by means of one-pot, table top chemical reactors: from the simplest, Finite automata, to the most complex, Turing machines. Language acceptance/rejection criteria by automata can be formulated using energy considerations. Our Turing machine uses the Belousov-Zhabotinsky chemical reaction and checks the same symbol in an Avogadro′s number of processors. Our findings have implications for chemical and general computing, artificial intelligence, bioengineering, the study of the origin and presence of life on other planets, and for artificial biology.}
}
@article{TRAGER20241555,
title = {The human touch: Utilizing AlphaFold 3 to analyze structures of endogenous metabolons},
journal = {Structure},
volume = {32},
number = {10},
pages = {1555-1562},
year = {2024},
issn = {0969-2126},
doi = {https://doi.org/10.1016/j.str.2024.08.018},
url = {https://www.sciencedirect.com/science/article/pii/S0969212624003356},
author = {Toni K. Träger and Christian Tüting and Panagiotis L. Kastritis},
abstract = {Summary
Computational structural biology aims to accurately predict biomolecular complexes with AlphaFold 3 spearheading the field. However, challenges loom for structural analysis, especially when complex assemblies such as the pyruvate dehydrogenase complex (PDHc), which catalyzes the link reaction in cellular respiration, are studied. PDHc subcomplexes are challenging to predict, particularly interactions involving weaker, lower-affinity subcomplexes. Supervised modeling, i.e., integrative structural biology, will continue to play a role in fine-tuning this type of prediction (e.g., removing clashes, rebuilding loops/disordered regions, and redocking interfaces). 3D analysis of endogenous metabolic complexes continues to require, in addition to AI, precise and multi-faceted interrogation methods.}
}
@article{JIANG2024122157,
title = {Explicit potential function and fast algorithm for computing potentials in α×β conic surface resistor network},
journal = {Expert Systems with Applications},
volume = {238},
pages = {122157},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122157},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423026593},
author = {Xiaoyu Jiang and Gaojun Zhang and Yanpeng Zheng and Zhaolin Jiang},
keywords = {Resistor network, Chebyshev polynomials, DST-IV, Fast algorithm},
abstract = {Resistor network research is of great importance, yet many resistor networks and their large-scale fast computations have not received sufficient attention. This paper proposes a new resistor network with idiosyncratic shape, i.e., a α×β conic surface (CS) resistor network that resembles the upper part of a three-dimensional Dirac function. Utilizing the Recursion Transform (RT-V) method of Tan, a recursive matrix equation model is constructed based on Kirchhoff’s law and nodal voltages, which contains the modified tridiagonal Toeplitz matrix. By using the orthogonal matrix transformation, the eigenvalues and eigenvectors of the modified tridiagonal Toeplitz are obtained. The discrete sine transform of the fourth type (DST−IV) is utilized to solve node voltages, while the explicit potential function is represented by the Chebyshev polynomials of the second kind. In addition, explicit potential functions for some special cases are provided, and the potential distribution is illustrated using dynamic three-dimensional graph. To achieve a rapid calculation of the potential, a fast algorithm based on the multiplication of DST-IV with a vector is proposed. In the end, analysis of computational efficiency for the explicit potential function and the fast algorithm are shown.}
}
@article{DORESWAMY2023,
title = {Attributes That Influence Human Decision-Making in Complex Health Services: Scoping Review},
journal = {JMIR Human Factors},
volume = {10},
year = {2023},
issn = {2292-9495},
doi = {https://doi.org/10.2196/46490},
url = {https://www.sciencedirect.com/science/article/pii/S2292949523001153},
author = {Nandini Doreswamy and Louise Horstmanshof},
keywords = {human attributes, human decision-making, rationality, rational decision-making, health policy, health regulation, health services, },
abstract = {Background
Humans currently dominate decision-making in both clinical health services and complex health services such as health policy and health regulation. Many assumptions inherent in health service models today are underpinned by Ramsey’s Expected Utility Theory, a prominent theory in the field of economics that is rooted in rationality. Rational, evidence-based metrics currently dominate the culture of decision-making in health policy and regulation. However, as the COVID-19 pandemic has shown, rational metrics alone may not suffice in making better policy and regulatory decisions. There are ethical and moral considerations and other complex factors that cannot be reduced to evidence-based rationality alone. Therefore, this scoping review was undertaken to identify and map the attributes that influence human decision-making in complex health services.
Objective
The objective is to identify and map the attributes that influence human decision-making in complex health services that have been reported in the peer-reviewed literature.
Methods
This scoping review was designed to answer the following research question: what attributes have been reported in the literature that influence human decision-making in complex health services? A clear, reproducible methodology is provided. It is reported in accordance with the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews) standards and a recognized framework. As the topic of interest merited broad review to scope and understand literature from a holistic viewpoint, a scoping review of literature was appropriate here. Inclusion and exclusion criteria were developed, and a database search undertaken within 4 search systems—ProQuest, Scopus, PubMed, and Web of Science.
Results
The results span 46 years, from 1976 to 2022. A total of 167 papers were identified. After removing duplicates, 81 papers remained. Of these, 77 papers were excluded based on the inclusion and exclusion criteria. The remaining 4 papers were found to be relevant. Citation tracking was undertaken, identifying 4 more relevant papers. Thus, a total of 8 papers were included. These papers were reviewed in detail to identify the human attributes mentioned and count the frequency of mentions. A thematic analysis was conducted to identify the themes.
Conclusions
The results highlight key themes that underline the complex and nuanced nature of human decision-making. The results suggest that rationality is entrenched and may influence the lexicon of our thinking about decision-making. The results also highlight the counter narrative of decision-making underpinned by uniquely human attributes. This may have ramifications for decision-making in complex health services today. The review itself takes a rational approach, and the methods used were suited to this.
International Registered Report Identifier (IRRID)
RR2-10.2196/42353}
}
@article{GUPTA20062290,
title = {Towards a new paradigm for innovative training methods for capacity building in remote sensing},
journal = {Advances in Space Research},
volume = {38},
number = {10},
pages = {2290-2298},
year = {2006},
note = {Remote Sensing of Oceanographic Processes and Land Surfaces; Space Science Education and Outreach},
issn = {0273-1177},
doi = {https://doi.org/10.1016/j.asr.2006.06.017},
url = {https://www.sciencedirect.com/science/article/pii/S0273117706004285},
author = {R.K. Gupta and P.M. Bala Manikavelu and D. Vijayan and T.S. Prasad},
keywords = {Thinking curricula, Innovative training methods, Capacity building, Remote sensing},
abstract = {Everybody uses a bulb to illustrate an idea but nobody shows where the current comes from. Majority of remote sensing user community comes from natural and social sciences domain while remote sensing technology evolves from physical and engineering sciences. To ensure inculcation and internalization of remote sensing technology by application/resource scientists, trainer needs to transfer physical and engineering concepts in geometric manner. Here, the steering for the transfer of knowledge (facts, procedures, concepts and principles) and skills (thinking, acting, reacting and interacting) needs to take the trainees from Known to Unknown, Concrete to Abstract, Observation to Theory and Simple to Complex. In the initial stage of training/education, experiential learning by instructor led exploring of thematic details in false colour composite (FCC) as well as in individual black and white spectral band(s) imagery by trainees not only creates interest, confidence build-up and orientation towards purposeful learning but also helps them to overcome their inhibitions towards the physical and engineering basal. The methodology to be adopted has to inculcate productive learning, emphasizing more on thinking and trial and error aspects as opposed to reproductive learning based dominantly on being told and imitation. The delivery by trainer needs to ensure dynamic, stimulating and effective discussions through deluging questions pertaining to analysis, synthesis and evaluation nature. This would ensure proactive participation from trainees. Hands-on module leads to creative concretization of concepts. To keep the trainees inspired to learn in an auto mode during post-training period, they need to consciously swim in the current and emerging knowledge pool during training programme. This is achieved through assignment of seminar delivery task to the trainees. During the delivery of seminar, peers and co-trainees drive the trainee to communicate the seminar content not only in what but also in how and why mode. The interest culminated in this manner keeps the entropy of the trainee minimized even during post-training professional life. So, such germinated trainee would always generate positive induction among colleagues; thus, helping in realizing multiplier effect. Based upon above thought process(es), the paper discusses the concept of “thinking curricula” and associated cares needed in training deliveries.}
}
@article{PUDANE2017517,
title = {Human Emotional Behavior Simulation in Intelligent Agents: Processes and Architecture},
journal = {Procedia Computer Science},
volume = {104},
pages = {517-524},
year = {2017},
note = {ICTE 2016, Riga Technical University, Latvia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.01.167},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917301680},
author = {Mara Pudane and Egons Lavendelis and Michael A. Radin},
keywords = {Affective agents, Emotive agents, Human behavior simulation, Agent internal architecture},
abstract = {The paper describes and discusses processes needed for human emotional behaviour simulation, in particular, emotion incorporation into rational thinking, as well as presents corresponding agent architecture. Such system would enable various application fields, perhaps one of the most important being enhancing smart devices with emotions. Decreasing frequency of social contact has become an urgent issue, particularly among young people. Emotional and social intelligence are however highly desired set of skills which is impossible to develop without interacting with others. Although this problem has been acknowledged, and there are some efforts to facilitate social contact, e.g., by augmented virtual reality games, that is still not enough. There is a need to develop environment that would allow learning exactly social and emotional skills. This on-going research aims at developing intelligent agents that are able to express and incorporate affects into rational processes.}
}
@article{DELEERSNYDER2024105602,
title = {A multidimensional AI-trained correction to the 1D approximate model for Airborne TDEM sensing},
journal = {Computers & Geosciences},
volume = {188},
pages = {105602},
year = {2024},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2024.105602},
url = {https://www.sciencedirect.com/science/article/pii/S0098300424000852},
author = {Wouter Deleersnyder and David Dudal and Thomas Hermans},
keywords = {Forward modelling, Machine Learning, Surrogate modelling, Electromagnetics, Airborne},
abstract = {The computational resources required to solve the full 3D inversion of time-domain electromagnetic data are immense. To overcome the time-consuming 3D simulations, we construct a surrogate model, more precisely, a data-driven statistical model to replace the 3D simulations. It is trained on 3D data and predicts the approximate output much faster. We construct a surrogate model that predicts the discrepancy between a 1D subsurface model and a deviation of the 1D assumption. The latter response is fastly computable with a semi-analytical 1D forward model. We exemplify the approach on a two-layered case. The results are encouraging even with few training samples. Given the computational cost related to the 3D simulations, there are limitations in the number of training samples that can be generated. In addition, certain applications require a wide range of parameters to be sampled, such as the electrical conductivity parameters in a saltwater intrusion case. The challenge of this work is achieving the best possible accuracy with only a few thousand samples. We propose to view the performance in terms of learning gain, representing the gain from the surrogate model whilst still acknowledging a residual discrepancy. Our works open new avenues for effectively simulating 3D TDEM data.}
}
@incollection{TONDEUR202424,
title = {Chapter 3 - Quality improvement movements},
editor = {Yves Tondeur},
booktitle = {Sustainable Quality Improvements for Isotope Dilution in Molecular Ultratrace Analyses},
publisher = {Elsevier},
pages = {24-70},
year = {2024},
isbn = {978-0-443-29034-3},
doi = {https://doi.org/10.1016/B978-0-443-29034-3.00022-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443290343000223},
author = {Yves Tondeur},
keywords = {Accreditation & technology-in-use, Commitment-based approach, Empirical vs. rational methods, Isomer selectivity, Isotope dilution, Known & documented quality, Legislating competition, Methods innovation rule, Performance assessment, Precision & trueness, Purpose of quality control samples, Quick fixes vs. fundamental solution, Structural conflicts},
abstract = {Emerging over recent years is the notion that quality improvements are hard to come by, when in fact, countless opportunities to integrate new developments are overlooked primarily because of the restrictive ways in which analytical protocols have been written and enforced, or the misconceptions about their function. A point of instability was reached. The manifestation of a quality malaise can be seen through the efforts by many to enhance quality by attempting to transfer the responsibility for quality back to those doing the work and to those who need the work products. The testing industry is now forced into abandoning old formal structures, mental models, and behaviors. Realigning our thinking, discovering the limits, and identifying the structural conflicts are essential if one wants to improve quality. This chapter makes clear that doing the same thing than before better is not enough, or to solely implement quick fixes can be wasteful; somehow, we need to ensure the application of the method coevolves with its environment. As chemists, what can we do?}
}
@incollection{JONATHANCOHEN1986597,
title = {Semantics and the Computational Metaphor},
editor = {Ruth {Barcan Marcus} and Georg J.W. Dorn and Paul Weingartner},
series = {Studies in Logic and the Foundations of Mathematics},
publisher = {Elsevier},
volume = {114},
pages = {597-621},
year = {1986},
booktitle = {Logic, Methodology and Philosophy of Science VII},
issn = {0049-237X},
doi = {https://doi.org/10.1016/S0049-237X(09)70715-3},
url = {https://www.sciencedirect.com/science/article/pii/S0049237X09707153},
author = {L. {Jonathan Cohen}},
abstract = {Publisher Summary
The computational hypothesis, irrespective of any particular experimental outcomes, has been claimed to carry some rather specific implications for the semantics of natural language. The purpose of the chapter is to criticize that claim. The chapter is occupied solely with the implications of the computational hypothesis and not at all with its merits as a strategy for scientific research. The computational hypothesis, which has come to dominate cognitive psychology, in the sense that it expects the computational analogy to be more successful than any other in generating a variety of theories that are not only testable, but also worthwhile testing, about how particular mental processes operate. Memory, visual imagery, concept formation, problem solving, speech comprehension, etc., are treated as fields of research in which experiments may be used to test theories that such-and-such a combination of iteration, recursion, chunking, horizontal searching, vertical searching, geometrical coding, linguistic coding or other mode of information-processing is at work. Commonly the researcher first constructs or sketches or surveys a suitably wide range of computer-programs (implementable on a suitably wide range of computer-architectures) that, when compared with the mental process in question, would provide analogous outputs for analogous inputs. He then devises experiments on the performance of human subjects to determine, which of these computer-simulations is closest to the kind of explanatory mechanism required by the results of the experiments.}
}
@article{MCGOWEN2010169,
title = {Metaphor or Met-Before? The effects of previouos experience on practice and theory of learning mathematics},
journal = {The Journal of Mathematical Behavior},
volume = {29},
number = {3},
pages = {169-179},
year = {2010},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2010.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0732312310000404},
author = {Mercedes A. McGowen and David O. Tall},
keywords = {Metaphor, Met-before, Epistemological obstacle, Embodiment, Local straightness},
abstract = {While the general notion of ‘metaphor’ may offer a thoughtful analysis of the nature of mathematical thinking, this paper suggests that it is even more important to take into account the particular mental structures available to the individual that have been built from experience that the individual has ‘met-before.’ The notion of ‘met-before’ offers not only a principle to analyse the changing meanings in mathematics and the difficulties faced by the learner—which we illustrate by the problematic case of the minus sign—it can also be used to analyse the met-befores of mathematicians, mathematics educators and those who develop theories of learning to reveal implicit assumptions that support our thinking in some ways and act as impediments in others.}
}
@article{FINGER2025101535,
title = {When kids juggle it all: Biliteracy instruction and the development of discourse connectedness in L1 and L2 writing},
journal = {Cognitive Development},
volume = {73},
pages = {101535},
year = {2025},
issn = {0885-2014},
doi = {https://doi.org/10.1016/j.cogdev.2024.101535},
url = {https://www.sciencedirect.com/science/article/pii/S0885201424001205},
author = {Ingrid Finger and Cristiane Ely Lemke and Larissa da Silva Cury and Natália Bezerra Mota and Janaina Weissheimer},
keywords = {Bilingual writing development, Discourse connectedness, Graph analysis, Narrative writing},
abstract = {The present longitudinal study explored how bilingual educational contexts shape children's cognitive and linguistic development. Its main goal was to investigate the development of discourse connectedness (measured by long-range connectedness - LSC) in written narratives in Portuguese (L1) and English (L2) by 78 children of a bilingual school in Brazil within a year span (from 2021 to 2022). Participants created a narrative in their L1 or L2 based on a sequence of five images, which were analyzed with the computational tool SpeechGraphs (Mota et al., 2014). Connectedness scores were expected to vary as a function of Language (L1, L2) and of Year of data collection (Time 1, Time 2), favoring, respectively, the L1 and Time 2. The results confirmed our hypotheses, with long-range recurrence (LSC) scores in the L1 narratives higher than in the L2 at both times of data collection. In addition, the longitudinal analysis revealed higher connectedness scores for narratives written in Time 2 in both languages. Overall, our findings indicate that the children's performance in terms of connectedness progressed in a parallel way in the two languages during the school years, with an expected advantage for the narratives written in their dominant language. In addition, they highlight the potential of using SpeechGraphs - a cost-effective, non-invasive computational tool - to analyze children's use of two prestige languages in a particular bilingual educational context.}
}
@article{ORAN1992251,
title = {Reactive-flow computations on a massively parallel computer},
journal = {Fluid Dynamics Research},
volume = {10},
number = {4},
pages = {251-271},
year = {1992},
issn = {0169-5983},
doi = {https://doi.org/10.1016/0169-5983(92)90025-R},
url = {https://www.sciencedirect.com/science/article/pii/016959839290025R},
author = {Elaine S. Oran and Jay P. Boris and C.Richard DeVore},
abstract = {Results are described of recent research and model developments for performing large-scale multidimensional compressible reacting-flow computations on the Connection Machine, a very fine-grained, parallel computer capable of multigigaflop performance. We are interested both in having general-purpose computer programs for routine production computations and in evaluating the architecture of the computer for a wide range of computational fluid dynamics and reacting-flow applications. We describe the hurdles involved in rethinking the structure and the algorithms to best suit this kind of computer, provide some relative timings for different programs, and describe ways of dealing with special constraints (such as periodic boundary conditions) imposed by the architecture. Finally, representative results are presented for several reacting and nonreacting computations, including the development and propagation of a spark in a hydrogen-oxygen mixture, an imploding detonation, and the generation of beam-channel turbulence.}
}
@incollection{RUNCO200771,
title = {Chapter 3 - Biological Perspectives on Creativity},
editor = {Mark A. Runco},
booktitle = {Creativity},
publisher = {Academic Press},
address = {Burlington},
pages = {71-113},
year = {2007},
isbn = {978-0-12-602400-5},
doi = {https://doi.org/10.1016/B978-012602400-5/50003-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780126024005500034},
author = {Mark A. Runco},
abstract = {Publisher Summary
This chapter discusses various aspects of biological perspectives on creativity. Some of the research on creativity as of late involves the brain and biological correlates of originality, novelty, and insight. Handedness is sometimes used as an indication of hemispheric dominance or hemisphericity, with right-handed people being compared to left-handed people. There are several reports of left-handed persons outnumbering the right-handed in creative and eminent samples. Hemisphericity and other important brain structures and processes contributing to creative thinking and behavior have been studied with EEG, PET, cerebral blood flow, and MRI techniques. Numerous EEG studies suggest that there are particular brain-wave patterns and brain structures that are associated with creative problem solving, or at least specific phases within the problem solving process. EEGs suggest a complex kind of activity while individuals work on divergent thinking tasks. The complexity disappears when those same individuals work on convergent thinking tasks. It is found that the role of the prefrontal cortex in creative thinking and behavior comes from several sources and uses different methodologies.}
}
@article{IGNUTACIUNCANU2025105459,
title = {Discrete Svelteness: Evaluating flow structures in generative constructal design},
journal = {BioSystems},
volume = {251},
pages = {105459},
year = {2025},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2025.105459},
url = {https://www.sciencedirect.com/science/article/pii/S0303264725000693},
author = {Matei C. Ignuta-Ciuncanu and Ricardo F. Martinez-Botas},
keywords = {Thermal design, Constructal law, Generative design, Evolutionary optimization, Svelteness},
abstract = {Constructal design theory posits that natural and engineered systems evolve toward improved flow efficiency, ensuring maximal access and adaptability. Generative design, in this context, functions as an evolutionary computational framework, leveraging algorithmic methods to simulate processes analogous to natural selection and self-organization. This approach enables the exploration and refinement of complex design spaces, fostering higher modeling resolution and morphological diversity beyond traditional parametric methods. In this study, we introduce Discrete Svelteness (DS), a spatially resolved metric that quantifies geometric efficiency at every point in a domain, addressing the limitations of traditional, global Svelteness measures. We apply this framework to generative designs, specifically area-to-point (ATP), circle-to-point (CTP), and vascular flow (VF) configurations, demonstrating how DS reveals performance differences shaped by local adaptations to the environment and emergent patterns that conventional metrics overlook. Our analysis reveals that DS provides critical design insights aligned with the Constructal Law, which predicts that systems evolve to enhance flow efficiency through increased branching and spatial access. Furthermore, we examine the probability density functions (PDFs) of DS values, identifying distinct power-law and lighter-tailed right-skewed distributions (such as gamma or log-normal) that reflect the statistical signatures of self-organizing, evolutionary systems found in nature. Additionally, we explore the trade-offs and optimization challenges in generative design, showing that increased degrees of freedom lead to more robust, diverse, and high-performing solutions. These dynamics parallel evolutionary processes in biological systems, where adaptability and efficiency emerge from complex interactions between structural constraints and environmental demands. Our findings position DS as a powerful tool for evaluating and guiding the evolution of flow architectures in both natural and engineered systems. By bridging global efficiency metrics with localized refinement, this work advances multi-scale evolutionary constructal design methodologies and offers new insights into the computational modeling of biological self-organization and evolutionary optimization.}
}
@incollection{ZHUGE201655,
title = {4 - The think lens},
editor = {Hai Zhuge},
booktitle = {Multi-Dimensional Summarization in Cyber-Physical Society},
publisher = {Morgan Kaufmann},
pages = {55-65},
year = {2016},
series = {Computer Science Reviews and Trends},
isbn = {978-0-12-803455-2},
doi = {https://doi.org/10.1016/B978-0-12-803455-2.00004-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128034552000044},
author = {Hai Zhuge},
keywords = {Think lens, Semantic lens, models, semantic images, multi-dimensional, semantic link network},
abstract = {The nature of many research problems is about scale and dimension of observation and thinking. Whether the patterns and rules on one scale still hold on the other scale? Whether the patterns and rules on one dimension or some dimensions still hold on the other dimension or some other dimensions? Summarization is also about the scale and the dimension of motivation, representation and thinking. Human eyes can focus on not only a part of a representation but also the whole from a certain distance like the lens of camera. The think lens is a mechanism that can zoom in and out while observing, searching, mapping, analysing, planning, predicting, calculating, reasoning, imaging, and representing patterns through semantic computing on various representations according to some principles and rules. This section presents a concept model of the think lens for realising general summarisation in cyber-physical society.}
}
@article{KOWALSKI2020103693,
title = {Effects of attention training technique on brain function in high- and low-cognitive-attentional syndrome individuals: Regional dynamics before, during, and after a single session of ATT},
journal = {Behaviour Research and Therapy},
volume = {132},
pages = {103693},
year = {2020},
issn = {0005-7967},
doi = {https://doi.org/10.1016/j.brat.2020.103693},
url = {https://www.sciencedirect.com/science/article/pii/S0005796720301479},
author = {Joachim Kowalski and Małgorzata Wierzba and Marek Wypych and Artur Marchewka and Małgorzata Dragan},
keywords = {Attention training technique, Metacognitve therapy, Attention, fMRI, S-REF},
abstract = {Objective
Attention Training Technique (ATT) is a key therapeutic tool in metacognitive therapy. There are numerous studies on the behavioral effects of ATT, however the neural mechanisms at work in the training are yet to be uncovered. To date there have been no controlled fMRI studies of ATT.
Method
We conducted a randomized double-blind controlled study of two groups with varying levels of cognitive-attentional syndrome (CAS). Groups with high (n = 43) and low (n = 46) levels of CAS underwent a single session of ATT or a control condition (CON) in an MRI scanner. Participants underwent resting state functional MRI (rsfMRI) sessions and rumination induction sessions both pre- and post-intervention Functional connectivity analyses and inter-subject correlations analyses were computed. We also collected data on emotion and attention functioning pre- and post-intervention.
Results
We did not observe any behavioral effects of ATT. However, direct comparison between ATT and CON sessions revealed greater inter-subject correlations in almost all hubs belonging to the studied functional networks. Moreover, subjects who received ATT showed diminished connectivity in the fronto-parietal network during ruminations and diminished connectivity of the precuneus with lateral occipital cortices and the intraparietal sulcus in abstract thinking and rsfMRI, respectively. Furthermore, some of the observed effects in functional connectivity and inter-subject correlations were specific to different levels of CAS.
Conclusions
Our results may support a proposed neural mechanism for ATT: disengagement of attention from CAS-type processing in either low- or high-CAS individuals. It is also possible that some neural effects of ATT are specific to individuals with different levels of CAS.}
}
@article{BAILEY20158,
title = {Metacognitive beliefs moderate the relationship between catastrophic misinterpretation and health anxiety},
journal = {Journal of Anxiety Disorders},
volume = {34},
pages = {8-14},
year = {2015},
issn = {0887-6185},
doi = {https://doi.org/10.1016/j.janxdis.2015.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0887618515000791},
author = {Robin Bailey and Adrian Wells},
keywords = {Health anxiety, Metacognition, Catastrophic misinterpretation, Moderation, S-REF model},
abstract = {Catastrophic misinterpretations of bodily symptoms have a central role in cognitive-behavioural models of health anxiety. However, the metacognitive (S-REF) model postulates that psychological disturbance is linked more to beliefs about thinking i.e., metacognition. Equally the relationship between catastrophic misinterpretation and health anxiety should be moderated by metacognition, in particular negative beliefs about the uncontrollability and danger of thinking (MCQNeg). Participants (N=351) completed measures to examine the relationship between these variables. Results indicated positive relationships between metacognition, catastrophic misinterpretation, and health anxiety. Moderation analysis showed that the effect of catastrophic misinterpretations on health anxiety was explained by the proposed interaction with metacognition. Follow-up regression analysis demonstrated the interaction term explained variance in health anxiety when controlling for other variables, and was a stronger unique predictor of health anxiety than catastrophic misinterpretation. Metacognition appears to be an important factor in the relationship between catastrophic misinterpretation and health anxiety, and would have important implications for existing models and treatment.}
}
@incollection{PAGEL201749,
title = {Chapter Four - Testing for Machine Consciousness},
editor = {J.F. Pagel and Philip Kirshtein},
booktitle = {Machine Dreaming and Consciousness},
publisher = {Academic Press},
address = {San Diego},
pages = {49-65},
year = {2017},
isbn = {978-0-12-803720-1},
doi = {https://doi.org/10.1016/B978-0-12-803720-1.00004-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128037201000049},
author = {J.F. Pagel and Philip Kirshtein},
keywords = {Thinking, intelligence, attention, intentionality, volition, self-awareness, artificial intelligence, AI, autonomous entity, Turing Test, Chinese Room Test},
abstract = {Thinking, intelligence, data integration, and attention are aspects of consciousness for which tests have been designed. A short history of the Computer Science field, a description, and an assessment of results obtained to this point for the Turing Test and Chinese Room Test are part of this chapter. Alternative definitions of artificial intelligence are presented. Applied tests for consciousness including those for intelligence, attention, intentionality, volition, and self-awareness are discussed as applied to the assessment of machine systems. Strong AI and the concept of autonomous entities are defined and addressed. The presence of dream-equivalent states is discussed as a potential marker for human-equivalent consciousness.}
}
@article{KIRIMTAY2025111785,
title = {Tau and MAP6 establish labile and stable domains on microtubules},
journal = {iScience},
volume = {28},
number = {3},
pages = {111785},
year = {2025},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2025.111785},
url = {https://www.sciencedirect.com/science/article/pii/S2589004225000446},
author = {Koray Kirimtay and Wenqiang Huang and Xiaohuan Sun and Liang Qiang and Dong V. Wang and Calvin T. Sprouse and Erin M. Craig and Peter W. Baas},
keywords = {Cell Biology, Cellular neuroscience},
abstract = {Summary
We previously documented that individual microtubules in the axons of cultured juvenile rodent neurons consist of a labile domain and a stable domain and that experimental depletion of tau results in selective shortening and partial stabilization of the labile domain. After first confirming these findings in adult axons, we sought to understand the mechanism that accounts for the formation and maintenance of these microtubule domains. We found that fluorescent tau and MAP6 ectopically expressed in RFL-6 fibroblasts predominantly segregate on different microtubules or different domains on the same microtubule, with the tau-rich ones becoming more labile than in control cells and the MAP6-rich ones being more stable than in control cells. These and other experimental findings, which we studied further using computational modeling with tunable parameters, indicate that these two MAPs do not merely bind to pre-existing stable and labile domains but actually create stable and labile domains on microtubules.}
}
@article{SCHULTZ2022104766,
title = {Animacy and the prediction of behaviour},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {140},
pages = {104766},
year = {2022},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2022.104766},
url = {https://www.sciencedirect.com/science/article/pii/S014976342200255X},
author = {Johannes Schultz and Chris D. Frith},
keywords = {Animacy, Action prediction, Goal-directed action, Mentalizing, Theory-of-Mind, Intentions, Economic games, Social cognition},
abstract = {To survive, all animals need to predict what other agents are going to do next. We review neural mechanisms involved in the steps required for this ability. The first step is to determine whether an object is an agent, and if so, how sophisticated it is. This involves brain regions carrying representations of animate agents. The movements of the agent can then be anticipated in the short term based solely on physical constraints. In the longer term, taking into account the agent’s goals and intentions is useful. Observing goal directed behaviour activates the neural action observation network, and predicting future goal directed behaviour is helped by the observer’s own action generating mechanisms. Intentions are critically important in determining actions when interacting with other agents, as several intentions can lie behind an action. Here, interpretation is helped by prior beliefs about the agent and the brain’s mentalising system is engaged. Biologically-constrained computational models of action recognition exist, but equivalent models for understanding intentional agents remain to be developed.}
}
@article{MENG201248,
title = {Extracting linguistic rules from data sets using fuzzy logic and genetic algorithms},
journal = {Neurocomputing},
volume = {78},
number = {1},
pages = {48-54},
year = {2012},
note = {Selected papers from the 8th International Symposium on Neural Networks (ISNN 2011)},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2011.05.029},
url = {https://www.sciencedirect.com/science/article/pii/S0925231211004711},
author = {Dan Meng and Zheng Pei},
keywords = {Computing with Words, Linguistic rules, Fuzzy logic, Genetic algorithms},
abstract = {Linguistic rules in natural language are useful and consistent with human way of thinking. They are very important in multi-criteria decision making due to their interpretability. In this paper, our discussions concentrate on extracting linguistic rules from data sets. In the end, we firstly analyze how to extract complex linguistic data summaries based on fuzzy logic. Then, we formalize linguistic rules based on complex linguistic data summaries, in which, the degree of confidence of linguistic rules from a data set can be explained by linguistic quantifiers and its linguistic truth from the fuzzy logical point of view. In order to obtain a linguistic rule with a higher degree of linguistic truth, a genetic algorithm is used to optimize the number and parameters of membership functions of linguistic values. Computational results show that the proposed method is an alternative method for extracting linguistic rules with linguistic truth from data sets.}
}
@article{CHANG2017160,
title = {Dynamic modeling approaches to characterize the functioning of health systems: A systematic review of the literature},
journal = {Social Science & Medicine},
volume = {194},
pages = {160-167},
year = {2017},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2017.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0277953617305300},
author = {Angela Y. Chang and Osondu Ogbuoji and Rifat Atun and Stéphane Verguet},
keywords = {Health systems, Dynamic modeling, Systems thinking, System dynamics},
abstract = {Universal Health Coverage (UHC) is one of the targets for the United Nations Sustainable Development Goal 3. The impetus for UHC has led to an increased demand for time-sensitive tools to enhance our knowledge of how health systems function and to evaluate impact of system interventions. We define the field of “health system modeling” (HSM) as an area of research where dynamic mathematical models can be designed in order to describe, predict, and quantitatively capture the functioning of health systems. HSM can be used to explore the dynamic relationships among different system components, including organizational design, financing and other resources (such as investments in resources and supply chain management systems) – what we call “inputs” – on access, coverage, and quality of care – what we call “outputs”, toward improved health system “outcomes”, namely increased levels and fairer distributions of population health and financial risk protection. We undertook a systematic review to identify the existing approaches used in HSM. We identified “systems thinking” – a conceptual and qualitative description of the critical interactions within a health system – as an important underlying precursor to HSM, and collated a critical collection of such articles. We then reviewed and categorized articles from two schools of thoughts: “system dynamics” (SD)” and “susceptible-infected-recovered-plus” (SIR+). SD emphasizes the notion of accumulations of stocks in the system, inflows and outflows, and causal feedback structure to predict intended and unintended consequences of policy interventions. The SIR + models link a typical disease transmission model with another that captures certain aspects of the system that impact the outcomes of the main model. These existing methods provide critical insights in informing the design of HSM, and provide a departure point to extend this research agenda. We highlight the opportunity to advance modeling methods to further understand the dynamics between health system inputs and outputs.}
}
@incollection{MONLEZUN2023159,
title = {Chapter 6 - AI+patient safety: adaptive, embedded, intelligent},
editor = {Dominique J. Monlezun},
booktitle = {The Thinking Healthcare System},
publisher = {Academic Press},
pages = {159-182},
year = {2023},
isbn = {978-0-443-18906-7},
doi = {https://doi.org/10.1016/B978-0-443-18906-7.00007-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443189067000076},
author = {Dominique J. Monlezun},
keywords = {Bias, Blockchain, Clinical alarms, Clinical reports, Command center intelligence, Data privacy, Data security, Design thinking, Drug safety, Explainability, Patient safety, Reproducibility},
abstract = {This chapter discusses the AI transformation of patient safety within modern healthcare systems, particularly those digitally extended with telehealth. It carefully details the definitions and debates in patient safety (including the major actors and researchers in this community who share a general critique about the slow to absent sustained, substantive, and equally shared improvements in healthcare systems' safe delivery of patient care). The chapter provides the standard (including WHO) and innovative though still practical conceptualizations of patient safety and then progresses to recent more promising recent advances in human-centered, standardized, and AI-enabled patient safety (including safety as design thinking and system strategy). The chapter illustrates these developments with concrete use cases (in AI-enabled drug safety, clinical reports, and alarms) and augmentation with automation (including embedded, ambient, and command center safety intelligence). The chapter concludes by considering new and growing challenges in AI-driven patient safety (including data security, privacy, bias, and inconsistency) and emerging solutions (including blockchain, bias reduction, reproducibility, explainability, effectiveness, and safety in embedded design).}
}
@article{CAO2020118,
title = {Computational parameter identification of strongest influence on the shear resistance of reinforced concrete beams by fiber reinforcement polymer},
journal = {Structures},
volume = {27},
pages = {118-127},
year = {2020},
issn = {2352-0124},
doi = {https://doi.org/10.1016/j.istruc.2020.05.031},
url = {https://www.sciencedirect.com/science/article/pii/S2352012420302435},
author = {Yan Cao and Qingming Fan and Sadaf {Mahmoudi Azar} and Rayed Alyousef and Salim T. Yousif and Karzan Wakil and Kittisak Jermsittiparsert and Lanh {Si Ho} and Hisham Alabduljabbar and Abdulaziz Alaskar},
keywords = {FRP: reinforced concrete, Shear resistance, Selection procedure, ANFIS},
abstract = {Bars made of fiber reinforcement polymer (FRP) are in common usage for concrete reinforcing instead of steel reinforcing since steel could be affected by corrosion. The concrete beams reinforced by FRP bars have been studied mostly in longitudinal direction without shear reinforcement. The primary objective of this investigation was to design and advance an algorithm for selection procedure of the parameters influence on prediction of shear resistance of reinforced concrete beams by FRP. Six input parameters were used which represent geometric and mechanical properties of the bars as well as shear features. These parameters are: web width, tensile reinforcement depth, ratio of shear and depth, concrete compressive strength, ratio of FRP reinforcement, FRP modulus of elasticity and beam shear resistance. The searching algorithm is based on combination of artificial neural network and fuzzy logic principle or adaptive neuro fuzzy inference system (ANFIS). Based on the obtained results ratio of shear and depth has the strongest influence on the prediction of shear resistance of reinforced concrete beams by FRP. Moreover, combination of tensile reinforcement depth and ratio of shear and depth is the most influential combination of two parameters on the prediction of shear resistance of reinforced concrete beams by FRP. Finally, combination of tensile reinforcement depth, ratio of shear and depth and FRP modulus of elasticity is the most influential combination of three parameters on the prediction of shear resistance of reinforced concrete beams by FRP.}
}
@article{PALIOURA2025105186,
title = {“Storytelling and educational robotics: A scoping review (2004–2024)”},
journal = {Computers & Education},
volume = {225},
pages = {105186},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2024.105186},
url = {https://www.sciencedirect.com/science/article/pii/S0360131524002008},
author = {Maria Palioura and Theodosios Sapounidis},
keywords = {Educational robotics, Storytelling, Scoping review},
abstract = {Storytelling has been used for years in educational practice and Educational Robotics is a rapidly growing field worldwide. Accordingly, researchers have attempted to combine Storytelling and Robotics in education. However, no systematic record exists on this combination. Therefore, we conducted a scoping review of 82 papers out of 5272 articles published in 5 Databases in the last 20 years to map the conducted research so far. In detail: the educational levels and the school subjects in which storytelling and educational robotics are applied, the types of robots used, the duration, the sample size, participants' age and the skills that students may develop through this combination. Additionally, we analyzed, grouped, and presented the tools used for measuring the potential effects of storytelling and educational robotics. Finally, the students' role in the activities was sought. Based on our findings, most interventions mainly addressed preschool and primary school students, the robots used the most are humanoid, most interventions did not exceed 6 h, and the number of participants was less than 20 students. Besides, most interventions tried to develop students’ skills (communication, creativity, collaboration) and attitudes (engagement, motivation, participation) with qualitative tools borrowed from other domains (e.g. psychology, healthcare). This scoping demonstrates a gap in the use of storytelling and educational robotics in secondary and university education and subjects like history, geography, etc. Finally, this combination seems to have the potential to enhance the educational process, but more research is needed to shed light on all the aspects of the combination.}
}
@article{LEE1993255,
title = {Interval computation as deduction in chip},
journal = {The Journal of Logic Programming},
volume = {16},
number = {3},
pages = {255-276},
year = {1993},
issn = {0743-1066},
doi = {https://doi.org/10.1016/0743-1066(93)90045-I},
url = {https://www.sciencedirect.com/science/article/pii/074310669390045I},
author = {J.H.M. Lee and M.H. {Van Emden}},
abstract = {Logic programming realizes the ideal of “computation is deduction,” but not when floating-point numbers are involved. In that respect logic programming languages are as careless as conventional computation: they ignore the fact that floating-point operations are only approximate and that it is not easy to tell how good the approximation is. It is our aim to extend the benefits of logic programming to computation involving floating-point arithmetic. Our starting points are the ideas of Cleary and the CHIP programming language. Cleary proposed a relational form of interval arithmetic that was incorporated in BNR Prolog in such a way that variables already bound can be bound again. In this way the usual logical interpretation of computation no longer holds. In this paper we develop a technique for narrowing intervals that we relate both to Cleary's work and to the constraint-satisfaction techniques of artificial intelligence. We then modify CHIP by allowing domains to be intervals of real numbers. To reduce arithmetic primitives with interval domains, we use our interval narrowing technique as an implementation of the looking-ahead inference rule. We show that the result is a system where answers are logical consequences of a declarative logic program, even when floating-point computations have been used. We believe ours is the first system with this property.}
}
@article{CADART2025113107,
title = {An optimal penalty method for the joint stiffening in beam models of additively manufactured lattice structures},
journal = {International Journal of Solids and Structures},
volume = {306},
pages = {113107},
year = {2025},
issn = {0020-7683},
doi = {https://doi.org/10.1016/j.ijsolstr.2024.113107},
url = {https://www.sciencedirect.com/science/article/pii/S0020768324004669},
author = {T. Cadart and T. Hirschler and S. Bahi and S. Roth and F. Demoly and N. Lebaal},
keywords = {Lattice structure, Beam formulation, Penalty method, Joint stiffening, Optimization, Additive manufacturing, Material jetting},
abstract = {Additive manufacturing is revolutionizing structural design, with lattice structures becoming increasingly prominent due to their superior mechanical properties. However, simulating these structures quickly and accurately using the finite element method (FEM) remains challenging. Recent research has highlighted beam element simulation within FEM as a more efficient alternative to traditional solid FE simulations, achieving similar accuracy with reduced computational resources. However, a significant challenge is managing the lack of rigidity at nodes and the prevalence of low aspect ratio beams. While various methodologies have been proposed to address these issues, there is still a gap in the comprehensive evaluation of their limitations. An optimal node penalization methodology is required to expand the limited range of accurately represented lattice behavior. A preliminary study investigates lattice geometries through comparative analysis of solid and beam FE simulations. Built on this, we developed a methodology suitable to linear, dynamics and nonlinear beam FE simulations, contributing to enhanced computational speed and accuracy. Several lattice structures were printed using material jetting and quasi-static compressive tests were conducted to validate the methodology’s accuracy. The numerical results reveal a good accuracy between the proposed beam FE methodology and the experimental data, offering a better alternative to conventional FEM for energy absorption in terms of computing time.}
}
@article{SHEARER2021,
title = {Foodborne Illness Outbreak Investigation for One Health Postsecondary Education},
journal = {Journal of Microbiology & Biology Education},
volume = {22},
number = {2},
year = {2021},
issn = {1935-7877},
doi = {https://doi.org/10.1128/jmbe.00129-21},
url = {https://www.sciencedirect.com/science/article/pii/S193578772100143X},
author = {Adrienne E. H. Shearer and Kalmia E. Kniel},
keywords = {food safety, investigation, One Health, education, microbiology, public health, escape room, problem-based learning, epidemiology, environment},
abstract = {One Health concepts were incorporated in a foodborne disease outbreak investigation with game features of data presented as visual and manipulative clues. Postsecondary pre-veterinary medicine and animal biosciences students and food science students (n = 319) enrolled in an introductory animal and food sciences course over a 3-year period received a brief introduction to foodborne illness, an outbreak scenario, and investigative tasks to complete individually or in groups.
ABSTRACT
One Health concepts were incorporated in a foodborne disease outbreak investigation with game features of data presented as visual and manipulative clues. Postsecondary pre-veterinary medicine and animal biosciences students and food science students (n = 319) enrolled in an introductory animal and food sciences course over a 3-year period received a brief introduction to foodborne illness, an outbreak scenario, and investigative tasks to complete individually or in groups. Tasks addressed epidemiology, laboratory, environment, traceback, recall, and prevention concepts. Gamification of the exercise involved generation of a numerical code to unlock a combination lock as an indication of successful organization, compilation, and interpretation of data. Students presented investigation findings and responses to critical thought questions on their roles. Student surveys on engagement and self-perceived change in conceptual understanding indicated that nearly all expressed increased understanding of outbreak investigations, safe food production, and environmental water as a transmission vehicle. Volunteered learned concepts indicated enhanced appreciation for the complexity of food safety and interdisciplinary connections. Students enjoyed the exercise (92%) and cited the clues and group interaction among the most enjoyable features. Objective assessment of student conceptual learning with the subset of students who conducted the investigation individually (n = 58) demonstrated significant increase in correct test responses (49% pretest; 76% posttest) after completion of the investigation for all questions combined and across all learning objectives. These data demonstrate the value of a foodborne disease investigation with escape room gamification features for engaging students in One Health concepts and exercising problem-solving, critical thinking, and skills for independent and collaborative work.}
}
@article{BLELLOCH199490,
title = {Parallel solutions to geometric problems in the scan model of computation},
journal = {Journal of Computer and System Sciences},
volume = {48},
number = {1},
pages = {90-115},
year = {1994},
issn = {0022-0000},
doi = {https://doi.org/10.1016/S0022-0000(05)80023-6},
url = {https://www.sciencedirect.com/science/article/pii/S0022000005800236},
author = {Guy E. Blelloch and James J. Little},
abstract = {This paper describes several parallel algorithms that solve geometric problems. The algorithms are based on a vector model of computation-the scan model. The purpose of this paper is both to show how the model can be used and to formulate a set of practical algorithms. The scan model is based on a small set of operations on vectors of atomic values. It differs from the P-RAM models both in that it includes a set of scan primitives, also called parallel prefix computations, and in that it is a strictly data-parallel model. A very useful abstraction in the scan model is the segment abstraction, the subdivision of a vector into a collection of independent smaller vectors. The segment abstraction permits a clean formulation of divide-and-conquer algorithms and is used heavily in the algorithms described in this paper. Within the scan model, using the operations and routines defined, the paper describes a k-D tree algorithm requiring O(lg n) calls to the primitives for n points, a closest-pair algorithm requiring O(lg n) calls to the primitives, a line-drawing algorithm requiring O(1) calls to the primitives, a line-of-sight algorithm requiring O(1) calls to the primitives, and finally, three different convex-hull algorithms. The last convex-hull algorithm, merge-hull, utilizes a generalized binary search technique using divide-and-conquer with the segment abstraction. The paper also describes how to implement the CREW version of Cole's merge sort in O(lg n) calls to the primitives. All these algorithms should be noted for their simplicity rather than their complexity; many of them are parallel versions of known serial algorithms. Most of the algorithms discussed in this paper have been implemented on the Connection Machine, a highly parallel single instruction multiple data computer.}
}
@incollection{ANGELETOS2023613,
title = {Chapter 20 - Dampening general equilibrium: incomplete information and bounded rationality☆☆This chapter subsumes an older paper of ours, entitled “Dampening General Equilibrium: From Micro to Macro” (Angeletos and Lian, 2017). We have benefited from the comments of various colleagues, especially those of the editors, Rüdiger Bachmann, Wilbert van der Klaauw, and Giorgio Topa. Angeletos acknowledges the support of the National Science Foundation under Grant Number SES-1757198.},
editor = {Rüdiger Bachmann and Giorgio Topa and Wilbert {van der Klaauw}},
booktitle = {Handbook of Economic Expectations},
publisher = {Academic Press},
pages = {613-645},
year = {2023},
isbn = {978-0-12-822927-9},
doi = {https://doi.org/10.1016/B978-0-12-822927-9.00028-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128229279000288},
author = {George-Marios Angeletos and Chen Lian},
keywords = {General equilibrium, incomplete information, higher-order beliefs, Level- Thinking, reflective equilibrium, coordination, beauty contests},
abstract = {We review how realistic frictions in information and/or rationality arrest general equilibrium (GE) feedbacks. In one specification, we maintain rational expectations but remove common knowledge of aggregate shocks. In another, we replace rational expectations with Level-k Thinking or a smooth variant thereof. Two other approaches, heterogeneous priors and cognitive discounting, capture the same essence while offering a gain in tractability. Relative to the full-information rational-expectation (FIRE) benchmark, all these modifications amount to attenuation of GE effects, especially in the short run. This in turn translates to either under- or overreaction in aggregate outcomes, depending on whether GE feedbacks are positive or negative in the first place. We review a few applications, with emphasis on monetary and fiscal policy. We finally discuss how the available evidence on expectations, along with other considerations, can help guide the choice among the various alternatives, as well as between them and FIRE.}
}
@article{WANG2025112377,
title = {Identifying falling-from-height hazards in building information models: From the perspectives of time and location},
journal = {Journal of Building Engineering},
volume = {104},
pages = {112377},
year = {2025},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2025.112377},
url = {https://www.sciencedirect.com/science/article/pii/S235271022500614X},
author = {Qiankun Wang and Chuxiong Shen and Zeng Guo and Chao Tang},
keywords = {Falling-from-Height, Building information modeling, Voxelization, Temporal information, Spatial information},
abstract = {Falling-from-Height (FFH) is the leading hazard among the 'Fatal Five' in the architectural/engineering/construction (AEC) industry, often resulting in severe consequences. Therefore, preventing these accidents is critical at the earliest stage. However, current FFH hazard identification methods suffer from a high workload, computational intensity, and lack of comprehensiveness. We propose a voxelization-based method to identify the temporal and spatial information on FFH hazards in Building Information Modeling (BIM). This method considers the falling, accessibility, and temporal conditions. We compared this method with a traditional FFH hazard identification method. The results indicate that the proposed method can rapidly identify all potential FFH hazard information, with a recall of 100 % and a precision of 81.5 %. It is 30 times faster than the offset geometry method and is not affected by the curvature of model surfaces, improving the effectiveness of FFH hazard identification. The proposed method is highly suitable for preventing FFH accidents and can be implemented during the design and construction preparation stages, increasing the effectiveness of FFH accident control.}
}
@article{GINOSAR20231858,
title = {Are grid cells used for navigation? On local metrics, subjective spaces, and black holes},
journal = {Neuron},
volume = {111},
number = {12},
pages = {1858-1875},
year = {2023},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2023.03.027},
url = {https://www.sciencedirect.com/science/article/pii/S0896627323002234},
author = {Gily Ginosar and Johnatan Aljadeff and Liora Las and Dori Derdikman and Nachum Ulanovsky},
abstract = {Summary
The symmetric, lattice-like spatial pattern of grid-cell activity is thought to provide a neuronal global metric for space. This view is compatible with grid cells recorded in empty boxes but inconsistent with data from more naturalistic settings. We review evidence arguing against the global-metric notion, including the distortion and disintegration of the grid pattern in complex and three-dimensional environments. We argue that deviations from lattice symmetry are key for understanding grid-cell function. We propose three possible functions for grid cells, which treat real-world grid distortions as a feature rather than a bug. First, grid cells may constitute a local metric for proximal space rather than a global metric for all space. Second, grid cells could form a metric for subjective action-relevant space rather than physical space. Third, distortions may represent salient locations. Finally, we discuss mechanisms that can underlie these functions. These ideas may transform our thinking about grid cells.}
}
@article{CHUNG201356,
title = {Table-top role playing game and creativity},
journal = {Thinking Skills and Creativity},
volume = {8},
pages = {56-71},
year = {2013},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2012.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1871187112000478},
author = {Tsui-shan Chung},
keywords = {Creativity, Role playing game, Divergent thinking test, Priming},
abstract = {The current study aims to observe whether individuals who engaged in table-top role playing game (TRPG) were more creative. Participants total 170 (52 TRPG players, 54 electronic role playing game (ERPG) players and 64 Non-players) aged from 19 to 63. In the current study, an online questionnaire is used, adopting the verbal subtests of Wallach–Kogan Creativity Tests and the McCrae and Costa Big Five Personality Inventory. It is found that TRPG players score higher in divergent thinking tests. Priming and instruction giving methods lower the performance of all participants, in particular, when the instruction is memory provoking. ERPG players score lowest among the three groups. TRPG could be regarded as a form of improvisation. It could also be a preferable activity for the promotion of creativity. It is low cost and no formal setting is required to play. Many ERPGs are originated from TRPGs, therefore, with the popularity of ERPG, there should be advantages in promoting TRPG.}
}
@article{WILSON1989171,
title = {Grand challenges to computational science},
journal = {Future Generation Computer Systems},
volume = {5},
number = {2},
pages = {171-189},
year = {1989},
note = {Grand Challenges to Computational Science},
issn = {0167-739X},
doi = {https://doi.org/10.1016/0167-739X(89)90038-1},
url = {https://www.sciencedirect.com/science/article/pii/0167739X89900381},
author = {Kenneth G. Wilson},
abstract = {Computational Science is at the very beginning of centuries of growth, comparable to the four centuries of experimental advances since Galileo. The Grand Challenges to Computational Science are unsolved scientific problems of extraordinary breadth and importance which will demand continuing computational advances throughout the forthcoming computational era. Supercomputers can be used to see phenomena not directly accessible to experiment in key scientific and engineering areas such as atmospheric science, astronomy, materials science, molecular biology, aerodynamics, and elementary particle physics. However, the benefits of supercomputers will be greatly increased if some major difficulties are overcome. In this paper, I address some of the tougher requirements on current grand challenge research to ensure that it has enduring value. The problems of algorithm development, error control, software productivity, and the fostering of technological advances are especially important.}
}
@article{ISLAM2022100280,
title = {Industry 4.0: Skill set for employability},
journal = {Social Sciences & Humanities Open},
volume = {6},
number = {1},
pages = {100280},
year = {2022},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2022.100280},
url = {https://www.sciencedirect.com/science/article/pii/S2590291122000341},
author = {Md. Aminul Islam},
keywords = {Industry 4.0, Skills, Competencies, Graduates, A lower middle-income country, Bangladesh},
abstract = {This paper aims at finding whether students are ready to perform in the modern competitive business job arena. Most importantly, if they have the required skills and competencies to catch the opportunity offered by companies at the fourth industrial revolution where we notice the trend of automation and data exchange and IoT, cloud computing and cognitive computing have taken the lead. Our target participants in the survey include students from public and private universities in Bangladesh who will perform in the job market and who are already in the market. This is how we can bridge the gap between employers' expectations and students' perceptions of skills and competencies they acquire before entering the job market. After surveying and analyzing data collected from 361 undergraduate and graduate-level students, we found that both business and technology impact employment. Students are aware of the changing job market scenario, and they are trying to have those skills which will make them competent compared to the early years, but they are not prepared enough to accept the challenges faced in industry 4.0. This paper will be helpful for both the academicians to be aware of the future trend of the market so that they can prepare students to fight the challenges and do future research on them. At the same time, employers can get some ideas how students are thinking right now and how much training and development opportunity they should arrange for the newly recruited graduates who have lack expertise but if they are trained up, can be a source of strength for the companies.}
}
@article{BROWN201511,
title = {On unifiers, diversifiers, and the nature of pattern recognition},
journal = {Pattern Recognition Letters},
volume = {64},
pages = {11-20},
year = {2015},
note = {Philosophical Aspects of Pattern Recognition},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2015.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167865515001312},
author = {Gavin Brown},
keywords = {Nature of pattern recognition, Unifying, Diversifying, Dyson},
abstract = {We study a dichotomy of scientific styles, unifying and diversifying, as proposed by Freeman J. Dyson. We discuss the extent to which the dichotomy transfers from the natural sciences (where Dyson proposed it) to the field of Pattern Recognition. To address this we must firstly ask what it means to be a “unifier” or “diversifier” in a field, and what are the relative merits of each style of thinking. Secondly, given that Dyson applied this to the sciences, does it also apply in a field known to be a blend of science and engineering? Parallels are drawn to Platonic/Aristotelian views, and to Cartesian/Baconian science, and questions are asked on what drives the Kuhnian paradigm shifts of our field. This article is intended not to marginalise individuals into categories (unifier/diversifier) but instead to demonstrate the utility of philosophical reflection on our field, showing the depth and complexities a seemingly simple idea can unearth.}
}
@article{DEVINK20222744,
title = {Cooperativity as quantification and optimization paradigm for nuclear receptor modulators††Electronic supplementary information (ESI) available: Experimental details, supporting figures and tables. See DOI: 10.1039/d1sc06426f},
journal = {Chemical Science},
volume = {13},
number = {9},
pages = {2744-2752},
year = {2022},
issn = {2041-6520},
doi = {https://doi.org/10.1039/d1sc06426f},
url = {https://www.sciencedirect.com/science/article/pii/S2041652023017297},
author = {Pim J. {de Vink} and Auke A. Koops and Giulia D'Arrigo and Gabriele Cruciani and Francesca Spyrakis and Luc Brunsveld},
abstract = {ABSTRACT
Nuclear Receptors (NRs) are highly relevant drug targets, for which small molecule modulation goes beyond a simple ligand/receptor interaction. NR–ligands modulate Protein–Protein Interactions (PPIs) with coregulator proteins. Here we bring forward a cooperativity mechanism for small molecule modulation of NR PPIs, using the Peroxisome Proliferator Activated Receptor γ (PPARγ), which describes NR–ligands as allosteric molecular glues. The cooperativity framework uses a thermodynamic model based on three-body binding events, to dissect and quantify reciprocal effects of NR–coregulator binding (KID) and NR–ligand binding (KIID), jointly recapitulated in the cooperativity factor (α) for each specific ternary ligand·NR·coregulator complex formation. These fundamental thermodynamic parameters allow for a conceptually new way of thinking about structure–activity-relationships for NR–ligands and can steer NR modulator discovery and optimization via a completely novel approach.}
}
@article{NEVES2009834,
title = {Structuring an MCDA model using SSM: A case study in energy efficiency},
journal = {European Journal of Operational Research},
volume = {199},
number = {3},
pages = {834-845},
year = {2009},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2009.01.053},
url = {https://www.sciencedirect.com/science/article/pii/S0377221709002033},
author = {L.P. Neves and L.C. Dias and C.H. Antunes and A.G. Martins},
keywords = {Problem structuring methods, Multiple criteria analysis, SSM, Value Focused Thinking, Energy efficiency},
abstract = {This work presents the use of a problem structuring method, Soft Systems Methodology (SSM), to structure a Multi-Criteria Decision Analysis (MCDA) model, aimed at appraising energy efficiency initiatives. SSM was useful to help defining clearly the decision problem context and the main actors involved, as well as to unveil the relevant objectives for each stakeholder. Keeney’s Value Focused Thinking approach was then used to refine and structure the list of objectives according to the perspective of the main evaluators identified. In addition to describing this particular case study, this paper aims at providing some general guidelines on how SSM may facilitate the emergence of objectives for MCDA models.}
}
@incollection{KRAAK2009468,
title = {Geovisualization},
editor = {Rob Kitchin and Nigel Thrift},
booktitle = {International Encyclopedia of Human Geography},
publisher = {Elsevier},
address = {Oxford},
pages = {468-480},
year = {2009},
isbn = {978-0-08-044910-4},
doi = {https://doi.org/10.1016/B978-008044910-4.00033-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008044910400033X},
author = {M.-J. Kraak},
keywords = {Alternative visualization, Cartography, Cognition, Coordinated-multiple-views, Geocomputation, Geoservices, Geovisualization, Information visualization, Interfaces, Maps, Representation, Spatiotemporal data, Usability, Visual exploration, Visual representation, Visual thinking},
abstract = {Recent developments in information and communication technology (ICT) have introduced many new opportunities, and have influenced many scientific disciplines in application of their methods and techniques. From a mapping perspective, this includes cartography and related disciplines like scientific visualization, image analysis and remote sensing, information visualization, exploratory data analysis, visual analytics, and GI Science. Interactivity and dynamics are prominent keywords and allow one not only to apply maps and diagrams to present-known facts but also to analyze and explore unknown data. The environment in which the maps and diagrams are used has also changed and often includes coordinated multiple views display via the Internet. This allows for simultaneous alternative views of the data and stimulates visual thinking, resulting in geovisualization.}
}
@article{ROBERTSON2009136,
title = {Impact of CAD tools on creative problem solving in engineering design},
journal = {Computer-Aided Design},
volume = {41},
number = {3},
pages = {136-146},
year = {2009},
note = {Computer Support for Conceptual Design},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2008.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0010448508001334},
author = {B.F. Robertson and D.F. Radcliffe},
keywords = {CAD, Creativity, Conceptual design},
abstract = {This paper presents the results of a survey of CAD users that examined the ways in which their computational environment may influence their ability to design creatively. This extensive online survey builds upon the findings of an earlier observational case study of the use of computer tools by a small engineering team. The case study was conducted during the conceptual and detailed stages of the design of a first-to-world product. Four mechanisms by which CAD tools may influence the creative problem solving process were investigated: enhanced visualisation and communication, circumscribed thinking, premature design fixation and bounded ideation. The prevalence of these mechanisms was examined via a series of questions that probed the user’s mode of working, attitudes, and responses to hypothetical situations. The survey showed good support for the first three mechanisms and moderate support for the fourth. The results have important implications for both the users and designers of CAD tools.}
}
@incollection{MARTIGNON2001382,
title = {Algorithms},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {382-385},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/00549-0},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767005490},
author = {L. Martignon},
abstract = {The concept of algorithm is central to the modern view of a thinking machine, be it the human mind or the modern computer. An algorithm is a well-defined mathematical recipe for the solution of a well-defined task. It is presented as a finite set of steps or instructions that can be applied to unlimited sets of possibilities. There is a clear-cut rule for the operation to be performed at each step, as well as a clear-cut specification of the conditions under which to terminate the process. An algorithm may contain loops, that is, there may be steps that return to previous steps. Algorithms can be sequential or parallel. An algorithm that produces a ‘yes’ or ‘no’ answer is, decision algorithm. An algorithm that constructs or determines a specific solution to a given problem is a computation algorithm.}
}
@article{DEMSZKY2025105183,
title = {Automated feedback improves teachers’ questioning quality in brick-and-mortar classrooms: Opportunities for further enhancement},
journal = {Computers & Education},
volume = {227},
pages = {105183},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2024.105183},
url = {https://www.sciencedirect.com/science/article/pii/S0360131524001970},
author = {Dorottya Demszky and Jing Liu and Heather C. Hill and Shyamoli Sanghi and Ariel Chung},
keywords = {Computer-assisted instruction, Brick-and-mortar classroom, Natural language processing, Automated teacher feedback, Randomized controlled trial, Focusing questions, Mixed-methods study, K-12 instruction},
abstract = {AI-powered professional learning tools that provide teachers with individualized feedback on their instruction have proven effective at improving instruction and student engagement in virtual learning contexts. Despite the need for consistent, personalized professional learning in K-12 settings, the effectiveness of automated feedback tools in traditional classrooms remains unexplored. We present results from 224 Utah mathematics and science teachers who engaged in a pre-registered randomized controlled trial, conducted in partnership with TeachFX, to assess the impact of automated feedback in K-12 classrooms. This feedback targeted “focusing questions” — questions that probe students’ thinking by pressing for explanations and reflection. We find that teachers opened emails containing the automated feedback about 53–65% of the time, and the feedback increased their use of focusing questions by 20% (p < 0.01) compared to the control group. The feedback did not impact other teaching practices. Qualitative interviews with 13 teachers revealed mixed perceptions of the automated feedback. Some teachers appreciated the reflective insights, while others faced barriers such as skepticism about accuracy, data privacy concerns, and time constraints. Our findings highlight the promises and areas of improvement for implementing effective and teacher-friendly automated professional learning tools in brick-and-mortar classrooms.}
}
@article{NASSIRI201329,
title = {Computational modelling of long bone fractures fixed with locking plates – How can the risk of implant failure be reduced?},
journal = {Journal of Orthopaedics},
volume = {10},
number = {1},
pages = {29-37},
year = {2013},
issn = {0972-978X},
doi = {https://doi.org/10.1016/j.jor.2013.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0972978X13000020},
author = {M. Nassiri and B. MacDonald and J.M. O'Byrne},
keywords = {Modeling, Fracture, Locking, Plate, Failure},
abstract = {Background and purpose
The Locking Compression Plate (LCP) is part of a new plate generation requiring an adapted surgical technique and new thinking about commonly used concepts of internal fixation using plates. Knowledge of the fixation stability provided by these new plates is very limited and clarification is still necessary to determine how the mechanical stability and the risk of implant failure can best be controlled.
Methods
Upon validation, a finite element model of an LCP attached to a cylinder was developed to simulate and analyse the biomechanics of a transverse long bone fracture fixed with a locking plate. Of special interest were the factors influencing the mechanical conditions at the fracture site, the control of interfragmentary movement and implant failure.
Results
Several factors were shown to influence stability in compression. Increasing translation and/or fracture angle post fixation reduced construct stability. Axial stiffness was also influenced by the working length and plate-bone distance. The fracture gap had no effect on the construct stability when no bone contact occurred during loading. Stress analysis of the LCP demonstrated that the maximum Von Mises stresses were found in the innermost screws at the screw-head junction.
Interpretation
For the clinical use of the LCP as a locked internal fixator in fractures with an interfragmentary gap of 1 mm, at least two to four plate holes near the fracture gap should be omitted to allow fracture motion and bone contact to occur. This will also achieve a larger area of stress distribution on the plate and reduce the likelihood of fatigue failure due to cyclic loading.}
}
@article{GOMEZMARTINEZ2025129806,
title = {A bioinspired model of decision making guided by reward dimensions and a motivational state},
journal = {Neurocomputing},
volume = {634},
pages = {129806},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129806},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225004783},
author = {Diana G. Gómez-Martínez and Alison Muñoz-Capote and Oscar Hernández and Francisco Robles and Félix Ramos},
keywords = {Decision-making, Magnitude reward, Probability reward, Decision criteria, Motivational state, Nondeterministic model},
abstract = {The decision-making process is a critical component of computational systems, whose processing involves the evaluation of various alternatives presented as possible solutions to a given problem, depending on the current context. This paper seeks to show how a neuroscience-based decision-making mechanism (DMM) integrating decision criteria, knowledge of reward stimuli, and motivational information helps to contribute to producing human-like adaptive behavior. To fulfill this objective, a computational model on DMM is proposed. The alternatives in this proposed model are constructed based on preferences, and the selection of the best alternative is guided by a goal-directed control scheme influenced by a motivational state (MS). The formation of preferences considers some dimensions of the reward, e.g., magnitude, probability of receiving the reward, incentive salience, and affective value. To validate the model exhibits a behavior considering parameters human being uses to compute its behavior, a case study was proposed. The case study’s objective is to gain the maximum reward (food) from the choice of a 4-choice card (a variation of Iowa Gambling Test), each card has a reward and a contingency probability associated with it. The analysis of the results of the case study shows that the model presents a short exploitation stage to find the contingency rule and choose the best option frequently according to some studies, also observed that the utility value of the card influenced the MS of hunger and other factors play a critical role in the DMM.}
}
@article{PIVIK2012548,
title = {Eating breakfast enhances the efficiency of neural networks engaged during mental arithmetic in school-aged children},
journal = {Physiology & Behavior},
volume = {106},
number = {4},
pages = {548-555},
year = {2012},
issn = {0031-9384},
doi = {https://doi.org/10.1016/j.physbeh.2012.03.034},
url = {https://www.sciencedirect.com/science/article/pii/S0031938412001394},
author = {R.T. Pivik and Kevin B. Tennal and Stephen D. Chapman and Yuyuan Gu},
keywords = {Morning nutrition, Mental arithmetic, Preadolescents, Time–frequency analysis},
abstract = {To determine the influence of a morning meal on complex mental functions in children (8–11y), time–frequency analyses were applied to electroencephalographic (EEG) activity recorded while children solved simple addition problems after an overnight fast and again after having either eaten or skipped breakfast. Power of low frequency EEG activity [2Hertz (Hz) bands in the 2–12Hz range] was determined from recordings over frontal and parietal brain regions associated with mathematical thinking during mental calculation of correctly answered problems. Analyses were adjusted for background variables known to influence or reflect the development of mathematical skills, i.e., age and measures of math competence and math fluency. Relative to fed children, those who continued to fast showed greater power increases in upper theta (6–8Hz) and both alpha bands (8–10Hz; 10–12Hz) across sites. Increased theta suggests greater demands on working memory. Increased alpha may facilitate task-essential activity by suppressing non-task-essential activity. Fasting children also had greater delta (2–4Hz) and greater lower-theta (4–6Hz) power in left frontal recordings—indicating a region-specific emphasis on both working memory for mental calculation (theta) and activation of processes that suppress interfering activity (delta). Fed children also showed a significant increase in correct responses while children who continued to fast did not. Taken together the findings suggest that neural network activity involved in processing numerical information is functionally enhanced and performance is improved in children who have eaten breakfast, whereas greater mental effort is required for this mathematical thinking in children who skip breakfast.}
}
@article{BIRJALI201765,
title = {Machine Learning and Semantic Sentiment Analysis based Algorithms for Suicide Sentiment Prediction in Social Networks},
journal = {Procedia Computer Science},
volume = {113},
pages = {65-72},
year = {2017},
note = {The 8th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2017) / The 7th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2017) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.08.290},
url = {https://www.sciencedirect.com/science/article/pii/S187705091731699X},
author = {Marouane Birjali and Abderrahim Beni-Hssane and Mohammed Erritali},
keywords = {Sentiment Analysis, Machine Learning, Suicide, Social Networks, Tweets, Semantic Sentiment Analysis},
abstract = {Sentiment analysis is one of the new challenges appeared in automatic language processing with the advent of social networks. Taking advantage of the amount of information is now available, research and industry have sought ways to automatically analyze sentiments and user opinions expressed in social networks. In this paper, we place ourselves in a difficult context, on the sentiments that could thinking of suicide. In particular, we propose to address the lack of terminological resources related to suicide by a method of constructing a vocabulary associated with suicide. We then propose, for a better analysis, to investigate Weka as a tool of data mining based on machine learning algorithms that can extract useful information from Twitter data collected by Twitter4J. Therefore, an algorithm of computing semantic analysis between tweets in training set and tweets in data set based on WordNet is proposed. Experimental results demonstrate that our method based on machine learning algorithms and semantic sentiment analysis can extract predictions of suicidal ideation using Twitter Data. In addition, this work verify the effectiveness of performance in term of accuracy and precision on semantic sentiment analysis that could thinking of suicide.}
}
@article{REZAPOUR2024108003,
title = {Learning experience assessment through players chat content in multiplayer online games},
journal = {Computers in Human Behavior},
volume = {151},
pages = {108003},
year = {2024},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.108003},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223003540},
author = {Mohammad Mahdi Rezapour and Afsaneh Fatemi and Mohammad Ali Nematbakhsh},
keywords = {Learning experience assessment, Multiplayer online game, Natural language processing, BERT, Stealth assessment, Game-based learning},
abstract = {Assessing players’ learning experiences in a proper manner is a fundamental aspect of successful game-based learning programs. One notable characteristic of these programs is stealth assessment, which involves integrating formative assessment into the learning environment without disrupting the learning process. In multiplayer online games (MOGs), the in-game online chat system is a commonly used tool that enables players to communicate through text or voice messages during gameplay. However, there is a lack of specific research on incorporating players’ in-game chat content for computational learning experience assessment, which could enhance the validity of stealth assessment. This study proposes a stealth assessment method based on natural language processing to highlight the significance of players’ in-game chat data in estimating learners’ skills in MOGs. A natural language processing model is developed using a distilled version of the Google BERT pre-trained model. The evaluations demonstrate that the proposed method accurately estimates a player’s skill level by analyzing a few chat messages from the player. This method has the potential to make a profound impact on the field of game-based learning by enabling more precise assessment and supporting the design of tailored interventions and adaptive learning systems. This study pioneers computational skill assessment through chats in MOGs, opening up new opportunities for future investigations in skill assessment and having the potential to transform the field of game-based learning.}
}
@article{XIAO2025200240,
title = {Integrating CNN and RANSAC for improved object recognition in industrial robotics},
journal = {Systems and Soft Computing},
volume = {7},
pages = {200240},
year = {2025},
issn = {2772-9419},
doi = {https://doi.org/10.1016/j.sasc.2025.200240},
url = {https://www.sciencedirect.com/science/article/pii/S2772941925000584},
author = {Yingding Xiao},
keywords = {CNN, Grasp detection, Industrial robots, ORB, RANSAC, VGG19},
abstract = {This research introduces a robotic grasping system that merges ORB (Oriented FAST and Rotated BRIEF) feature detection, VGG19 convolutional neural networks, and RANSAC (Random Sample Consensus) geometric verification to achieve high-precision object manipulation in unstructured environments. The framework synergizes ORB's efficient, rotation-invariant keypoints with deep semantic features extracted from intermediate layers of VGG19 enabling robust object recognition under occlusions and lighting variations. ORB detects scale-agnostic keypoints and generates binary descriptors, while VGG19’s hierarchical features provide contextual understanding of object geometry. These complementary features are fused into compact descriptors, combining ORB's 256-bit binary patterns with aggregated VGG19 layer outputs to balance accuracy and computational efficiency. RANSAC is then employed to eliminate mismatched features and estimate precise spatial alignments through iterative homography calculations, ensuring reliable mapping between detected objects and the robot's workspace. Experimental validation on industrial dataset trials demonstrates a 99 % grasp success rate, highlighting the system's ability to address challenges in dynamic, cluttered settings. By bridging deep learning's perceptual capabilities with geometric verification, this work advances autonomous robotic systems, offering a scalable solution for industrial automation that prioritizes precision and adaptability.}
}