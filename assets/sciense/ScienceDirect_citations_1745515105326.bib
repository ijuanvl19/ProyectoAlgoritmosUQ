@article{KOMPALLI2016534,
title = {Clusters of Genetic-based Attributes Selection of Cancer Data},
journal = {Procedia Computer Science},
volume = {89},
pages = {534-539},
year = {2016},
note = {Twelfth International Conference on Communication Networks, ICCN 2016, August 19– 21, 2016, Bangalore, India Twelfth International Conference on Data Mining and Warehousing, ICDMW 2016, August 19-21, 2016, Bangalore, India Twelfth International Conference on Image and Signal Processing, ICISP 2016, August 19-21, 2016, Bangalore, India},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.06.098},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916311632},
author = {Vijaya Sri Kompalli and K. Usha Rani},
keywords = {Cluster, Coupling, Cohesion, Genetic Algorithm, Fuzzy C-Means.},
abstract = {Clustering of data simplifies the task of data analysis and results in better disease diagnosis. Well-existing K-Means clustering hard computes clusters. Due to which the data may be centered to a specific cluster having less concentration on the effect of the coupling of clusters. Soft Computing methods are widely used in medical field as it contains fuzzy natured data. A Soft Computing approach of clustering called Fuzzy C-Means (FCM) deals with coupling. FCM clustering soft computes the clusters to determine the clusters based on the probability of having memberships in each of the clusters. The probability function used, determines the extent of coupling among the clusters. In order to achieve the computational efficiency and binding of features genetic evaluation is introduced. Genetic-based features are identified having more cohesion based on the fitness function values and then the coupling of the clusters is done using K-Means clustering in one trial and FCM in another trial. Analysis of coupling and cohesion is performed on Wisconsin Breast Cancer Dataset. Nature of clusters formations are observed with respect to coupling and cohesion.}
}
@incollection{ESFELD2001859,
title = {Atomism and Holism: Philosophical Aspects},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {859-864},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/01005-6},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767010056},
author = {M. Esfeld},
abstract = {Social atomism is the thesis that an individual considered in isolation can have thoughts with a determinate conceptual content. Social holism, by contrast, is the thesis that social relations are essential for a human being in order to be a ‘thinking’ being. The discussion on atomism vs. holism extends to aspects of thoughts as well. Semantic atomism is the thesis that each thought has a meaning independently of other thoughts. Semantic holism, in reverse, is the thesis that the meaning of a thought consists in its inferential relations to other thoughts in a system of thoughts. Confirmation atomism is the thesis that thoughts can be empirically confirmed one by one. Confirmation holism, by contrast, is the thesis that only a whole system of thoughts or a whole theory can be confirmed by experience. Social atomism in modern philosophy goes back to Hobbes. Social holism comes up in romanticism and its predecessors; it is worked out by Hegel. In today's discussion, the rule-following considerations that are developed by Kripke on behalf of the later Wittgenstein are the main argument for social holism. Social atomists counter this argument by a naturalistic account of rule following in terms of certain dispositions to behavior.}
}
@article{MARSHALL2024R950,
title = {Where physics and biology meet},
journal = {Current Biology},
volume = {34},
number = {20},
pages = {R950-R960},
year = {2024},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2024.08.022},
url = {https://www.sciencedirect.com/science/article/pii/S0960982224011345},
author = {Wallace Marshall and Buzz Baum and Adrienne Fairhall and Carl-Philipp Heisenberg and Elena Koslover and Andrea Liu and Yanlan Mao and Alex Mogilner and Celeste M. Nelson and Ewa K. Paluch and Xavier Trepat and Alpha Yap}
}
@article{MILLI2021104881,
title = {A rational reinterpretation of dual-process theories},
journal = {Cognition},
volume = {217},
pages = {104881},
year = {2021},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2021.104881},
url = {https://www.sciencedirect.com/science/article/pii/S0010027721003024},
author = {Smitha Milli and Falk Lieder and Thomas L. Griffiths},
keywords = {Bounded rationality, Dual-process theories, Meta-decision making, Bounded optimality, Metareasoning, Resource-rationality},
abstract = {Highly influential “dual-process” accounts of human cognition postulate the coexistence of a slow accurate system with a fast error-prone system. But why would there be just two systems rather than, say, one or 93? Here, we argue that a dual-process architecture might reflect a rational tradeoff between the cognitive flexibility afforded by multiple systems and the time and effort required to choose between them. We investigate what the optimal set and number of cognitive systems would be depending on the structure of the environment. We find that the optimal number of systems depends on the variability of the environment and the difficulty of deciding when which system should be used. Furthermore, we find that there is a plausible range of conditions under which it is optimal to be equipped with a fast system that performs no deliberation (“System 1”) and a slow system that achieves a higher expected accuracy through deliberation (“System 2”). Our findings thereby suggest a rational reinterpretation of dual-process theories.}
}
@article{MAO2022109671,
title = {A decision support engine for infill drilling attractiveness evaluation using rule-based cognitive computing under expert uncertainties},
journal = {Journal of Petroleum Science and Engineering},
volume = {208},
pages = {109671},
year = {2022},
issn = {0920-4105},
doi = {https://doi.org/10.1016/j.petrol.2021.109671},
url = {https://www.sciencedirect.com/science/article/pii/S0920410521013000},
author = {Qiangqiang Mao and Xiaohua Ma and Yuhe Wang},
keywords = {Cognitive computing, Fuzzy inference, Infill well placement, Drilling attractiveness evaluation, Expert uncertainties quantification},
abstract = {Optimally drilling new wells in a developed reservoir is an essential strategy to potentially tap remaining oil for a complete life circle of oilfield development. Further, the determination of optimal infill drilling targets is a challenging issue which involves the integration of data, experts' knowledge and human decisions. The decision process can be essentially regarded as a systematic evaluation of drilling attractiveness. To automate drilling attractiveness evaluation, we develop a decision support engine using rule-based cognitive computing to rank and recommend drilling candidates. Such drilling candidates are chosen by the quantification of regional drilling attractiveness. Then we use two cases with different settings to show its general applicability and human-like reasoning abilities. The reasoning process considers expertise and human-involved uncertainties. The expertise is characterized by certain representation of fuzzy rules sets. Our results highlight the potential of our recommendation engine in pinpointing the most productive drilling location. And our method avoids the expensive reservoir simulation runs. Moreover, fuzzy drilling attractiveness evaluation can serve as an alternative initialization method of model-based infill well optimization, which avoids local optimum problem and greatly saves iteration time. Our approach extends human's reasoning capability and accelerates human's decision-making process with very low computational cost.}
}
@article{WEN201811,
title = {Fast ranking nodes importance in complex networks based on LS-SVM method},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {506},
pages = {11-23},
year = {2018},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2018.03.076},
url = {https://www.sciencedirect.com/science/article/pii/S0378437118303947},
author = {Xiangxi Wen and Congliang Tu and Minggong Wu and Xurui Jiang},
keywords = {Complex network, Node importance, AHP, LS-SVM},
abstract = {Achieving high accuracy and comprehensiveness in node importance evaluation of complex networks is time-consuming. To solve this problem, a method based on Least Square Support Vector Machine (LS-SVM) was proposed. Firstly, four complicated importance indicators which reflect the node importance globally and comprehensively were selected. Then analytic hierarchy process (AHP) method was applied to obtain the node’s importance evaluation. On this basis, three simple indicators with low computational complexity were proposed, and LS-SVM was adopted to find the mapping rules between simple indicators and AHP evaluation. The experiments on artificial network and actual network show the validity of proposed method: the evaluation based on complicated indicators is consistent with reality and reflects node importance accurately; simple indicators evaluation by LS-SVM saved a lot of computational time and improved the evaluating efficiency. Our method can provide guidance on influential node identification in large scale complex networks.}
}
@article{NOBRE2019132,
title = {Premembering Experience: A Hierarchy of Time-Scales for Proactive Attention},
journal = {Neuron},
volume = {104},
number = {1},
pages = {132-146},
year = {2019},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2019.08.030},
url = {https://www.sciencedirect.com/science/article/pii/S0896627319307366},
author = {Anna C. Nobre and Mark G. Stokes},
keywords = {memory, attention, decision-making, hippocampus, prefrontal cortex, priming, working memory, episodic memory, implicit memory},
abstract = {Memories are about the past, but they serve the future. Memory research often emphasizes the former aspect: focusing on the functions that re-constitute (re-member) experience and elucidating the various types of memories and their interrelations, timescales, and neural bases. Here we highlight the prospective nature of memory in guiding selective attention, focusing on functions that use previous experience to anticipate the relevant events about to unfold—to “premember” experience. Memories of various types and timescales play a fundamental role in guiding perception and performance adaptively, proactively, and dynamically. Consonant with this perspective, memories are often recorded according to expected future demands. Using working memory as an example, we consider how mnemonic content is selected and represented for future use. This perspective moves away from the traditional representational account of memory toward a functional account in which forward-looking memory traces are informationally and computationally tuned for interacting with incoming sensory signals to guide adaptive behavior.}
}
@article{VELAVELUPILLAI201436,
title = {Constructive and computable Hahn–Banach theorems for the (second) fundamental theorem of welfare economics},
journal = {Journal of Mathematical Economics},
volume = {54},
pages = {36-39},
year = {2014},
issn = {0304-4068},
doi = {https://doi.org/10.1016/j.jmateco.2014.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0304406814001062},
author = {K. {Vela Velupillai}},
keywords = {Fundamental theorems of welfare economics, Hahn–Banach theorem, Constructive analysis, Computable analysis},
abstract = {The Hahn–Banach Theorem plays a crucial role in the second fundamental theorem of welfare economics. To date, all mathematical economics and advanced general equilibrium textbooks concentrate on using non-constructive or incomputable versions of this celebrated theorem. In this paper we argue for the introduction of constructive or computable Hahn–Banach theorems in mathematical economics and advanced general equilibrium theory. The suggested modification would make applied and policy-oriented economics intrinsically computational.}
}
@article{BAHK2013298,
title = {Analytical investigation of tooth profile modification effects on planetary gear dynamics},
journal = {Mechanism and Machine Theory},
volume = {70},
pages = {298-319},
year = {2013},
issn = {0094-114X},
doi = {https://doi.org/10.1016/j.mechmachtheory.2013.07.018},
url = {https://www.sciencedirect.com/science/article/pii/S0094114X13001584},
author = {Cheon-Jae Bahk and Robert G. Parker},
keywords = {Tooth profile modification, Planetary gear, Vibration, Nonlinear, Perturbation method},
abstract = {This study investigates the impact of tooth profile modification on spur planetary gear vibration. An analytical model is proposed to capture the excitation from tooth profile modifications at the sun–planet and ring–planet meshes. The accuracy of the proposed model for dynamic analysis is correlated against a benchmark finite element analysis. Perturbation analysis yields a closed-form approximation of the vibration response with tooth profile modifications. The perturbation solution is used to investigate the effects of tooth profile modification. The tooth profile modification parameters that minimize response are readily obtained. Static transmission error and dynamic response are minimized at different amounts of profile modification, which contradicts common practical thinking regarding the correlation between static transmission error and dynamic response. Contrary to expectations, the optimal sun–planet and ring–planet tooth profile modifications that minimize response when applied individually may increase dynamic response when applied simultaneously. System parameters such as mesh stiffness and mesh phase significantly affect the influence of tooth profile modification.}
}
@article{QIAO2024120105,
title = {Towards retraining-free RNA modification prediction with incremental learning},
journal = {Information Sciences},
volume = {660},
pages = {120105},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.120105},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524000185},
author = {Jianbo Qiao and Junru Jin and Haoqing Yu and Leyi Wei},
keywords = {RNA modification, Deep learning, Incremental learning},
abstract = {RNA modifications are important for deciphering the function of cells and their regulatory mechanisms. In recent years, researchers have developed many deep learning methods to identify specific modifications. However, these methods require model retraining for each new RNA modification and cannot progressively identify the newly identified RNA modifications. To address this challenge, we propose an innovative incremental learning framework that incorporates multiple incremental learning methods. Our experimental results confirm the efficacy of incremental learning strategies in addressing the RNA modification challenge. By uniquely targeting 10 RNA modification types in a class incremental setting, our framework exhibits superior performance. Notably, it can be extended to new category methylation predictions without the need for retraining with previous data, improving computational efficiency. Through the accumulation of knowledge, the model is able to evolve and continuously learn the differences across methylation, mitigating the problem of catastrophic forgetting during deep learning model training. Overall, our framework provides various alternatives to enhance the prediction of novel RNA modifications and illuminates the potential of incremental learning in tacking numerous genome data.}
}
@article{WANG2007254,
title = {An efficient algorithm for generalized discriminant analysis using incomplete Cholesky decomposition},
journal = {Pattern Recognition Letters},
volume = {28},
number = {2},
pages = {254-259},
year = {2007},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2006.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167865506001966},
author = {Haixian Wang and Zilan Hu and Yu’e Zhao},
keywords = {Generalized discriminant analysis, Nonlinear feature extraction, Eigenvalue decomposition, Gram–Schmidt orthonormalization, Incomplete Cholesky decomposition},
abstract = {Generalized discriminant analysis (GDA) has provided an extremely powerful approach to extracting nonlinear features via kernel trick. And it has been suggested for a number of applications, such as classification problem. Whereas the GDA could be solved by the utilization of Mercer kernels, a drawback of the standard GDA is that it may suffer from computational problem for large scale data set. Besides, there is still attendant problem of numerical accuracy when computing the eigenvalue problem of large matrices. Also, the GDA would occupy large memory (to store the kernel matrix). To overcome these deficiencies, we use Gram–Schmidt orthonormalization and incomplete Cholesky decomposition to find a basis for the entire training samples, and then formulate GDA as another eigenvalue problem of matrix whose size is much smaller than that of the kernel matrix by using the basis, while still working out the optimal discriminant vectors from all training samples. The theoretical analysis and experimental results on both artificial and real data set have shown the superiority of the proposed method for performing GDA in terms of computational efficiency and even the recognition accuracy, especially when the training samples size is large.}
}
@article{ZHAO2025100817,
title = {User entertainment experience analysis of artificial intelligence entertainment robots based on convolutional neural networks in park plant landscape design},
journal = {Entertainment Computing},
volume = {52},
pages = {100817},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100817},
url = {https://www.sciencedirect.com/science/article/pii/S187595212400185X},
author = {Jingjing Zhao and Juan Yin and Yaqi Shi and Liang Qiao and Guihua Ma},
keywords = {Convolutional neural network, AI entertainment robots, Park plants, Landscape design, User experience},
abstract = {Currently, the application of artificial intelligence entertainment robots in park plant landscape design has attracted increasing attention. This study aims to design an artificial intelligence entertainment robot that can provide a high-quality user experience. Through virtual reality and robotics technology, designers can be provided with visual and entertaining design solutions, and more interactive experiences can be provided for design clients. Convolutional neural networks can effectively extract features from images, and utilizing spectral feature extraction technology to further improve the accuracy of image recognition. Subsequently, this study designed a robot control system and calibrated the hand eye system. The robot control system can coordinate the various functions of the robot and ensure its smooth operation in the park plant landscape design. The calibration of the hand eye system is to ensure that the robot can accurately perceive the environment and locate its own position. Through real-time control strategies, robots can respond and adjust in a timely manner based on current environmental changes and user needs. By comparing with the actual position on the ground, the accuracy of robot positioning is obtained, and the system is further optimized and improved.}
}
@article{GUI2024111972,
title = {Molten salt-promoted MgO-based CO2 adsorbents: Selective adsorption on polycrystalline surfaces},
journal = {Journal of Environmental Chemical Engineering},
volume = {12},
number = {2},
pages = {111972},
year = {2024},
issn = {2213-3437},
doi = {https://doi.org/10.1016/j.jece.2024.111972},
url = {https://www.sciencedirect.com/science/article/pii/S2213343724001027},
author = {Changqing Gui and Zirui Wang and Changjian Ling and Zhongfeng Tang},
keywords = {CO, MgCl·6 HO, Molten salt, MgO, Capture},
abstract = {Molten salt-doped MgO adsorbent is considered one of the most promising CO2 adsorbents in the field. In this work, MgO-based adsorbents were prepared by one-step calcination using MgCl2·6 H2O as magnesium source. The CO2 adsorption performance of MgO-based adsorbents was investigated via different methods. Results showed that the maximum CO2 adsorption capacity of MgO doped by LiNO3-NaNO3-KNO3 was 57.1% at the CO2 concentration of 80% and 350 ℃, and the MgO-based adsorbents showed good regeneration. The nanosheet structure of the MgO-based adsorbents decreased with the increase in the number of cycles, whereas the crystal structures of MgO and alkali metal nitrates did not change because of multiple decarbonization. DFT computation revealed selective adsorption of CO2 on different crystal faces of MgO. The (200) crystal face of molten salt-doped MgO did not have CO2 trap ability. In addition, the doped nitrate did not directly participate in the reaction but reduced the adsorption energy of MgO carbonation. The adsorption energies of the MgO (220) and (222) crystal faces after doping with nitrate were reduced to − 2.07 and − 3.26 eV, respectively. The overall energy level of adsorption decreased as the number of resonance peaks and the stability of the structure increased. This study explains why MgO currently fails to reach the theoretical adsorption capacity and reveals the underlying mechanism of molten salts.}
}
@article{SCHNASE2017198,
title = {MERRA Analytic Services: Meeting the Big Data challenges of climate science through cloud-enabled Climate Analytics-as-a-Service},
journal = {Computers, Environment and Urban Systems},
volume = {61},
pages = {198-211},
year = {2017},
note = {Geospatial Cloud Computing and Big Data},
issn = {0198-9715},
doi = {https://doi.org/10.1016/j.compenvurbsys.2013.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S019897151300118X},
author = {John L. Schnase and Daniel Q. Duffy and Glenn S. Tamkin and Denis Nadeau and John H. Thompson and Cristina M. Grieg and Mark A. McInerney and William P. Webster},
keywords = {MapReduce, Hadoop, Data analytics, Data services, Cloud Computing, Generativity, iRODS, MERRA, ESGF, BAER},
abstract = {Climate science is a Big Data domain that is experiencing unprecedented growth. In our efforts to address the Big Data challenges of climate science, we are moving toward a notion of Climate Analytics-as-a-Service (CAaaS). We focus on analytics, because it is the knowledge gained from our interactions with Big Data that ultimately produce societal benefits. We focus on CAaaS because we believe it provides a useful way of thinking about the problem: a specialization of the concept of business process-as-a-service, which is an evolving extension of IaaS, PaaS, and SaaS enabled by Cloud Computing. Within this framework, Cloud Computing plays an important role; however, we see it as only one element in a constellation of capabilities that are essential to delivering climate analytics as a service. These elements are essential because in the aggregate they lead to generativity, a capacity for self-assembly that we feel is the key to solving many of the Big Data challenges in this domain. MERRA Analytic Services (MERRA/AS) is an example of cloud-enabled CAaaS built on this principle. MERRA/AS enables MapReduce analytics over NASA’s Modern-Era Retrospective Analysis for Research and Applications (MERRA) data collection. The MERRA reanalysis integrates observational data with numerical models to produce a global temporally and spatially consistent synthesis of 26 key climate variables. It represents a type of data product that is of growing importance to scientists doing climate change research and a wide range of decision support applications. MERRA/AS brings together the following generative elements in a full, end-to-end demonstration of CAaaS capabilities: (1) high-performance, data proximal analytics, (2) scalable data management, (3) software appliance virtualization, (4) adaptive analytics, and (5) a domain-harmonized API. The effectiveness of MERRA/AS has been demonstrated in several applications. In our experience, Cloud Computing lowers the barriers and risk to organizational change, fosters innovation and experimentation, facilitates technology transfer, and provides the agility required to meet our customers’ increasing and changing needs. Cloud Computing is providing a new tier in the data services stack that helps connect earthbound, enterprise-level data and computational resources to new customers and new mobility-driven applications and modes of work. For climate science, Cloud Computing’s capacity to engage communities in the construction of new capabilities is perhaps the most important link between Cloud Computing and Big Data.}
}
@incollection{MURRAY202119,
title = {Chapter Two - The neurocognitive mechanisms of responsibility: A framework for normatively relevant neuroscience},
editor = {Martín Hevia},
series = {Developments in Neuroethics and Bioethics},
publisher = {Academic Press},
volume = {4},
pages = {19-40},
year = {2021},
booktitle = {Regulating Neuroscience: Transnational Legal Challenges},
issn = {2589-2959},
doi = {https://doi.org/10.1016/bs.dnb.2021.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S2589295921000023},
author = {Samuel Murray and Felipe {De Brigard}},
keywords = {Moral responsibility, Autonomy of ethics, Moral neuroscience, Decision-making, Practical reasoning, Moral agency},
abstract = {We argue that research in cognitive neuroscience can contribute meaningfully to some normative theorizing. To make our case, we develop one instance where ethical inquiry progressed through empirical research into the computational basis of decision-making. From this, we draw some general considerations about the kinds of normative inquiry where research in cognitive neuroscience might be relevant.}
}
@article{KEBEDE2024131461,
title = {Transfer learning-based deep learning models for proton exchange membrane fuel remaining useful life prediction},
journal = {Fuel},
volume = {367},
pages = {131461},
year = {2024},
issn = {0016-2361},
doi = {https://doi.org/10.1016/j.fuel.2024.131461},
url = {https://www.sciencedirect.com/science/article/pii/S0016236124006094},
author = {Getnet Awoke Kebede and Shih-Che Lo and Fu-Kwun Wang and Jia-Hong Chou},
keywords = {Drop method, Long-short term memory with attention, Remaining useful life prediction, Transfer learning, Variational autoencoder},
abstract = {Proton exchange membrane fuel cells (PEMFCs) offer power generation capabilities for diverse applications including commercial enterprises, industrial sectors, and residential technologies. Nevertheless, the comprehensive integration of PEMFC applications could be improved by challenges related to degradation and durability. The imperative development of efficient performance prognostic models assumes a pivotal role in the prognosis of remaining useful life (RUL), health monitoring, and effective utilization of PEMFCs. This paper centers on the prognostication of critical components within PEMFCs and introduces a transfer learning approach based on variational autoencoder and bi-directional long short-term memory with an attention mechanism (Bi-LSTM-AM) model. This approach combines feature fusion, knee-point detection, and a sophisticated deep-learning-based predictive model. Notably, incorporating the variational autoencoder as the framework for feature fusion introduces a novel perspective previously unexplored. Identifying the knee point and knowing the start point on the training data, facilitates optimized parameter computation. The application of transfer learning facilitates the transfer of optimal model parameters and weights from a source to a target dataset. Conclusively, the estimation of stack voltage degradation and real-time RUL prediction based on the test dataset is executed by implementing our proposed method. The stack voltage prediction findings showcase the Bi-LSTM-AM model’s superior performance relative to comparison models. The proposed online rolling prediction model, utilizing a sliding window technique for RUL prediction, yields significantly enhanced accuracy, culminating in a relative error margin ranging from approximately 1.69% to 5.04%.}
}
@article{LEPP2025100642,
title = {Does generative AI help in learning programming: Students’ perceptions, reported use and relation to performance},
journal = {Computers in Human Behavior Reports},
volume = {18},
pages = {100642},
year = {2025},
issn = {2451-9588},
doi = {https://doi.org/10.1016/j.chbr.2025.100642},
url = {https://www.sciencedirect.com/science/article/pii/S2451958825000570},
author = {Marina Lepp and Joosep Kaimre},
keywords = {Artificial Intelligence (AI), Programming education, Higher education, Student perceptions, Academic performance},
abstract = {In 2022, the release of ChatGPT marked a significant advancement in the use of Artificial Intelligence (AI) chatbots, particularly impacting fields like computer science and education. The ability to generate code snippets using AI chatbots has introduced new opportunities and challenges in teaching programming. However, there is limited agreement on how students integrate them into their learning processes. This study aims to explore how students utilize AI chatbots in the "Object-Oriented Programming" course and examine the relationship between chatbot usage and academic performance. To address this, 231 students completed a survey assessing the frequency and manner of chatbot usage. Descriptive statistical methods were employed to analyze usage and perceptions, while Spearman's correlation was used to investigate the connection between chatbot usage and course performance. Results indicated that students primarily relied on AI chatbots for programming tasks. Interestingly, students' performance negatively correlates with the reported frequency of using these tools. These findings provide valuable insights for programming educators, offering a better understanding of students' perceptions and use of AI chatbots. This knowledge can inform strategies for integrating these tools effectively into computer science education.}
}
@article{SWARTZ2004773,
title = {A multimethod approach to the combat air forces mix and deployment problem},
journal = {Mathematical and Computer Modelling},
volume = {39},
number = {6},
pages = {773-797},
year = {2004},
note = {Defense transportation: Algorithms, models, and applications for the 21st century},
issn = {0895-7177},
doi = {https://doi.org/10.1016/S0895-7177(04)90554-7},
url = {https://www.sciencedirect.com/science/article/pii/S0895717704905547},
author = {S.M Swartz and A.W Johnson},
keywords = {Multiattribute decision analysis, Ranking and selection, Heuristics},
abstract = {The purpose of military logistics is to ensure that the material elements of combat capability come together at the right place and time and in the right configuration to be useful to the supported commander. These material elements are constrained in both quantity and location. The usefulness of any element to a commander is dependent upon both its extrinsic (qualitative; situation dependent) and intrinsic (quantitative; inherent) characteristics. Our research provides a methodology for rationally assigning relative value to material resources over time, in order to improve the linkage between what arrives (becomes available for use) in theater at any given time, and what is actually needed at that time. A blend of qualitative (value focused thinking and hierarchical weighting) and quantitative (a greedy matching algorithm) methods were used against the lift-constrained combat forces material selection/movement problem. The intent is to provide a decision support tool for the formulation of force mixes that best support desired time-phased battlefield objectives, given constraints on available transportation resources. This methodology is applicable to general crisis response planning, such as for disaster relief.}
}
@article{YAMANE2021102520,
title = {Humor meets morality: Joke generation based on moral judgement},
journal = {Information Processing & Management},
volume = {58},
number = {3},
pages = {102520},
year = {2021},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2021.102520},
url = {https://www.sciencedirect.com/science/article/pii/S0306457321000297},
author = {Hiroaki Yamane and Yusuke Mori and Tatsuya Harada},
keywords = {Computational humor, Morality, Recurrent neural networks, Joke generation},
abstract = {Although humor enriches human lives, some jokes fail to amuse people because of a lack of morality. In this paper, we propose a mechanism capable of selecting humor based on moral criteria. To this end, we first construct a model based on an N-gram corpus and generate joke candidates using various template patterns. We then employ a moral judgement classifier based on a recurrent neural network and utilize the trained model for humor selection. The experimental results obtained from best–worst scaling demonstrate that this scheme is able to generate jokes with moral category labels. We confirmed that jokes about the classifier categorized as Loyalty and Authority, which are regarded as good in our study, are funnier than jokes about Fairness, Purity, Harm, Cheating, and Degradation. Although we did not confirm that there was a difference in the funny level between good and bad moral jokes, the results demonstrate that moral categories of humor can affect the funny level.}
}
@article{BERGER2024537,
title = {Enmeshed with the digital: satellite navigation and the phenomenology of drivers’ spaces},
journal = {Mobilities},
volume = {19},
number = {3},
pages = {537-555},
year = {2024},
issn = {1745-0101},
doi = {https://doi.org/10.1080/17450101.2023.2285304},
url = {https://www.sciencedirect.com/science/article/pii/S1745010123001431},
author = {Viktor Berger},
keywords = {Satellite navigation, GPS, driving, automobilities, Merleau-Ponty, hybrid spaces, mesh, mediatization},
abstract = {This paper aims to develop a theoretical interpretation of how satellite navigation transforms drivers’ experience of automotive spaces. The use of satellite navigation has, so far, been predominantly studied from a cognitivist perspective based on the computer model of cognition and the theory of spatial disengagement. Experimental studies have concluded that over-reliance on digital navigation tools diminishes spatial orientation and spatial memory. According to the dominant interpretation, satellite navigation causes disengagement from space. After addressing these approaches, the paper introduces an embodied perspective of satellite navigation. This is accomplished by applying the phenomenology of perception of Maurice Merleau-Ponty, whose notions, such as perception, body schema, motor habit, and virtual body, illuminate otherwise undertheorized dimensions of drivers’ spaces. By using digital tools for wayfinding, drivers’ body schema, virtual body, and perception of space are modified, thereby enabling an engagement with convoluted ‘mesh spaces.’ This new term is integral to the interpretation of drivers’ spaces, as well as being distinct from that of ‘hybrid space,’ although both aim to conceptualize spaces, including physical objects and their visual representations. Conclusions will be drawn against the broader context of the mediatization of everyday life.}
}
@article{HE2025110716,
title = {The comprehensive safety assessment method for complex construction crane accidents based on scenario analysis – A case study of crane accidents},
journal = {Computers & Industrial Engineering},
volume = {199},
pages = {110716},
year = {2025},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2024.110716},
url = {https://www.sciencedirect.com/science/article/pii/S0360835224008386},
author = {Wei He and Zelong Lin and Wei Li and CJ Wong and Dewei Kong and W.M. Edmund Loh},
keywords = {Scenario Analysis Theory, Improved Bayesian Network, Crane Accidents, Safety Assessment, Emergency Management},
abstract = {Crane accidents pose a significant safety hazard in the infrastructure construction process, making a scientifically reliable safety assessment crucial. Addressing the limitations of traditional methods in adequately considering the complexity of crane accidents, this study proposes a safety assessment model based on Scenario Analysis Theory (SAT) and an improved Bayesian Network (BN) algorithm. The model constructs accident scenario elements, utilizes improved BN to model influencing factors and their interactions, and designs safety assessment functions for a quantitative analysis of crane accident safety. This study demonstrates that the proposed safety assessment model more comprehensively reflects the dynamic evolution of crane accidents. It provides more accurate and interpretable assessment outcomes, significantly aiding in risk prediction and decision-making for emergency management. Key stakeholders, including site management teams, and regulatory bodies, can leverage these findings to enhance emergency management capabilities and reduce the risk of accidents in construction projects.}
}
@article{RZHETSKY20089,
title = {Seeking a New Biology through Text Mining},
journal = {Cell},
volume = {134},
number = {1},
pages = {9-13},
year = {2008},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2008.06.029},
url = {https://www.sciencedirect.com/science/article/pii/S0092867408008167},
author = {Andrey Rzhetsky and Michael Seringhaus and Mark Gerstein},
abstract = {Tens of thousands of biomedical journals exist, and the deluge of new articles in the biomedical sciences is leading to information overload. Hence, there is much interest in text mining, the use of computational tools to enhance the human ability to parse and understand complex text.}
}
@incollection{ROSENBERG2023157,
title = {Chapter 9 - Machine learning and precision medicine},
editor = {Gary A. Rosenberg},
booktitle = {Neuroinflammation in Vascular Dementia},
publisher = {Academic Press},
pages = {157-173},
year = {2023},
isbn = {978-0-12-823455-6},
doi = {https://doi.org/10.1016/B978-0-12-823455-6.00005-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234556000055},
author = {Gary A. Rosenberg},
keywords = {Principal component analysis (PCA), exploratory factor analysis (EFA), Binswanger’s disease score (BDS), The Alzheimer Disease Neuroimaging Initiative (ADNI), hierarchical clustering analysis (HCA)},
abstract = {Clinical medicine is experiencing a massive increase in the amount of information available to the physician caring for a patient. Certain medical fields have incorporated this deluge of information into patient care while others are lagging behind. Neurology has been slow to adopt the new methods to use the large amount of information, but it is rapidly learning from other fields. Cancer diagnosis and treatment has been at the forefront of this revolution; not only have there been an extensive number of genes associated with different cancers discovered, but this information has been used to formulate treatment plans. Other fields such as radiology and dermatology are using computer-aided imaging to diagnose illness by analysis of radiographs and to automate diagnoses of skin cancers. The concepts behind the use of machine learning in diagnosis originated from early work in the field of “cybernetics,” which is a transdisciplinary approach for exploring regulatory systems – their structures, constraints, and possibilities. Norbert Wiener defined cybernetics in 1948 as “the scientific study of control and communication in the animal and the machine.” From the early work on control theory by Wiener and others has slowly evolved the modern concepts of artificial intelligence (AI) and machine learning. There are various definitions of AI or machine learning. The term is used to describe computers that perform cognitive functions that we associate with the human mind; these “thinking machines” can beat experts in chess and the Chinese game of Go. In medicine, there are capable of analyzing large amounts of data to arrive at a diagnosis through pattern recognition. Antibiotic drugs have been designed by AI in ways that were unavailable to humans, pointing to the future of molecular discovery in medicine.}
}
@article{EVANS1989499,
title = {A review and synthesis of OR/MS and creative problem solving (Parts 1 and 2)},
journal = {Omega},
volume = {17},
number = {6},
pages = {499-524},
year = {1989},
issn = {0305-0483},
doi = {https://doi.org/10.1016/0305-0483(89)90055-8},
url = {https://www.sciencedirect.com/science/article/pii/0305048389900558},
author = {JR Evans},
keywords = {creativity, problem-solving, OR/MS methodology},
abstract = {Problem solving in operations research and management science is both a science and an art. While much has been written about the science of OR/MS, relatively little has been written about the art. Art, by its very nature, is a creative discipline. This implies that creative thinking should be an important component of OR/MS methodology. A rich literature on creative thinking exists, mostly in the domains of psychology and design. Creativity has been indirectly discussed in OR/MS research and practice, but seldom as a central theme. The purpose of this paper is to review the literature on creative thinking and problem solving that has special relevance to traditional OR/MS methodology. In this part we focus on problem solving, the need for creative thinking, and fundamental concepts of creativity. In Part 2 we synthesize the OR/MS literature that relates to creative thinking, and provide a framework for integrating structured creative thinking processes with OR/MS methodology. Finally, we discuss implications for education, research, and practice.}
}
@article{NOORMAN2017677,
title = {Biochemical engineering’s grand adventure},
journal = {Chemical Engineering Science},
volume = {170},
pages = {677-693},
year = {2017},
note = {13th International Conference on Gas-Liquid and Gas-Liquid-Solid Reactor Engineering},
issn = {0009-2509},
doi = {https://doi.org/10.1016/j.ces.2016.12.065},
url = {https://www.sciencedirect.com/science/article/pii/S0009250916307266},
author = {Henk J. Noorman and Joseph J. Heijnen},
keywords = {Lifeline modeling, Bioprocess design, Scale-down, Bio-economy, Renewable feedstocks, Bio-products},
abstract = {Building on the recent revolution in molecular biology, enabling a wealth of bio-product innovations made from renewable feedstocks, the biotechnology field is in a transition phase to bring the products to the market. This requires a shift from natural sciences to engineering sciences with first conception of new, efficient large-scale bioprocess designs, followed by implementation of the most promising design in practice. Inspired by a former publication by O. Levenspiel in 1988, an outline is presented of main challenges that the field of biochemical engineering is currently facing, in a context of major global sustainability trends. The critical stage is the conceptual design phase. Issues can best be addressed and overcome by adopting an attitude where one begins with the end in mind. This applies to three principal components: 1. the bioprocess value chain, where the product specifications and downstream purification schemes should be set before defining the upstream sections, 2. the time perspective, starting in the future assuming that feedstock and product-market combinations are already in place and then going back to today, and 3. the scale of operation, where the industrial operation sets the boundaries for all labscale research and development, and not vice versa. In this way, and ideal process is defined taking constraints from anticipated manufacturing into account. For illustration, three bioprocess design examples are provided, that show how new, ideal conceptual designs can be generated. These also make clear that the engineering sciences are undergoing a revolution, where bio-based approaches replace fossil routes, and gross simplification is replaced by highly detailed computational methods. For biochemical processes, lifeline modeling frameworks are highlighted as powerful means to reconcile the competing needs for high speed and high quality in biochemical engineering, both in the design and implementation stages, thereby enabling significant growth of the bio-based economy.}
}
@article{PATON1997245,
title = {The organisations of hereditary information},
journal = {Biosystems},
volume = {40},
number = {3},
pages = {245-255},
year = {1997},
issn = {0303-2647},
doi = {https://doi.org/10.1016/S0303-2647(96)01652-8},
url = {https://www.sciencedirect.com/science/article/pii/S0303264796016528},
author = {Ray Paton},
keywords = {Gene, Syntax/semantics, Hierarchy, Epigenetic system, Talkback},
abstract = {The meaning of hereditary information is not simple. It includes not only what a system receives and transmits but particularly what it makes. The syntactic basis to hereditary information is also not straightforward. For example, is DNA instructions, or data, or both? The answer to this question requires an appreciation of the meaning of the information yet there are a number of possible semantic systems for describing hereditary information including proteins and development. The descriptive boundaries of hereditary information are examined by locating some general organising themes including hierarchy, ecology, regulation, epigenetic systems and talkback. Though metaphors have limits in terms of their explanatory power, a number have influenced the development of biological thinking and biosystems have variously been represented as chemical laboratories, computers, electromechanical machines and societies. In this article a further metaphor is discussed, that of life-as-a-play or dance in which the trio of script (genome), cast (metabolism) and stage (cellular structure) co-exist and pre-exist the phenotypic life history which inherits them. A fuller examination of this trio provides an important perspective on the study of the organisations of information processing in hereditary systems.}
}
@article{GANNON2025R152,
title = {Motion integration: A case of misdirection},
journal = {Current Biology},
volume = {35},
number = {4},
pages = {R152-R154},
year = {2025},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2025.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S0960982225000168},
author = {Sara M. Gannon and Lindsey L. Glickfeld},
abstract = {Summary
Integrating complex motion signals from the environment is essential for behavior. A recent study in the mouse has revealed that both encoding in the superior colliculus and the optokinetic reflex follow a novel motion integration rule.}
}
@article{SURYADI2023730,
title = {”Read on”: comprehending challenging texts at university through gamification App},
journal = {Procedia Computer Science},
volume = {216},
pages = {730-738},
year = {2023},
note = {7th International Conference on Computer Science and Computational Intelligence 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.190},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922022682},
author = {Phillip Suryadi and Irfan Rifai and Hady Pranoto},
keywords = {gamification, reading, application, students, texts},
abstract = {Despite the common misperception of playing games as wasteful activity, studies found that some of its components may contribute to users’ knowledge generation, soft skill improvement, and foreign language learning. This article reports the development of an application for reading and the initial impacts of the gamification-based application on students’ reading comprehension in English. The application was aimed to support generation Z's university students who are well exposed to gadgets with the ability in comprehending challenging texts. In addition to the sociocultural theory of learning and second language acquisition theories, we considered factors like university students as users, texts’ complexity offered at the university level, and gamification features in designing the application. The study resulted in a prototype of a gaming activity called” ReadOn”. Surveys, interviews, and experiments were carried out on a small group of participants during, and after designing processes. The Survey data was used as a foundation to design the app while the interview and the experiments provided data to explore the usability of the newly built prototype. The data of students’ experience in using the prototype was used as feedback for future development of the platform.}
}
@article{BELLA2023123268,
title = {Vibrationally resolved deep–red circularly polarised luminescence spectra of C70 derivative through Gaussian curvature analysis of ground and excited states},
journal = {Journal of Molecular Liquids},
volume = {391},
pages = {123268},
year = {2023},
issn = {0167-7322},
doi = {https://doi.org/10.1016/j.molliq.2023.123268},
url = {https://www.sciencedirect.com/science/article/pii/S0167732223020743},
author = {Giovanni Bella and Giuseppe Bruno and Antonio Santoro},
keywords = {Fullerene, Chirality, Curvature, Vibronic, Circularly polarized luminescence},
abstract = {Over the last years, the interaction of fullerene with circularly polarized light has attracted growing attention for potential electronic and optical applications. However, in literature there is only one example of fullerene derivative capable of emitting circularly polarized light, showing an active circularly polarized luminescence (CPL) signal in the deep-red visible region. This unique luminophore offered us the possibility to study the connection between the topological features of C70 spheroid and its chiral emission properties. In light of these considerations, we proposed a theoretical protocol that combines three different step: (1) The Ball Pivoting Algorithm for C70 surface reconstruction. (2) The discrete gaussian curvature analysis in the ground (S0) and excited states (S1). (3) The computation of the vibrationally-resolved CPL spectrum. The first step allowed us to extract useful information that linked the topological shape of C70 to the sp2 carbon network chemistry. The DFT benchmark in the second step guided us in grasping the best functional for the C70 curvature simulation in the ground state, spotlighting how B97D3 excellently succeed for this task. The curvature investigation in the first excited state showed that (for all the twenty exchange–correlation functional tested) the C70 fragment is more curved in S1 than in S0. The final step collected the topological information from the previous sections to provide a detailed overview of the theoretical factors (such as the quantum formalism, the potential energy surface description and the transition dipole moment approximation) impacting on the C70 vibrationally resolved CPL spectrum. We found that the adiabatic hessian model coupled with the Franck-Condon Herzberg-Teller approximation computed at PW6B95D3/6-311G(d,p) level provides excellent results in emulating the band-shape and position of the experimental CPL spectrum.}
}
@article{TERZIYAN20242540,
title = {Can ChatGPT Challenge the Scientific Impact of Published Research, Particularly in the Context of Industry 4.0 and Smart Manufacturing?},
journal = {Procedia Computer Science},
volume = {232},
pages = {2540-2550},
year = {2024},
note = {5th International Conference on Industry 4.0 and Smart Manufacturing (ISM 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.02.072},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924002497},
author = {Vagan Terziyan and Olena Kaikova and Mariia Golovianko and Oleksandra Vitko},
keywords = {Artificial Intelligence, ChatGPT, Industry 4.0, Smart Manufacturing, academic impact},
abstract = {The released ChatGPT as a powerful language model is capable of assisting with a wide range of tasks, including answering questions, summarizing, paraphrasing, proofreading, classifying, and integrating texts. In this study, we tested ChatGPT capability to assist researchers in evaluating the academic articles’ contribution. We suggest a dialogue schema in which ChatGPT is asked to answer research questions from the target article and then to compare its own answers with the answers from the article. Finally, ChatGPT is asked to integrate both solutions coherently. We experimented with Proceedings of ISM-2022 Conference on Industry 4.0 and Smart Manufacturing, utilizing explicit research questions. The chat context enabled assessing studied articles’ contributions to Industry 4.0, uncovering advancements beyond the state-of-the-art. However, ChatGPT demonstrates limitations in content understanding and contribution evaluation. We conclude that while it collaborates with humans on academic tasks, human guidance remains essential, while ChatGPT's assistance efficiently complements traditional academic processes.}
}
@article{TALAAT2021164,
title = {The validity of an artificial intelligence application for assessment of orthodontic treatment need from clinical images},
journal = {Seminars in Orthodontics},
volume = {27},
number = {2},
pages = {164-171},
year = {2021},
note = {Artificial Intelligence applications in Orthodontics -An update},
issn = {1073-8746},
doi = {https://doi.org/10.1053/j.sodo.2021.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S1073874621000359},
author = {Sameh Talaat and Ahmed Kaboudan and Wael Talaat and Budi Kusnoto and Flavio Sanchez and Mohammed H. Elnagar and Christoph Bourauel and Ahmed Ghoneima},
abstract = {Aim: To assess the validity of a Convolutional Neural Network (CNN) digital model to detect and localize orthodontic malocclusions from intraoral clinical images. Materials and methods: The sample of this study consisted of the intraoral images of 700 Subjects. All images were intraoral clinical images, in one of the following views: Left Occlusion, Right Occlusion, Front Occlusion, Upper Occlusal, and Lower Occlusal. The following malocclusion conditions were localized: crowding, spacing, increased overjet, cross bite, open bite, deep bite. The images annotations were repeated by the same investigator (S.T) with a one week interval (ICC ≥ 0.9). The CNN model used for this research study was the “You Only Look Once” model. This model can detect and localize multiple objects or multiple instances of the same object in each image. It is a fully convolutional deep neural network; 24 convolutional layers followed by 2 fully connected layers. This model was implemented using the TensorFlow framework freely available from Google. Results: The created CNN model was able to detect and localize the malocclusions with an accuracy of 99.99%, precision of 99.79%, and a recall of 100%. Conclusions: The use of computational deep convolutional neural networks to identify and localize orthodontic problems from clinical images proved valid. The built AI engine accurately detected and localized malocclusion from different views of intra-oral clinical images.}
}
@article{CARLI2012119,
title = {Efficient algorithms for large scale linear system identification using stable spline estimators},
journal = {IFAC Proceedings Volumes},
volume = {45},
number = {16},
pages = {119-124},
year = {2012},
note = {16th IFAC Symposium on System Identification},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20120711-3-BE-2027.00394},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015379386},
author = {Francesca P. Carli and Alessandro Chiuso and Gianluigi Pillonetto},
keywords = {Parametric prediction error methods, output error models, model complexity, marginal likelihood, kernel eigenfunctions},
abstract = {A new nonparametric approach for system identification has been recently proposed where, in place of postulating parametric classes of impulse responses, the estimation process starts from an infinite-dimensional space. In particular, the impulse response is seen as the realization of a zero-mean Gaussian process. Its covariance, the so called stable spline kernel, encodes information on system stability and depends on few hyperparameters estimated from data via marginal likelihood optimization. This approach has been proved to compare much favorably with classical parametric methods but, in data rich situations, a possible drawback may be represented by its computational complexity which scales with the cube of the number of available samples. In this work we design a new computational strategy which may reduce significantly the computational load required by the stable spline estimator, thus extending its practical applicability also to large-scale scenarios.}
}
@article{LOW2020e03083,
title = {Induction approach via P-Graph to rank clean technologies},
journal = {Heliyon},
volume = {6},
number = {1},
pages = {e03083},
year = {2020},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2019.e03083},
url = {https://www.sciencedirect.com/science/article/pii/S2405844019367428},
author = {C.X. Low and W.Y. Ng and Z.A. Putra and K.B. Aviso and M.A.B. Promentilla and R.R. Tan},
keywords = {Chemical engineering, Optimal selection, Simple additive weighting, Clean technologies, Induction, Decision analysis, P-Graph},
abstract = {Identification of appropriate clean technologies for industrial implementation requires systematic evaluation based on a set of criteria that normally reflect economic, technical, environmental and other aspects. Such multiple attribute decision-making (MADM) problems involve rating a finite set of alternatives with respect to multiple potentially conflicting criteria. Conventional MADM approaches often involve explicit trade-offs in between criteria based on the expert's or decision maker's priorities. In practice, many experts arrive at decisions based on their tacit knowledge. This paper presents a new induction approach, wherein the implicit preference rules that estimate the expert's thinking pathways can be induced. P-graph framework is applied to the induction approach as it adds the advantage of being able to determine both optimal and near-optimal solutions that best approximate the decision structure of an expert. The method elicits the knowledge of experts from their ranking of a small set of sample alternatives. Then, the information is processed to induce implicit rules which are subsequently used to rank new alternatives. Hence, the expert's preferences are approximated by the new rankings. The proposed induction approach is demonstrated in the case study on the ranking of Negative Emission Technologies (NETs) viability for industry implementation.}
}
@incollection{ALEKSANDER200599,
title = {Machine consciousness},
editor = {Steven Laureys},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {150},
pages = {99-108},
year = {2005},
booktitle = {The Boundaries of Consciousness: Neurobiology and Neuropathology},
issn = {0079-6123},
doi = {https://doi.org/10.1016/S0079-6123(05)50008-6},
url = {https://www.sciencedirect.com/science/article/pii/S0079612305500086},
author = {Igor Aleksander},
abstract = {The work from several laboratories on the modeling of consciousness is reviewed. This ranges, on one hand, from purely functional models where behavior is important and leads to an attribution of consciousness to, on the other hand, material work closely derived from the information about the anatomy of the brain. At the functional end of the spectrum, applications are described specifically directed at a job-finding problem, where the person being served should not discern between being served by a conscious human or a machine. This employs an implementation of global workspace theories. At the material end, attempts at modeling attentional brain mechanisms, and basic biochemical processes in children are discussed. There are also general prescriptions for functional schemas that facilitate discussions for the presence of consciousness in computational systems and axiomatic structures that define necessary architectural features without which it would be difficult to represent sensations. Another distinction between these two approaches is whether one attempts to model phenomenology (material end) or not (functional end). The former is sometimes called “synthetic phenomenology.” The upshot of this chapter is that studying consciousness through the design of machines is likely to have two major outcomes. The first is to provide a wide-ranging computational language to express the concept of consciousness. The second is to suggest a wide-ranging set of computational methods for building competent machinery that benefits from the flexibility of conscious representations.}
}
@article{ZHANG2025127717,
title = {CCMA: A framework for cascading cooperative multi-agent in autonomous driving merging using Large Language Models},
journal = {Expert Systems with Applications},
pages = {127717},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127717},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425013399},
author = {Miao Zhang and Zhenlong Fang and Tianyi Wang and Shuai Lu and Xueqian Wang and Tianyu Shi},
keywords = {Large Language Model, Autonomous driving, Reinforcement Learning, In-context learning, Multi-agent system, Cooperative merging},
abstract = {Traditional Reinforcement Learning (RL) suffers from challenges in replicating human-like behaviors, generalizing effectively in multi-agent scenarios, and overcoming inherent interpretability issues. These tasks become even more difficult when they require a deep understanding of the environment, coordination of agents’ intentions and driving styles across various scenarios, and the overall optimization of safety, efficiency, and comfort in dynamic environments. Recently, Large Language Model (LLM) enhanced methods have shown promise in improving generalization and interoperability. However, these approaches primarily focus on single-agent scenarios and often neglect the necessary coordination among multiple road users. Therefore, in this paper, we introduce the Cascading Cooperative Multi-agent (CCMA) framework, designed to address these challenges by enhancing human-like behaviors and fostering multi-level cooperation across diverse multi-agent driving tasks, ultimately improving both micro and macro-level performance in complex driving environments. Specifically, the CCMA framework integrates RL for individual interactions, a fine-tuned LLM for regional cooperation, a reward function for global optimization, and the Retrieval-augmented Generation mechanism to dynamically optimize decision-making across complex driving scenarios. Our experiments demonstrate that our CCMA method not only enhances human-like behaviors and interpretability, but also outperforms other state-of-the-art RL methods in multi-agent environments. These findings highlight the significant impact of cascading coordinated communication and dynamic functional alignment in advanced, human-like multi-agent autonomous driving environments. Our project page is https://miaorain.github.io/rainrun.github.io/.}
}
@article{VIEIRA2020106268,
title = {Symmetry exploitation to reduce impedance evaluations in grounding grids},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {123},
pages = {106268},
year = {2020},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2020.106268},
url = {https://www.sciencedirect.com/science/article/pii/S0142061519342188},
author = {Pedro H.N. Vieira and Rodolfo A.R. Moura and Marco Aurélio O. Schroeder and Antonio C.S. Lima},
keywords = {Electromagnetic analysis, Frequency response, Grounding, Method of moments, Numerical methods, Symmetry},
abstract = {One main concern on wideband evaluation of grounding systems is the high computational burden related to the determination of the impedance matrices. Traditionally, one has to divide any given conductor in a large number of segments which leads to a rather time consuming procedure. However, there are a number of geometrical symmetries that if exploited can significantly reduce the overall computational time. This work aims at investigating the adequacy of using some existing symmetries to reduce computer burden in the assessment of a wideband grounding system in models based on the Method of Moments. An algorithmic approach is proposed to extend the symmetry exploitation to arbitrarily oriented uniform rectangular grounding systems. Several topologies are used to assess the performance of the proposed approach. According to results, the proposed methodology can be more than 12 times faster than the traditional approach without loss of accuracy because it is not a numerical approximation.}
}
@article{GROOTHUIJSEN2024100290,
title = {AI chatbots in programming education: Students’ use in a scientific computing course and consequences for learning},
journal = {Computers and Education: Artificial Intelligence},
volume = {7},
pages = {100290},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100290},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000936},
author = {Suzanne Groothuijsen and Antoine {van den Beemt} and Joris C. Remmers and Ludo W. {van Meeuwen}},
keywords = {AI chatbots, ChatGPT, Programming education, Pair programming, Student learning, Engineering education},
abstract = {Teaching and learning in higher education require adaptation following students' inevitable use of AI chatbots. This study contributes to the empirical literature on students' use of AI chatbots and how they influence learning. The aim of this study is to identify how to adapt programming education in higher engineering education. A mixed-methods case study was conducted of a scientific computing course in a Mechanical Engineering Master's program at a Eindhoven University of Technology in the Netherlands. Data consisted of 29 student questionnaires, a semi-structured group interview with three students, a semi-structured interview with the teacher, and 29 students' grades. Results show that students used ChatGPT for error checking and debugging of code, increasing conceptual understanding, generating, and optimizing solution code, explaining code, and solving mathematical problems. While students reported advantages of using ChatGPT, the teacher expressed concerns over declining code quality and student learning. Furthermore, both students and teacher perceived a negative influence from ChatGPT usage on pair programming, and consequently on student collaboration. The findings suggest that learning objectives should be formulated in more detail, to highlight essential programming skills, and be expanded to include the use of AI tools. Complex programming assignments remain appropriate in programming education, but pair programming as a didactic approach should be reconsidered in light of the growing use of AI Chatbots.}
}
@article{ARTHURS2013443,
title = {Efficient simulation of cardiac electrical propagation using high-order finite elements II: Adaptive p-version},
journal = {Journal of Computational Physics},
volume = {253},
pages = {443-470},
year = {2013},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2013.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0021999113004841},
author = {Christopher J. Arthurs and Martin J. Bishop and David Kay},
keywords = {Adaptive finite element method, -version, Monodomain simulation, Computational cardiology, Numerical efficiency},
abstract = {We present a computationally efficient method of simulating cardiac electrical propagation using an adaptive high-order finite element method to automatically concentrate computational effort where it is most needed in space on each time-step. We drive the adaptivity using a residual-based error indicator, and demonstrate using norms of the error that the indicator allows us to control it successfully. Our results using two-dimensional domains of varying complexity demonstrate that significant improvements in efficiency are possible over the standard linear FEM in our single-thread studies, and our preliminary three-dimensional results suggest that improvements are also possible in 3D. We do not work in parallel or investigate the challenges for adaptivity such as dynamic load-balancing which are associated with parallelisation. However, based upon recent work demonstrating that in some circumstances and with moderate processor counts parallel h-adaptive methods are efficient, and upon the claim that p-adaptivity will outperform h-adaptivity, we argue that p-adaptivity should be investigated for efficiency in parallel for simulation on moderate numbers of processors.}
}
@incollection{GINSBURGH2006947,
title = {Chapter 27 The Computation of Prices Indices},
editor = {Victor A. Ginsburg and David Throsby},
series = {Handbook of the Economics of Art and Culture},
publisher = {Elsevier},
volume = {1},
pages = {947-979},
year = {2006},
issn = {1574-0676},
doi = {https://doi.org/10.1016/S1574-0676(06)01027-1},
url = {https://www.sciencedirect.com/science/article/pii/S1574067606010271},
author = {Victor Ginsburgh and Jianping Mei and Michael Moses},
keywords = {prices indices, repeat sales, hedonic pricing, auctions},
abstract = {While there are no significant investment characteristics that inhibit art from being considered as an asset, a major hurdle has long been the lack of a systematic measure of its financial performance. Due to its heterogeneity (each piece is different) and its infrequency of trading (the exact same piece does not come to the market very often), the determination of changes in market value is difficult to ascertain. Two estimation methods are commonly used to construct indices. Repeat-sales regression (RSR) uses prices of individual objects traded at two distinct moments in time. If the characteristics of an object do not change (which is usually so for collectibles), the heterogeneity issue is bypassed. The basic idea of the hedonic regression (HR) method is to regress prices on various attributes of objects (dimensions, artist, subject matter, etc.) and to use the residuals of the regression which can be considered as “characteristic-free prices” to compute the price index. The chapter deals with the basics of hedonic and repeat-sales estimators, and tries to interpret in economic terms what both are trying to achieve. It also goes into some more technical details which may be useful for researchers who want to construct such indices, and gives some guidelines on how to go about collecting data, and the choice between RSR and HR that this induces. Both methods are compared using simulated returns, pointing to which method should be used given the data at hand.}
}
@article{PRATO201888,
title = {Considering built environment and spatial correlation in modeling pedestrian injury severity},
journal = {Traffic Injury Prevention},
volume = {19},
number = {1},
pages = {88-93},
year = {2018},
issn = {1538-9588},
doi = {https://doi.org/10.1080/15389588.2017.1329535},
url = {https://www.sciencedirect.com/science/article/pii/S1538958822003630},
author = {Carlo G. Prato and Sigal Kaplan and Alexandre Patrier and Thomas K. Rasmussen},
keywords = {Pedestrian crashes, injury severity models, built environment, spatial correlation},
abstract = {ABSTRACT
Objective: This study looks at mitigating and aggravating factors that are associated with the injury severity of pedestrians when they have crashes with another road user and overcomes existing limitations in the literature by focusing attention on the built environment and considering spatial correlation across crashes. Method: Reports for 6,539 pedestrian crashes occurred in Denmark between 2006 and 2015 were merged with geographic information system resources containing detailed information about the built environment and exposure at the crash locations. A linearized spatial logit model estimated the probability of pedestrians sustaining a severe or fatal injury conditional on the occurrence of a crash with another road user. Results: This study confirms previous findings about older pedestrians and intoxicated pedestrians being the most vulnerable road users and crashes with heavy vehicles and in roads with higher speed limits being related to the most severe outcomes. This study provides novel perspectives by showing positive spatial correlations of crashes with the same severity outcomes and emphasizing the role of the built environment in the proximity of the crash. Conclusions: This study emphasizes the need for thinking about traffic calming measures, illumination solutions, road maintenance programs, and speed limit reductions. Moreover, this study emphasizes the role of the built environment, because shopping areas, residential areas, and walking traffic density are positively related to a reduction in pedestrian injury severity. Often, these areas have in common a larger pedestrian mass that is more likely to make other road users more aware and attentive, whereas the same does not seem to apply to areas with lower pedestrian density.}
}
@article{SRINIVAS199799,
title = {Strategic decision-making processes: network-based representation and stochastic simulation},
journal = {Decision Support Systems},
volume = {21},
number = {2},
pages = {99-110},
year = {1997},
note = {Special Issue: Expertise and Modeling Expert Decision Making},
issn = {0167-9236},
doi = {https://doi.org/10.1016/S0167-9236(97)00023-7},
url = {https://www.sciencedirect.com/science/article/pii/S0167923697000237},
author = {V. Srinivas and B. Shekar},
keywords = {Qualitative probabilistic networks, Stochastic simulation, Cognitive maps, Strategic thinking, Decision-making process, Network-based representation},
abstract = {Representation of decision-making in organizations is an intricate process. Qualitative Probabilistic Network (QPN)-based approach offers a scheme which is useful for representing processes involved in decision-making. This paper demonstrates the usefulness of QPN-based scheme with an illustrative case study. The focus of the case study is on understanding the strategic behavior of a key player in the Indian Automobile Industry. This is done by transforming Cognitive Maps developed into QPN-based formalisms and analyzing them. In addition to this, stochastic simulation experiment is performed on the QPN-based networks to generate hypothetical scenarios.}
}
@article{ERIOLI2011729,
title = {Interwoven landscape},
journal = {Procedia Engineering},
volume = {21},
pages = {729-736},
year = {2011},
note = {2011 International Conference on Green Buildings and Sustainable Cities},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2011.11.2071},
url = {https://www.sciencedirect.com/science/article/pii/S1877705811049058},
author = {Alessio Erioli and Mirco Bianchini and Piero Bruschi and Andrea Baschieri},
keywords = {architecture, ecology, infrastructure, highway, photocatalysis, dazzle, new materials ;},
abstract = {Human specie has always engineered the environment to set the conditions for its own settlement, producing in its evolutionary development superorganisms (cities) and the necessary networks of connections among them. Instead of rejecting cars as an extraneous object to a picturesque nature, this project starts from a perspective in which cities and technology are the metabolic extension of human specie and therefore a necessary part of its own nature; the vessels (vehicles) for human transportation, or better, the vehicle-host symbiotic system thus becomes a necessary part of human ecology, and so the network of connections upon which they live, operate and interact with: infrastructures. The project of an environmental enhancer for the Nogara mare highway in Veneto (Italy) provides the unique chance to bring together ecological thinking, host interaction and active materials. Its location (an open country planar area among cultivated fields) enucleates as critical variables the impact of pollutants and the phenomenon of dazzling. With respect to such criticalities, the project uses digital generative and parametric strategies to generate a performative structure in which densification and rarefaction of elements is a local morphological response to dazzle. The structure itself acts as a scaffold for a photo catalytic PET based material that, mimicking the behavior of coccoluti (marine microorganisms) is able to reduce CO2 (and potentially other pollutants) to salts and nitrates that are then naturally deployed to the neighboring cultivated fields as fertilizers. The material has been tested for photo catalytic integration and is currently under development. Present building and production techniques privilege the industrial assembly of inert materials, with a one-way flow of energy and process from raw material to finished product. Instead of this mono-directional energy consumption the project promotes the continuous exchange of information (as code and matter-energy) at all levels and from the digital to the material domains: use of dazzle information, morphogenetic rules and structural behavior to generate the scaffold, a photo catalytic material that responds to pollutants and produces fertilizers, making the structure symbiotic with their hosts and the environment.}
}
@article{YIN2024110392,
title = {Embrace sustainable AI: Dynamic data subset selection for image classification},
journal = {Pattern Recognition},
volume = {151},
pages = {110392},
year = {2024},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.110392},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324001432},
author = {Zimo Yin and Jian Pu and Ru Wan and Xiangyang Xue},
keywords = {Data selection, Dynamic subset selection, Weighted sampling, Class distribution, Training efficiency},
abstract = {Data selection is commonly used to reduce costs and energy usage by training on a subset of available data. However, determining the appropriate subset size requires extensive dataset knowledge and experimentation, limiting transferability. Varying the validation set also produces unstable results and wastes computational resources. In this paper, we propose a data selection method for dynamically determining subset ratios based on model performance using only a training set. The data search space is narrowed through weighted sampling, leveraging statistical selection patterns. Parallel analysis of class distributions identifies the most representative samples with high selection potential. Extensive experiments validate our approach and demonstrate improved training efficiency. Our method speeds up various subset ratios by up to 2.2x on CIFAR-10, 1.9x on CIFAR-100, 2.0x on TinyImageNet, and 2.3x on ImageNet with negligible accuracy drops.}
}
@incollection{HORRIGAN2004317,
title = {A Study in the Process of Planning, Designing and Executing a Survey Program: The BLS American Time-Use Survey},
series = {Contributions to Economic Analysis},
publisher = {Elsevier},
volume = {271},
pages = {317-350},
year = {2004},
booktitle = {The Economics of Time Use},
issn = {0573-8555},
doi = {https://doi.org/10.1016/S0573-8555(04)71012-3},
url = {https://www.sciencedirect.com/science/article/pii/S0573855504710123},
author = {Michael Horrigan and Diane Herz},
keywords = {US, time use, survey, American time-use survey},
abstract = {In this study, we describe the evolution of the American time-use survey (ATUS) from its inception as an issue of statistical policy interest in 1991 to its implementation in January 2003 as an ongoing monthly survey sponsored by the US Bureau of Labor Statistics. This 12-year process included four developmental phases. Each successive phase represented a deeper level of agency commitment and outside statistical support. The resulting reports referenced in the text reflect an evolution in our thinking on survey estimation objectives, units of measurement, universe frame and sampling plan, and data collection and coding protocols.}
}
@article{BANDARAGODA2019104424,
title = {Enabling Collaborative Numerical Modeling in Earth Sciences using Knowledge Infrastructure},
journal = {Environmental Modelling & Software},
volume = {120},
pages = {104424},
year = {2019},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2019.03.020},
url = {https://www.sciencedirect.com/science/article/pii/S1364815219301562},
author = {C. Bandaragoda and A. Castronova and E. Istanbulluoglu and R. Strauch and S.S. Nudurupati and J. Phuong and J.M. Adams and N.M. Gasparini and K. Barnhart and E.W.H. Hutton and D.E.J. Hobley and N.J. Lyons and G.E. Tucker and D.G. Tarboton and R. Idaszak and S. Wang},
keywords = {Cyberinfrastructure, Knowledge infrastructure, Reproducible modeling, Landlab, HydroShare, Earth science education},
abstract = {Knowledge infrastructure is an intellectual framework for creating, sharing, and distributing knowledge. In this paper, we use knowledge infrastructure to address common barriers to entry into numerical modeling in Earth sciences as demonstrated in three computational narratives: physical process modeling education, replicating published model results, and reusing published models to extend research. We outline six critical functional requirements: 1) workflows designed for new users; 2) community-supported collaborative web platform; 3) distributed data storage; 4) software environment; 5) personalized cloud-based high-performance computing platform; and 6) a standardized open source modeling framework. Our methods meet these functional requirements by providing three interactive computational narratives for hands-on, problem-based research using Landlab on HydroShare. Landlab is an open-source toolkit for building, coupling, and exploring two-dimensional numerical models. HydroShare is an online collaborative environment for the sharing of data and models. We describe the methods we are using to accelerate knowledge development by providing a suite of modular and interoperable process components that allows students, domain experts, collaborators, researchers, and sponsors to learn by exploring shared data and modeling resources. The system is designed to support uses on the continuum from fully-developed modeling applications to prototyping research software tools. Landlab notebooks are available for interactive computing on HydroShare at https://doi.org/10.4211/hs.fdc3a06e6ad842abacfa5b896df73a76 and for further development on Github at https://zenodo.org/badge/latestdoi/187289993.}
}
@article{PALANIYAPPAN2020109911,
title = {Cortical thickness and formal thought disorder in schizophrenia: An ultra high-field network-based morphometry study},
journal = {Progress in Neuro-Psychopharmacology and Biological Psychiatry},
volume = {101},
pages = {109911},
year = {2020},
issn = {0278-5846},
doi = {https://doi.org/10.1016/j.pnpbp.2020.109911},
url = {https://www.sciencedirect.com/science/article/pii/S0278584619310309},
author = {Lena Palaniyappan and Ali Al-Radaideh and Penny A. Gowland and Peter F. Liddle},
keywords = {Disorganisation, Thought disorder, Salience network, Cognitive control, Language network, Coherence},
abstract = {Background
Persistent formal thought disorder (FTD) is a core feature of schizophrenia. Recent cognitive and neuroimaging studies indicate a distinct mechanistic pathway underlying the persistent positive FTD (pFTD or disorganized thinking), though its structural determinants are still elusive. Using network-based cortical thickness estimates from ultra-high field 7-Tesla Magnetic Resonance Imaging (7T MRI), we investigated the structural correlates of pFTD.
Methods
We obtained speech samples and 7T MRI anatomical scans from medicated clinically stable patients with schizophrenia (n = 19) and healthy controls (n = 20). Network-based morphometry was used to estimate the mean cortical thickness of 17 functional networks covering the entire cortical surface from each subject. We also quantified the vertexwise variability of thickness within each network to quantify the spatial coherence of the 17 networks, estimated patients vs. controls differences, and related the thickness of the affected networks to the severity of pFTD.
Results
Patients had reduced thickness of the frontoparietal and default mode networks, and reduced spatial coherence affecting the salience and the frontoparietal control network. A higher burden of positive FTD related to reduced frontoparietal thickness and reduced spatial coherence of the salience network. The presence of positive FTD, but not its severity, related to the reduced thickness of the language network comprising of the superior temporal cortex.
Conclusions
These results suggest that cortical thickness of both cognitive control and language networks underlie the positive FTD in schizophrenia. The structural integrity of cognitive control networks is a critical determinant of the expressed severity of persistent FTD in schizophrenia.}
}
@article{YAHIAOUI20243958,
title = {Two parallel expansions for improving supersonic axisymmetric nozzle performance},
journal = {Advances in Space Research},
volume = {74},
number = {8},
pages = {3958-3982},
year = {2024},
issn = {0273-1177},
doi = {https://doi.org/10.1016/j.asr.2024.06.066},
url = {https://www.sciencedirect.com/science/article/pii/S0273117724006562},
author = {Toufik Yahiaoui},
keywords = {MLN, BPN, DEN, HT, MOC, Error computation},
abstract = {The aim of this work is to develop a numerical computation program allowing designing new contours of a supersonic axisymmetric nozzle having two expansions at the throat, named by DEN (Dual Expansion Nozzle). This new nozzle gives a uniform and parallel flow at the exit section, to improve considerably the performances compared to the conventional Minimum Length Nozzle (MLN), and the Best Performances Nozzle (BPN). The present nozzle has a two unknowns external and central body curved walls. Each of them is started by an initial expansion angle to give a uniform and horizontal flow at the exit section. Two others transition regions are calculated in parallel with the contours points to give the desired exit Mach number. The walls are determined point by point by the High Temperature Method of Characteristics (HT MOC) model. The resolution of the four compatibility and characteristics equations is done numerically by the finite difference predictor corrector algorithm. The validation of the results is controlled by the convergence of the calculated critical sections ratio to that given by the theory. The design depends on four parameters, where MLN and BPN become special cases of DEN. A comparison is made with MLN, since it is currently used in the aerospace propulsion and with BPN aiming to improve their performances. The comparison is made for the same critical mass flow rate. The results demonstrate a remarkable reduction up of 45 %, and 52 % in the mass of DEN when the exit Mach number ME = 3.00 and the stagnation temperature T0 = 2000 K. The application is made for air and for future aerospace missiles in order to improve their trajectory parameters. The chosen example demonstrates an improvement of 13 % and 16 % on the missile range compared, respectively to MLN, and BPN.}
}
@article{ALONSO2025100509,
title = {A novel approach for job matching and skill recommendation using transformers and the O*NET database},
journal = {Big Data Research},
volume = {39},
pages = {100509},
year = {2025},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2025.100509},
url = {https://www.sciencedirect.com/science/article/pii/S2214579625000048},
author = {Rubén Alonso and Danilo Dessí and Antonello Meloni and Diego {Reforgiato Recupero}},
keywords = {Information extraction, Transformers, Online enrolling process, Natural language processing, Course recommendation},
abstract = {Today we have tons of information posted on the web every day regarding job supply and demand which has heavily affected the job market. The online enrolling process has thus become efficient for applicants as it allows them to present their resumes using the Internet and, as such, simultaneously to numerous organizations. Online systems such as Monster.com, OfferZen, and LinkedIn contain millions of job offers and resumes of potential candidates leaving to companies with the hard task to face an enormous amount of data to manage to select the most suitable applicant. The task of assessing the resumes of candidates and providing automatic recommendations on which one suits a particular position best has, therefore, become essential to speed up the hiring process. Similarly, it is important to help applicants to quickly find a job appropriate to their skills and provide recommendations about what they need to master to become eligible for certain jobs. Our approach lies in this context and proposes a new method to identify skills from candidates' resumes and match resumes with job descriptions. We employed the O*NET database entities related to different skills and abilities required by different jobs; moreover, we leveraged deep learning technologies to compute the semantic similarity between O*NET entities and part of text extracted from candidates' resumes. The ultimate goal is to identify the most suitable job for a certain resume according to the information there contained. We have defined two scenarios: i) given a resume, identify the top O*NET occupations with the highest match with the resume, ii) given a candidate's resume and a set of job descriptions, identify which one of the input jobs is the most suitable for the candidate. The evaluation that has been carried out indicates that the proposed approach outperforms the baselines in the two scenarios. Finally, we provide a use case for candidates where it is possible to recommend courses with the goal to fill certain skills and make them qualified for a certain job.}
}
@article{PIRES2024625,
title = {Selection of Naval Bases and Stations for submarines: a multimethodological approach},
journal = {Procedia Computer Science},
volume = {242},
pages = {625-632},
year = {2024},
note = {11th International Conference on Information Technology and Quantitative Management (ITQM 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.08.119},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924018386},
author = {Tullio Pires and Celio Manso {de Azevedo Junior} and Mateus Vanzetta and Marcos {dos Santos} and Carlos Francisco {Simões Gomes}},
keywords = {Submarines, Naval Base, Multicriteria, MPSI-MARA, SCA},
abstract = {With the PROSUB program, the Brazilian Navy (MB) has been renewing its feet of submarines. However, this is not a movement that is exclusively Brazilian. With the worsening of crises around the world, many countries are in the process of expanding their armed forces, and coastal nations, in particular, are paying significant attention to their submarine weapons. However, as it is not only convenient to acquire submarines but also to operate them, it is necessary to define from where they will do so. Given the above, this current work aims to present a framework with a multimethodological focus, that is, presenting a combined use of the problem structuring method (PSM) Strategic Choice Approach with the multicriteria method MPSI-MARA. As a result, the ordering of some points along the Brazilian coast, made non-specific, is presented as a suggestion for the implementation of new Submarine Bases and/or Naval Support Stations.}
}
@article{YUCEL2019352,
title = {Battling gender stereotypes: A user study of a code-learning game, “Code Combat,” with middle school children},
journal = {Computers in Human Behavior},
volume = {99},
pages = {352-365},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2019.05.029},
url = {https://www.sciencedirect.com/science/article/pii/S0747563219302109},
author = {Yeliz Yücel and Kerem Rızvanoğlu},
keywords = {User experience (UX), Digital gender divide, Gender stereotypes, Stereotype threat, Games, Serious games},
abstract = {Abstract.
Gender has been consistently controlled as a variable in usability and playability tests. However, there is no consensus on whether and how gender differences should influence the design of digital environments. According to some research, digital environments may be unintentionally designed especially for males as a result of the existing gender biases which risks reproducing gender-polarized culture in a computational field. This study attempts to highlight that females are still being negatively affected by existing gender stereotypes and prescribed gender identities despite relatively equal access and use of computer technology. This qualitative study aims to provide insights about the first-time user experience in a home environment of 16 middle school children in Turkey (8 males - 8 females), aged between 11 and 14 years, with a code learning game named “Code Combat”. The analysis is supported with complementary quantitative findings. The present study investigates the participants' conceptualizations and opinions toward coding concept and this specific coding game. Further, it explores how existing gender stereotypes and gender biased expectations impact their behaviors and attitudes in the context of game experience. Our results indicated that perceived computer competence and perceived coding difficulty had important effects on the participants’ performance relatedly with their gender identity. According to our findings, there are important gender differences to be found in our 9 constructs, namely; perceived computer competence, perceived coding difficulty, identification, perceived game difficulty, perceived success, level of enjoyment, level of anxiety, the likelihood of playing it another time and the likelihood of trying new features.}
}
@incollection{YANG2001291,
title = {Curved Beam Theories and Related Computational Aspects},
editor = {S. Valliappan and N. Khalili},
booktitle = {Computational Mechanics–New Frontiers for the New Millennium},
publisher = {Elsevier},
address = {Oxford},
pages = {291-298},
year = {2001},
isbn = {978-0-08-043981-5},
doi = {https://doi.org/10.1016/B978-0-08-043981-5.50046-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780080439815500468},
author = {Yeong-Bin Yang},
keywords = {Buckling, curved beam, curved beam element, joint equilibrium, stability, straight beam element},
abstract = {ABSTRACT
The theories of buckling for horizontal curved beams presented by Timoshenko, Vlasov, Yoo, and Yang and Kuo are first reviewed, with their key features identified. The previous argument concerning the incapability of straight beam elements to predict the buckling loads of curved beams in incorrect, due to overlook of the conditions of equilibrium for structural joints connecting non-aligned members in the deformed position, as implied by conventional finite element approaches. If such conditions are duly taken into account, then the straight beam elements derived, which are referred to as the semitangential elements, can be used as a reliable tool for predicting the buckling loads of curved beams. Moreover, by simulating a curved beam in the limit as an infinite number of infinitesimal elements, the theory for straight beams can be manipulated through use of the concept of transfer matrix to yield the ones for curved beams for the cases of uniform bending and uniform compression. It is in this sense that the theories of straight beams and curved beams are unified.}
}
@article{UPADHYAY2024109796,
title = {Advancements in Alzheimer's disease classification using deep learning frameworks for multimodal neuroimaging: A comprehensive review},
journal = {Computers and Electrical Engineering},
volume = {120},
pages = {109796},
year = {2024},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2024.109796},
url = {https://www.sciencedirect.com/science/article/pii/S0045790624007237},
author = {Prashant Upadhyay and Pradeep Tomar and Satya Prakash Yadav},
keywords = {Multimodal, Neuroimaging, Alzheimer's disease, Classification, Images, Feature extraction},
abstract = {Over the past years, Alzheimer's disease has emerged as a serious concern for people's health. Researchers are facing challenges in effectively categorizing and diagnosing the different stages of Alzheimer's disease (AD). Current promising studies have shown that multimodal Neuroimaging has the potential to offer vital information about the structural and functional alterations associated with Alzheimer's. Using advanced computational techniques, Machine Learning calculations have been demonstrated to be highly precise in deciphering patterns and connections within the multimodal Neuroimaging data, eventually aiding in the arrangement of Alzheimer's illness stages. This research aimed to survey the adequacy of Machine Learning techniques in correctly categorizing stages of Alzheimer's disease by working on multiple neuroimaging modalities. In this review, a detailed analysis was carried out on the classification algorithms included. The study specifically examines publications published between 2016 and 2024. From the review, it was found that deep learning frameworks are more robust in Alzheimer's disease classification.}
}
@article{YUAN202317,
title = {MFGAD: Multi-fuzzy granules anomaly detection},
journal = {Information Fusion},
volume = {95},
pages = {17-25},
year = {2023},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523000490},
author = {Zhong Yuan and Hongmei Chen and Chuan Luo and Dezhong Peng},
keywords = {Granular computing, Fuzzy rough set theory, Unsupervised anomaly detection, Multi-granularity, Hybrid data},
abstract = {Unsupervised anomaly detection is an important research direction in the process of unsupervised knowledge acquisition. It has been successfully applied in many fields, such as online fraud identification, loan approval, and medical diagnosis. Multi-granularity thinking is an effective information fusion method for solving problems in a multi-granular environment, which allows people to understand and analyze problems from multiple perspectives. However, there are few studies on building anomaly detection models using the idea of multi-fuzzy granules. To this end, this paper constructs a multi-fuzzy granules anomaly detection method by using a fuzzy rough computing model. In this method, a hybrid metric is first used to calculate the fuzzy relations. Then, two ranking sequences are constructed based on the significance of attributes. Furthermore, forward and reverse multi-fuzzy granules are constructed to define anomaly scores based on the ranking sequences. Finally, a multi-fuzzy granules-based anomaly detection algorithm is designed to detect anomalies. The experimental results compared with existing algorithms show the effectiveness of the proposed algorithm.}
}
@article{RICH2018110,
title = {Participatory systems approaches for urban and peri-urban agriculture planning: The role of system dynamics and spatial group model building},
journal = {Agricultural Systems},
volume = {160},
pages = {110-123},
year = {2018},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2016.09.022},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X16305959},
author = {Karl M. Rich and Magda Rich and Kanar Dizyee},
keywords = {Urban agriculture, System dynamics, Spatial group model building, Participatory processes, Planning, Christchurch},
abstract = {Urban agriculture has become an important research theme in recent years. Over the past decade, a number of different, diverse value chains have been established in the urban areas of developed and developing countries alike, with increasing convergence in their motivations related to food security and livelihoods development, particularly for poor and disadvantaged segments of society. However, for urban agriculture to be sustainable as a livelihoods and resilience strategy will require decision-support tools that allow planners and participants alike to jointly develop strategies and assess potential leverage points within urban food value chains. In this paper, we argue that system dynamics (SD) models combined with participatory approaches have important roles in bridging this gap, though these will need to be adapted to the spatial influences that exist in urban settings. We first review elements of urban agriculture and some of the policy challenges faced in this growing phenomenon. We follow this by motivating the role of SD models in the context of urban agriculture and note their potential utility in overlaying quantitative models of urban food value chains alongside their land-use characteristics, highlighting the dynamic feedbacks between intensive processes within changing urban food systems and extensive processes associated with land-use and planning. From this background, we introduce the concept of spatial group model building (SGMB), which adapts standard group model building concepts to account for both the spatial context of urban agriculture and enables a spatially sensitive, participatory approach to qualitative and quantitative model building. We provide a qualitative proof-of-concept of SGMB principles and techniques in the context of describing the setting and dynamic issues facing organic urban agriculture value chains in Christchurch, New Zealand. Our approach fills an important space between participatory GIS practices and the development of complex spatial system dynamics models, infusing systems thinking principles to participatory processes, while showing a way to enhance the future development of quantitative spatial system dynamics models more generally.}
}
@incollection{CHOUBEY2025319,
title = {Chapter 25 - Future directions on systems biology},
editor = {Babak Sokouti},
booktitle = {Systems Biology and In-Depth Applications for Unlocking Diseases},
publisher = {Academic Press},
pages = {319-328},
year = {2025},
isbn = {978-0-443-22326-6},
doi = {https://doi.org/10.1016/B978-0-443-22326-6.00025-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443223266000250},
author = {Jyotsna Choubey and Jyoti Kant Choudhari and Biju Prava Sahariah},
keywords = {Agriculture, Biomedical research, Bioremediation, Bioresources, Drug discovery, Genetics, Healthcare and medicine, Proteomics},
abstract = {Biologists integrate engineering principles to design, construct, and transform biological systems for specific intents. Biological engineering involves creating new biological components, technologies, and systems, as well as redesigning existing ones, to execute specific functions and solve specific biological problems. To accomplish this objective, engineers utilize their expertise in the creation, assembly, and manipulation of biological systems, such as DNA and cells. The shift in the research paradigm toward systems biology can be largely attributed to significant advancements in protein and DNA sequencing technology. This approach is characterized by its emphasis on creating predictive models that can be applied to all levels of structural hierarchies found in biological systems. In addition, there is a strong focus on integrating data from various scales. The ultimate goal of systems biology is to create bio-based technologies that can be applied in a wide range of fields, including pharmaceuticals, health science, environmental remediation, energy production, and biotechnology. The impact of systems biology is increasingly being observed in various aspects of our lives, and this influence is anticipated to gain momentum in the future. This chapter centers on the historical background and extent of systems biology, its implementation in diverse domains, and its prospective future in various branches of biology.}
}
@article{TSOTSOS2021305,
title = {On the control of attentional processes in vision},
journal = {Cortex},
volume = {137},
pages = {305-329},
year = {2021},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2021.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0010945221000150},
author = {John K. Tsotsos and Omar Abid and Iuliia Kotseruba and Markus D. Solbach},
keywords = {Vision, Attention, Control, Cognitive program, Selective tuning},
abstract = {The study of attentional processing in vision has a long and deep history. Recently, several papers have presented insightful perspectives into how the coordination of multiple attentional functions in the brain might occur. These begin with experimental observations and the authors propose structures, processes, and computations that might explain those observations. Here, we consider a perspective that past works have not, as a complementary approach to the experimentally-grounded ones. We approach the same problem as past authors but from the other end of the computational spectrum, from the problem nature, as Marr's Computational Level would prescribe. What problem must the brain solve when orchestrating attentional processes in order to successfully complete one of the myriad possible visuospatial tasks at which we as humans excel? The hope, of course, is for the approaches to eventually meet and thus form a complete theory, but this is likely not soon. We make the first steps towards this by addressing the necessity of attentional control, examining the breadth and computational difficulty of the visuospatial and attentional tasks seen in human behavior, and suggesting a sketch of how attentional control might arise in the brain. The key conclusions of this paper are that an executive controller is necessary for human attentional function in vision, and that there is a 'first principles' computational approach to its understanding that is complementary to the previous approaches that focus on modelling or learning from experimental observations directly.}
}
@article{LAFORCADE2010347,
title = {A Domain-Specific Modeling approach for supporting the specification of Visual Instructional Design Languages and the building of dedicated editors},
journal = {Journal of Visual Languages & Computing},
volume = {21},
number = {6},
pages = {347-358},
year = {2010},
note = {Special Issue on Visual Instructional Design Languages},
issn = {1045-926X},
doi = {https://doi.org/10.1016/j.jvlc.2010.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S1045926X10000492},
author = {Pierre Laforcade},
keywords = {Visual Instructional Design Languages Domain Specific Modeling, Visual and executable models},
abstract = {This paper presents, illustrates and discusses theories and practices about the application of a domain-specific modeling (DSM) approach to facilitate the specification of Visual Instructional Design Languages (VIDLs) and the development of dedicated graphical editors. Although this approach still requires software engineering skills, it tackles the need of building VIDLs allowing both visual models for human-interpretation purposes (explicit designs, communication, thinking, etc.) and machine-readable notations for deployment or other instructional design activities. This article proposes a theoretical application and a categorization, based on a domain-oriented separation of concerns of instructional design. It also presents some practical illustrations from experiments of specific DSM tooling. Key lessons learned as well as observed obstacles and challenges to deal with are discussed in order to further develop such an approach.}
}
@incollection{PREISIG20121242,
title = {HAZOP - an automaton-inspired approach},
editor = {Ian David Lockhart Bogle and Michael Fairweather},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {30},
pages = {1242-1246},
year = {2012},
booktitle = {22nd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-59520-1.50107-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044459520150107X},
author = {Heinz A Preisig and Flavio Manenti},
keywords = {dynamic system, safety, HAZOP},
abstract = {If there exists a gradient pointing outwards a save-operation region, then there exists a possible path for the plant to cross out of the save-operation region. A method is introduced that allows the identification of such surface pieces in the phase space of the state variable for the analysed equipment. The idea is based on splitting the phase space into subspaces separated by the zero-dynamic component surface. The computation requires only root solving of the right-hand side of the dynamic equations, one at the time.}
}
@article{PHILLIPS200930,
title = {Fiber tractography reveals disruption of temporal lobe white matter tracts in schizophrenia},
journal = {Schizophrenia Research},
volume = {107},
number = {1},
pages = {30-38},
year = {2009},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2008.10.019},
url = {https://www.sciencedirect.com/science/article/pii/S0920996408004854},
author = {Owen R. Phillips and Keith H. Nuechterlein and Kristi A. Clark and Liberty S. Hamilton and Robert F. Asarnow and Nathan S. Hageman and Arthur W. Toga and Katherine L. Narr},
keywords = {Diffusion tensor imaging, White matter, Uncinate fasciculus, Inferior longitudinal fasciculus, Arcuate fasciculus, Fractional anisotropy},
abstract = {Diffusion tensor imaging (DTI) studies have demonstrated abnormal anisotropic diffusion in schizophrenia. However, examining data with low spatial resolution and/or a low number of gradient directions and limitations associated with analysis approaches sensitive to registration confounds may have contributed to mixed findings concerning the regional specificity and direction of results. This study examined three major white matter tracts connecting lateral and medial temporal lobe regions with neocortical association regions widely implicated in systems-level functional and structural disturbances in schizophrenia. Using DTIstudio, a previously validated regions of interest tractography method was applied to 30 direction diffusion weighted imaging data collected from demographically similar schizophrenia (n=23) and healthy control subjects (n=22). The diffusion tensor was computed at each voxel after intra-subject registration of diffusion-weighted images. Three-dimensional tract reconstruction was performed using the Fiber Assignment by Continuous Tracking (FACT) algorithm. Tractography results showed reduced fractional anisotropy (FA) of the arcuate fasciculi (AF) and inferior longitudinal fasciculi (ILF) in patients compared to controls. FA changes within the right ILF were negatively correlated with measures of thinking disorder. Reduced volume of the left AF was also observed in patients. These results, which avoid registration issues associated with voxel-based analyses of DTI data, support that fiber pathways connecting lateral and medial temporal lobe regions with neocortical regions are compromised in schizophrenia. Disruptions of connectivity within these pathways may potentially contribute to the disturbances of memory, language, and social cognitive processing that characterize the disorder.}
}
@article{BENDER2024156,
title = {Dimension results for extremal-generic polynomial systems over complete toric varieties},
journal = {Journal of Algebra},
volume = {646},
pages = {156-182},
year = {2024},
issn = {0021-8693},
doi = {https://doi.org/10.1016/j.jalgebra.2024.01.029},
url = {https://www.sciencedirect.com/science/article/pii/S0021869324000553},
author = {Matías Bender and Pierre-Jean Spaenlehauer},
keywords = {Sparse polynomial systems, Toric varieties},
abstract = {We study polynomial systems with prescribed monomial supports in the Cox ring of a toric variety built from a complete polyhedral fan. We present combinatorial formulas for the dimension of their associated subvarieties under genericity assumptions on the coefficients of the polynomials. Using these formulas, we identify at which degrees generic systems in polytopal algebras form regular sequences. Our motivation comes from sparse elimination theory, where knowing the expected dimension of these subvarieties leads to specialized algorithms and to large speed-ups for solving sparse polynomial systems. As a special case, we classify the degrees at which regular sequences defined by weighted homogeneous polynomials can be found, answering an open question in the Gröbner bases literature. We also show that deciding whether a sparse system is generically a regular sequence in a polytopal algebra is hard from the point of view of theoretical computational complexity.}
}
@article{ABDALLAH2024102341,
title = {An evaluation of the use of air cooling to enhance photovoltaic performance},
journal = {Thermal Science and Engineering Progress},
volume = {47},
pages = {102341},
year = {2024},
issn = {2451-9049},
doi = {https://doi.org/10.1016/j.tsep.2023.102341},
url = {https://www.sciencedirect.com/science/article/pii/S2451904923006947},
author = {Ramez Abdallah and Tamer Haddad and Mohammad Zayed and Adel Juaidi and Tareq Salameh},
keywords = {Photovoltaic, ANSYS fluent, CFD, PV cooling, Heat sink},
abstract = {The rapid rise in global energy consumption and its consequences on climate change has made incorporating renewable energy sources like solar photovoltaics into the building envelope easier. However, in spite of extensive uses and significant technological advances, the lower solar panel efficiencies caused by high temperatures remain a significant barrier to the viability of deploying photovoltaic technology in regions with hot climates utilizing computational fluid dynamics (CFD). This research examines the cooling effectiveness of air-cooled photovoltaic (PV) under the climate of Nablus - Palestine. This study presents a numerical model designed to cool solar panels using various air-cooled channel configurations. Rectangular fins made of high thermal conductivity materials such as copper were used in this study. The parametric study was based on the changing baseplate thickness, fin spacing, height, and thickness through a stepwise optimization process to enhance the heat transfer mechanism. The results show that the optimum design of average volume temperatures for the PV cell models in air-cooled channel configurations with and without fins were 40.28 °C and 42.58 °C, respectively. The optimum design was obtained at 3, 110, 60, and 4 mm for baseplate thickness, fin spacing, height, and thickness, respectively. This optimum design was responsible for the average PV panel temperature drop by 1.6 %, 1.3 %, 5.9 %, and 6.2 % for baseplate thickness, fin spacing, height, and thickness, respectively. The optimum design of an air-cooled cooling channel for PV is an important insight provided by this work, and it may help in the future development of more effective and affordable cooling methods.}
}
@article{LIU2024115,
title = {Water Quality System Informatics: An Emerging Inter-Discipline of Environmental Engineering},
journal = {Engineering},
volume = {43},
pages = {115-124},
year = {2024},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2024.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S2095809924002601},
author = {Hong Liu and Zhaoming Chen and Zhiwei Wang and Ming Xu and Yutao Wang and Jinju Geng and Fengjun Yin},
keywords = {Water quality system, Water quality system informatics, Environmental engineering, Emerging interdisciplinary, Research pattern},
abstract = {Water quality system informatics (WQSI) is an emerging field that employs cybernetics to collect and digitize data associated with water quality. It involves monitoring the physical, chemical, and biological processes that affect water quality and the ecological impacts and interconnections within water quality systems. WQSI integrates theories and methods from water quality engineering, information engineering, and system control theory, enabling the intelligent management and control of water quality. This integration revolutionizes the understanding and management of water quality systems with greater precision and higher resolution. WQSI is a new stage of development in environmental engineering that is driven by the digital age. This work explores the fundamental concepts, research topics, and methods of WQSI and its features and potential to promote disciplinary development. The innovation and development of WQSI are crucial for driving the digital and intelligent transformation of national industry patterns in China, positioning China at the forefront of environmental engineering and ecological environment research on a global scale.}
}
@article{SELKER2005410,
title = {Fostering motivation and creativity for computer users},
journal = {International Journal of Human-Computer Studies},
volume = {63},
number = {4},
pages = {410-421},
year = {2005},
note = {Computer support for creativity},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2005.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S1071581905000443},
author = {Ted Selker},
keywords = {Communicate, Communication, Community, Computers, Creative, Creativity, Creativity-enhancing, Design, Engineering, Filter, Graphics, Human–computer, Idea, Interaction, Motivation, Product, Programming, Social, Support, Text, User interface},
abstract = {Creativity might be viewed as any process which results in a novel and useful product. People use computers for creative tasks; they flesh out ideas for text, graphics, engineering solutions, etc. Computer programming is an especially creative activity, but few tools for programming aid creativity. Computers can be designed to foster creativity as well. As a start, all computer programs should help users enumerate ideas, remember alternatives and support various ways to compare them. More sophisticated thinking aids could implement other successful techniques as well. Most computers are used in solitude; however, people depend on social supports for creativity. User scenarios can provide the important social support and gracious cues normally offered by collaborators that keep people motivated and help them consider alternatives. People also use computers to build community and to communicate. Computers should also support and filter these potentially creativity-enhancing communication acts. User-interface designers are so busy exposing features and fighting bugs that they might ignore their users’ needs for motivation and creativity support. This paper develops the notion that creativity and motivation enhancement can easily be aligned with the design of high-quality human–computer interaction. User interface toolkits and evaluations should include support for motivation and creativity-enhancing approaches.}
}
@article{ZHANG2024603,
title = {An intelligent management and decision model of operational research for edge computing aided planning},
journal = {Alexandria Engineering Journal},
volume = {109},
pages = {603-609},
year = {2024},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2024.09.072},
url = {https://www.sciencedirect.com/science/article/pii/S1110016824010962},
author = {Yuanshou Zhang},
keywords = {Intelligent Decision-making Model, Edge Algorithm, Task Offloading, Mobile Edge Computing Server},
abstract = {With the development of “Internet plus” and the deep integration of information and network based technology and network technology, Industry 4.0 has become a hot topic. Industrial production is faced with a large number of tasks, complex and changeable demands, and difficult to predict. In order to solve a series of objective problems such as task delay in actual production, this paper proposed a method based on edge computing (EC) to reduce the delay to meet the real-time requirements of industrial production. However, due to the limitation of its computing power and storage capacity, it is difficult to adapt to large-scale data decision-making. In terms of the lag rate of the algorithm in the experiment of the intelligent decision model, when the number of tasks of EC algorithm was 15 and 6, the lag rate was the highest and the lowest, and its values were 16 % and 2 % respectively. Therefore, it can be seen that EC algorithm can play a good role in the intelligent management and decision-making model of operational research.}
}
@article{CASAJUS202488,
title = {Random partitions, potential, value, and externalities},
journal = {Games and Economic Behavior},
volume = {147},
pages = {88-106},
year = {2024},
issn = {0899-8256},
doi = {https://doi.org/10.1016/j.geb.2024.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S089982562400085X},
author = {André Casajus and Yukihiko Funaki and Frank Huettner},
keywords = {Shapley value, Partition function form, Random partition, Restriction operator, Ewens distribution, Chinese restaurant process, Potential, Externalities, Null player, Expected accumulated worth},
abstract = {The Shapley value equals a player's contribution to the potential of a game. The potential is a most natural one-number summary of a game, which can be computed as the expected accumulated worth of a random partition of the players. This computation integrates the coalition formation of all players and readily extends to games with externalities. We investigate those potential functions for games with externalities that can be computed this way. It turns out that the potential that corresponds to the MPW solution introduced by Macho-Stadler et al. (2007, J. Econ. Theory 135, 339–356) is unique in the following sense. It is obtained as the expected accumulated worth of a random partition, it generalizes the potential for games without externalities, and it induces a solution that satisfies the null player property even in the presence of externalities.}
}
@article{PERRIN20227006,
title = {Malonic Anhydrides, Challenges from a Simple Structure},
journal = {The Journal of Organic Chemistry},
volume = {87},
number = {11},
pages = {7006-7012},
year = {2022},
issn = {0022-3263},
doi = {https://doi.org/10.1021/acs.joc.2c00453},
url = {https://www.sciencedirect.com/science/article/pii/S0022326322022642},
author = {Charles L. Perrin},
abstract = {ABSTRACT
After many years of unsuccessful attempts, monomeric malonic anhydrides were prepared by ozonolysis of ketene dimers, a procedure validated by model studies. The structure proof relied most heavily on IR absorption at 1820 cm–1 and a Raman band at 1947 cm–1. Malonic anhydrides are unstable, decomposing below room temperature to a ketene plus carbon dioxide. Surprisingly, according to kinetic studies, the dimethyl derivative is slightly less unstable than the parent, and the monomethyl is the fastest to decompose, with an enthalpy of activation of only 12.6 kcal/mol. Computations rationalize this behavior in terms of a concerted [2s + 2a] cycloreversion that requires a more highly organized transition state, as also manifested by a negative entropy of activation.}
}
@article{WARNIER201715,
title = {Distributed monitoring for the prevention of cascading failures in operational power grids},
journal = {International Journal of Critical Infrastructure Protection},
volume = {17},
pages = {15-27},
year = {2017},
issn = {1874-5482},
doi = {https://doi.org/10.1016/j.ijcip.2017.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S1874548216300427},
author = {Martijn Warnier and Stefan Dulman and Yakup Koç and Eric Pauwels},
keywords = {Power Grids, Cascading Failures, Robustness, Real-Time Monitoring, Distributed Computation},
abstract = {Electrical power grids are vulnerable to cascading failures that can lead to large blackouts. The detection and prevention of cascading failures in power grids are important problems. Currently, grid operators mainly monitor the states (loading levels) of individual components in a power grid. The complex architecture of a power grid, with its many interdependencies, makes it difficult to aggregate the data provided by local components in a meaningful and timely manner. Indeed, monitoring the resilience of an operational power grid to cascading failures is a major challenge. This paper attempts to address this challenge. It presents a robustness metric based on the topology and operative state of a power grid to quantify the robustness of the grid. Also, it presents a distributed computation method with self-stabilizing properties that can be used for near real-time monitoring of grid robustness. The research thus provides insights into the resilience of a dynamic operational power grid to cascading failures during real-time in a manner that is both scalable and robust. Computations are pushed to the power grid network, making the results available at each node and enabling automated distributed control mechanisms to be implemented.}
}
@article{SMOLARCZYK2024100669,
title = {Let’s get them on board: Focus group discussions with adolescents on empowering leisure engagement in Fab Labs and makerspaces},
journal = {International Journal of Child-Computer Interaction},
volume = {41},
pages = {100669},
year = {2024},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100669},
url = {https://www.sciencedirect.com/science/article/pii/S2212868924000370},
author = {Kathrin Smolarczyk and Marios Mouratidis and Sophie Uhing and Rolf Becker and Stephan Kröner},
keywords = {Maker activities, Leisure, Youth, Focus groups, Sustainability},
abstract = {Makerspaces and Fab Labs are growing in number all over the world, holding the potential to empower children and adolescents. They form an important pathway to provide young people with access to digital manufacturing technologies while fostering self-determination, collaboration, and creativity. We explore how the engagement in Fab Lab based leisure maker activities may be promoted, taking into account both the perspectives of adolescents and the potential of surrounding systems. For this, we conducted focus group discussions with N = 61 non-maker, adolescent girls and boys from 6th to 9th grade, to scrutinize hindering and promoting factors of their engagement in leisure maker activities, and to explore their preferences regarding the involvement of parents, teachers and peers while considering the ecological sustainability of the activities. A reflexive thematic analysis identified the hindering and promoting factors across different aspects of maker activities such as the purpose, location and setting, content, and learning processes. Implications for the promotion and design of maker activities, as well as implications for further research, are discussed.}
}
@article{KAZIEVA20242933,
title = {Unpacking Complex Concepts to Enhance Use of Dynamic Simulations},
journal = {Procedia Computer Science},
volume = {246},
pages = {2933-2942},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.377},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924024098},
author = {Victoria Kazieva},
keywords = {simulation modeling, decision-making, complex systems, dynamic simulations, agent-based modeling, system dynamics},
abstract = {The paper suggests a research framework that can aid simulation model designers and users in understanding and modeling complex concepts. The aim is to enhance the role of simulation in supporting decision-making with agent-based modeling and system dynamics by investigating literature that outlines challenges in the field. This study advocates for involving decision-makers in unpacking the overarching concept into manageable components that are associates with the top concept. The research framework generates insights into the interpretation of complex concepts and guides model designers in formulating concrete variables for simulation modeling through successive iterations of the unpacking practices.}
}
@article{PETRELLA2024139,
title = {The AI Future of Emergency Medicine},
journal = {Annals of Emergency Medicine},
volume = {84},
number = {2},
pages = {139-153},
year = {2024},
issn = {0196-0644},
doi = {https://doi.org/10.1016/j.annemergmed.2024.01.031},
url = {https://www.sciencedirect.com/science/article/pii/S019606442400043X},
author = {Robert J. Petrella},
abstract = {In the coming years, artificial intelligence (AI) and machine learning will likely give rise to profound changes in the field of emergency medicine, and medicine more broadly. This article discusses these anticipated changes in terms of 3 overlapping yet distinct stages of AI development. It reviews some fundamental concepts in AI and explores their relation to clinical practice, with a focus on emergency medicine. In addition, it describes some of the applications of AI in disease diagnosis, prognosis, and treatment, as well as some of the practical issues that they raise, the barriers to their implementation, and some of the legal and regulatory challenges they create.}
}
@article{LIMASILVA2024109436,
title = {Dynamical homotopy transient-based technique to improve the convergence of ill-posed power flow problem},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {155},
pages = {109436},
year = {2024},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2023.109436},
url = {https://www.sciencedirect.com/science/article/pii/S0142061523004933},
author = {Alisson Lima-Silva and Francisco Damasceno Freitas},
keywords = {Euler method, Homotopy, Newton–Raphson method, Numerical integration, Power flow problem, Ill-posed problem},
abstract = {This paper proposes a hybrid technique to solve the ill-posed Power Flow Problem (PFP), considering a homotopy approach. The primary proposal is to solve large-scale problems where the traditional Newton–Raphson (NR) fails to converge, as in the case of ill-posed systems. The method explores a dynamical homotopy transient-based technique to improve the convergence of the ill-conditioned problem instead of using the classical static method. Depending on the integration selected scheme and the integration step, the result furnished by the dynamical homotopy method has low accuracy. Then, the NR method is employed to refine the low-accuracy result and accurately determine the ill-posed PFP solution. The proposed approach can be implemented efficiently using only one Jacobian matrix computation and LU factorization per point of the homotopy path. In the static homotopy problem, a PFP using previous results must be solved per path point. In this case, some LU factorizations are necessary for each path point. The technique’s performance was evaluated through experiments, including a 70,000-bus large-scale system. The approximate dynamical homotopy result used as an initial estimate provided appropriate convergence quality for the NR method to determine a high-precision solution to the PFP.}
}
@article{REHDER201454,
title = {Independence and dependence in human causal reasoning},
journal = {Cognitive Psychology},
volume = {72},
pages = {54-107},
year = {2014},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2014.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0010028514000176},
author = {Bob Rehder},
keywords = {Causal reasoning, Causal inference, Causal Markov condition, Conditional independence, Screening off},
abstract = {Causal graphical models (CGMs) are a popular formalism used to model human causal reasoning and learning. The key property of CGMs is the causal Markov condition, which stipulates patterns of independence and dependence among causally related variables. Five experiments found that while adult’s causal inferences exhibited aspects of veridical causal reasoning, they also exhibited a small but tenacious tendency to violate the Markov condition. They also failed to exhibit robust discounting in which the presence of one cause as an explanation of an effect makes the presence of another less likely. Instead, subjects often reasoned “associatively,” that is, assumed that the presence of one variable implied the presence of other, causally related variables, even those that were (according to the Markov condition) conditionally independent. This tendency was unaffected by manipulations (e.g., response deadlines) known to influence fast and intuitive reasoning processes, suggesting that an associative response to a causal reasoning question is sometimes the product of careful and deliberate thinking. That about 60% of the erroneous associative inferences were made by about a quarter of the subjects suggests the presence of substantial individual differences in this tendency. There was also evidence that inferences were influenced by subjects’ assumptions about factors that disable causal relations and their use of a conjunctive reasoning strategy. Theories that strive to provide high fidelity accounts of human causal reasoning will need to relax the independence constraints imposed by CGMs.}
}
@article{DARROCH2023105989,
title = {The rangeomorph Pectinifrons abyssalis: Hydrodynamic function at the dawn of animal life},
journal = {iScience},
volume = {26},
number = {2},
pages = {105989},
year = {2023},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2023.105989},
url = {https://www.sciencedirect.com/science/article/pii/S2589004223000664},
author = {Simon A.F. Darroch and Susana Gutarra and Hale Masaki and Andrei Olaru and Brandt M. Gibson and Frances S. Dunn and Emily G. Mitchell and Rachel A. Racicot and Gregory Burzynski and Imran A. Rahman},
keywords = {Zoology, Evolutionary biology, Paleobiology},
abstract = {Summary
Rangeomorphs are among the oldest putative eumetazoans known from the fossil record. Establishing how they fed is thus key to understanding the structure and function of the earliest animal ecosystems. Here, we use computational fluid dynamics to test hypothesized feeding modes for the fence-like rangeomorph Pectinifrons abyssalis, comparing this to the morphologically similar extant carnivorous sponge Chondrocladia lyra. Our results reveal complex patterns of flow around P. abyssalis unlike those previously reconstructed for any other Ediacaran taxon. Comparisons with C. lyra reveal substantial differences between the two organisms, suggesting they converged on a similar fence-like morphology for different functions. We argue that the flow patterns recovered for P. abyssalis do not support either a suspension feeding or osmotrophic feeding habit. Instead, our results indicate that rangeomorph fronds may represent organs adapted for gas exchange. If correct, this interpretation could require a dramatic reinterpretation of the oldest macroscopic animals.}
}
@article{BAIDYA2024100680,
title = {Comprehensive survey on resource allocation for edge-computing-enabled metaverse},
journal = {Computer Science Review},
volume = {54},
pages = {100680},
year = {2024},
issn = {1574-0137},
doi = {https://doi.org/10.1016/j.cosrev.2024.100680},
url = {https://www.sciencedirect.com/science/article/pii/S1574013724000649},
author = {Tanmay Baidya and Sangman Moh},
keywords = {Augmented reality, Edge computing, Metaverse, Offloading, Resource allocation, Virtual reality},
abstract = {With the rapid evaluation of virtual and augmented reality, massive Internet of Things networks and upcoming 6 G communication give rise to an emerging concept termed the “metaverse,” which promises to revolutionize how we interact with the digital world by offering immersive experiences between reality and virtuality. Edge computing, another novel paradigm, propels the metaverse functionality by enhancing real-time interaction and reducing latency, providing a responsive and seamless virtual environment. However, realizing the full potential of the metaverse requires dynamic and efficient resource-allocation strategies to handle the immense demand for communicational, computational, and storage resources required by its diverse applications. This survey comprehensively explores resource-allocation strategies in the context of an edge-computing-enabled metaverse, investigating various challenges, existing techniques, and emerging trends in this rapidly expanding field. We first explore the underlying metaverse characteristics and pivotal role of edge computing, after which we investigate various types of resources and their key issues and challenges. We also provide a brief discussion on offloading and caching strategies, which are the most prominent research issues in this context. In this study, we compare and analyze 35 different resource-allocation strategies, benchmark 19 algorithms, and investigate their suitability across diverse metaverse scenarios, offering a broader scope than existing surveys. The survey aims to serve as a comprehensive guide for researchers and practitioners, helping them navigate the complexities of resource allocation in the metaverse and supporting the development of more efficient, scalable, and user-centric virtual environments.}
}
@article{MARGINEANU2014131,
title = {Systems biology, complexity, and the impact on antiepileptic drug discovery},
journal = {Epilepsy & Behavior},
volume = {38},
pages = {131-142},
year = {2014},
note = {SI: NEWroscience 2013},
issn = {1525-5050},
doi = {https://doi.org/10.1016/j.yebeh.2013.08.029},
url = {https://www.sciencedirect.com/science/article/pii/S1525505013004344},
author = {Doru Georg Margineanu},
keywords = {Systems biology, Systems/network pharmacology, Drug resistance in epilepsy, Antiepileptic drug, Polypharmacology, Multitarget drug, Phenotypic screening, Modeling, Drug discovery},
abstract = {The number of available anticonvulsant drugs increased in the period spanning over more than a century, amounting to the current panoply of nearly two dozen so-called antiepileptic drugs (AEDs). However, none of them actually prevents/reduces the post-brain insult development of epilepsy in man, and in no less than a third of patients with epilepsy, the seizures are not drug-controlled. Plausibly, the enduring limitation of AEDs' efficacy derives from the insufficient understanding of epileptic pathology. This review pinpoints the unbalanced reductionism of the analytic approaches that overlook the intrinsic complexity of epilepsy and of the drug resistance in epilepsy as the core conceptual flaw hampering the discovery of truly antiepileptogenic drugs. A rising awareness of the complexity of epileptic pathology is, however, brought about by the emergence of nonreductionist systems biology (SB) that considers the networks of interactions underlying the normal organismic functions and of SB-based systems (network) pharmacology that aims to restore pathological networks. By now, the systems pharmacology approaches of AED discovery are fairly meager, but their forthcoming development is both a necessity and a realistic prospect, explored in this review. This article is part of a Special Issue entitled “NEWroscience 2013”.}
}
@article{BOCK2020100960,
title = {On the semantics for spreadsheets with sheet-defined functions},
journal = {Journal of Computer Languages},
volume = {57},
pages = {100960},
year = {2020},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2020.100960},
url = {https://www.sciencedirect.com/science/article/pii/S2590118420300204},
author = {Alexander Asp Bock and Thomas Bøgholm and Peter Sestoft and Bent Thomsen and Lone Leth Thomsen},
keywords = {Spreadsheet, Semantics, Funcalc, Sheet-defined function, Recalculation},
abstract = {We give an operational semantics for the evaluation of spreadsheets, including sheet-defined and built-in numeric functions in the Funcalc spreadsheet platform. The semantics allows for different implementations and we discuss sheet-defined functions implemented using both interpretation and run-time code generation. The semantics specifies the expected result of a computation, also considering non-deterministic functions, independently of an evaluation mechanism. It can be extended to include the cost of formula evaluation for a cost analysis e.g. for use in parallelization of computations. An interesting future direction is to investigate experimentally how close our semantics is to that of major spreadsheet implementations.}
}
@incollection{LUDLOW2025,
title = {Competence/Performance},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2025},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00557-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041005573},
author = {Peter Ludlow},
keywords = {Linguistic competence, Linguistic performance, Cognizing rules, Knowing rules, Rule following, Natural logic, Norms},
abstract = {The competence/performance distinction has played a role in linguistic theorizing since 1965. The key idea is that one can distinguish between the grammar that an agent has and the performance errors that violate the posited rules or principles of the grammar. The first question to consider is what counts as a performance error and whether it can be defined in a way that avoids bailing out defective theories. The next question concerns what competence consists in—in what sense do we have linguistic rules or principles? One idea is that the rule plays a normative (guiding) role. Another idea is that we merely “cognize” or represent the rule. These lead to different conceptions of performance errors. We then turn to versions of the competence/performance distinction in other fields, including ethics and logic.}
}
@article{OYELADE2025127455,
title = {SMAR + NIE IdeaGen: A knowledge graph based node importance estimation with analogical reasoning on large language model for idea generation},
journal = {Expert Systems with Applications},
volume = {279},
pages = {127455},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127455},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425010772},
author = {Olaide N. Oyelade and Hui Wang and Karen Rafferty},
keywords = {Knowledge graphs (KGs), Large language model (LLMs), Idea generation, Novelty, Analogical reasoning, Node importance estimation, Natural language processing (NLP), Isomorphic subgraphs},
abstract = {Idea generation describes a creative process involving reasoning over some knowledge to derive new information. Traditional approaches such as mind-map and brainstorming are limited and often fail due to lack of quality ideas and ineffective methods. The reasoning capability of large language models (LLMs) have been investigated for ideation tasks and have reported interesting performance. However, these models suffer from limited logical reasoning capability which hinders the use of structural and factual real-world knowledge in discovery of latent insight and predict possible outcome when applied to ideation. In addition, the possibility of LLMs regurgitating knowledge learnt from datasets might adversely impact the degree of novel ideas the models can generate. In this paper, a two-stage logical reasoning approach is applied to initiate the search for candidate idea pathways based on the knowledge graphs (KGs) to address the problem of reasoning, domain-specificity and novelty. The divergence stage this reasoning explores utilizes a new node importance estimation (NIE) technique over KGs to discover latent connections supporting idea generation. In the convergence stage of this reasoning, subgraph matching using analogical reasoning (SMAR) is applied to find matching patterns to describe a new idea. The use of SMAR + NIE and KGs helps to achieve an improvement in reasoning over KGs before transferring such reasoning to LLMs for translation of idea into natural language. To evaluate the degree of novelty of ideas generated, a relevance-to-novelty scoring metrics is proposed based on multiple premise entailment (MPE). We combined this metric with other popular metrics to evaluate the performance of SMAR + NIE on benchmark datasets, and as well on the quality of ideas generated. Findings from the study showed that this approach demonstrates competitive performance with mainstream LLMs in idea generation tasks.}
}
@incollection{SYMMONDS20123,
title = {Chapter 1 - The Neurobiology of Preferences},
editor = {Raymond Dolan and Tali Sharot},
booktitle = {Neuroscience of Preference and Choice},
publisher = {Academic Press},
address = {San Diego},
pages = {3-31},
year = {2012},
isbn = {978-0-12-381431-9},
doi = {https://doi.org/10.1016/B978-0-12-381431-9.00001-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780123814319000012},
author = {Mkael Symmonds and Raymond J. Dolan},
keywords = {Preference, choice, neuroscience, neuroeconomics, decision-making, action, value},
abstract = {Publisher Summary
The neuroscience of choice and preference dates back to the nineteenth century, with the emergence of the idea of functional specialization as a fundamental organizational principle of the brain. The development of neuroimaging techniques—in particular, functional magnetic resonance imaging (fMRI)—has meant that questions related to choice and preference can now be addressed non-invasively in humans. There are important examples where choices do not accord with internal wants. An addict may perform an action in the present despite expressing a desire to avoid doing this very action on a prior occasion. A major conundrum when thinking about neurobiological mechanisms in decision-making is the fact that choices are often noisy or stochastic. A different network of regions in precuneus, left prefrontal, and temproparietal cortex reflected endogenous inequity aversion across subjects, illustrating that even within the context of a specific task, preferences for the same stimulus feature can be expressed in different regions and modulated in a distinct manner.}
}
@article{POVALA2022114712,
title = {Variational Bayesian approximation of inverse problems using sparse precision matrices},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {393},
pages = {114712},
year = {2022},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2022.114712},
url = {https://www.sciencedirect.com/science/article/pii/S0045782522000822},
author = {Jan Povala and Ieva Kazlauskaite and Eky Febrianto and Fehmi Cirak and Mark Girolami},
keywords = {Inverse problems, Bayesian inference, Variational Bayes, Precision matrix, Uncertainty quantification},
abstract = {Inverse problems involving partial differential equations (PDEs) are widely used in science and engineering. Although such problems are generally ill-posed, different regularisation approaches have been developed to ameliorate this problem. Among them is the Bayesian formulation, where a prior probability measure is placed on the quantity of interest. The resulting posterior probability measure is usually analytically intractable. The Markov Chain Monte Carlo (MCMC) method has been the go-to method for sampling from those posterior measures. MCMC is computationally infeasible for large-scale problems that arise in engineering practice. Lately, Variational Bayes (VB) has been recognised as a more computationally tractable method for Bayesian inference, approximating a Bayesian posterior distribution with a simpler trial distribution by solving an optimisation problem. In this work, we argue, through an empirical assessment, that VB methods are a flexible and efficient alternative to MCMC for this class of problems. We propose a natural choice of a family of Gaussian trial distributions parametrised by precision matrices, thus taking advantage of the inherent sparsity of the inverse problem encoded in its finite element discretisation. We utilise stochastic optimisation to efficiently estimate the variational objective and assess not only the error in the solution mean but also the ability to quantify the uncertainty of the estimate. We test this on PDEs based on the Poisson equation in 1D and 2D. A Tensorflow implementation is made publicly available on GitHub.}
}
@article{KRISHNANNAIR202437,
title = {Empowering Tomorrow's Scientists: ‘Girls In Control’ Workshop Promotes STEM Education For Young Girls},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {25},
pages = {37-42},
year = {2024},
note = {3rd Control Conference Africa CCA 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.10.234},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324020093},
author = {S. Krishnannair and A. Krishnannair},
keywords = {STEM education, Control Engineering, Girls in Control, Empowering Girls},
abstract = {Enhancement of female students’ access to and success in STEM-related subjects and courses has acquired an unprecedented level of traction in academia. Considering female students’ increased participation in STEM education as a moral imperative, the University of Zululand (South Africa) in partnership with SACAC, South Africa hosted two Girls in Control Workshops at its Science Centre during 2022 and 2023. This article reports on the workshop's purpose and the experiences of the participants. It also makes references to the workshop's implications on various aspects of girls’ participation in STEM education in general and control engineering in particular. The article thus places the need for instilling a high degree of receptiveness to STEM-related careers among girls from previously marginalized communities.}
}
@article{JIANG2020556,
title = {Energy aware edge computing: A survey},
journal = {Computer Communications},
volume = {151},
pages = {556-580},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S014036641930831X},
author = {Congfeng Jiang and Tiantian Fan and Honghao Gao and Weisong Shi and Liangkai Liu and Christophe Cérin and Jian Wan},
keywords = {Edge computing, Energy efficiency, Computing offloading, Benchmarking, Computation partitioning},
abstract = {Edge computing is an emerging paradigm for the increasing computing and networking demands from end devices to smart things. Edge computing allows the computation to be offloaded from the cloud data centers to the network edge and edge nodes for lower latency, security and privacy preservation. Although energy efficiency in cloud data centers has been broadly investigated, energy efficiency in edge computing is largely left uninvestigated due to the complicated interactions between edge devices, edge servers, and cloud data centers. In order to achieve energy efficiency in edge computing, a systematic review on energy efficiency of edge devices, edge servers, and cloud data centers is required. In this paper, we survey the state-of-the-art research work on energy-aware edge computing, and identify related research challenges and directions, including architecture, operating system, middleware, applications services, and computation offloading.}
}
@article{GAO201464,
title = {Unconscious processing modulates creative problem solving: Evidence from an electrophysiological study},
journal = {Consciousness and Cognition},
volume = {26},
pages = {64-73},
year = {2014},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2014.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1053810014000464},
author = {Ying Gao and Hao Zhang},
keywords = {Unconscious processing, Divergent thinking, Creative problem solving, Creativity, Event-related potential},
abstract = {Previous behavioral studies have identified the significant role of subliminal cues in creative problem solving. However, neural mechanisms of such unconscious processing remain poorly understood. Here we utilized an event-related potential (ERP) approach and sandwich mask technique to investigate cerebral activities underlying the unconscious processing of cues in creative problem solving. College students were instructed to solve divergent problems under three different conditions (conscious cue, unconscious cue and no-cue conditions). Our data showed that creative problem solving can benefit from unconscious cues, although not as much as from conscious cues. More importantly, we found that there are crucial ERP components associated with unconscious processing of cues in solving divergent problems. Similar to the processing of conscious cues, processing unconscious cues in problem solving involves the semantic activation of unconscious cues (N280–340) in the right inferior parietal lobule (BA 40), new association formation (P350–450) in the right parahippocampal gyrus (BA 36), and mental representation transformation (P500–760) in the right superior temporal gyrus (BA 22). The present results suggest that creative problem solving can be modulated by unconscious processing of enlightening information that is weakly diffused in the semantic network beyond our conscious awareness.}
}
@article{BASOV2020101433,
title = {Socio-semantic and other dualities},
journal = {Poetics},
volume = {78},
pages = {101433},
year = {2020},
note = {Discourse, Meaning, and Networks: Advances in Socio-Semantic Analysis},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2020.101433},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X19304073},
author = {Nikita Basov and Ronald Breiger and Iina Hellsten},
keywords = {Social network, Semantic network, Socio-semantic network, Duality, Culture, Special Issue},
abstract = {The social and the cultural orders are dual – that is, they constitute each other. To understand either we need to account for both. Socio-semantic network analysis brings together the study of relations among actors (social networks), relations among elements of actors’ cultural structures (their semantic networks), and relations among these two orders of networks. In this introductory essay, we describe how the duality of the social and semantic networks that constitute each other, as well as other related dualities (including material / symbolic, micro / macro, computational / qualitative, in-presence contexts / online contexts, ‘Big’ data / ‘thick’ data), have evolved in recent decades to mold socio-semantic network analysis into its present form. In doing so, we delineate the current state of the art and the main features of socio-semantic network analysis as highlighted by the papers included in this Special Issue. These articles range from in-depth analysis of ‘thick’ data on small group interactions to automated analysis of ‘Big’ online data in contexts extending from Renaissance parliamentary discussions to cutting-edge global scientific fields of the 21st century. We conclude by delineating current problems of and future prospects for socio-semantic network analysis.}
}
@article{COVINGTON2016869,
title = {Expanding the Language Network: Direct Contributions from the Hippocampus},
journal = {Trends in Cognitive Sciences},
volume = {20},
number = {12},
pages = {869-870},
year = {2016},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2016.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S1364661316301759},
author = {Natalie V. Covington and Melissa C. Duff},
keywords = {hippocampus, language, memory, online processing, theta oscillations},
abstract = {New research suggests that the same hippocampal computations used in support of memory are also used for language processing, providing direct neurophysiological evidence of a shared neural mechanism for memory and language. This work expands classic memory and language models and represents a new opportunity for studying the memory–language interface.}
}
@article{KHACHATURIAN20253,
title = {Perspective on “Brain Network Disorders”},
journal = {Brain Network Disorders},
volume = {1},
number = {1},
pages = {3-6},
year = {2025},
issn = {3050-6239},
doi = {https://doi.org/10.1016/j.bnd.2024.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S3050623924000129},
author = {Zaven Khachaturian and Jean-Marie C. Boutellier and Jiri Damborsky and Ara S. Khachaturian}
}
@article{VELOSO2023104997,
title = {Spatial synthesis for architectural design as an interactive simulation with multiple agents},
journal = {Automation in Construction},
volume = {154},
pages = {104997},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2023.104997},
url = {https://www.sciencedirect.com/science/article/pii/S0926580523002571},
author = {Pedro Veloso and Ramesh Krishnamurti},
keywords = {Spatial synthesis, Interactive simulation, Artificial intelligence, Agent-based modeling, Multi-agent deep reinforcement learning},
abstract = {Motivated by reflection-in-action in architectural design, this article introduces a spatial synthesis artifact that relies on multi-agent reinforcement learning to address spatial goals with fine-grained control in a simulation. It relies on parameter sharing with proximal policy optimization and a parameterized reward function to train robust agent policies in random environments with random spatial problems. The agents are evaluated in three design cases: a house design with 12 agents in three sites, a museum with 18 agents in an interstitial urban site, and a speculative design of a housing complex with 96 agents on a large empty site. The policies performed well in all the cases and produced morphologically consistent solutions. However, in cases with a larger number of agents, the system largely benefited from a spring layout algorithm for the initialization. Future research will address more complex spatial synthesis problems and mechanisms for human-computer interaction.}
}
@incollection{KALNOOR2021575,
title = {Chapter 24 - The brain-machine interface, nanosensor technology, and artificial intelligence: Their convergence with a novel frontier},
editor = {Chaudhery Mustansar Hussain and Suresh Kumar Kailasa},
booktitle = {Handbook of Nanomaterials for Sensing Applications},
publisher = {Elsevier},
pages = {575-587},
year = {2021},
series = {Micro and Nano Technologies},
isbn = {978-0-12-820783-3},
doi = {https://doi.org/10.1016/B978-0-12-820783-3.00013-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128207833000130},
author = {Gauri Kalnoor},
keywords = {Neuroscience, Machine learning, Nanotechnology, Artificial intelligence (AI), Brain-computer interface, Brain-machine interface (BMI), Computational neuroscience},
abstract = {A confluence of technological capabilities is creating an opportunity for machine learning and artificial intelligence (AI) to enable “smart” nanoengineered brain-machine interfaces (BMI). This new generation of technologies will be able to communicate with the brain in ways that support contextual learning and adaptation to change functional requirements. This applies to both invasive technologies aimed at restoring neurological function, as in the case of neural prosthesis, as well as noninvasive technologies enabled by signals such as electroencephalograph (EEG). Advances in computation, hardware, and algorithms that learn and adapt in a contextually dependent way will be able to leverage the capabilities that nanoengineering offers the design and functionality of BMI. We explore the enabling capabilities that these devices may exhibit, why they matter, and the state of the technologies necessary to build them. We also discuss a number of open technical challenges and problems that will need to be solved to achieve this.}
}
@article{YANOVA201682,
title = {Relativistic Psychometrics in Subjective Scaling},
journal = {Procedia Computer Science},
volume = {102},
pages = {82-89},
year = {2016},
note = {12th International Conference on Application of Fuzzy Systems and Soft Computing, ICAFS 2016, 29-30 August 2016, Vienna, Austria},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.09.373},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916325534},
author = {Natalia Yanova},
keywords = {mental representation, relativistic psychometrics, computational theory of perceptions, image understanding, significance, expert test system, psychosemiotics},
abstract = {The article announces the possibilities of semantic modeling in the development of feedback tools in social sciences. A new approach to the computational theory of perceptions (CTP) for analysis of mental object is proposed. The article demonstrates the implementation of relativistic psychometrics for the study of mental response (opinions, expectations and attitudes). The problem of image understanding and its significance is considered in combination of soft and hard computing. It is shown that the modeling of object (its coding and decoding in ‘mental map’) obeys the semiotic and mathematical logic. Computing with perceptions for the rules of mental representation proves their identity to the laws of conservation. The article demonstrates the versatility of the semiotic description of objects in Minkowski space. It also confirms by mathematical solution C. S. Peirce's metaphor, according to which the semiology of language is a truly universal algebra of relations.}
}
@article{MATLI2024100286,
title = {Extending the theory of information poverty to deepfake technology},
journal = {International Journal of Information Management Data Insights},
volume = {4},
number = {2},
pages = {100286},
year = {2024},
issn = {2667-0968},
doi = {https://doi.org/10.1016/j.jjimei.2024.100286},
url = {https://www.sciencedirect.com/science/article/pii/S2667096824000752},
author = {Walter Matli},
keywords = {Deepfake technology, Information poverty theory, Artificial intelligence (AI), Synthetic media, Societal implications, Technological advancements},
abstract = {The advent of deepfake technology has introduced complex challenges to the information technology landscape, simultaneously presenting benefits and novel risks and ethical considerations. This paper delves into the evolution of deepfakes through the prism of information poverty theory, scrutinising how deepfakes may contribute to a growing information access/use inequality. The research focuses on the risks of misinformation and the ensuing expansion of digital divides, particularly when manipulative media could delude individuals lacking access to legitimate information sources. The study outlines the potential exacerbation of information asymmetries and examines the societal implications across various demographics. By integrating an analytical discussion on the risks associated with deepfakes, the study aligns the observed trends with the theoretical underpinnings of information poverty. As part of its contribution, the paper offers actionable policy-making recommendations and educational strategies to combat the proliferation of harmful deepfake content. The article aims to ensure a more equitable distribution of authentic information and foster media literacy. Through a multifaceted approach, this study endeavours to provide a foundational understanding for stakeholders to navigate the ethical minefield posed by deepfakes and to instil a framework for information equity in the digital era. The article provides critical insights into the discourse on deepfake technology and its relation to information poverty, underscoring the urgent need for equitable access to informed digital spaces. As deepfake technology evolves and more data emerges, a societal demand exists for comprehensive knowledge about deepfakes to promote discernment, decision-making and awareness. Policymakers are tasked with recognising the significance of widening access to sophisticated information technologies whilst addressing their negative repercussions. Their efforts will be particularly crucial for disseminating knowledge about deepfakes to those with limited or non-existent information and communication awareness and infrastructures. Learning from past successes and failures becomes pivotal in shaping effective strategies to address the challenges posed by deepfakes and fostering accessible, informed digital communities.}
}
@article{PROKOPENKO2019134,
title = {Self-referential basis of undecidable dynamics: From the Liar paradox and the halting problem to the edge of chaos},
journal = {Physics of Life Reviews},
volume = {31},
pages = {134-156},
year = {2019},
note = {Physics of Mind},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2018.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S1571064519300077},
author = {Mikhail Prokopenko and Michael Harré and Joseph Lizier and Fabio Boschetti and Pavlos Peppas and Stuart Kauffman},
keywords = {Self-reference, Diagonalization, Undecidability, Incomputability, Program-data duality, Complexity},
abstract = {In this paper we explore several fundamental relations between formal systems, algorithms, and dynamical systems, focussing on the roles of undecidability, universality, diagonalization, and self-reference in each of these computational frameworks. Some of these interconnections are well-known, while some are clarified in this study as a result of a fine-grained comparison between recursive formal systems, Turing machines, and Cellular Automata (CAs). In particular, we elaborate on the diagonalization argument applied to distributed computation carried out by CAs, illustrating the key elements of Gödel's proof for CAs. The comparative analysis emphasizes three factors which underlie the capacity to generate undecidable dynamics within the examined computational frameworks: (i) the program-data duality; (ii) the potential to access an infinite computational medium; and (iii) the ability to implement negation. The considered adaptations of Gödel's proof distinguish between computational universality and undecidability, and show how the diagonalization argument exploits, on several levels, the self-referential basis of undecidability.}
}
@article{BURKE2024102382,
title = {A chance for models to show their quality: Stochastic process model-log dimensions},
journal = {Information Systems},
volume = {124},
pages = {102382},
year = {2024},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2024.102382},
url = {https://www.sciencedirect.com/science/article/pii/S0306437924000401},
author = {Adam T. Burke and Sander J.J. Leemans and Moe T. Wynn and Wil M.P. {van der Aalst} and Arthur H.M. {ter Hofstede}},
keywords = {Stochastic process mining, Process conformance, Stochastic Petri nets, Adhesion, Relevance, Simplicity},
abstract = {Process models describe the desired or observed behaviour of organisations. In stochastic process mining, computational analysis of trace data yields process models which describe process paths and their probability of execution. To understand the quality of these models, and to compare them, quantitative quality measures are used. This research investigates model comparison empirically, using stochastic process models built from real-life logs. The experimental design collects a large number of models generated randomly and using process discovery techniques. Twenty-five different metrics are taken on these models, using both existing process model metrics and new, exploratory ones. The results are analysed quantitatively, making particular use of principal component analysis. Based on this analysis, we suggest three stochastic process model dimensions: adhesion, relevance and simplicity. We also suggest possible metrics for these dimensions, and demonstrate their use on example models.}
}
@article{PIANTADOSI2012199,
title = {Bootstrapping in a language of thought: A formal model of numerical concept learning},
journal = {Cognition},
volume = {123},
number = {2},
pages = {199-217},
year = {2012},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2011.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0010027711002769},
author = {Steven T. Piantadosi and Joshua B. Tenenbaum and Noah D. Goodman},
keywords = {Number word learning, Bootstrapping, Cognitive development, Bayesian model, Language of thought, CP transition},
abstract = {In acquiring number words, children exhibit a qualitative leap in which they transition from understanding a few number words, to possessing a rich system of interrelated numerical concepts. We present a computational framework for understanding this inductive leap as the consequence of statistical inference over a sufficiently powerful representational system. We provide an implemented model that is powerful enough to learn number word meanings and other related conceptual systems from naturalistic data. The model shows that bootstrapping can be made computationally and philosophically well-founded as a theory of number learning. Our approach demonstrates how learners may combine core cognitive operations to build sophisticated representations during the course of development, and how this process explains observed developmental patterns in number word learning.}
}
@article{DELATORRE2014653,
title = {Monte Carlo advances and concentrated solar applications},
journal = {Solar Energy},
volume = {103},
pages = {653-681},
year = {2014},
issn = {0038-092X},
doi = {https://doi.org/10.1016/j.solener.2013.02.035},
url = {https://www.sciencedirect.com/science/article/pii/S0038092X13001448},
author = {J. Delatorre and G. Baud and J.J. Bézian and S. Blanco and C. Caliot and J.F. Cornet and C. Coustet and J. Dauchet and M. {El Hafi} and V. Eymet and R. Fournier and J. Gautrais and O. Gourmel and D. Joseph and N. Meilhac and A. Pajot and M. Paulin and P. Perez and B. Piaud and M. Roger and J. Rolland and F. Veynandt and S. Weitz},
keywords = {Monte Carlo algorithm, Concentrated solar energy, Solar energy flux density distribution, Solar concentrators design optimization, Sensitivity computation},
abstract = {The Monte Carlo method is partially reviewed with the objective of illustrating how some of the most recent methodological advances can benefit to concentrated solar research. This review puts forward the practical consequences of writing down and handling the integral formulation associated to each Monte Carlo algorithm. Starting with simple examples and up to the most complex multiple reflection, multiple scattering configurations, we try to argue that these formulations are very much accessible to the non specialist and that they allow a straightforward entry to sensitivity computations (for assistance in design optimization processes) and to convergence enhancement techniques involving subtle concepts such as control variate and zero variance. All illustration examples makePROMES - UPR CNRS 8521 - 7, rue du Four Solaire, 66120 Font Romeu Odeillo, France use of the public domain development environment EDStar (including advanced parallelized computer graphics libraries) and are meant to serve as start basis either for the upgrading of existing Monte Carlo codes, or for fast implementation of ad hoc codes when specific needs cannot be answered with standard concentrated solar codes (in particular as far as the new generation of solar receivers is concerned).}
}
@article{JIN201559,
title = {Significance and Challenges of Big Data Research},
journal = {Big Data Research},
volume = {2},
number = {2},
pages = {59-64},
year = {2015},
note = {Visions on Big Data},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2015.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S2214579615000076},
author = {Xiaolong Jin and Benjamin W. Wah and Xueqi Cheng and Yuanzhuo Wang},
keywords = {Big data, Data complexity, Computational complexity, System complexity},
abstract = {In recent years, the rapid development of Internet, Internet of Things, and Cloud Computing have led to the explosive growth of data in almost every industry and business area. Big data has rapidly developed into a hot topic that attracts extensive attention from academia, industry, and governments around the world. In this position paper, we first briefly introduce the concept of big data, including its definition, features, and value. We then identify from different perspectives the significance and opportunities that big data brings to us. Next, we present representative big data initiatives all over the world. We describe the grand challenges (namely, data complexity, computational complexity, and system complexity), as well as possible solutions to address these challenges. Finally, we conclude the paper by presenting several suggestions on carrying out big data projects.}
}
@article{GAO2021107161,
title = {Optical waves/modes in a multicomponent inhomogeneous optical fiber via a three-coupled variable-coefficient nonlinear Schrödinger system},
journal = {Applied Mathematics Letters},
volume = {120},
pages = {107161},
year = {2021},
issn = {0893-9659},
doi = {https://doi.org/10.1016/j.aml.2021.107161},
url = {https://www.sciencedirect.com/science/article/pii/S089396592100080X},
author = {Xin-Yi Gao and Yong-Jiang Guo and Wen-Rui Shan},
keywords = {Optical waves/modes, Multicomponent inhomogeneous optical fiber, Symbolic computation, Three-coupled variable-coefficient nonlinear Schrödinger system, Similarity reduction, Bäcklund transformation with analytic solutions},
abstract = {Recent progress in optical fibers is impressive, while nonlinear Schrödinger-type models are seen in fiber optics and other fields (such as ferromagnetism, plasma physics, Bose–Einstein condensation and oceanography). Hereby, our symbolic computation on a three-coupled variable-coefficient nonlinear Schrödinger system is performed, for the picosecond-pulse attenuation/amplification in a multicomponent inhomogeneous optical fiber with diverse polarizations/frequencies. For the slowly-varying envelopes of optical modes, we obtain a similarity reduction, an auto-Bäcklund transformation and some analytic solutions, which rely on the optical-fiber variable coefficients, i.e., the fiber loss/gain, nonlinearity and group velocity dispersion. Relevant variable-coefficient constraints are presented. Our results might be of some use in the construction of logic gates, optical computing, soliton switching, design of fiber directional couplers, quantum information processing, soliton amplification in the wavelength division multiplexing systems, solitonic studies in the all-optical devices and birefringence fiber systems.}
}
@article{SURYANARAYANA2024100495,
title = {Artificial Intelligence Enhanced Digital Learning for the Sustainability of Education Management System},
journal = {The Journal of High Technology Management Research},
volume = {35},
number = {2},
pages = {100495},
year = {2024},
issn = {1047-8310},
doi = {https://doi.org/10.1016/j.hitech.2024.100495},
url = {https://www.sciencedirect.com/science/article/pii/S104783102400004X},
author = {K.S. Suryanarayana and V.S. Prasad Kandi and G. Pavani and Akuthota Sankar Rao and Sandeep Rout and T. {Siva Rama Krishna}},
keywords = {Artificial intelligence, Digital education, Sustainability, Role of AI in education, Educational management},
abstract = {Maintenance schedules are scheduled ahead of time and automatically based on the continuous monitoring of the equipment by statistical methods, thanks to artificial intelligence-enabled digital transformation and the best fit model based on Machine Management Index in a pedagogical system. One of the most important aspects of universities is the widespread use of machine learning methods to evaluate students' progress. Machine learning approaches are designed to speed up the learning process without sacrificing accuracy. The dynamics of teaching and learning have shifted since the introduction of modern technological tools. The educational system as a whole has changed and developed over time. These days, people can get an education outside of the classroom as well, thanks to the proliferation of online courses and resources. Everyone's professional life begins with their education. By analyzing past data, artificial intelligence methods can resolve existing problems. When applied properly, artificial intelligence can be a highly efficient method for solving problems with a predictable and repeatable solution space. The learner's personality can be predicted based on a number of factors using machine learning approaches. This article examines how AI may improve digital learning in education management systems to sustain the education ecosystem. AI in education improves student results, learning experiences, and administrative processes. This study discusses AI applications in education management systems and associated problems and opportunities. We also explore ethical issues and the roadmap for using AI to improve education. Educational institutions can provide individualized curriculum for students based on their unique personalities and areas of interest. Institutions of higher learning can benefit greatly from this instrument for personality prediction by recommending a course of study that will better prepare students to enter the field of their choice and achieve professional success.}
}
@article{JARRAHI2018577,
title = {Artificial intelligence and the future of work: Human-AI symbiosis in organizational decision making},
journal = {Business Horizons},
volume = {61},
number = {4},
pages = {577-586},
year = {2018},
issn = {0007-6813},
doi = {https://doi.org/10.1016/j.bushor.2018.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0007681318300387},
author = {Mohammad Hossein Jarrahi},
keywords = {Artificial intelligence, Organizational decision making, Human-machine symbiosis, Human augmentation, Analytical and intuitive decision making},
abstract = {Artificial intelligence (AI) has penetrated many organizational processes, resulting in a growing fear that smart machines will soon replace many humans in decision making. To provide a more proactive and pragmatic perspective, this article highlights the complementarity of humans and AI and examines how each can bring their own strength in organizational decision-making processes typically characterized by uncertainty, complexity, and equivocality. With a greater computational information processing capacity and an analytical approach, AI can extend humans’ cognition when addressing complexity, whereas humans can still offer a more holistic, intuitive approach in dealing with uncertainty and equivocality in organizational decision making. This premise mirrors the idea of intelligence augmentation, which states that AI systems should be designed with the intention of augmenting, not replacing, human contributions.}
}
@article{DECARVALHOBOTEGA2022109893,
title = {A data-driven Machine Learning approach to creativity and innovation techniques selection in solution development},
journal = {Knowledge-Based Systems},
volume = {257},
pages = {109893},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2022.109893},
url = {https://www.sciencedirect.com/science/article/pii/S0950705122009868},
author = {Luiz Fernando {de Carvalho Botega} and Jonny Carlos {da Silva}},
keywords = {Decision support system, Creativity, Artificial intelligence, Design},
abstract = {The creation and refinement of new ideas is a strategic competence for teams and organization to innovate and prosper. This paper addresses the challenge of finding adequate creativity and innovation techniques (CITs) for improving individual or team creativity through the use of Machine Learning (ML). The process of choosing which CIT to use is complex and demanding, especially when taking into consideration the existence of hundreds of techniques and the plurality of different design contexts. This empiric knowledge, usually retained in an expert’s repertoire, can be extracted and implemented in a computational system, making it more available and permanent. This research focused on developing a Decision Support System embedded in an online application with a two-stage ML inference process able to evaluate users’ design scenario through an online form, and infer the most appropriate CITs from the database that would fit their needs. This paper presents two iterative development cycles of the prototype, first focused on core knowledge acquisition, representation, ML implementation, and verification; while second focused on system expansion, addition of web interface, and initial validation. After essaying 12 algorithms, the two-stage model achieved uses a Gradient Boosted Regression Trees algorithm using user provided information about the context to infer the required CITs characteristics; followed by a Logistic Regression classification-ranking algorithm that uses outputs from first model to define which CITs to present to users. To the best of our efforts, no other system was found to use ML approaches to address the problem of CIT selection.}
}
@article{YOUNG1987309,
title = {The metaphor machine: A database method for creativity support},
journal = {Decision Support Systems},
volume = {3},
number = {4},
pages = {309-317},
year = {1987},
issn = {0167-9236},
doi = {https://doi.org/10.1016/0167-9236(87)90102-3},
url = {https://www.sciencedirect.com/science/article/pii/0167923687901023},
author = {Lawrence F Young},
keywords = {Creativity Support Systems, Methaphor Generation by Computer, Database Methods for Metaphor Generation, Idea Processing Support, Computer Support of Metaphorical Thinking, Support of Creative Thinking, Qualitative Support Systems, A Relational Calculus for Metaphor Generation, Relational Database Methods for Metaphor Generation, Interactive Support Systems for Creativity, Right-Brained Support Systems, Relational Algebra for Metaphor Generation, Database Structures for Metaphor Generation, Computer Support of Divergent Thinking},
abstract = {This paper shows how a data base method can be applied to the automatic generation of metaphors. The utility of automatic metaphor generation is based on providing interactive support to creative human thinking processes. Such interactive support systems have been called Idea Processing systems, and are seen as special qualitative types of Decision Support Systems (DSS). They include functions to support metaphorical thinking as well as other modes of creative idea development. The paper presents brief backgrounds references on creativity and the relevance of metaphors, as well as to previous work in Idea Processing. It then presents a relational data base method for automatic metaphor generation. The method is described and illustrated, as well as shown in relational algebra and relational calculus notation. In conclusion, the paper indicates how the relational data base method presented can be operationalized through using existing data base software or by integration with a specialized interface for the particular application of metaphor generation.}
}