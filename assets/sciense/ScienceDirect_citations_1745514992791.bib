@incollection{ERICSSON200112256,
title = {Protocol Analysis in Psychology},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {12256-12262},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/01598-9},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767015989},
author = {K.A. Ericsson},
abstract = {Protocol analysis is the name for the methodology for eliciting, transcribing, and encoding verbal reports of thoughts into objective data for evaluating and testing theories of thinking. Philosophers since Aristotle have introspected on their own thinking as a means to analyze the structure of their thought processes. However, introspective analysis of one's thoughts and behavior was found to be reactive. In response to these criticisms a general theoretical framework was developed for how participants could verbalize their thinking without influencing the course of their thinking. Instructions to elicit such immediate reports were developed and shown to uncover thinking without the reactive effects due to explanations and descriptions of their thinking. Rigorous methods for analyzing verbal reports have been developed based on a formal analysis of the tasks. Short segments of verbal reports are coded into formal categories and the resulting data show similar reliability and validity as other forms of data on cognitive processes, such as reaction times and eye fixations. This general framework for collecting and analyzing verbal reports of thinking has been applied to laboratory studies of memory, problem solving, and decision making, and to everyday life in the study of expert performance and text comprehension.}
}
@article{BERNALMANRIQUE202086,
title = {Effect of acceptance and commitment therapy in improving interpersonal skills in adolescents: A randomized waitlist control trial},
journal = {Journal of Contextual Behavioral Science},
volume = {17},
pages = {86-94},
year = {2020},
issn = {2212-1447},
doi = {https://doi.org/10.1016/j.jcbs.2020.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S2212144720301551},
author = {Koryn N. Bernal-Manrique and María B. García-Martín and Francisco J. Ruiz},
keywords = {Acceptance and commitment therapy, Interpersonal skills, Emotional disorders, Psychological flexibility, Repetitive negative thinking},
abstract = {This parallel randomized controlled trial evaluated the effect of acceptance and commitment therapy (ACT) focused on repetitive negative thinking (RNT) versus a waitlist control (WLC) in improving interpersonal skills in adolescents with problems of social and school adaptation. Forty-two adolescents (11–17 years) agreed to participate. Participants were allocated through simple randomization to the intervention condition or the waitlist control condition. The intervention was a 3-session, group-based, RNT-focused ACT protocol. The primary outcome was the performance on a test of interpersonal skills (Interpersonal Conflict Resolution Assessment, ESCI). At posttreatment, repeated measures ANOVA showed that the intervention was efficacious in increasing overall interpersonal skills (d = 2.62), progress in values (d = 1.23), and reducing emotional symptoms (d = 0.98). No adverse events were found. A brief RNT-focused ACT intervention was highly efficacious in improving interpersonal skills and reducing emotional symptoms in adolescents.}
}
@article{GILL2018733,
title = {Data to Decision and Judgment Making – a Question of Wisdom},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {30},
pages = {733-738},
year = {2018},
note = {18th IFAC Conference on Technology, Culture and International Stability TECIS 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.11.205},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318328702},
author = {Karamjit S Gill},
keywords = {algorithms, artificial intelligence, big data, calculation, decision, judgment, wisdom},
abstract = {The technological waves of super artificial intelligence, big data, algorithms, and machine learning continue to impact our thinking and actions, thereby affecting the ways individuals, professions and institutions make judgments. On the one hand, there is an argument that more data and knowledge together with the cyber physical system of industry4.0 will automatically push society along some track toward a better world for all. On the other hand, we hear worrying voices of the imponderable downsides of powerful new cyber-, bio-, Nano-technologies, and synthetic biology. In the age of uncertainties, big data and the algorithm, how is the decision and judgment making process being affected?}
}
@article{BUTZ2025105948,
title = {Contextualizing predictive minds},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {168},
pages = {105948},
year = {2025},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2024.105948},
url = {https://www.sciencedirect.com/science/article/pii/S0149763424004172},
author = {Martin V. Butz and Maximilian Mittenbühler and Sarah Schwöbel and Asya Achimova and Christian Gumbsch and Sebastian Otte and Stefan Kiebel},
keywords = {Prediction, Cognitive modeling, Events, Active inference, Learning, Behavior, Free energy, Abstraction, Context inference, Deep learning},
abstract = {The structure of human memory seems to be optimized for efficient prediction, planning, and behavior. We propose that these capacities rely on a tripartite structure of memory that includes concepts, events, and contexts—three layers that constitute the mental world model. We suggest that the mechanism that critically increases adaptivity and flexibility is the tendency to contextualize. This tendency promotes local, context-encoding abstractions, which focus event- and concept-based planning and inference processes on the task and situation at hand. As a result, cognitive contextualization offers a solution to the frame problem—the need to select relevant features of the environment from the rich stream of sensorimotor signals. We draw evidence for our proposal from developmental psychology and neuroscience. Adopting a computational stance, we present evidence from cognitive modeling research which suggests that context sensitivity is a feature that is critical for maximizing the efficiency of cognitive processes. Finally, we turn to recent deep-learning architectures which independently demonstrate how context-sensitive memory can emerge in a self-organized learning system constrained by cognitively-inspired inductive biases.}
}
@article{TURNQUIST2024112726,
title = {Adaptive mesh methods on compact manifolds via Optimal Transport and Optimal Information Transport},
journal = {Journal of Computational Physics},
volume = {500},
pages = {112726},
year = {2024},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2023.112726},
url = {https://www.sciencedirect.com/science/article/pii/S0021999123008215},
author = {Axel G.R. Turnquist},
keywords = {Optimal transport, Optimal information transport, Diffeomorphic density matching, Moving mesh methods, Convergent numerical methods, Compact manifolds},
abstract = {Moving mesh methods were devised to redistribute a mesh in a smooth way, while keeping the number of vertices of the mesh and their connectivity unchanged. A fruitful theoretical point-of-view is to take such moving mesh methods and think of them as an application of the diffeomorphic density matching problem. Given two probability measures μ0 and μ1, the diffeomorphic density matching problem consists of finding a diffeomorphic pushforward map T such that T#μ0=μ1. Moving mesh methods are seen to be an instance of the diffeomorphic density matching problem by treating the probability density as the local density of nodes in the mesh. It is preferable that the restructuring of the mesh be done in a smooth way that avoids tangling the connections between nodes, which would lead to numerical instability when the mesh is used in computational applications. This then suggests that a diffeomorphic map T is desirable to avoid tangling. The first tool employed to solve the moving mesh problem between source and target probability densities on the sphere was Optimal Transport (OT). Recently Optimal Information Transport (OIT) was rigorously derived and developed allowing for the computation of a diffeomorphic mapping by simply solving a Poisson equation. Not only is the equation simpler to solve numerically in OIT, but with Optimal Transport there is no guarantee that the mapping between probability density functions defines a diffeomorphism for general 2D compact manifolds. In this manuscript, we perform a side-by-side comparison of using Optimal Transport and Optimal Information Transport on the sphere for adaptive mesh problems. We choose to perform this comparison with recently developed provably convergent solvers, but these are, of course, not the only numerical methods that may be used. We believe that Optimal Information Transport is preferable in computations due to the fact that the partial differential equation (PDE) solve step is simply a Poisson equation. For more general surfaces M, we show how the Optimal Transport and Optimal Information Transport problems can be reduced to solving on the sphere, provided that there exists a diffeomorphic mapping Φ:M→S2. This implies that the Optimal Transport problem on M with a special cost function can be solved with regularity guarantees, while computations for the problem are performed on the unit sphere.}
}
@incollection{SAWLEY1995181,
title = { - A serial data-parallel multi-block method for compressible flow computations},
editor = {A. Ecer and J. Hauser and P. Leca and J. Periaux},
booktitle = {Parallel Computational Fluid Dynamics 1993},
publisher = {North-Holland},
address = {Amsterdam},
pages = {181-188},
year = {1995},
isbn = {978-0-444-81999-4},
doi = {https://doi.org/10.1016/B978-044481999-4/50148-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780444819994501481},
author = {M.L. Sawley and J.K. Tegnér and C.M. Bergman},
abstract = {Publisher Summary
Block structured meshes not only provide the possibility to compute flows in complex geometries but also lend themselves in a natural way to coarse-grain parallel processing via the distribution of different blocks to different processors. Nevertheless, for some flow computations, a fine-grain data parallel implementation may be more appropriate. This chapter presents a study of such an implementation, which utilizes the simplicity of the data parallel approach. Particular attention is placed on a dynamic block management strategy that allows computations to be undertaken only in blocks where useful work is to be performed. The question of code portability among four different parallel computer systems is addressed in the chapter. This chapter concludes that the serial data-parallel multi-block method provides a number of advantages: (1) it retains the simplicity of the above-mentioned data parallel methods, because each block is treated individually in the same manner as for a single block computation; (2) it does not impose any parallelization constraints on the mesh generation procedure, in principle, any number of blocks of unequal size can be employed; the transfer of data between two blocks (block connectivity) is performed in a transparent manner via globally addressable memory contrasting with the explicit data transfer required by message passing implementations; (3) because individual blocks are treated sequentially, a simple dynamic block management algorithm can be applied to avoid performing unnecessary operations; and (4) the use of standard Fortran 90 facilitates code portability among different platforms supporting the data parallel programming method.}
}
@article{AMADEI2020120149,
title = {Revisiting positive peace using systems tools},
journal = {Technological Forecasting and Social Change},
volume = {158},
pages = {120149},
year = {2020},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120149},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520309756},
author = {Bernard Amadei},
keywords = {Complex systems, Systems thinking, System dynamics, Cross-impact analysis, Network analysis, Positive peace, Peace geometry},
abstract = {This paper looks at peace with an integrated perspective. As a state, peace cannot be measured directly and requires the use of proxies and indicators. This paper revisits the positive peace index (PPI) introduced by the Institute for Economy and Peace (IEP) through the lens of systems thinking and modeling. Three sets of systems tools (cross-impact analysis, network analysis, and system dynamics) are proposed to explicitly account for the different levels of influence and dependence among the eight domains used to determine the PPI at the country level. Although more comprehensive than the original IEP formulation, the integrated approach proposed herein requires decisionmakers to be systems thinkers and able to conduct a detailed analysis of how the eight domains influence (impact) or depend on (sensitive to) each other. The proposed approach allows decisionmakers to capture the multidimensional and cross-disciplinary nature of positive peace better. This paper also shows that the three components of peace (positive, negative, and cultural) initially proposed by Johan Galtung can be represented using three-dimensional geometric features.}
}
@article{NAZIDIZAJI2015318,
title = {Does the smartest designer design better? Effect of intelligence quotient on students’ design skills in architectural design studio},
journal = {Frontiers of Architectural Research},
volume = {4},
number = {4},
pages = {318-329},
year = {2015},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2015.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S2095263515000394},
author = {Sajjad Nazidizaji and Ana Tomé and Francisco Regateiro},
keywords = {Architectural design studio, Intelligence quotient (IQ), Design education, Human factors, Design thinking},
abstract = {Understanding the cognitive processes of the human mind is necessary to further learn about design thinking processes. Cognitive studies are also significant in the research about design studio. The aim of this study is to examine the effect of designers intelligence quotient (IQ) on their designs. The statistical population in this study consisted of all Deylaman Institute of Higher Education architecture graduate students enrolled in 2011. Sixty of these students were selected via simple random sampling based on the finite population sample size calculation formula. The students’ IQ was measured using Raven’s Progressive Matrices. The students’ scores in Architecture Design Studio (ADS) courses from first grade (ADS-1) to fifth grade (ADS-5) and the mean scores of the design courses were used in determining the students’ design ability. Inferential statistics, as well as correlation analysis and mean comparison test for independent samples with SPSS, were also employed to analyze the research data. Results indicated that the students’ IQ, ADS-1 to ADS-4 scores, and the mean scores of the students’ design courses were not significantly correlated. By contrast, the students’ IQ and ADS-5 scores were significantly correlated. As the complexity of the design problem and designers’ experience increased, the effect of IQ on design seemingly intensified.}
}
@article{HEILMAN20041,
title = {Computational models of epileptiform activity in single neurons},
journal = {Biosystems},
volume = {78},
number = {1},
pages = {1-21},
year = {2004},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2004.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0303264704000978},
author = {Avram D. Heilman and James Quattrochi},
keywords = {Paroxysmal depolarizing shifts (PDS), Sustained depolarizations (SD), Hippocampus, Autaptic CA1/CA3 pyramidal neuron, Voltage-gated Ca channels, Ca-dependent K channels},
abstract = {A series of original computational models written in NEURON of increasing physiological and morphological complexity were developed to determine the dominant causes of epileptiform behavior. Current injections to a model hippocampal pyramidal neuron consisting of three compartments produced the sustained depolarizations (SD) and simple paroxysmal depolarizing shifts (PDS) characteristic of ictal and interictal behavior in a cell, respectively. Our results indicate that SDs are the result of the semi-saturation of Na+, Ca2+ and K+ active channels, particularly the CaN, with regular Na+/K+ spikes riding atop a saturated depolarization; PDS rides on a similar semi-saturated depolarization whose shape depends more heavily on interactions between low-threshold voltage-gated Ca2+ channels (CaT) and Ca2+-dependent K+ channels. Our results reflect and predict recent physiological data, and we report here a cellular basis of epilepsy whose mechanisms reside mainly in the membrane channels, and not in specific morphology or network interactions, advancing a possible resolution to the cellular/network debate over the etiology of epileptiform activity.}
}
@article{PETERSON20223586,
title = {Physical computing for materials acceleration platforms},
journal = {Matter},
volume = {5},
number = {11},
pages = {3586-3596},
year = {2022},
issn = {2590-2385},
doi = {https://doi.org/10.1016/j.matt.2022.09.022},
url = {https://www.sciencedirect.com/science/article/pii/S2590238522005409},
author = {Erik Peterson and Alexander Lavin},
keywords = {materials acceleration platforms, AI-driven science, simulation intelligence, physical computing, self-driving labs, inverse design, computational metamaterials},
abstract = {Summary
A “technology lottery” describes a research idea or technology succeeding over others because it is suited to available software/hardware and not necessarily because it is superior. The nascent field of self-driving laboratories, particularly materials acceleration platforms (MAPs), is at risk: while it is logical and opportunistic to inject existing lab equipment and workflows with artificial intelligence (AI) and automation, such MAPs can constrain research by proliferating existing biases in science, mechatronics, and general-purpose computing. Rather than conformity, MAPs present opportunity to pursue new vectors of engineering physics with advances in cyber-physical learning and closed-loop, self-optimizing systems. We outline a simulation-based MAP program to design computers that use physics to solve optimization problems: the physical computing (PC)-MAP can mitigate hardware-software-substrate-user information losses present in all other MAP classes and eliminate lotteries by perfecting alignment between computing problems and media. We describe early PC advances and research pursuits toward optimal design of new materials and computing media.}
}
@article{ZHU2023126915,
title = {SPAR: An efficient self-attention network using Switching Partition Strategy for skeleton-based action recognition},
journal = {Neurocomputing},
volume = {562},
pages = {126915},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126915},
url = {https://www.sciencedirect.com/science/article/pii/S092523122301038X},
author = {ZiJie Zhu and RenDong Ying and Fei Wen and PeiLin Liu},
keywords = {Action recognition, Self-attention, 3D-skeleton, Graph convolutional networks},
abstract = {Graph convolutional networks (GCN) have become the mainstream in skeleton-based action recognition. For further performance improvement, existing methods propose to utilize self-attention to model long-range features of joints. However, these methods cannot balance accuracy with computational efficiency. In this paper, we propose the Switching Partition Strategy (SPAR) Network that uses the self-attention mechanism for the simultaneous and efficient extraction of spatial–temporal long-range information from the skeleton. We design two partition strategies that reduce the computational cost and improve the efficiency of the computation of self-attention. Extensive experiments are conducted on two large-scale datasets, i.e. NTU RGB+D 60 and NTU RGB+D 120, to evaluate the performance of the proposed SPAR network. The results demonstrate that our method outperforms the state-of-the-art on accuracy as well as computational cost.}
}
@article{AMACHER2020100022,
title = {Specificity in PDZ-peptide interaction networks: Computational analysis and review},
journal = {Journal of Structural Biology: X},
volume = {4},
pages = {100022},
year = {2020},
issn = {2590-1524},
doi = {https://doi.org/10.1016/j.yjsbx.2020.100022},
url = {https://www.sciencedirect.com/science/article/pii/S2590152420300040},
author = {Jeanine F. Amacher and Lionel Brooks and Thomas H. Hampton and Dean R. Madden},
keywords = {Protein-protein interactions, PDZ, Peptide-binding domains, Therapeutic targets},
abstract = {Globular PDZ domains typically serve as protein–protein interaction modules that regulate a wide variety of cellular functions via recognition of short linear motifs (SLiMs). Often, PDZ mediated-interactions are essential components of macromolecular complexes, and disruption affects the entire scaffold. Due to their roles as linchpins in trafficking and signaling pathways, PDZ domains are attractive targets: both for controlling viral pathogens, which bind PDZ domains and hijack cellular machinery, as well as for developing therapies to combat human disease. However, successful therapeutic interventions that avoid off-target effects are a challenge, because each PDZ domain interacts with a number of cellular targets, and specific binding preferences can be difficult to decipher. Over twenty-five years of research has produced a wealth of data on the stereochemical preferences of individual PDZ proteins and their binding partners. Currently the field lacks a central repository for this information. Here, we provide this important resource and provide a manually curated, comprehensive list of the 271 human PDZ domains. We use individual domain, as well as recent genomic and proteomic, data in order to gain a holistic view of PDZ domains and interaction networks, arguing this knowledge is critical to optimize targeting selectivity and to benefit human health.}
}
@article{FLATER2018144,
title = {Architecture for software-assisted quantity calculus},
journal = {Computer Standards & Interfaces},
volume = {56},
pages = {144-147},
year = {2018},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2017.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0920548917303069},
author = {David Flater},
keywords = {SI, Quantity, Unit, Uncertainty, Value, Unit 1},
abstract = {A quantity value, such as 5 kg, consists of a number and a reference (often an International System of Units (SI) unit) that together express the magnitude of a quantity. Many software libraries, packages, and ontologies that implement “quantities and units” functions are available. Although all of them begin with SI and associated practices, they differ in how they address issues such as ad hoc counting units, ratios of two quantities of the same kind, and uncertainty. This short article describes an architecture that addresses the complete set of functions in a simple and consistent fashion. Its goal is to encourage more convergent thinking about the functions and the underlying concepts so that the many disparate implementations, present and future, will become more consistent with one another.}
}
@article{SEVERENGIZ2018429,
title = {Influence of Gaming Elements on Summative Assessment in Engineering Education for Sustainable Manufacturing},
journal = {Procedia Manufacturing},
volume = {21},
pages = {429-437},
year = {2018},
note = {15th Global Conference on Sustainable Manufacturing},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.02.141},
url = {https://www.sciencedirect.com/science/article/pii/S235197891830180X},
author = {Mustafa Severengiz and Ina Roeder and Kristina Schindler and Günther Seliger},
keywords = {summative assessment, gamification, higher education, engineering education},
abstract = {Regarding the massive sustainability challenge mankind is currently facing, there is an indisputable need to implement sustainability as the key reference point into higher engineering education in order to prepare the stakeholders of tomorrow. This requires networked thinking on the part of the learner and increases the learning goals’ complexity dramatically. The actual achieved learning outcomes are often evaluated by assessing factual knowledge in higher education. However, it has been shown many times that students choose the examination format for orientation when studying. Thus, the authors propose a gamified summative assessment approach that requires networked thinking to direct students’ learning efforts towards broad competency building. In a study with 25 students of a master engineering course, the effects of a gamified examination design are investigated.}
}
@article{BONCHEKDOKOW201444,
title = {Towards computational models of intention detection and intention prediction},
journal = {Cognitive Systems Research},
volume = {28},
pages = {44-79},
year = {2014},
note = {Special Issue on Mindreading},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2013.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S1389041713000399},
author = {Elisheva Bonchek-Dokow and Gal A. Kaminka},
keywords = {Intention recognition, Intention prediction, Cognitive modeling},
abstract = {Intention recognition is one of the core components of mindreading, an important process in social cognition. Human beings, from age of 18months, have been shown to be able to extrapolate intentions from observed actions, even when the performer failed at achieving the goal. Existing accounts of intention recognition emphasize the use of an intent (plan) library, which is matched against observed actions for recognition. These therefore cannot account for recognition of failed sequences of actions, nor novel actions. In this paper, we begin to tackle these open questions by examining computational models for components of human intention recognition, which emphasize the ability of humans to detect and identify intentions in a sequence of observed actions, based solely on the rationality of movement (its efficiency). We provide a high-level overview of intention recognition as a whole, and then elaborate on two components of the model, which we believe to be at its core, namely, those of intention detection and intention prediction. By intention detection we mean the ability to discern whether a sequence of actions has any underlying intention at all, or whether it was performed in an arbitrary manner with no goal in mind. By intention prediction we mean the ability to extend an incomplete sequence of actions to its most likely intended goal. We evaluate the model, and these two components, in context of existing literature, and in a number of experiments with more than 140 human subjects. For intention detection, our model was able to attribute high levels of intention to those traces perceived by humans as intentional, and vice versa. For intention prediction as well, our model performed in a way that closely matched that of humans. The work highlights the intimate relationship between the ability to generate plans, and the ability to recognize intentions.}
}
@article{LYU2023366,
title = {Application of Deep Learning in the Search and a Certain Celestial Body},
journal = {Procedia Computer Science},
volume = {228},
pages = {366-372},
year = {2023},
note = {3rd International Conference on Machine Learning and Big Data Analytics for IoT Security and Privacy},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.11.042},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923018665},
author = {Yang Lyu and Donglin Su},
keywords = {Deep Learning, Search Analysis, Research Applications, a Certain Celestial Body},
abstract = {With the development of the times, celestial body search has become increasingly common, and people want to enhance their understanding of the universe through further search of celestial bodies. Nowadays, many software and hardware have been invented to assist in celestial body search, but the computational efficiency and efficiency of current software are still insufficient. So this article focuses on the research and application of Deep Learning (DL) in the search and analysis of "a certain celestial body", aiming to improve celestial search through DL. Through experiments, this article uses DL to achieve a maximum computational rate of 78% and a minimum of 70% for celestial search. The computational efficiency of celestial search without DL can reach up to 68% and 57%, respectively. After using DL, the efficiency of celestial search reaches up to 82% and 70%, while before using DL, the efficiency of celestial search reaches up to 62% and 50%, respectively. From this data, it can be seen that DL can achieve good results in celestial search.}
}
@article{XU2024473,
title = {Review of the continuous catalytic ortho-para hydrogen conversion technology for hydrogen liquefaction},
journal = {International Journal of Hydrogen Energy},
volume = {62},
pages = {473-487},
year = {2024},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2024.03.085},
url = {https://www.sciencedirect.com/science/article/pii/S0360319924009170},
author = {Pan Xu and Jian Wen and Ke Li and Simin Wang and Yanzhong Li},
keywords = {Hydrogen liquefaction, Continuous catalytic ortho-para hydrogen conversion technology, Ortho-para hydrogen conversion catalyst, Packing layer, Plate-fin heat exchanger},
abstract = {In order to meet rapidly growing demand of liquid hydrogen in the future hydrogen industry and energy structure, the continuous catalytic ortho-para hydrogen conversion technology (CCOPHCT) has been once again proposed and has become an important choice to improve the hydrogen liquefaction units (HLUs). The origin, concept, and research progress of the CCOPHCT are systematically reviewed for the first time in this paper. However, the research depth and breadth of the CCOPHCT are insufficient to support its current application. To solve it, for the continuous catalytic ortho-para hydrogen conversion plate-fin heat exchanger (CCOPHC-PFHE) with better comprehensive performances, this paper comprehensively summarizes the research achievements in the related fields from the perspective of the unit analysis, including the ortho-para hydrogen conversion (OPHC), packing layer and plate-fin heat exchanger (PFHE), which to provide further thinking for the development of the CCOPHC-PFHE. Further, some suggestions for the CCOPHCT are proposed based on the existed research foundations, including preparing the effective ortho-para hydrogen conversion catalyst (OPHCC), developing the accurate OPHC dynamical model, revealing the coupling mechanism in the packing layer filled with the OPHCC and establishing an effective design method and standard of the CCOPHC-PFHE. In addition, considering the special conditions on the CCOPHC-PFHE, the importance of the experimental research is emphasized. And based on the established hydrogen experimental platform with the comprehensive supporting implementations, the experimental device of the CCOPHC-PFHE has been completed and a series of experimental tests are currently in progressing.}
}
@article{CAO2024101200,
title = {Explanatory models in neuroscience, Part 2: Functional intelligibility and the contravariance principle},
journal = {Cognitive Systems Research},
volume = {85},
pages = {101200},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.101200},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723001341},
author = {Rosa Cao and Daniel Yamins},
keywords = {Evolution, Contravariance, Intelligibility, Function, Optimization, No-miracles, Instrumentalism, Realism, Philosophy, Constraints, Evolutionary landscape, Models, Explanation, Evo-devo, Development, Learning, Deep learning, Abstraction},
abstract = {Computational modeling plays an increasingly important role in neuroscience, highlighting the philosophical question of how computational models explain. In the particular case of neural network models, concerns have been raised about their intelligibility, and how these models relate (if at all) to what is found in the brain. We claim that what makes a system intelligible is an understanding of the dependencies between its behavior and the factors that are responsible for that behavior. In biology, many of these dependencies are naturally “top-down”, as ethological imperatives interact with evolutionary and developmental constraints under natural selection to produce systems with capabilities and behaviors appropriate to their evolutionary needs. We describe how the optimization techniques used to construct neural network models capture some key aspects of these dependencies, and thus help explain why brain systems are as they are — because when a challenging ecologically-relevant goal is shared by a neural network and the brain, it places constraints on the possible mechanisms exhibited in both kinds of systems. The presence and strength of these constraints explain why some outcomes are more likely than others. By combining two familiar modes of explanation — one based on bottom-up mechanistic description (whose relation to neural network models we address in a companion paper) and the other based on top-down constraints, these models have the potential to illuminate brain function.}
}
@article{SUDHESHWAR2024108305,
title = {Learning from Safe-by-Design for Safe-and-Sustainable-by-Design: Mapping the current landscape of Safe-by-Design reviews, case studies, and frameworks},
journal = {Environment International},
volume = {183},
pages = {108305},
year = {2024},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2023.108305},
url = {https://www.sciencedirect.com/science/article/pii/S0160412023005780},
author = {Akshat Sudheshwar and Christina Apel and Klaus Kümmerer and Zhanyun Wang and Lya G. Soeteman-Hernández and Eugenia Valsami-Jones and Claudia Som and Bernd Nowack},
keywords = {Safe-by-Design (SbD), Safe and Sustainable-by-Design (SSbD), Literature mapping, SSbD implementation},
abstract = {With the introduction of the European Commission's “Safe and Sustainable-by-Design” (SSbD) framework, the interest in understanding the implications of safety and sustainability assessments of chemicals, materials, and processes at early-innovation stages has skyrocketed. Our study focuses on the “Safe-by-Design” (SbD) approach from the nanomaterials sector, which predates the SSbD framework. In this assessment, SbD studies have been compiled and categorized into reviews, case studies, and frameworks. Reviews of SbD tools have been further classified as quantitative, qualitative, or toolboxes and repositories. We assessed the SbD case studies and classified them into three categories: safe(r)-by-modeling, safe(r)-by-selection, or safe(r)-by-redesign. This classification enabled us to understand past SbD work and subsequently use it to define future SSbD work so as to avoid confusion and possibilities of “SSbD-washing” (similar to greenwashing). Finally, the preexisting SbD frameworks have been studied and contextualized against the SSbD framework. Several key recommendations for SSbD based on our analysis can be made. Knowledge gained from existing approaches such as SbD, green and sustainable chemistry, and benign-by-design approaches needs to be preserved and effectively transferred to SSbD. Better incorporation of chemical and material functionality into the SSbD framework is required. The concept of lifecycle thinking and the stage-gate innovation model need to be reconciled for SSbD. The development of high-throughput screening models is critical for the operationalization of SSbD. We conclude that the rapid pace of both SbD and SSbD development necessitates a regular mapping of the newly published literature that is relevant to this field.}
}
@incollection{MILLER2023169,
title = {Chapter 9 - Doctoral and professional programs},
editor = {Susan Miller and Walter Moos and Barbara Munk and Stephen Munk and Charles Hart and David Spellmeyer},
booktitle = {Managing the Drug Discovery Process (Second Edition)},
publisher = {Woodhead Publishing},
edition = {Second Edition},
pages = {169-196},
year = {2023},
series = {Woodhead Publishing Series in Biomedicine},
isbn = {978-0-12-824304-6},
doi = {https://doi.org/10.1016/B978-0-12-824304-6.00013-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128243046000134},
author = {Susan Miller and Walter Moos and Barbara Munk and Stephen Munk and Charles Hart and David Spellmeyer},
keywords = {Basic/applied/clinical, Career/job, Collaboration/teams, Critical thinking, -index, PhD/PharmD, Postdoc/postdoctoral, Problem identification, Research design, Writing/publishing},
abstract = {In this chapter on graduate and professional education, we explore doctoral and professional programs, posing a number of key questions you should ask yourself. Where to apply to graduate school or a postdoc, and why? With whom should you work? A PhD or a PharmD? What must you do to be successful? Moreover, we touch on traits important to becoming an independent researcher and ask whether success in graduate school or a postdoctoral fellowship requires different skills than undergraduate degrees. Critical thinking habits underpin this discussion. We outline possible career choices—jobs!—touching on the knowledge and expertise used by drug hunters, and also ask what might be of most value to potential employers. Each of us is different, and what’s best for you is something you will have to decipher, but hopefully you will consult with family, friends, and advisors or mentors before making a final decision. Regardless, “the big leap” is coming, so get ready.}
}
@article{COELHOLOPES2023103555,
title = {The structure of a strategic crisis management model: The context and characteristics of a brazilian community college},
journal = {International Journal of Disaster Risk Reduction},
volume = {87},
pages = {103555},
year = {2023},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2023.103555},
url = {https://www.sciencedirect.com/science/article/pii/S2212420923000353},
author = {Gisele Silveira {Coelho Lopes} and Carlos Ricardo Rossetto and Micheline {Ramos de Oliveira} and Jorge Oneide Sausen and Rudimar {Antunes da Rocha}},
keywords = {Crisis management, Organizational strategy, Community university},
abstract = {This study presents the structure of a strategic crisis management model, considering the context and characteristics of a Brazilian community college. Strategic crisis management associated with coordination and control mechanisms theoretically underpinned the problem under study. It is a single case study, with grounded theory as the strategy for data treatment and content analysis to interpret and present the findings. In the model, the strategic and tactical stages systematized the dynamics of crisis management in a coordinated manner. Strategic crisis management was considered a continuous process rather than a strictly punctual one. The dynamism of the model's operationalization considered some premises that guided the behavior of the leadership and the crisis management team: i) pragmatic strategic thinking shaped by rationality; ii) quick responses in facing the crisis; iii) simplicity in actions; iv) reversible decisions susceptible to flexibilization; v) creativity and boldness to innovate and set new standards; v) collaboration for common causes.}
}
@article{JAYAPRAKASAM2015229,
title = {PSOGSA-Explore: A new hybrid metaheuristic approach for beampattern optimization in collaborative beamforming},
journal = {Applied Soft Computing},
volume = {30},
pages = {229-237},
year = {2015},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2015.01.024},
url = {https://www.sciencedirect.com/science/article/pii/S1568494615000435},
author = {S. Jayaprakasam and S.K.A. Rahim and Chee Yen Leow},
keywords = {Collaborative beamforming, Random array, Sidelobe suppression, Particle swarm optimization (PSO), Gravitational search algorithm (GSA)},
abstract = {A conventional collaborative beamforming (CB) system suffers from high sidelobes due to the random positioning of the nodes. This paper introduces a hybrid metaheuristic optimization algorithm called the Particle Swarm Optimization and Gravitational Search Algorithm-Explore (PSOGSA-E) to suppress the peak sidelobe level (PSL) in CB, by the means of finding the best weight for each node. The proposed algorithm combines the local search ability of the gravitational search algorithm (GSA) with the social thinking skills of the legacy particle swarm optimization (PSO) and allows exploration to avoid premature convergence. The proposed algorithm also simplifies the cost of variable parameter tuning compared to the legacy optimization algorithms. Simulations show that the proposed PSOGSA-E outperforms the conventional, the legacy PSO, GSA and PSOGSA optimized collaborative beamformer by obtaining better results faster, producing up to 100% improvement in PSL reduction when the disk size is small.}
}
@article{PARTTO2012442,
title = {Explaining failures in innovative thought processes in engineering design},
journal = {Procedia - Social and Behavioral Sciences},
volume = {41},
pages = {442-449},
year = {2012},
note = {The First International Conference on Leadership, Technology and Innovation Management},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2012.04.053},
url = {https://www.sciencedirect.com/science/article/pii/S1877042812009330},
author = {Minna Pärttö and Pertti Saariluoma},
keywords = {Microinnovation processes, engineering design, thought errors, thought failures},
abstract = {The aim of this study is to explore factors causing failures in innovative thought processes in engineering design. An innovation process is here understood as a complex and multi-phased thinking and problem solving process generating new and mostly unforeseeable solutions. The phases are partly overlapping and simultaneous. This complicated nature of innovation process demands a lot from innovation management, and thus it is not unusual that innovation processes fail. Identifying problems and shortcomings is important because it helps organizations to eliminate them in the future. This study focus on thought processes of individual participants in an innovation process, which is referred by us as microinnovation approach. This approach understands innovations as being based on human thinking.This study shows that factors related to knowledge, management and interaction are causing failures in engineering design. We found haste to be the most common reason for failures. Other contributing factors were lack of long-term thinking and inability to understand others’ perspective.}
}
@article{KALBANDE2023138474,
title = {Machine learning based quantification of VOC contribution in surface ozone prediction},
journal = {Chemosphere},
volume = {326},
pages = {138474},
year = {2023},
issn = {0045-6535},
doi = {https://doi.org/10.1016/j.chemosphere.2023.138474},
url = {https://www.sciencedirect.com/science/article/pii/S0045653523007415},
author = {Ritesh Kalbande and Bipin Kumar and Sujit Maji and Ravi Yadav and Kaustubh Atey and Devendra Singh Rathore and Gufran Beig},
keywords = {Ozone, VOCs, Machine learning, Meteorology, Isoprene},
abstract = {The prediction of surface ozone is essential attributing to its impact on human and environmental health. Volatile organic compounds (VOCs) are crucial in driving ozone concentration; particularly in urban areas where VOC limited regimes are prominent. The limited measurements of VOCs, however, hinder assessing the VOC-ozone relationship. This work applies machine learning (ML) algorithms for temporal forecasting of surface ozone over a metropolitan city in India. The availability of continuous VOCs measurement data along with meteorology and other pollutants during 2014–2016 makes it possible to deduce the influence of various input parameters on surface ozone prediction. After evaluating the best ML model for ozone prediction, simulations were carried out using varied input combinations. The combination with isoprene, meteorology, NOx, and CO (Isop + MNC) was the best with RMSE 4.41 ppbv and MAPE 6.77%. A season-wise comparison of simulations having all data, only meteorological data and Isop + MNC as input showed that Isop + MNC simulation gives the best results during the summer season (RMSE: 5.86 ppbv, MAPE: 7.05%). This shows the increased ability of the model to capture ozone peaks (high ozone during summer) relatively better when isoprene data is used. The overall results highlight that using all available data doesn't necessarily give best prediction results; also critical thinking is essential when evaluating the model results.}
}
@incollection{PAUL2005431,
title = {Chapter 15 - Models of Computation for Systems-on-Chips},
editor = {Ahmed Amine Jerraya and Wayne Wolf},
booktitle = {Multiprocessor Systems-on-Chips},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {431-463},
year = {2005},
series = {Systems on Silicon},
issn = {18759661},
doi = {https://doi.org/10.1016/B978-012385251-9/50031-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780123852519500311},
author = {JoAnn M. Paul and Donald E. Thomas},
abstract = {Publisher Summary
This chapter describes system modeling and its relationship to models of computation. It compares several different models of computation and evaluates their usefulness at various stages in system design. It also describes the modeling environment for software and hardware (MESH) environment for hardware and software modeling. Models of computation (MoCs) are abstract representations of computing systems. Computer modeling can be separated into three areas—formal MoCs, computer artifacts, and computer design tools. A formal MoC is generally considered to be one with a mathematical basis. Simulations of formal models may be more efficient for large systems; however, the properties of formal models permit the representation of the system to be manipulated purely mathematically. Computer artifacts are the objects of computer architects. They include software, hardware, or both. Design tools are computer programs that are used to assist the construction of instances of computers as well as the conceptualization of computer artifacts. Design tools may be considered synonymous with design artifacts because they are objects, or entities, used to facilitate the design process. They may have a formal mathematical basis.}
}
@incollection{GISZTER2007323,
title = {Primitives, premotor drives, and pattern generation: a combined computational and neuroethological perspective},
editor = {Paul Cisek and Trevor Drew and John F. Kalaska},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {165},
pages = {323-346},
year = {2007},
booktitle = {Computational Neuroscience: Theoretical Insights into Brain Function},
issn = {0079-6123},
doi = {https://doi.org/10.1016/S0079-6123(06)65020-6},
url = {https://www.sciencedirect.com/science/article/pii/S0079612306650206},
author = {Simon Giszter and Vidyangi Patil and Corey Hart},
keywords = {primitives, motor synergies, force-fields, modularity, feedback, motor pattern analysis, decomposition, rhythm generation, pattern shaping},
abstract = {A modular motor organization may be needed to solve the degrees of freedom problem in biological motor control. Reflex elements, kinematic primitives, muscle synergies, force-field primitives and/or pattern generators all have experimental support as modular elements. We discuss the possible relations of force-field primitives, spinal feedback systems, and pattern generation and shaping systems in detail, and review methods for examining underlying motor pattern structure in intact or semi-intact behaving animals. The divisions of systems into primitives, synergies, and rhythmic elements or oscillators suggest specific functions and methods of construction of movement. We briefly discuss the limitations and caveats needed in these interpretations given current knowledge, together with some of the hypotheses arising from these frameworks.}
}
@article{CHEN1998475,
title = {A computationally attractive nonlinear predictive control scheme with guaranteed stability for stable systems},
journal = {Journal of Process Control},
volume = {8},
number = {5},
pages = {475-485},
year = {1998},
note = {ADCHEM '97 IFAC Symposium: Advanced Control of Chemical Processes},
issn = {0959-1524},
doi = {https://doi.org/10.1016/S0959-1524(98)00021-3},
url = {https://www.sciencedirect.com/science/article/pii/S0959152498000213},
author = {H. Chen and F. Allgöwer},
keywords = {nonlinear predictive control, constraints, stability, terminal conditions},
abstract = {We introduce in this paper a nonlinear model predictive control scheme for open-loop stable systems subject to input and state constraints. Closed-loop stability is guaranteed by an appropriate choice of the finite prediction horizon, independent of the specification of the desired control performance. In addition, this control scheme is likely to allow ‘real time’ implementation, because of its computational attractiveness. The theoretical results are demonstrated and discussed with a CSTR control application.}
}
@article{SONI2024100016,
title = {Advancements in MXene-based electrocatalysts for hydrogen evolution reaction processes: A comprehensive review},
journal = {Journal of Alloys and Compounds Communications},
volume = {3},
pages = {100016},
year = {2024},
issn = {2950-2845},
doi = {https://doi.org/10.1016/j.jacomc.2024.100016},
url = {https://www.sciencedirect.com/science/article/pii/S295028452400016X},
author = {Kunjal Soni and Rakesh Kumar Ameta},
keywords = {MXenes, 2D materials, Two-electron transfer process, Hydrogen evolution process, Electrocatalysts},
abstract = {MXenes are a newly emerging family of two-dimensional (2D) materials that include carbonitrides, nitrides, and carbides of transition metals. They have attracted much interest from scientists and researchers due to their potential use in electrocatalysts, where a two-electron transfer process is applied. Their remarkable properties, such as strong chemical and structural stability, high electrical conductivity, and large active surface area, make them effective for their potential in advanced hydrogen evolution reactions (HER). This thorough analysis starts by carefully outlining the forward-thinking advances in MXene synthesis and development. It then explores the theoretical and empirical aspects of MXene-based HER electrocatalysts. This review paper presents methods for improving the HER catalytic activity of MXene, including terminal modification, metal-atom doping, and the creation of various nanostructures to increase the density of active sites. The study clarifies current issues and new opportunities and provides a valuable framework for the future development of effective MXene-based electrocatalysts for HERs.}
}
@article{TALAMI2025115242,
title = {Evaluating the effectiveness, reliability and efficiency of a multi-objective sequential optimization approach for building performance design},
journal = {Energy and Buildings},
volume = {329},
pages = {115242},
year = {2025},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.115242},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824013586},
author = {Riccardo Talami and Jonathan Wright and Bianca Howard},
keywords = {Building optimization, Building performance, Algorithm, Building simulation, Building design},
abstract = {The complexity of performance-based building design stems from the evaluation of numerous candidate design options, driven by the plethora of variables, objectives, and constraints inherent in multi-disciplinary projects. This necessitates optimization approaches to support the identification of well performing designs while reducing the computational time of performance evaluation. In response, this paper proposes and evaluates a sequential approach for multi-objective design optimization of building geometry, fabric, HVAC system and controls for building performance. This approach involves sequential optimizations with optimal solutions from previous stages passed to the next. The performance of the sequential approach is benchmarked against a full factorial search, assessing its effectiveness in finding global optima, solution quality, reliability to scale and variations of problem formulations, and computational efficiency compared to the NSGA-II algorithm. 24 configurations of the sequential approach are tested on a multi-scale case study, simulating 874 to 4,147,200 design options for an office building, aiming to minimize energy demand while maintaining thermal comfort. A two-stage sequential process-(building geometry + fabric) and (HVAC system + controls) identified the same Pareto-optimal solutions as the full factorial search across all four scales and variations of problem formulations, demonstrating 100 % effectiveness and reliability. This approach required 100,700 function evaluations, representing a 91.2 % reduction in computational effort compared to the full factorial search. In contrast, NSGA-II achieved only 73.5 % of the global optima with the same number of function evaluations. This research indicates that a sequential optimization approach is a highly efficient and robust alternative to the standard NSGA-II algorithm.}
}
@incollection{JAIN2015181,
title = {Chapter Seven - Computational Methods for RNA Structure Validation and Improvement},
editor = {Sarah A. Woodson and Frédéric H.T. Allain},
series = {Methods in Enzymology},
publisher = {Academic Press},
volume = {558},
pages = {181-212},
year = {2015},
booktitle = {Structures of Large RNA Molecules and Their Complexes},
issn = {0076-6879},
doi = {https://doi.org/10.1016/bs.mie.2015.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0076687915000208},
author = {Swati Jain and David C. Richardson and Jane S. Richardson},
keywords = {RNA crystallography, RNA backbone conformers, Ribose pucker, Clash correction, MolProbity, PHENIX, ERRASER, wwPDB validation},
abstract = {With increasing recognition of the roles RNA molecules and RNA/protein complexes play in an unexpected variety of biological processes, understanding of RNA structure–function relationships is of high current importance. To make clean biological interpretations from three-dimensional structures, it is imperative to have high-quality, accurate RNA crystal structures available, and the community has thoroughly embraced that goal. However, due to the many degrees of freedom inherent in RNA structure (especially for the backbone), it is a significant challenge to succeed in building accurate experimental models for RNA structures. This chapter describes the tools and techniques our research group and our collaborators have developed over the years to help RNA structural biologists both evaluate and achieve better accuracy. Expert analysis of large, high-resolution, quality-conscious RNA datasets provides the fundamental information that enables automated methods for robust and efficient error diagnosis in validating RNA structures at all resolutions. The even more crucial goal of correcting the diagnosed outliers has steadily developed toward highly effective, computationally based techniques. Automation enables solving complex issues in large RNA structures, but cannot circumvent the need for thoughtful examination of local details, and so we also provide some guidance for interpreting and acting on the results of current structure validation for RNA.}
}
@article{VANHOOIJDONK2022101044,
title = {Creativity and change of context: The influence of object-context (in)congruency on cognitive flexibility},
journal = {Thinking Skills and Creativity},
volume = {45},
pages = {101044},
year = {2022},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2022.101044},
url = {https://www.sciencedirect.com/science/article/pii/S1871187122000475},
author = {Mare {van Hooijdonk} and Simone M. Ritter and Marcel Linka and Evelyn Kroesbergen},
keywords = {Creativity, Spatial context, Object congruence, Cognitive flexibility, Stimulating creativity},
abstract = {Specific environmental features, such as natural settings or spatial design, can foster creativity. The effect of object-context congruency on creativity has not yet been investigated. While congruence between an object and its visual context provides meaning to the object, it may hamper creativity due to mental fixation effects. In the current study, virtual reality technology (VR) was employed to examine the hypothesis that people display more cognitive flexibility - a key element of creativity, representing the ability to overcome mental fixation - when thinking about an object while being in an incongruent than in a congruent environment. Participants (N = 184) performed an Alternative Uses Task, in which they had to name as many uses for a book as possible, while being immersed in a virtual environment that was either object-context congruent (i.e., places where you would expect a book; e.g., a library or a living room; n = 91) or object-context incongruent (i.e., places where a book is not expected; e.g., a clothing store or a car workshop; n = 93). The effect of object (in)congruency was also assessed for three other indices of creativity: fluency (i.e., the number of ideas generated), originality and usefulness. In line with our hypothesis, participants scored higher on pure cognitive flexibility in the object-context incongruent than in the object-context congruent environment. Moreover, participants in the object-context incongruent environment condition generated more original ideas. The theoretical and practical implications of the current findings are discussed.}
}
@article{PUPUNWIWAT2011827,
title = {Conceptual Selective RFID Anti-Collision Technique Management},
journal = {Procedia Computer Science},
volume = {5},
pages = {827-834},
year = {2011},
note = {The 2nd International Conference on Ambient Systems, Networks and Technologies (ANT-2011) / The 8th International Conference on Mobile Web Information Systems (MobiWIS 2011)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2011.07.114},
url = {https://www.sciencedirect.com/science/article/pii/S187705091100439X},
author = {Prapassara Pupunwiwat and Peter Darcy and Bela Stantic},
keywords = {Radio Frequency Identification (RFID), Anti-Collision, Decision Tree, Six Thinking Hats},
abstract = {Radio Frequency Identification (RFID) uses wireless radio frequency technology to automatically identify tagged objects. Despite the extensive development of RFID technology, tag collisions still remains a major drawback. The collision issue can be solved by using anti-collision techniques. While existing research has focused on improving anti-collision methods alone, it is also essential that a suitable type of anti-collision algorithm is selected for the specific circumstance. In this work, we evaluate anti-collision techniques and perform a comparative analysis in order to find the advantages and disadvantages of each approach. To identify the best anti-collision selection method in various scenarios, we have proposed two strategies for selective anti-collision technique management: a “Novel Decision Tree Strategy” and a “Six Thinking Hats Strategy”. We have shown that the selection of the correct technique for specific scenarios improve the quality of the data collection which, in turn, will increase the integrity of the data after being transformed, aggregated, and used for event processing.}
}
@article{JUDD1997907,
title = {Computational economics and economic theory: Substitutes or complements?},
journal = {Journal of Economic Dynamics and Control},
volume = {21},
number = {6},
pages = {907-942},
year = {1997},
note = {Society of Computational Economics Conference},
issn = {0165-1889},
doi = {https://doi.org/10.1016/S0165-1889(97)00010-9},
url = {https://www.sciencedirect.com/science/article/pii/S0165188997000109},
author = {Kenneth L. Judd},
keywords = {Computational approach, Theoretical analysis},
abstract = {This essay examines the idea and potential of a ‘computational approach to theory’, discusses methodological issues raised by such computational methods, and outlines the problems associated with the dissemination of computational methods and the exposition of computational results. We argue that the study of a theory need not be confined to proving theorems, that current and future computer technologies create new possibilities for theoretical analysis, and that by resolving these issues we will create an intellectual atmosphere in which computational methods can make substantial contributions to economic analysis.}
}
@article{ZHU2025117923,
title = {Prediction of damage evolution in CMCs considering the real microstructures through a deep-learning scheme},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {439},
pages = {117923},
year = {2025},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2025.117923},
url = {https://www.sciencedirect.com/science/article/pii/S0045782525001951},
author = {Rongqi Zhu and Guohao Niu and Panding Wang and Chunwang He and Zhaoliang Qu and Daining Fang},
keywords = {Deep learning, Damage evolution, Ceramic matrix composites, Microstructure, Computed tomography},
abstract = {The real microstructures of ceramic matrix composites (CMCs) play a crucial role in determining their damage behavior. However, considering the real microstructure within the high-fidelity numerical simulation usually leads to expensive computational costs. In this study, an end-to-end deep-learning (DL) framework is proposed to predict the evolution of damage fields for CMCs from their real microstructures, which are characterized through computed tomography (CT). Three sub-networks, including the microstructure processing network (MPN), elastic deformation prediction network (EPN), and damage sequence prediction network (DPN), are used to construct a two-stage DL model. In the first stage, the geometrical characteristics of real microstructure are precisely captured by the MPN with over 92 % precision for the yarns and matrix. In the second stage, the elastic deformation predicted by the EPN is taken as the intermediate variable to motivate the damage prediction of DPN with the MPN-predicted microstructure as input. The damage evolution of real microstructure is finally predicted with a mean relative error of 10.8 % for the primary damage variable fields. The high-damage regions in the microstructure can also be accurately captured with a mean precision of 87.9 %. The proposed model is further validated by the in-situ tensile experiment. The micro-cracks are proven to initiate and propagate in the high-damage regions. Compared with the high-fidelity numerical methods, this DL-based method can predict the damage evolution on the fly, avoiding time-consuming computation and poor convergence during the damage analysis.}
}
@article{ZHAO2024109027,
title = {Towards the definition of spatial granules},
journal = {Fuzzy Sets and Systems},
volume = {490},
pages = {109027},
year = {2024},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2024.109027},
url = {https://www.sciencedirect.com/science/article/pii/S0165011424001738},
author = {Liquan Zhao and Yiyu Yao},
keywords = {Granularity, Fineness, Subsethood, Coarse-fine relation, Quotient join, Quotient meet, Granular space, Spatial granular computing},
abstract = {Three basic issues of granular computing are construction or definition of granules, measures of granules, and computation or reasoning with granules. This paper reviews the main theories of granular computing and introduces the definition of spatial granules. A granule is composed of one or more atomic granules. The rationality of this definition is explained from the four aspects: simplicity, applicability, measurability and visualization. A one-to-one correspondence is established between the granules and the points in the unit hypercube, and the coarsening and refining of the granules are the descending and ascending dimensions of the points, respectively. The weak fuzzy tolerance relation and weak fuzzy equivalence relation are defined so as to study on all fuzzy binary relations. The notion of layer granularity/fineness is introduced and each granule can be easily denoted by two numbers, which can be used to pre-process macro knowledge space and greatly improve the search speed. This paper also discusses the main properties of granules including the necessary and sufficient conditions of coarse-fine relation and the main principles of granular space.}
}
@article{AMEL2023104187,
title = {Toward an automatic detection of cardiac structures in short and long axis views},
journal = {Biomedical Signal Processing and Control},
volume = {79},
pages = {104187},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.104187},
url = {https://www.sciencedirect.com/science/article/pii/S1746809422006413},
author = {Laidi Amel and Mohammed Ammar and Mostafa {El Habib Daho} and Said Mahmoudi},
keywords = {Cardiac MRI Segmentation, Shape Descriptors, Particle Swarm Optimization, Residual Network, Interpretability},
abstract = {Objective
This work aims to create an automatic detection process of cardiac structures in both short-axis and long-axis views. A workflow inspired by human thinking process, for better explainability.
Methods
we began by separating the images into two classes: long axis and short axis, using a Residual Network model. Then, we used Particle Swarm Optimization for general segmentation. After segmentation, a characterization step based on shape descriptors calculated from bounding box and ANOVA for features selection were applied on the binary images to detect the location of each region of interest: lung, left and right ventricle in the short-axis view, the aorta, the left heart (left atrium and ventricle), and the right heart (right atrium and ventricle) in the long axis view.
Results
we achieved a 90% accuracy on view separation. We have selected: Elongation, Compactness, Circularity, Type Factor, for short axis identification; and:Area, Centre of Mass Y, Moment of Inertia XY, Moment of Inertia YY, for long axis identification.
Conclusion
a successful separation of long axis and short axis views allows for a better characterization and detection of segmented cardiac structures. After that, any method can be applied for segmentation, attribute selection, and classification.
Significance
an attempt to introduce explainability into cardiac image segmentation, we tried to mimic the human workflow while computerizing each step. The process seems to be valid and added clarity and interpretability to the detection.}
}
@article{CHEN202321,
title = {Varieties of specification: Redefining over- and under-specification},
journal = {Journal of Pragmatics},
volume = {216},
pages = {21-42},
year = {2023},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2023.07.019},
url = {https://www.sciencedirect.com/science/article/pii/S037821662300200X},
author = {Guanyi Chen and Kees {van Deemter}},
keywords = {Referring expressions, Over-specification, Under-specification},
abstract = {A long tradition of research in theoretical, experimental and computational pragmatics has investigated over-specification and under-specification in referring expressions. Along broadly Gricean lines, these studies compare the amount of information expressed by a referring expression against the amount of information that is required. Often, however, these studies offer no formal definition of what “required” means, and how the comparison should be performed. In this paper, we use a simple set-theoretic perspective to define some communicatively important types of over-/under-specification. We argue that our perspective enables an enhanced understanding of reference phenomena that can pay important dividends for the analysis of reference in corpora and for the evaluation of computational models of referring. To illustrate and substantiate our claims, we analyse two corpora, containing Chinese and English referring expressions respectively, using the new perspective. The results show that interesting new monolingual and cross-linguistic insights can be obtained from our perspective.}
}
@article{JORAJURIA2022664,
title = {Oscillatory Source Tensor Discriminant Analysis (OSTDA): A regularized tensor pipeline for SSVEP-based BCI systems},
journal = {Neurocomputing},
volume = {492},
pages = {664-675},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.07.103},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221018956},
author = {Tania Jorajuría and Mina {Jamshidi Idaji} and Zafer İşcan and Marisol Gómez and Vadim V. Nikulin and Carmen Vidaurre},
keywords = {Brain-computer interface, Steady-state visual evoked potential, Spatio-spectral decomposition, Higher order discriminant analysis, Analytical regularization, Tensor-based feature reduction},
abstract = {Periodic signals called Steady-State Visual Evoked Potentials (SSVEP) are elicited in the brain by flickering stimuli. They are usually detected by means of regression techniques that need relatively long trial lengths to provide feedback and/or sufficient number of calibration trials to be reliably estimated in the context of brain-computer interface (BCI). Thus, for BCI systems designed to operate with SSVEP signals, reliability is achieved at the expense of speed or extra recording time. Furthermore, regardless of the trial length, calibration free regression-based methods have been shown to suffer from significant performance drops when cognitive perturbations are present affecting the attention to the flickering stimuli. In this study we present a novel technique called Oscillatory Source Tensor Discriminant Analysis (OSTDA) that extracts oscillatory sources and classifies them using the newly developed tensor-based discriminant analysis with shrinkage. The proposed approach is robust for small sample size settings where only a few calibration trials are available. Besides, it works well with both low- and high-number-of-channel settings, using trials as short as one second. OSTDA performs similarly or significantly better than other three benchmarked state-of-the-art techniques under different experimental settings, including those with cognitive disturbances (i.e. four datasets with control, listening, speaking and thinking conditions). Overall, in this paper we show that OSTDA is the only pipeline among all the studied ones that can achieve optimal results in all analyzed conditions.}
}
@article{CHAN2022102109,
title = {Slow down to speed up: Longer pause time before solving problems relates to higher strategy efficiency},
journal = {Learning and Individual Differences},
volume = {93},
pages = {102109},
year = {2022},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2021.102109},
url = {https://www.sciencedirect.com/science/article/pii/S1041608021001461},
author = {Jenny Yun-Chen Chan and Erin R. Ottmar and Ji-Eun Lee},
keywords = {Pause time, Strategy efficiency, Algebra problem-solving, Online learning environment, Metacognitive skills},
abstract = {We examined the influences of pre-solving pause time, algebraic knowledge, mathematics self-efficacy, and mathematics anxiety on middle-schoolers' strategy efficiency in an algebra learning game. We measured strategy efficiency using (a) the number of steps taken to complete a problem, (b) the proportion of problems completed on the initial attempt, and (c) the number of resets prior to completing the problems. Using the log data from the game, we found that longer pre-solving pause time was associated with more efficient strategies, as indicated by fewer solution steps, higher initial completion rate, and fewer resets. Higher algebraic knowledge was associated with higher initial completion rate and fewer resets. Mathematics self-efficacy and mathematics anxiety was not associated with any measures of strategy efficiency. The results suggest that pause time may be an indicator of student thinking before problem-solving, and provide insights into using data from online learning platforms to examine students' problem-solving processes.}
}
@article{ROY2020106210,
title = {Fixed subgroups and computation of auto-fixed closures in free-abelian times free groups},
journal = {Journal of Pure and Applied Algebra},
volume = {224},
number = {4},
pages = {106210},
year = {2020},
issn = {0022-4049},
doi = {https://doi.org/10.1016/j.jpaa.2019.106210},
url = {https://www.sciencedirect.com/science/article/pii/S0022404919302178},
author = {Mallika Roy and Enric Ventura},
keywords = {Free-abelian times free, Automorphism, Fixed subgroup, Periodic subgroup, Auto-fixed closure},
abstract = {The classical result by Dyer–Scott about fixed subgroups of finite order automorphisms of Fn being free factors of Fn is no longer true in Zm×Fn. Within this more general context, we prove a relaxed version in the spirit of Bestvina–Handel Theorem: the rank of fixed subgroups of finite order automorphisms is uniformly bounded in terms of m,n. We also study periodic points of endomorphisms of Zm×Fn, and give an algorithm to compute auto-fixed closures of finitely generated subgroups of Zm×Fn. On the way, we prove the analog of Day's Theorem for real elements in Zm×Fn, contributing a modest step into the project of doing so for any right angled Artin group (as McCool did with respect to Whitehead's Theorem in the free context).}
}
@article{FAELENS2021106510,
title = {Social media use and well-being: A prospective experience-sampling study},
journal = {Computers in Human Behavior},
volume = {114},
pages = {106510},
year = {2021},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2020.106510},
url = {https://www.sciencedirect.com/science/article/pii/S0747563220302624},
author = {Lien Faelens and Kristof Hoorelbeke and Bart Soenens and Kyle {Van Gaeveren} and Lieven {De Marez} and Rudi {De Raedt} and Ernst H.W. Koster},
keywords = {Social media, Social comparison, Self-esteem, Repetitive negative thinking, Negative affect},
abstract = {Facebook and Instagram are currently the most popular Social Network Sites (SNS) for young adults. A large amount of research examined the relationship between these SNS and well-being, and possible intermediate constructs such as social comparison, self-esteem, and repetitive negative thinking (RNT). However, most of these studies have cross-sectional designs and use self-report indicators of SNS use. Therefore, their conclusions should be interpreted cautiously. Consequently, the goal of the current experience sampling study was to examine the temporal dynamics between objective indicators of SNS use, and self-reports of social comparison, RNT, and daily fluctuations in negative affect. More specifically, we assessed 98 participants 6 times per day during 14 days to examine reciprocal relationships between SNS use, negative affect, emotion regulation, and key psychological constructs. Results indicate that (1) both Facebook and Instagram use predicted reduced well-being, and (2) self-esteem and RNT appear to be important intermediate constructs in these relationships. Future longitudinal and experimental studies are needed to further support and extend the current research findings.}
}
@article{EDELMAN1997296,
title = {Computational theories of object recognition},
journal = {Trends in Cognitive Sciences},
volume = {1},
number = {8},
pages = {296-304},
year = {1997},
issn = {1364-6613},
doi = {https://doi.org/10.1016/S1364-6613(97)01090-5},
url = {https://www.sciencedirect.com/science/article/pii/S1364661397010905},
author = {S. Edelman},
abstract = {This paper examines four current theoretical approaches to the representation and recognition of visual objects: structural descriptions, geometric constraints, multidimensional feature spaces and shape-space approximation. The strengths and weaknesses of the four theories are considered, with a special focus on their approach to categorization — a computationally challenging task which is not widely addressed in computer vision, where the stress is rather on the generalization of recognition across changes of viewpoint.}
}
@article{JIANG2024109208,
title = {Surrogate-based Shape Optimization Design for the Stable Descent of Mars Parachutes},
journal = {Aerospace Science and Technology},
volume = {150},
pages = {109208},
year = {2024},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2024.109208},
url = {https://www.sciencedirect.com/science/article/pii/S1270963824003390},
author = {Lulu Jiang and Guanhua Chen and Xiaopeng Xue and Xin Pan and Gang Chen},
keywords = {Supersonic Parachute, Mars atmosphere, Aerodynamic Optimization, Shape design, Wide Speed Range},
abstract = {The crucial role that the supersonic parachute plays in space exploration missions has been widely recognized, as it directly impacts the safe landing of probes. However, parachute models with optimization on different aerodynamic performances often involve design conflicts with each other. Additionally, the parachute design focusing on a single point cannot fully adapt to different speed ranges during stable descent. This complexity makes it challenging to use traditional shape design methods, which rely on empirical knowledge, to address these coupled design issues. Faced with the design challenges of Mars parachutes, this study, inspired by aircraft aerodynamic optimization principles, establishes a shape design method specifically for the stable descent phase of Mars parachutes. The method combines numerical simulation and surrogate-based optimization strategies, aiming to enhance the overall performance during stable descent and meet various demands of different exploration missions. Meanwhile, by providing a rapid estimate of the shape during the design phase, the method significantly improves computational efficiency. The optimal models effectively balance comprehensive performance in the supersonic-transonic-subsonic speed domain by conducting shape optimization research on the disk-gap-band parachute using the surrogate-based optimization strategy. Also, it exhibits better deceleration and stability across the entire speed range compared to the base model, even when deviating from the design Mach number. Importantly, the advantages of canopy-only optimization for drag performance extend to the capsule-canopy two-body system, enhancing the drag performance of the canopy in the two-body system. This strategic approach reduces the transient calculation time for the two-body system, further improving computational efficiency. The method provides a practical and forward-thinking solution for the design of Mars parachutes.}
}
@article{ZHANG2025111031,
title = {Constrained multi-scale dense connections for biomedical image segmentation},
journal = {Pattern Recognition},
volume = {158},
pages = {111031},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111031},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007829},
author = {Jiawei Zhang and Yanchun Zhang and Hailong Qiu and Tianchen Wang and Xiaomeng Li and Shanfeng Zhu and Meiping Huang and Jian Zhuang and Yiyu Shi and Xiaowei Xu},
keywords = {Multi-scale dense connections, Image segmentation, Network architecture search, Feature fusion},
abstract = {Multi-scale dense connection has been widely used in the biomedical image community to enhance the segmentation performance. In this way, features from all or most scales are aggregated or iteratively fused. However, by analyzing the details, we discover that some connections involving distant scales may not contribute to, or even harm, the performance, while they always introduce a noticeable increase in computational cost. In this paper, we propose constrained multi-scale dense connections (CMDC) for biomedical image segmentation. In contrast to current general lightweight approaches, we first introduce two methods, a naive method and a network architecture search (NAS)-based method, to remove redundant connections and verify the optimal connection configuration, thereby improving overall efficiency and accuracy. The results demonstrate that the two approaches obtain a similar optimal configuration in which most features at the adjacent scales are connected. Then, we applied the optimal configuration to various backbone networks to build constrained multi-scale dense networks (CMD-Net). Experimental results evaluated on eight image segmentation datasets covering biomedical images and natural images demonstrate the effectiveness of CMD-Net across a variety of backbone networks (FCN, U-Net, DeepLabV3, SegNet, FCNsa, ConvUNeXt) with a much lower increase in computational cost. Furthermore, CMD-Net achieves state-of-the-art performance on four publicly available datasets. We believe that the CMDC method can offer valuable insight for ways to engage in dense connectivity at multiple scales within communities. The source code has been made available at https://github.com/JerRuy/CMD-Net.}
}
@article{NUNEZV2024101173,
title = {Recommendation system using bio-inspired algorithms for urban orchards},
journal = {Internet of Things},
volume = {26},
pages = {101173},
year = {2024},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2024.101173},
url = {https://www.sciencedirect.com/science/article/pii/S2542660524001148},
author = {Juan M. {Núñez V.} and Juan M. Corchado and Diana M. Giraldo and Sara Rodríguez-González and Fernando {De la Prieta}},
keywords = {Internet of Things, Bio-inspired algorithms, Urban orchards, Lettuce crops, Social design},
abstract = {According to the Food and Agriculture Organization of the United Nations (FAO), climate change is exponentially affecting agricultural production worldwide, with food prices expected to increase by up to 90 percent by 2030 and hunger and malnutrition rates to rise by 2050. This paper presents the development of a platform based on the Internet of Things (IoT) for monitoring urban gardens as a strategy to mitigate hunger, promote food sovereignty and circular economy in areas of food shortage. To this end, an Internet of Things (IoT) architecture is proposed and implemented that involves a social design layer that allows an effective transfer of knowledge to communities and a recommendation system based on evolutionary computation to optimize and maximize the productivity of urban orchards, and thus contribute to the 2030 agenda of the Sustainable Development Goals (SDGs). Finally, three experiments in urban gardens are shown to validate evolutionary computation and artificial intelligence models, such as multiple linear regression, genetic algorithms, ant colony algorithms and spatial estimation and inference algorithms such as the Kriging algorithm. The productivity of urban lettuce orchards is increased between 25 and 45%.}
}
@incollection{KISS2019109,
title = {Process Systems Engineering from an industrial and academic perspective},
editor = {Anton A. Kiss and Edwin Zondervan and Richard Lakerveld and Leyla Özkan},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {46},
pages = {109-114},
year = {2019},
booktitle = {29th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-12-818634-3.50019-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186343500199},
author = {Anton A. Kiss and Johan Grievink},
keywords = {PSE, industry, education, research, interface, perspectives},
abstract = {Process Systems Engineering (PSE) deals with decision-making, at all levels and scales, by understanding complex process systems using a holistic view. Computer Aided Process Engineering (CAPE) is a complementary field that focuses on developing methods and providing solution through systematic computer aided techniques for problems related to the design, control and operation of chemical systems. The ‘PSE’ term suffers from a branding issue to the point that PSE does not get the recognition it deserves. This work aims to provide an informative industrial and academic perspective on PSE, arguing that the ‘systems thinking’ and ‘systems problem solving’ have to be prioritized ahead of just applications of computational problem solving methods. A multi-level view of the PSE field is provided within the academic and industrial context, and enhancements for PSE are suggested at their industrial and academic interfaces.}
}
@article{KAZANINA2023996,
title = {The neural ingredients for a language of thought are available},
journal = {Trends in Cognitive Sciences},
volume = {27},
number = {11},
pages = {996-1007},
year = {2023},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2023.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S1364661323001936},
author = {Nina Kazanina and David Poeppel},
keywords = {language-of-thought, symbolic representation, computational theory of mind, spatial navigation, compositionality},
abstract = {The classical notion of a ‘language of thought’ (LoT), advanced prominently by the philosopher Jerry Fodor, is an influential position in cognitive science whereby the mental representations underpinning thought are considered to be compositional and productive, enabling the construction of new complex thoughts from more primitive symbolic concepts. LoT theory has been challenged because a neural implementation has been deemed implausible. We disagree. Examples of critical computational ingredients needed for a neural implementation of a LoT have in fact been demonstrated, in particular in the hippocampal spatial navigation system of rodents. Here, we show that cell types found in spatial navigation (border cells, object cells, head-direction cells, etc.) provide key types of representation and computation required for the LoT, underscoring its neurobiological viability.}
}
@article{TARMIZI2010384,
title = {Effects of Problem-based Learning Approach in Learning of Statistics among University Students},
journal = {Procedia - Social and Behavioral Sciences},
volume = {8},
pages = {384-392},
year = {2010},
note = {International Conference on Mathematics Education Research 2010 (ICMER 2010)},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2010.12.054},
url = {https://www.sciencedirect.com/science/article/pii/S1877042810021592},
author = {Rohani Ahmad Tarmizi and Sahar Bayat},
keywords = {Problem-based learning, E-learning, Statistic learning performance, Mental load},
abstract = {Current Mathematics Curriculum concerns are focused on students’ needs to think mathematically rather than just mathematical computation. Students should be able to develop more complex, abstract, and powerful mathematical structures. This can dramatically enable them to solve a broad variety of meaningful problems. Furthermore, students ought to become autonomous and self-motivated in their mathematical activities such as acquiring mathematical concepts, skills and problem solving; meta-cognitively aware of their mathematical thinking; highly motivated in mathematics learning and develop positive attitudes towards mathematical task. To achieve this learning goal, an investigation into efficient learning mode, the problem-based learning (PBL) was undertaken. PBL has been successfully applied in medical, engineering, economics, and accounting field but lack of evidence in mathematics field. This study examined possible outcomes of PBL among postgraduate students who were taking Educational Statistic course. Three statistic tests were employed to assess the students’ performance in statistic learning. The Meta-cognitive Awareness Inventory (MAI), which comprises of 52 items was used to assess the students’ meta-cognitive strategy in solving Educational Statistics problems. Students’ motivation towards the PBL learning was measured by Keller's Motivational Design Questionnaire with 36 items. Comparison of students’ performance based on three tests showed that there is significant diffrence between mean performance (F [2,28]=5.571, p<0.05). In addition, results indicated that there is significant positive effects on students meta-cognitive awareness (t [30]=3.358, p<0.05) and on students motivation level (t [30]=2.484, p<0.05) after undergoing PBL intervention.}
}
@article{BAI2011364,
title = {Prediction of human voluntary movement before it occurs},
journal = {Clinical Neurophysiology},
volume = {122},
number = {2},
pages = {364-372},
year = {2011},
issn = {1388-2457},
doi = {https://doi.org/10.1016/j.clinph.2010.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S1388245710005699},
author = {Ou Bai and Varun Rathi and Peter Lin and Dandan Huang and Harsha Battapady and Ding-Yu Fei and Logan Schneider and Elise Houdayer and Xuedong Chen and Mark Hallett},
keywords = {Human intention, Voluntary movement, Prediction, Movement-related cortical potentials (MRCP), Event-related desynchronization (ERD), Electroencephalography (EEG), Brain–computer interface (BCI), Consciousness},
abstract = {Objective
Human voluntary movement is associated with two changes in electroencephalography (EEG) that can be observed as early as 1.5s prior to movement: slow DC potentials and frequency power shifts in the alpha and beta bands. Our goal was to determine whether and when we can reliably predict human natural movement BEFORE it occurs from EEG signals ONLINE IN REAL-TIME.
Methods
We developed a computational algorithm to support online prediction. Seven healthy volunteers participated in this study and performed wrist extensions at their own pace.
Results
The average online prediction time was 0.62±0.25s before actual movement monitored by EMG signals. There were also predictions that occurred without subsequent actual movements, where subjects often reported that they were thinking about making a movement.
Conclusion
Human voluntary movement can be predicted before movement occurs.
Significance
The successful prediction of human movement intention will provide further insight into how the brain prepares for movement, as well as the potential for direct cortical control of a device which may be faster than normal physical control.}
}
@article{KOTIR2024140042,
title = {Field experiences and lessons learned from applying participatory system dynamics modelling to sustainable water and agri-food systems},
journal = {Journal of Cleaner Production},
volume = {434},
pages = {140042},
year = {2024},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2023.140042},
url = {https://www.sciencedirect.com/science/article/pii/S0959652623042002},
author = {Julius H. Kotir and Renata Jagustovic and George Papachristos and Robert B. Zougmore and Aad Kessler and Martin Reynolds and Mathieu Ouedraogo and Coen J. Ritsema and Ammar Abdul Aziz and Ron Johnstone},
keywords = {Africa, Group model building, Systems thinking, Stakeholder engagement, System modelling, Sustainable development},
abstract = {Achieving the objectives of sustainable development in water and agri-food systems requires the utilisation of decision-support tools in stakeholder-driven processes to construct and simulate various scenarios and evaluate the outcomes of associated policy interventions. While it is common practice to involve stakeholders in participatory modelling processes, their comprehensive documentation and the lessons learned remain scarce. In this paper, we share our experience of engaging stakeholders throughout the entire system dynamics modelling process. We draw on two projects implemented in the Volta River Basin, West Africa, to understand the dynamics of water and agri-food systems under changing environmental and socioeconomic conditions. We outline eight key insights and lessons as practical guides derived from each stage of the participatory modelling process, including the pre-workshop stage, problem definition, model conceptualization, simulation model formulation, model testing and verification, and policy design and evaluation. Our findings demonstrate that stakeholders can actively contribute to all phases of the system dynamics modelling process, including parameter estimation, sensitivity analysis, and numerical simulation experiments. However, we encountered notable challenges, including the time-intensive nature of the process, the struggle to reach a consensus on the modelled problem, and the difficulty of translating the conceptual model into a simulation model using stock and flow diagrams – all of which were addressed through a structured facilitation process. While the projects were anchored in the specific context of West Africa, the key lessons and insights highlighted have broader significance, particularly for researchers employing PSDM in regions characterised by multifaceted human-environmental systems and where stakeholder involvement is crucial for holistic understanding and effective policy interventions. This paper contributes practical guidance for future efforts with participatory modelling, particularly in regions worldwide grappling with sustainable development challenges in water and agri-food systems, and where stakeholder involvement is crucial for holistic understanding of the multiple challenges and for designing effective policy interventions.}
}
@article{CUI2025106320,
title = {From complex networks to urban heat island governance: Structural analysis and intervention in xiamen island's heat island network},
journal = {Sustainable Cities and Society},
volume = {124},
pages = {106320},
year = {2025},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2025.106320},
url = {https://www.sciencedirect.com/science/article/pii/S2210670725001970},
author = {Huanjia Cui and Yu Wang and Qiang Yu and Jikai Zhao and Weijie Sun and Xinyu Yang and Bowen Chi and Ji Long and Buyanbaatar Avirmed},
keywords = {Urban heat island, Morphological spatial pattern analysis, Circuit theory, Complex network theory, Robustness analysis},
abstract = {Rapid urbanization, under global warming, has intensified the urban heat island effect, causing significant environmental and social challenges. Many studies propose governance measures for individual landscape patches, but their effectiveness remains limited. A global perspective is essential for systematically analyzing urban heat environments. This study uses Xiamen Island as a case study and, based on the "source-sink" landscape theory, applies a reverse-thinking approach to construct urban heat island network that needs to be disrupted. By introducing complex network theory and aligning it with urban needs, a novel framework for optimizing and governing the global heat environment is proposed. The results show that: (1) Xiamen Island has 86 urban heat island sources and 202 heat transfer corridors; (2) 7 potential heat island sources were identified through pinch point analysis; (3) Based on barrier point analysis, 23 heat transfer barriers and 19 priority corridors for governance were found; (4) 6 potential heat transfer corridors were identified using complex network theory and LST data; (5) Robustness analysis of the urban heat island network revealed a governance strategy prioritizing closeness centrality, identifying 15 sources for prioritized governance. This study provides practical new approaches and strategies for comprehensive governance of urban heat environments.}
}
@article{DALLAQUA2021422,
title = {ForestEyes Project: Conception, enhancements, and challenges},
journal = {Future Generation Computer Systems},
volume = {124},
pages = {422-435},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21001965},
author = {Fernanda B.J.R. Dallaqua and Álvaro L. Fazenda and Fabio A. Faria},
keywords = {Citizen Science, Deforestation area detection, Rainforest, Tropical forest, Volunteered Thinking},
abstract = {Rainforests play an important role in the global ecosystem. However, significant regions of them are facing deforestation and degradation due to several reasons. Diverse government and private initiatives were created to monitor and alert for deforestation increases from remote sensing images, using different ways to deal with the notable amount of generated data. Citizen Science projects can also be used to reach the same goal. Citizen Science consists of scientific research involving nonprofessional volunteers for analyzing, collecting data, and using their computational resources to outcome advancements in science and to increase the public’s understanding of problems in specific knowledge areas such as astronomy, chemistry, mathematics, and physics. In this sense, this work presents a Citizen Science project called ForestEyes, which uses volunteer’s answers through the analysis and classification of remote sensing images to monitor deforestation regions in rainforests. To evaluate the quality of those answers, different campaigns/workflows were launched using remote sensing images from Brazilian Legal Amazon and their results were compared to an official groundtruth from the Amazon Deforestation Monitoring Project PRODES. In this work, the first two workflows that enclose the State of Rondônia in the years 2013 and 2016 received more than 35,000 answers from 383 volunteers in the 2,050 created tasks in only two and a half weeks after their launch. For the other four workflows, even enclosing the same area (Rondônia) and different setups (e.g., image segmentation method, image spatial resolution, and detection target), they received 51,035 volunteers’ answers gathered from 281 volunteers in 3,358 tasks. In the performed experiments, it was possible to observe that the volunteers achieved satisfactory overall accuracy, higher than 75%, in the classification of forestation and non-forestation areas using the ForestEyes project. Furthermore, considering an efficient segmentation and a better image spatial resolution, they achieved almost 66% accuracy in the classification of recent deforestation, which is a great challenge to overcome. Therefore, these results show that Citizen Science might be a powerful tool in monitoring deforestation regions in rainforests as well as in obtaining high-quality labeled data.}
}
@incollection{ASHBY2024255,
title = {Chapter 10 - Circular Materials Economics},
editor = {Michael F. Ashby},
booktitle = {Materials and Sustainable Development (Second Edition)},
publisher = {Butterworth-Heinemann},
edition = {Second Edition},
pages = {255-295},
year = {2024},
isbn = {978-0-323-98361-7},
doi = {https://doi.org/10.1016/B978-0-323-98361-7.00010-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323983617000105},
author = {Michael F. Ashby},
keywords = {Circularity, Material efficiency, Linear materials economy, Circular materials economy, Reuse, Repair, Recycling, Take-back legislation, Recycling targets, Increased product life, Urban mining, Business models, Measuring circularity, Modelling circularity, Limits to circularity},
abstract = {When products come to the end of their lives, the materials they contain are still there. Repurposing, repair or recycling can return them to active use, creating a technological cycle that, in some ways, parallels the carbon, nitrogen and hydrological cycles of the biosphere. In developed nations they lost urgency as the cost of materials fell and that of labour rose, making it cheaper to make new products than to fix old ones, leading to a materials economy that is largely linear, characterized by the sequence “take – make – use – dispose”. Increasing population and affluence, and the limited capacity for the planet to provide resources and absorb waste direct thinking towards a more circular way of using materials. Governments have sought to reduce waste by imposing take-back regulations, setting mandatory recycling targets and requiring minimum service lives. These, and the efficiency movement – eco-efficiency, material-efficiency, energy efficiency – seek to allow business as usual with reduced drain on natural resources without any real change of behaviour. The ‘circularity’ concept is a way of thinking that looks not just for efficiencies but also for new ways to provide functions. The idea of deploying rather than consuming materials, of using them not once but many times has economic as well as environmental appeal. This chapter examines the background, the successes, the difficulties, and the ultimate limits of implementing a circular materials economy.}
}
@article{DECARO200758,
title = {Methodologies for examining problem solving success and failure},
journal = {Methods},
volume = {42},
number = {1},
pages = {58-67},
year = {2007},
note = {Neurocognitive Mechanisms of Creativity: A Toolkit},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2006.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S1046202306002982},
author = {Marci S. DeCaro and Mareike Wieth and Sian L. Beilock},
keywords = {Working memory, Performance, Pressure, Individual differences, Problem solving, Creativity, Short term memory, Stress, Math},
abstract = {When designing research to examine the variables underlying creative thinking and problem solving success, one must not only consider (a) the demands of the task being performed, but (b) the characteristics of the individual performing the task and (c) the constraints of the skill execution environment. In the current paper we describe methodologies that allow one to effectively study creative thinking by capturing interactions among the individual, task, and problem solving situation. In doing so, we demonstrate that the relation between executive functioning and problem solving success is not always as straightforward as one might initially believe.}
}
@article{RYDER2022100703,
title = {Rethinking reflective practice: John Boyd’s OODA loop as an alternative to Kolb},
journal = {The International Journal of Management Education},
volume = {20},
number = {3},
pages = {100703},
year = {2022},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2022.100703},
url = {https://www.sciencedirect.com/science/article/pii/S1472811722001057},
author = {Mike Ryder and Carolyn Downs},
keywords = {OODA, Reflective practice, Experiential learning, Work-based learning, Employability, Business education},
abstract = {The world is changing and business schools are struggling to keep up. Theories of reflective practice developed by the likes of Schon (1983), Gibbs (1988), Driscoll (1994, 2007) and Kolb (1984, 2015) are outdated and unfit for current purposes. Problems include the chronology of events, the orientation of the observer, the impact of external inputs, and the fact that neither education nor the workplace follow a structured, linear path. In response to these challenges, we propose a new ‘solution’: John Boyd's OODA loop. We argue that OODA loops offer the chance to reshape reflective practice and work-based learning for a world in which individuals must cope with ‘an unfolding evolving reality that is uncertain, ever changing and unpredictable’ (Boyd, 1995, slide 1). By embracing the philosophy of John Boyd and his OODA loop theory, business schools can develop greater resilience and employability in graduates, preparing them to embrace change while also embedding the concept of life-long learning to make them better equipped to face the uncertainty that the modern world brings.}
}
@article{FERREIRA20131446,
title = {Fostering the Creative Development of Computer Science Students in Programming and Interaction Design},
journal = {Procedia Computer Science},
volume = {18},
pages = {1446-1455},
year = {2013},
note = {2013 International Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.05.312},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913004559},
author = {Deller James Ferreira},
keywords = {Creativity, Programming, Interaction design},
abstract = {This study explores the enhancement of creativity in undergraduate students studying computer science. We assume that everybody has creative potential. As a teacher, we can explicitly encourage creative thinking, providing space to let students collaboratively discover and explore their creativity. This paper presents a dialogical framework to help the teacher fostering creativity among students of computer science in programming and interaction design. The framework presented here involves underlying dialogic processes from seven collaborative and creative dimensions that allow students to develop creativity. The use of the pedagogical framework makes it possible to teachers create significant interaction design and computer programming experiences to students, motivating them to activate mental processes underlying creativity. Students can simultaneously activate two or more ideas, images, or thoughts and have them interact, prompt thought experiments, change cognitive perspectives, raise new points of view, and risk category mistakes.}
}
@article{TIAN2018104,
title = {The association between visual creativity and cortical thickness in healthy adults},
journal = {Neuroscience Letters},
volume = {683},
pages = {104-110},
year = {2018},
issn = {0304-3940},
doi = {https://doi.org/10.1016/j.neulet.2018.06.036},
url = {https://www.sciencedirect.com/science/article/pii/S0304394018304397},
author = {Fang Tian and Qunlin Chen and Wenfeng Zhu and Yongming Wang and Wenjing Yang and Xingxing Zhu and Xue Tian and Qinglin Zhang and Guikang Cao and Jiang Qiu},
keywords = {Visual creativity, Cortical thickness, Prefrontal cortex, Supplementary motor cortex, Insula},
abstract = {Creativity is necessary to human survival, human prosperity, civilization and well-being. Visual creativity is an important part of creativity and is the ability to create products of novel and useful visual forms, playing important role in many fields such as art, painting and sculpture. There have been several neuroimaging studies exploring the neural basis of visual creativity. However, to date, little is known about the relationship between cortical structure and visual creativity as measured by the Torrance Tests of Creative Thinking. Here, we investigated the association between cortical thickness and visual creativity in a large sample of 310 healthy adults. We used multiple regression to analyze the correlation between cortical thickness and visual creativity, adjusting for gender, age and general intelligence. The results showed that visual creativity was significantly negatively correlated with cortical thickness in the left middle frontal gyrus (MFG), right inferior frontal gyrus (IFG), right supplementary motor cortex (SMA) and the left insula. These observations have implications for understanding that a thinner prefrontal cortex (PFC) (e.g. IFG, MFG), SMA and insula correspond to higher visual creative performance, presumably due to their role in executive attention, cognitive control, motor planning and dynamic switching.}
}
@article{GOLDBERG20007,
title = {The Design of Innovation: Lessons from Genetic Algorithms, Lessons for the Real World},
journal = {Technological Forecasting and Social Change},
volume = {64},
number = {1},
pages = {7-12},
year = {2000},
issn = {0040-1625},
doi = {https://doi.org/10.1016/S0040-1625(99)00079-7},
url = {https://www.sciencedirect.com/science/article/pii/S0040162599000797},
author = {David E Goldberg},
abstract = {This article considers some of the connections between genetic algorithms (GAs)—search procedures based on the mechanics of natural selection and natural genetics—and human innovation. Simply stated, innovation has been a source of inspiration for thinking about genetic algorithms, and as the algorithms have improved, GAs have become increasingly interesting computational models of the processes of innovation. The article reviews the basics of genetic algorithm operation and connects the basic mechanics to two processes of innovation: continual improvement and discontinuous change. Thereafter, some of the technical lessons of genetic algorithm processing are reviewed and their implications are briefly explored in the context of organizational change.}
}
@article{BLOSS2016,
title = {Reimagining Human Research Protections for 21st Century Science},
journal = {Journal of Medical Internet Research},
volume = {18},
number = {12},
year = {2016},
issn = {1438-8871},
doi = {https://doi.org/10.2196/jmir.6634},
url = {https://www.sciencedirect.com/science/article/pii/S1438887116003204},
author = {Cinnamon Bloss and Camille Nebeker and Matthew Bietz and Deborah Bae and Barbara Bigby and Mary Devereaux and James Fowler and Ann Waldo and Nadir Weibel and Kevin Patrick and Scott Klemmer and Lori Melichar},
keywords = {ethics committees, research, biomedical research, telemedicine, informed consent, behavioral research},
abstract = {Background
Evolving research practices and new forms of research enabled by technological advances require a redesigned research oversight system that respects and protects human research participants.
Objective
Our objective was to generate creative ideas for redesigning our current human research oversight system.
Methods
A total of 11 researchers and institutional review board (IRB) professionals participated in a January 2015 design thinking workshop to develop ideas for redesigning the IRB system.
Results
Ideas in 5 major domains were generated. The areas of focus were (1) improving the consent form and process, (2) empowering researchers to protect their participants, (3) creating a system to learn from mistakes, (4) improving IRB efficiency, and (5) facilitating review of research that leverages technological advances.
Conclusions
We describe the impetus for and results of a design thinking workshop to reimagine a human research protections system that is responsive to 21st century science.}
}
@article{FASOLINO2025125111,
title = {Dynamics in action: Exploring economic impacts of drought through a systemic approach},
journal = {Journal of Environmental Management},
volume = {380},
pages = {125111},
year = {2025},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2025.125111},
url = {https://www.sciencedirect.com/science/article/pii/S0301479725010874},
author = {Nunzia Gabriella Fasolino and Emilia Pellegrini and Meri Raggi and Davide Viaggi},
keywords = {Water energy nexus, Natural hazards, Climate change, Impacts assessment, System thinking},
abstract = {Droughts are generally associated with direct and indirect impacts, but their relationships and propagation dynamics are often unknown. This study aims to fill this research gap by exploring the spill-over effects of drought impacts across systems and, in doing so, identifying leverage points within the systems to promote effective mitigation strategies. To achieve these objectives, the effects of the 2022 drought in a portion of the Po River Basin District are analysed through a qualitative application of the System Dynamics methodology called Causal Loop Diagrams, systemic representations that can effectively capture the interdependencies across complex subsystems. The findings reveal that the drought increased production costs across all the water-using sectors due to higher energy demands driven by energy-intensive activities such as irrigation, pumping and potabilization. The empirical results offer a novel insight into the identification and propagation of impacts, providing further evidence on the water-energy nexus. Therefore, policy approaches that enhance blue and green water reserves, while addressing their interconnections, are encouraged for mitigating drought impacts. This underscores the importance of adopting a proactive approach in drought management rather than reactive responses during the occurrence of extreme events. Finally, in fields characterized by high policy fragmentation, such as water policy, the study demonstrates how representing the system behaviour through causal maps can support the integration of different sectoral policies, providing decision-makers with tools to evaluate and address the complexity of droughts.}
}
@article{ALY2014206,
title = {Atmospheric boundary-layer simulation for the built environment: Past, present and future},
journal = {Building and Environment},
volume = {75},
pages = {206-221},
year = {2014},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2014.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0360132314000444},
author = {Aly Mousaad Aly},
keywords = {Aerodynamics, Aeroelasticity, Atmospheric boundary-layer, Built environment, Experimental/computational wind engineering},
abstract = {This paper summarizes the state-of-the-art techniques used to simulate hurricane winds in atmospheric boundary-layer (ABL) for wind engineering testing. The wind tunnel simulation concept is presented along with its potential applications, advantages and challenges. ABL simulation at open-jet simulators is presented along with an application example followed by a discussion on the advantages and challenges of testing at these facilities. Some of the challenges and advantages of using computational fluid dynamics (CFD) are presented with an application example. The paper show that the way the wind can be simulated is complex and matching one parameter at full-scale may lead to a mismatch of other parameters. For instance, while large-scale testing is expected to improve Reynolds number and hence approach the full-scale scenario, it is challenging to generate large-scale turbulence in an artificially created wind. New testing protocols for low-rise structures and small-size architectural features are presented as an answer to challenging questions associated with both wind tunnel and open-jet testing. Results show that it is the testing protocol that can be adapted to enhance the prediction of full-scale physics in nature. Thinking out of the box and accepting non-traditional ABL is necessary to compensate for Reynolds effects and to allow for convenient experimentation. New research directions with focus on wind, rain and waves as well as other types of non-synoptic winds are needed, in addition to a more focus on the flow physics in the lower part of the ABL, where the major part of the infrastructure exists.}
}
@article{KNAPP2017370,
title = {Energy-efficient Legionella control that mimics nature and an open-source computational model to aid system design},
journal = {Applied Thermal Engineering},
volume = {127},
pages = {370-377},
year = {2017},
issn = {1359-4311},
doi = {https://doi.org/10.1016/j.applthermaleng.2017.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S135943111633664X},
author = {Samuel Knapp and Bo Nordell},
keywords = {Thermal model, Heat exchanger, Pasteurization, Legionnaire’s disease, Microsoft Excel},
abstract = {Although there is no direct connection, the incidence of Legionnaire’s disease has increased concurrently with increased usage of energy efficient domestic hot water (DHW) systems, which serve as ideal growth environments for Legionella pneumophila, the bacteria responsible for Legionnaire’s disease. The Duck Foot Heat Exchange Model (DFHXM) was developed to aid design of energy efficient thermal pasteurization systems with Legionella control specifically in mind. The model simulates a system design imitating the countercurrent heat exchange in the feet of ducks, an evolutionary adaption reducing environmental heat losses in cold climates. Such systems use a heat exchanger to preheat fluids prior to pasteurization and cool the same fluid after pasteurization. Thus, the design requires minimal addition of heat to achieve pasteurization temperatures and to cover environmental heat losses. This article describes the underlying principles and use of the freely available Microsoft Excel model, as well as compares results from the DFHXM to measurements of an experimental pilot system. Simulation outputs agreed well with experimental results for transient and steady-state temperatures, the largest discrepancy in steady-state temperatures being 4.6%. Lastly, we discuss the flexibility of the DFHXM to simulate a wide variety of designs with special emphasis on Legionella control and solar-thermal water disinfection.}
}
@article{WEICHBROTH20223798,
title = {A note on the affective computing systems and machines: a classification and appraisal},
journal = {Procedia Computer Science},
volume = {207},
pages = {3798-3807},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.441},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922013345},
author = {Paweł Weichbroth and Wiktor Sroka},
keywords = {Affective Computing, Artificial Emotional Intelligence, Classification, System, Machine},
abstract = {Affective computing (AfC) is a continuously growing multidisciplinary field, spanning areas from artificial intelligence, throughout engineering, psychology, education, cognitive science, to sociology. Therefore, many studies have been devoted to the aim of addressing numerous issues, regarding different facets of AfC solutions. However, there is a lack of classification of the AfC systems. This study aims to fill this gap by reviewing and evaluating the state-of-the-art studies in a qualitative manner. In this line of thinking, we put forward a threefold classification that breaks down to desktop and mobile AfC systems, and AfC machines. Moreover, we identified four types of AfC systems, based on the features extracted. In our opinion, the results of this study can serve as a guide for future affect-related research and design, on the one hand, and provide a better understanding on the role of emotions and affect in human-computer interaction, on the other hand.}
}
@article{LI2024109502,
title = {Numerical study on heat transfer performance of printed circuit heat exchanger with anisotropic thermal conductivity},
journal = {International Journal of Heat and Fluid Flow},
volume = {109},
pages = {109502},
year = {2024},
issn = {0142-727X},
doi = {https://doi.org/10.1016/j.ijheatfluidflow.2024.109502},
url = {https://www.sciencedirect.com/science/article/pii/S0142727X24002273},
author = {Libo Li and Jiyuan Bi and Jingkai Ma and Xiaoxu Zhang and Qiuwang Wang and Ting Ma},
keywords = {Printed circuit heat exchanger, Anisotropic thermal conductivity, Numerical simulation, Thermal resistance, Heat exchanger efficiency},
abstract = {Printed Circuit Heat Exchangers are compact and efficient heat exchangers, widely used in nuclear engineering, very high-temperature reactors, and aerospace systems. This study investigates the heat transfer performance of a heat exchanger with anisotropic thermal conductivity, such as fiber reinforced composites. Numerical simulations were conducted to examine the synergistic effect of three-dimensional thermal resistance on heat exchanger performance. The most significant impact on performance is the z-direction thermal resistance, followed by the y-direction, while the x-direction has the least impact. Contrary to traditional design thinking, increasing the overall heat exchanger thermal resistance under the same thermal resistance ratio improves heat transfer efficiency at the studied conditions. The results suggest that it is necessary to design the lowest thermal conductivity direction as the z-direction and increase the y-direction thermal conductivity to enhance heat exchanger performance. In the numerical investigation presented in this study, the efficiency of the heat exchanger was improved by approximately 23 % under specific operating conditions by adjusting the thermal conductivity of anisotropic materials to control the thermal resistance in the x, y and z directions. It is evident that the manipulation of anisotropic material properties has a substantial influence on the performance of heat exchangers.}
}
@article{MAHMUD2025111321,
title = {RSPCA: Random Sample Partition and Clustering Approximation for ensemble learning of big data},
journal = {Pattern Recognition},
volume = {161},
pages = {111321},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111321},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010720},
author = {Mohammad Sultan Mahmud and Hua Zheng and Diego Garcia-Gil and Salvador García and Joshua Zhexue Huang},
keywords = {Clustering approximation, Ensemble clustering, Incremental clustering, Ensemble learning},
abstract = {Large-scale data clustering needs an approximate approach for improving computation efficiency and data scalability. In this paper, we propose a novel method for ensemble clustering of large-scale datasets that uses the Random Sample Partition and Clustering Approximation (RSPCA) to tackle the problems of big data computing in cluster analysis. In the RSPCA computing framework, a big dataset is first partitioned into a set of disjoint random samples, called RSP data blocks that remain distributions consistent with that of the original big dataset. In ensemble clustering, a few RSP data blocks are randomly selected, and a clustering operation is performed independently on each data block to generate the clustering result of the data block. All clustering results of selected data blocks are aggregated to the ensemble result as an approximate result of the entire big dataset. To improve the robustness of the ensemble result, the ensemble clustering process can be conducted incrementally using multiple batches of selected RSP data blocks. To improve computation efficiency, we use the I-niceDP algorithm to automatically find the number of clusters in RSP data blocks and the k-means algorithm to determine more accurate cluster centroids in RSP data blocks as inputs to the ensemble process. Spectral and correlation clustering methods are used as the consensus functions to handle irregular clusters. Comprehensive experiment results on both real and synthetic datasets demonstrate that the ensemble of clustering results on a few RSP data blocks is sufficient for a good global discovery of the entire big dataset, and the new approach is computationally efficient and scalable to big data.}
}
@article{VANDUN2023113880,
title = {ProcessGAN: Supporting the creation of business process improvement ideas through generative machine learning},
journal = {Decision Support Systems},
volume = {165},
pages = {113880},
year = {2023},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2022.113880},
url = {https://www.sciencedirect.com/science/article/pii/S0167923622001518},
author = {Christopher {van Dun} and Linda Moder and Wolfgang Kratsch and Maximilian Röglinger},
keywords = {Business process improvement, Business process redesign, Generative adversarial networks, Generative machine learning, Process mining},
abstract = {Business processes are a key driver of organizational success, which is why business process improvement (BPI) is a central activity of business process management. Despite an abundance of approaches, BPI as a creative task is time-consuming and labour-intensive. Most importantly, its level of computational support is low. The few computational BPI approaches hardly leverage the opportunities brought about by computational creativity, neglect process data, and rely on rather rigid improvement patterns. Given the increasing amount of process data in the form of event logs and the uptake of generative machine learning for automating creative tasks in various domains, there is huge potential for BPI. Hence, following the design science research paradigm, we specified, implemented, and evaluated ProcessGAN, a novel computational BPI approach based on generative adversarial networks that supports the creation of BPI ideas. Our evaluation shows that ProcessGAN improves the creativity of process designers, particularly the originality of BPI ideas, and shapes up useful in real-world settings. Moreover, ProcessGAN is the first approach to combine BPI and computational creativity.}
}
@article{SYCHEV2024101261,
title = {Educational models for cognition: Methodology of modeling intellectual skills for intelligent tutoring systems},
journal = {Cognitive Systems Research},
volume = {87},
pages = {101261},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101261},
url = {https://www.sciencedirect.com/science/article/pii/S138904172400055X},
author = {Oleg Sychev},
keywords = {Reasoning modeling, Constraint-based modeling, Intelligent tutoring systems},
abstract = {Automation of teaching people new skills requires modeling of human reasoning because human cognition involves active reasoning over the new subject domain to acquire skills that will later become automatic. The article presents Thought Process Trees — a language for modeling human reasoning that was created to facilitate the development of intelligent tutoring systems, which can perform the same reasoning that is expected of a student and find deficiencies in their line of thinking, providing explanatory messages and allowing them to learn from performance errors. The methodology of building trees which better reflect human learning is discussed, with examples of design choices during the modeling process and their consequences. The characteristics of educational modeling that impact building subject-domain models for intelligent tutoring systems are discussed. The trees were formalized and served as a basis for developing a framework for constructing intelligent tutoring systems. This significantly lowered the time required to build and debug a constraint-based subject-domain model. The framework has already been used to develop five intelligent tutoring systems and their prototypes and is being used to develop more of them.}
}
@article{SUDDENDORF201826,
title = {Prospection and natural selection},
journal = {Current Opinion in Behavioral Sciences},
volume = {24},
pages = {26-31},
year = {2018},
note = {Survival circuits},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2018.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S2352154617302449},
author = {T Suddendorf and A Bulley and B Miloyan},
abstract = {Prospection refers to thinking about the future, a capacity that has become the subject of increasing research in recent years. Here we first distinguish basic prospection, such as associative learning, from more complex prospection commonly observed in humans, such as episodic foresight, the ability to imagine diverse future situations and organize current actions accordingly. We review recent studies on complex prospection in various contexts, such as decision-making, planning, deliberate practice, information gathering, and social coordination. Prospection appears to play many important roles in human survival and reproduction. Foreseeing threats and opportunities before they arise, for instance, drives attempts at avoiding future harm and obtaining future benefits, and recognizing the future utility of a solution turns it into an innovation, motivating refinement and dissemination. Although we do not know about the original contexts in which complex prospection evolved, it is increasingly clear through research on the emergence of these capacities in childhood and on related disorders in various clinical conditions, that limitations in prospection can have profound functional consequences.}
}
@article{KALAY199837,
title = {P3: Computational environment to support design collaboration},
journal = {Automation in Construction},
volume = {8},
number = {1},
pages = {37-48},
year = {1998},
issn = {0926-5805},
doi = {https://doi.org/10.1016/S0926-5805(98)00064-8},
url = {https://www.sciencedirect.com/science/article/pii/S0926580598000648},
author = {Yehuda E Kalay},
keywords = {Collaborative design, Design environment, Product model, Performance model, Process model},
abstract = {The work reported in this paper addresses the paradoxical state of the construction industry (also known as A/E/C, for Architecture, Engineering and Construction), where the design of highly integrated facilities is undertaken by severely fragmented teams, leading to diminished performance of both processes and products. The construction industry has been trying to overcome this problem by partitioning the design process hierarchically or temporally. While these methods are procedurally efficient, their piecemeal nature diminishes the overall performance of the project. Computational methods intended to facilitate collaboration in the construction industry have, so far, focused primarily on improving the flow of information among the participants. They have largely met their stated objective of improved communication, but have done little to improve joint decision-making, and therefore have not significantly improved the quality of the design project itself. We suggest that the main impediment to effective collaboration and joint decision-making in the A/E/C industry is the divergence of disciplinary `world-views', which are the product of educational and professional processes through which the individuals participating in the design process have been socialized into their respective disciplines. To maximize the performance of the overall project, these different world-views must be reconciled, possibly at the expense of individual goals. Such reconciliation can only be accomplished if the participants find the attainment of the overall goals of the project more compelling than their individual disciplinary goals. This will happen when the participants have become cognizant and appreciative of world-views other than their own, including the objectives and concerns of other participants. To achieve this state of knowledge, we propose to avail to the participants of the design team highly specific, contextualized information, reflecting each participant's valuation of the proposed design actions. P3 is a semantically-rich computational environment, which is intended to fulfill this mission. It consists of: (1) a shared representation of the evolving design project, connected (through the World Wide Web) to (2) individual experts and their discipline-specific knowledge repositories; and (3) a computational project manager makes the individual valuations visible to all the participants, and helps them deliberate and negotiate their respective positions for the purpose of improving the overall performance of the project. The paper discusses the theories on which the three components are founded, their function, and the principles of their implementation.}
}
@incollection{WU202257,
title = {Chapter 3 - CTDA methodology},
editor = {Jiaping Wu and Junyu He and George Christakos},
booktitle = {Quantitative Analysis and Modeling of Earth and Environmental Data},
publisher = {Elsevier},
pages = {57-100},
year = {2022},
isbn = {978-0-12-816341-2},
doi = {https://doi.org/10.1016/B978-0-12-816341-2.00010-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128163412000101},
author = {Jiaping Wu and Junyu He and George Christakos},
keywords = {Methodological chain, Knowledge bases, Big data, Scales, Visualization, Chronotopologic statistics},
abstract = {Abstract The methodological characteristics of the chronotopologic data analysis chain are discussed. Various kinds of knowledge are considered and properly classified, and several illustrative examples in applied sciences are presented. Big data and data-driven analyses are critically reviewed, and their implementation carefully assessed. Data scale types are classifications considered in property- and attribute-oriented settings. Classical statistics inadequacies are pointed out and the need of a chronotopology-dependent statistics is outlined. The chronotopologic visualization thinking mode and techniques are briefly reviewed.}
}
@article{DEVOE2012466,
title = {Time, money, and happiness: How does putting a price on time affect our ability to smell the roses?},
journal = {Journal of Experimental Social Psychology},
volume = {48},
number = {2},
pages = {466-474},
year = {2012},
issn = {0022-1031},
doi = {https://doi.org/10.1016/j.jesp.2011.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0022103111002897},
author = {Sanford E. DeVoe and Julian House},
keywords = {Time, Money, Impatience, Happiness},
abstract = {In this paper, we investigate how the impatience that results from placing a price on time impairs individuals' ability to derive happiness from pleasurable experiences. Experiment 1 demonstrated that thinking about one's income as an hourly wage reduced the happiness that participants derived from leisure time on the internet. Experiment 2 revealed that a similar manipulation decreased participants' state of happiness after listening to a pleasant song and that this effect was fully mediated by the degree of impatience experienced during the music. Finally, Experiment 3 showed that the deleterious effect on happiness caused by impatience was attenuated by offering participants monetary compensation in exchange for time spent listening to music, suggesting that a sensation of unprofitably wasted time underlay the induced impatience. Together these experiments establish that thinking about time in terms of money can influence how people experience pleasurable events by instigating greater impatience during unpaid time.}
}
@article{LUNGU2008255,
title = {Partial current information and signal extraction in a rational expectations macroeconomic model: A computational solution},
journal = {Economic Modelling},
volume = {25},
number = {2},
pages = {255-273},
year = {2008},
issn = {0264-9993},
doi = {https://doi.org/10.1016/j.econmod.2007.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0264999307000818},
author = {L. Lungu and K.G.P. Matthews and A.P.L. Minford},
keywords = {Rational expectations, Partial current information, Signal extraction, Macroeconomic modelling},
abstract = {Previous attempts at modelling current observed endogenous financial variables in a macroeconomic model have concentrated on only one variable — the short-term rate of interest. This paper applies a general search algorithm to a macroeconomic model with an observed interest rate and exchange rate to solve the signal extraction problem. Firstly, the algorithm is tested against a linear model with a known analytical solution. Then, the algorithm is applied to all the observed current endogenous variables in a non-linear rational expectations model of the UK. The informational advantage of applying the signal extraction algorithm is evaluated in terms of the forecasting efficiency of the model.}
}
@article{RUTHERFORD2023102255,
title = {“Don't [ruminate], be happy”: A cognitive perspective linking depression and anhedonia},
journal = {Clinical Psychology Review},
volume = {101},
pages = {102255},
year = {2023},
issn = {0272-7358},
doi = {https://doi.org/10.1016/j.cpr.2023.102255},
url = {https://www.sciencedirect.com/science/article/pii/S0272735823000132},
author = {Ashleigh V. Rutherford and Samuel D. McDougle and Jutta Joormann},
keywords = {Rumination, Emotion regulation, Working memory, Reinforcement learning, Depression},
abstract = {Anhedonia, a lack of pleasure in things an individual once enjoyed, and rumination, the process of perseverative and repetitive attention to specific thoughts, are hallmark features of depression. Though these both contribute to the same debilitating disorder, they have often been studied independently and through different theoretical lenses (e.g., biological vs. cognitive). Cognitive theories and research on rumination have largely focused on understanding negative affect in depression with much less focus on the etiology and maintenance of anhedonia. In this paper, we argue that by examining the relation between cognitive constructs and deficits in positive affect, we may better understand anhedonia in depression thereby improving prevention and intervention efforts. We review the extant literature on cognitive deficits in depression and discuss how these dysfunctions may not only lead to sustained negative affect but, importantly, interfere with an ability to attend to social and environmental cues that could restore positive affect. Specifically, we discuss how rumination is associated to deficits in working memory and propose that these deficits in working memory may contribute to anhedonia in depression. We further argue that analytical approaches such as computational modeling are needed to study these questions and, finally, discuss implications for treatment.}
}
@article{POLZER2022100181,
title = {The rise of people analytics and the future of organizational research},
journal = {Research in Organizational Behavior},
volume = {42},
pages = {100181},
year = {2022},
issn = {0191-3085},
doi = {https://doi.org/10.1016/j.riob.2023.100181},
url = {https://www.sciencedirect.com/science/article/pii/S0191308523000011},
author = {Jeffrey T. Polzer},
keywords = {People analytics, Algorithms, Decision-making, Networks, Teams, Meetings, Culture, Monitoring, Computational social science, Organizational behavior},
abstract = {Organizations are transforming as they adopt new technologies and use new sources of data, changing the experiences of employees and pushing organizational researchers to respond. As employees perform their daily activities, they generate vast digital data. These data, when combined with established methods and new analytic techniques, create unprecedented opportunities for studying human behavior at work and have fueled the rise of people analytics as a new institutional field of practice. In this chapter, I describe the emerging field of people analytics and new organizational phenomena that accompany the use of data and algorithms. These practices are affecting how individuals, groups, and organizations function, ranging from decision-making processes and work procedures, to communication and collaboration, to attempts to monitor and control employees. In each of these domains, I describe recent research and propose new research directions. Many of these domains intersect with the emerging field of Computational Social Science, in which disciplinary scholars are applying computational methods to an expanding array of digitized data, pursuing interests that extend far into the organizational domain. Organizational scholars are well-positioned to bridge organizational and disciplinary advances to stay at the forefront of research on the future of work.}
}
@article{GONG2023105530,
title = {Continuous time causal structure induction with prevention and generation},
journal = {Cognition},
volume = {240},
pages = {105530},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2023.105530},
url = {https://www.sciencedirect.com/science/article/pii/S0010027723001646},
author = {Tianwei Gong and Neil R. Bramley},
keywords = {Causal learning, Time, Prevention, Structure induction, Summary statistics},
abstract = {Most research into causal learning has focused on atemporal contingency data settings while fewer studies have examined learning and reasoning about systems exhibiting events that unfold in continuous time. Of these, none have yet explored learning about preventative causal influences. How do people use temporal information to infer which components of a causal system are generating or preventing activity of other components? In what ways do generative and preventative causes interact in shaping the behavior of causal mechanisms and their learnability? We explore human causal structure learning within a space of hypotheses that combine generative and preventative causal relationships. Participants observe the behavior of causal devices as they are perturbed by fixed interventions and subject to either regular or irregular spontaneous activations. We find that participants are capable learners in this setting, successfully identifying the large majority of generative, preventative and non-causal relationships but making certain attribution errors. We lay out a computational-level framework for normative inference in this setting and propose a family of more cognitively plausible algorithmic approximations. We find that participants’ judgment patterns can be both qualitatively and quantitatively captured by a model that approximates normative inference via a simulation and summary statistics scheme based on structurally local computation using temporally local evidence.}
}
@article{SEWALL2020,
title = {Fiber Force: A Fiber Diet Intervention in an Advanced Course-Based Undergraduate Research Experience (CURE) Course},
journal = {Journal of Microbiology & Biology Education},
volume = {21},
number = {1},
year = {2020},
issn = {1935-7877},
doi = {https://doi.org/10.1128/jmbe.v21i1.1991},
url = {https://www.sciencedirect.com/science/article/pii/S1935787720000660},
author = {Julia Massimelli Sewall and Andrew Oliver and Kameryn Denaro and Alexander B. Chase and Claudia Weihe and Mi Lay and Jennifer B. H. Martiny and Katrine Whiteson},
abstract = {Course-based undergraduate research experiences (CUREs) are an effective way to introduce students to contemporary scientific research. Research experiences have been shown to promote critical thinking, improve understanding and proper use of the scientific method, and help students learn practical skills including writing and oral communication. We aimed to improve scientific training by engaging students enrolled in an upper division elective course in a human microbiome CURE. The “Fiber Force” course is aimed at studying the effect of a wholesome high-fiber diet (40 to 50 g/day for two weeks) on the students’ gut microbiomes. Enrolled students participated in a noninvasive diet intervention, designed health surveys, tested hypotheses on the effect of a diet intervention on the gut microbiome, and analyzed their own samples (as anonymized aggregates). The course involved learning laboratory techniques (e.g., DNA extraction, PCR, and 16S sequencing) and the incorporation of computational techniques to analyze microbiome data with QIIME2 and within the R software environment. In addition, the learning objectives focused on effective student performance in writing, data analysis, and oral communication. Enrolled students showed high performance grades on writing, data analysis and oral communication assignments. Pre- and post-course surveys indicate that the students found the experience favorable, increased their interest in science, and heightened awareness of their diet habits. Fiber Force constitutes a validated case of a research experience on microbiology with the capacity to improve research training and promote healthy dietary habits.}
}
@article{ELVEVAG2023115098,
title = {Reflections on measuring disordered thoughts as expressed via language},
journal = {Psychiatry Research},
volume = {322},
pages = {115098},
year = {2023},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2023.115098},
url = {https://www.sciencedirect.com/science/article/pii/S0165178123000513},
author = {Brita Elvevåg},
keywords = {Assessment, Language, Memory},
abstract = {Thought disorder, as inferred from disorganized and incoherent speech, is an important part of the clinical presentation in schizophrenia. Traditional measurement approaches essentially count occurrences of certain speech events which may have restricted their usefulness. Applying speech technologies in assessment can help automate traditional clinical rating tasks and thereby complement the process. Adopting these computational approaches affords clinical translational opportunities to enhance the traditional assessment by applying such methods remotely and scoring various parts of the assessment automatically. Further, digital measures of language may help detect subtle clinically significant signs and thus potentially disrupt the usual manner by which things are conducted. If proven beneficial to patient care, methods where patients’ voice are the primary data source could become core components of future clinical decision support systems that improve risk assessment. However, even if it is possible to measure thought disorder in a sensitive, reliable and efficient manner, there remain many challenges to then translate into a clinically implementable tool that can contribute towards providing better care. Indeed, embracing technology - notably artificial intelligence - requires vigorous standards for reporting underlying assumptions so as to ensure a trustworthy and ethical clinical science.}
}
@article{STORLIE20091735,
title = {Implementation and evaluation of nonparametric regression procedures for sensitivity analysis of computationally demanding models},
journal = {Reliability Engineering & System Safety},
volume = {94},
number = {11},
pages = {1735-1763},
year = {2009},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2009.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0951832009001112},
author = {Curtis B. Storlie and Laura P. Swiler and Jon C. Helton and Cedric J. Sallaberry},
keywords = {Bootstrap, Confidence intervals, Meta-model, Nonparametric regression, Sensitivity analysis, Surrogate model, Uncertainty analysis, Variance decomposition},
abstract = {The analysis of many physical and engineering problems involves running complex computational models (simulation models, computer codes). With problems of this type, it is important to understand the relationships between the input variables (whose values are often imprecisely known) and the output. The goal of sensitivity analysis (SA) is to study this relationship and identify the most significant factors or variables affecting the results of the model. In this presentation, an improvement on existing methods for SA of complex computer models is described for use when the model is too computationally expensive for a standard Monte-Carlo analysis. In these situations, a meta-model or surrogate model can be used to estimate the necessary sensitivity index for each input. A sensitivity index is a measure of the variance in the response that is due to the uncertainty in an input. Most existing approaches to this problem either do not work well with a large number of input variables and/or they ignore the error involved in estimating a sensitivity index. Here, a new approach to sensitivity index estimation using meta-models and bootstrap confidence intervals is described that provides solutions to these drawbacks. Further, an efficient yet effective approach to incorporate this methodology into an actual SA is presented. Several simulated and real examples illustrate the utility of this approach. This framework can be extended to uncertainty analysis as well.}
}
@article{OH2023100602,
title = {Making computing visible & tangible: A paper-based computing toolkit for codesigning inclusive computing education activities},
journal = {International Journal of Child-Computer Interaction},
volume = {38},
pages = {100602},
year = {2023},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2023.100602},
url = {https://www.sciencedirect.com/science/article/pii/S2212868923000399},
author = {HyunJoo Oh and Sherry Hsi and Noah Posner and Colin Dixon and Tymirra Smith and Tingyu Cheng},
keywords = {Paper-based computing, Codesign, Inclusive CS education, Learning through making},
abstract = {MCVT (Making Computing Visible and Tangible) Cards are a toolkit of paper-based computing cards intended for use in the codesign of inclusive computing education. Working with groups of teachers and students over multiple design sessions, we share our toolkit, design drivers and material considerations; and use cases drawn from a week-long codesign workshop where seven teachers made and adapted cards for their future classroom facilitation. Our findings suggest that teachers valued the MCVT toolkit as a resource for their own learning and perceived the cards to be useful for supporting new computational practices, specifically for learning through making and connecting to examples of everyday computing. Critically reviewed by teachers during codesign workshops, the toolkit however posed some implementation challenges and constraints for learning through making and troubleshooting circuitry. From teacher surveys, interviews, workshop video recordings, and teacher-constructed projects, we show how teachers codesigned new design prototypes and pedagogical activities while also adapting and extending paper-based computing materials so their students could take advantage of the unique technical and expressive affordances of MCVT Cards. Our design research contributes a new perspective on using interactive paper computing cards as a medium for instructional materials development to support more inclusive computing education.}
}
@article{RAHMAN20125541,
title = {Developing Mathematical Communication Skills of Engineering Students},
journal = {Procedia - Social and Behavioral Sciences},
volume = {46},
pages = {5541-5547},
year = {2012},
note = {4th WORLD CONFERENCE ON EDUCATIONAL SCIENCES (WCES-2012) 02-05 February 2012 Barcelona, Spain},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2012.06.472},
url = {https://www.sciencedirect.com/science/article/pii/S1877042812022082},
author = {Roselainy Abdul Rahman and Yudariah Mohammad Yusof and Hamidreza Kashefi and Sabariah Baharun},
keywords = {Communication, Mathematical Thinking, Multivariable Calculus, Student's Obstacles},
abstract = {In Malaysia and also elsewhere in the world the demands for graduates who have employability skills such as ability to think critically, solve problems and can communicate are highly sought in the workplace. In the early 2006, the development of such skills was recognized as integral goals of undergraduate education at Universiti Teknologi Malaysia. Since then rigorous efforts have been made to inculcate these skills amongst the undergraduates. In this paper, we will share some of our experiences in coping with the challenges of changing our teaching practices to accommodate this quest though focusing on communication. For mathematics learning to occur, we believed that students should participate actively in the knowledge construction and be able to take charge of their own learning. Taking these aspects into consideration, we had developed a framework of active learning and used it to guide our instruction in engineering mathematics at UTM. Here we will discuss the strategies that we had designed and employed in engaging students with the subject matter as well as to initiate and support student's thinking and communication in the language of mathematics. Some student's responses that gave indications of their struggle, progress and growth encountered in the research implementation will also be presented.}
}
@article{BARKER2023100569,
title = {An Inflection Point in Cancer Protein Biomarkers: What was and What's Next},
journal = {Molecular & Cellular Proteomics},
volume = {22},
number = {7},
pages = {100569},
year = {2023},
issn = {1535-9476},
doi = {https://doi.org/10.1016/j.mcpro.2023.100569},
url = {https://www.sciencedirect.com/science/article/pii/S1535947623000804},
author = {Anna D. Barker and Mario M. Alba and Parag Mallick and David B. Agus and Jerry S.H. Lee},
keywords = {proteomics, cancer biomarkers, protein biomarkers, complex adaptive systems, clinical proteomics},
abstract = {Biomarkers remain the highest value proposition in cancer medicine today—especially protein biomarkers. Despite decades of evolving regulatory frameworks to facilitate the review of emerging technologies, biomarkers have been mostly about promise with very little to show for improvements in human health. Cancer is an emergent property of a complex system, and deconvoluting the integrative and dynamic nature of the overall system through biomarkers is a daunting proposition. The last 2 decades have seen an explosion of multiomics profiling and a range of advanced technologies for precision medicine, including the emergence of liquid biopsy, exciting advances in single-cell analysis, artificial intelligence (machine and deep learning) for data analysis, and many other advanced technologies that promise to transform biomarker discovery. Combining multiple omics modalities to acquire a more comprehensive landscape of the disease state, we are increasingly developing biomarkers to support therapy selection and patient monitoring. Furthering precision medicine, especially in oncology, necessitates moving away from the lens of reductionist thinking toward viewing and understanding that complex diseases are, in fact, complex adaptive systems. As such, we believe it is necessary to redefine biomarkers as representations of biological system states at different hierarchical levels of biological order. This definition could include traditional molecular, histologic, radiographic, or physiological characteristics, as well as emerging classes of digital markers and complex algorithms. To succeed in the future, we must move past purely observational individual studies and instead start building a mechanistic framework to enable integrative analysis of new studies within the context of prior studies. Identifying information in complex systems and applying theoretical constructs, such as information theory, to study cancer as a disease of dysregulated communication could prove to be “game changing” for the clinical outcome of cancer patients.}
}
@article{MUSGRAVE2017137,
title = {Understanding and advancing graduate teaching assistants’ mathematical knowledge for teaching},
journal = {The Journal of Mathematical Behavior},
volume = {45},
pages = {137-149},
year = {2017},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2016.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S0732312316302012},
author = {Stacy Musgrave and Marilyn P. Carlson},
keywords = {Graduate student teaching assistant, Mathematical meanings, Average rate of change, Precalculus},
abstract = {Graduate student teaching assistants (GTAs) usually teach introductory level courses at the undergraduate level. Since GTAs constitute the majority of future mathematics faculty, their image of effective teaching and preparedness to lead instructional improvements will impact future directions in undergraduate mathematics curriculum and instruction. In this paper, we argue for the need to support GTAs in improving their mathematical meanings of foundational ideas and their ability to support productive student thinking. By investigating GTAs’ meanings for average rate of change, a key content area in precalculus and calculus, we found evidence that even mathematically sophisticated GTAs possess impoverished meanings of this key idea. We argue for the need, and highlight one approach, for supporting GTAs to improve their understanding of foundational mathematical ideas and how these ideas are learned.}
}
@article{SAIKIA2021129664,
title = {Study of interacting mechanism of amino acid and Alzheimer's drug using vibrational techniques and computational method},
journal = {Journal of Molecular Structure},
volume = {1227},
pages = {129664},
year = {2021},
issn = {0022-2860},
doi = {https://doi.org/10.1016/j.molstruc.2020.129664},
url = {https://www.sciencedirect.com/science/article/pii/S0022286020319773},
author = {Jyotshna Saikia and Bhargab Borah and Th. Gomti Devi},
keywords = {DL-Alanine, Memantine, Raman, FTIR, DFT},
abstract = {The present work is undertaken to investigate the molecular interaction between Memantine (Alzheimer's drug) and DL-Alanine (amino acid) using DFT and vibrational spectroscopic methods, in particular, Fourier Transform Infrared Spectroscopy (FTIR) and Raman techniques. The DFT calculations of these molecules are carried out using the B3LYP/6–311 ++ G (d, p) level of theory. The experimental FTIR and Raman spectra of the molecules are compared to the respective DFT computed wavenumbers. A satisfactory agreement is obtained between experimental and computed wavenumbers. Further, HOMO-LUMO energy gap, Natural Bond Orbital (NBO) analysis, total energy, zero-point vibrational energy, Molecular Electrostatic Potential (MEP), chemical potential, hardness, ionization energy, global electrophilicity index, dipole moments, and first-order hyperpolarizabilities of the interacting state are reported and compared to the respective parameters of the individual states. The NBO analysis of the molecules indicates the transfer of charge between DL-Alanine and Memantine through NH•••O intermolecular hydrogen bonds. The molecular docking studies of the molecules are  performed to investigate the binding affinity of the ligand with the 6DG7 receptor.}
}
@article{TALWAR2021102341,
title = {Has financial attitude impacted the trading activity of retail investors during the COVID-19 pandemic?},
journal = {Journal of Retailing and Consumer Services},
volume = {58},
pages = {102341},
year = {2021},
issn = {0969-6989},
doi = {https://doi.org/10.1016/j.jretconser.2020.102341},
url = {https://www.sciencedirect.com/science/article/pii/S0969698920313497},
author = {Manish Talwar and Shalini Talwar and Puneet Kaur and Naliniprava Tripathy and Amandeep Dhir},
keywords = {Artificial neural network, COVID-19, Financial behavior, Financial attitude, Financial anxiety, Pandemic},
abstract = {Financial attitude influences the financial behavior of retail investors. Although the extant research has acknowledged and examined this relationship, the measures of financial attitude and behavior still vary widely and are generally posed as a series of questions rather than statements. In addition to this, there is insufficient knowledge regarding retail investors' behavior in the face of a health crisis, such as the current COVID-19 pandemic. This study addresses these gaps in the prior literature by examining the relative influence of six dimensions of financial attitude, namely, financial anxiety, optimism, financial security, deliberative thinking, interest in financial issues, and needs for precautionary savings, on the trading activity of retail investors during the pandemic. Data were collected from 404 respondents and analyzed using the artificial neural network (ANN) method. The results revealed that all six dimensions had a positive influence on trading activity, with interest in financial issues exerting the strongest influence, followed by deliberative thinking. The study thus contributes important inferences for researchers and managers.}
}
@article{LI2023119775,
title = {Neural representations of self-generated thought during think-aloud fMRI},
journal = {NeuroImage},
volume = {265},
pages = {119775},
year = {2023},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2022.119775},
url = {https://www.sciencedirect.com/science/article/pii/S1053811922008965},
author = {Hui-Xian Li and Bin Lu and Yu-Wei Wang and Xue-Ying Li and Xiao Chen and Chao-Gan Yan},
keywords = {Self-generated thoughts, Think-aloud fMRI, Natural language processing, Representational similarity analysis},
abstract = {Is the brain at rest during the so-called resting state? Ongoing experiences in the resting state vary in unobserved and uncontrolled ways across time, individuals, and populations. However, the role of self-generated thoughts in resting-state fMRI remains largely unexplored. In this study, we collected real-time self-generated thoughts during “resting-state” fMRI scans via the think-aloud method (i.e., think-aloud fMRI), which required participants to report whatever they were currently thinking. We first investigated brain activation patterns during a think-aloud condition and found that significantly activated brain areas included all brain regions required for speech. We then calculated the relationship between divergence in thought content and brain activation during think-aloud and found that divergence in thought content was associated with many brain regions. Finally, we explored the neural representation of self-generated thoughts by performing representational similarity analysis (RSA) at three neural scales: a voxel-wise whole-brain searchlight level, a region-level whole-brain analysis using the Schaefer 400-parcels, and at the systems level using the Yeo seven-networks. We found that “resting-state” self-generated thoughts were distributed across a wide range of brain regions involving all seven Yeo networks. This study highlights the value of considering ongoing experiences during resting-state fMRI and providing preliminary methodological support for think-aloud fMRI.}
}
@article{LI2023106299,
title = {Constructing a link between multivariate titanium-based semiconductor band gaps and chemical formulae based on machine learning},
journal = {Materials Today Communications},
volume = {35},
pages = {106299},
year = {2023},
issn = {2352-4928},
doi = {https://doi.org/10.1016/j.mtcomm.2023.106299},
url = {https://www.sciencedirect.com/science/article/pii/S235249282300990X},
author = {Jiawei Li and Zhengxin Chen and Jiang Wu and Jia Lin and Ping He and Rui Zhu and Cheng Peng and Hai Zhang and Wenhao Li and Xu Fang and Hongtao Shen},
keywords = {Random forest model, Chemical formula, Components, Bandgap, Machine learning},
abstract = {Titanium-based semiconductors are wildly recognized as one of the most commonly used photocatalysts for photocatalysis. Energy band modulation is a key aspect of the catalytic activity of photocatalytic semiconductors, but the acquisition of semiconductor energy bands is still a complex and important task. In recent years, machine learning has played an important role in materials prediction, where the crystal structure of a material is usually used as input in energy band prediction. However, existing machine learning algorithms cannot accurately predict the energy bands of materials from structural components. Here, we convert the chemical formula into component descriptors after comparing first principles and ultraviolet-visible spectrophotometry (UV–vis) errors on bandgap values, and the component and bandgap values form a set of labeled data pairs. The chemical formula components are used as input to accurately predict the energy bands of the material. In our evaluation, the model outperforms existing machine learning methods in predicting energy bands, yielding mean absolute value errors of about 0.277 eV, and possesses a significant advantage in component prediction. In particular, this method of predicting the photocatalytic semiconductor energy band gap from chemical formula components provides a new way of thinking about photocatalyst selectivity.}
}
@article{GUSTAFSON199557,
title = {Theory and computation of periodic solutions of autonomous partial differential equation boundary value problems, with application to the driven cavity problem},
journal = {Mathematical and Computer Modelling},
volume = {22},
number = {9},
pages = {57-75},
year = {1995},
issn = {0895-7177},
doi = {https://doi.org/10.1016/0895-7177(95)00168-2},
url = {https://www.sciencedirect.com/science/article/pii/0895717795001682},
author = {K. Gustafson},
keywords = {Navier-Stokes equations, Driven cavity problem, Pressure boundary condition, Vortex shedding, Hopf bifurcation},
abstract = {In ordinary differential equations, one distinguishes two cases: autonomous and nonautonomous. Roughly speaking, the theory of the latter is built upon the theory of the former. The same distinction should be applied to partial differential equations, where much less is known. Here I will focus on the question of the generation of periodic solutions for autonomous partial differential equation boundary value problems. Specifically, I consider the incompressible Navier-Stokes equations, and the important driven cavity problem. For simplicity, attention is restricted to two bifurcation parameters, the Reynolds number and the Aspect ratio. Only Dirichlet velocity boundary conditions are considered. Both the known theory and known computational results for the driven cavity are surveyed. The importance of computationally adhering to the div u = 0 condition to accurately simulate unsteady flows which will be qualitatively correct for the incompressible Navier-Stokes equations is stressed. The dependence of sustained periodicity upon the existence of highly localized vortex shedding sequences somewhere along the boundary is pointed out. A new analysis of the pressure boundary condition, based upon a general regularity principle, is given. A conjectured Hopf bifurcation criticality curve is explained.}
}
@article{GALLISTEL2021104533,
title = {The physical basis of memory},
journal = {Cognition},
volume = {213},
pages = {104533},
year = {2021},
note = {Special Issue in Honour of Jacques Mehler, Cognition’s founding editor},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2020.104533},
url = {https://www.sciencedirect.com/science/article/pii/S0010027720303528},
author = {C.R. Gallistel},
keywords = {Engram, Communication channel, Plastic synapse, Molecules},
abstract = {Neuroscientists are searching for the engram within the conceptual framework established by John Locke's theory of mind. This framework was elaborated before the development of information theory, before the development of information processing machines and the science of computation, before the discovery that molecules carry hereditary information, before the discovery of the codon code and the molecular machinery for editing the messages written in this code and translating it into transcription factors that mark abstract features of organic structure such as anterior and distal. The search for the engram needs to abandon Locke's conceptual framework and work within a framework informed by these developments. The engram is the medium by which information extracted from past experience is transmitted to the computations that inform future behavior. The information-conveying symbols in the engram are rapidly generated in the course of computations, which implies that they are molecules.}
}
@article{ZOHDI20073927,
title = {Computation of strongly coupled multifield interaction in particle–fluid systems},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {196},
number = {37},
pages = {3927-3950},
year = {2007},
note = {Special Issue Honoring the 80th Birthday of Professor Ivo Babuška},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2006.10.040},
url = {https://www.sciencedirect.com/science/article/pii/S004578250700117X},
author = {T.I. Zohdi},
keywords = {Particle–fluid interaction, Multiple fields, Iterative methods},
abstract = {The present work develops a flexible and robust solution strategy to resolve coupled systems comprised of large numbers of flowing particles embedded within a fluid. A model problem, consisting of particles which may undergo inelastic collisions in the presence of near-field forces, is considered. The particles are surrounded by a continuous interstitial fluid which is assumed to obey the compressible Navier–Stokes equations. Thermal effects are also considered. Such particle/fluid systems are strongly coupled, due to the mechanical forces and heat transfer induced by the fluid onto the particles and vice-versa. Because the coupling of the various particle and fluid fields can dramatically change over the course of a flow process, a primary focus of this work is the development of a recursive “staggering” solution scheme, whereby the time-steps are adaptively adjusted to control the error associated with the incomplete resolution of the coupled interaction between the various solid particulate and continuum fluid fields. A central feature of the approach is the ability to account for the presence of particles within the fluid in a straightforward manner that can be easily incorporated within any standard computational fluid mechanics code based on finite difference, finite element or finite volume type discretization. A three dimensional example is provided to illustrate the overall approach.}
}
@article{XU2025121541,
title = {Adaptive sequential three-way decisions for dynamic time warping},
journal = {Information Sciences},
volume = {690},
pages = {121541},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121541},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524014555},
author = {Jianfeng Xu and Ruihua Wang and Yuanjian Zhang and Weiping Ding},
keywords = {Time-series data, Sequential three-way decisions, Dynamic time warping, Uncertainty},
abstract = {Dynamic time warping (DTW) algorithm is widely used in diversified applications due to its excellent anti-deformation and anti-interference in measuring time-series based similarity. However, the high time complexity of DTW restrains the applicability of real-time case. The existing DTW acceleration studies suffer from a loss of accuracy. How to accelerate computation while maintaining satisfying computational accuracy remains challenging. Motivated by sequential three-way decisions, this paper develops a novel model with adaptive sequential three-way decisions for dynamic time warping (AS3-DTW). Firstly, we systematically summarize distance differences under the context of adjacent tripartite search spaces for DTW, and propose five patterns of granularity adjustments of the search spaces. Furthermore, we present the corresponding calculation method of DTW adjacent tripartite search spaces distances difference. Finally, we construct a novel dynamism on adaptively adjusting time warping by combining sequence-based multi-granularity with sequential three-way decisions. Experimental results show that AS3-DTW effectively achieves promising trade-off between computational speed and accuracy on multiple UCR datasets when compared with the state-of-the-art algorithms.}
}
@article{BIELZA2000725,
title = {Structural, elicitation and computational issues faced when solving complex decision making problems with influence diagrams},
journal = {Computers & Operations Research},
volume = {27},
number = {7},
pages = {725-740},
year = {2000},
issn = {0305-0548},
doi = {https://doi.org/10.1016/S0305-0548(99)00113-6},
url = {https://www.sciencedirect.com/science/article/pii/S0305054899001136},
author = {C. Bielza and M. Gómez and S. Rı́os-Insua and J.A.Fernández {del Pozo}},
keywords = {Decision analysis, Influence diagrams, Implementation issues, Medical decision making, Neonatal jaundice},
abstract = {Influence diagrams have become a popular tool for representing and solving decision making problems under uncertainty (Shachter, Operations Research 1986;34:871–82). We show here some practical difficulties when using them to construct a medical decision support system. Specifically, it is hard to tackle issues related to the problem structuring, like the existence of constraints on the sequence of decisions, and the time evolution modeling; related to the knowledge-acquisition, like probability and utility assignment; and related to computational limitations, in memory storage and evaluation phases, as well as the explanation of results. We have recently developed a complex decision support system for neonatal jaundice management — a very common medical problem — , encountering all these difficulties. In this paper, we describe them and how they have been undertaken, providing insights into the community involved in the design and solution of decision models by means of influence diagrams.
Scope and purpose
Decision Analysis is a very well-known discipline that deals with the practice of Decision Theory (Clemen, Making hard decisions: an introduction to decision analysis, 2nd ed. Pacific Grove, CA: Duxbury, 1996). It comprises various steps usually implemented in a decision support system: definition of the alternatives and objectives, modelization of the structure of the decision problem, as well as the beliefs and preferences of the decision maker. The recommended alternative is the one with maximum expected utility, once all the assignments have been refined via sensitivity analyses. However, there are a number of difficulties faced in practice when solving large problems, that require an attentive study.}
}
@article{KHAZAEI2025115420,
title = {Renewable energy portfolio in Mexico for Industry 5.0 and SDGs: Hydrogen, wind, or solar?},
journal = {Renewable and Sustainable Energy Reviews},
volume = {213},
pages = {115420},
year = {2025},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2025.115420},
url = {https://www.sciencedirect.com/science/article/pii/S1364032125000930},
author = {Moein Khazaei and Fatemeh Gholian-Jouybari and Mahdi {Davari Dolatabadi} and Aryan {Pourebrahimi Alamdari} and Hamidreza Eskandari and Mostafa Hajiaghaei-Keshteli},
keywords = {Renewable energies, Industry 5.0, Sustainable development goals, Portfolio selection, Nearshoring},
abstract = {Despite a surge in Foreign Direct Investment (FDI) in Mexico, like nearshoring, the slow growth in international investment in renewables challenges the country's progress in achieving Sustainable Development Goals (SDGs) related to clean energy. To the best of current knowledge, this research is one of the first to explore the integration of renewable energy (Green/Blue/Turquoise Hydrogen, Solar, and Wind plants) in Mexico, emphasizing a diverse portfolio of projects aligned with SDGs and Industry 5.0. While previous works have focused on the nexus between energy, Industry 4.0, and sustainability, the present study advances this discourse by incorporating Industry 5.0 principles and a comprehensive methodological approach. Through a comprehensive methodology involving Value-Focused Thinking (VFT), fuzzy Decision-Making Trial and Evaluation Laboratory (DEMATEL), and multi-objective mathematical programming, the study identifies key criteria encompassing social, economic, environmental, and technological dimensions. The resulting criteria form a robust framework for evaluating project sustainability. The fuzzy DEMATEL analysis reveals intricate interrelations among criteria, emphasizing the need for balanced considerations. Results highlighted job creation, income equality, and microfinance support as key social considerations, while energy-related criteria emphasized sustainable practices. The proposed multi-objective programming model and COmbined COmpromise SOlution (COCOSO) method facilitated the selection of eight projects, with one project as the top-ranked option across various scoring strategies. Overall, this research provides a nuanced roadmap for effective decision-making in renewable energy projects, offering insights into project strengths, weaknesses, and potential areas for improvement.}
}
@article{GOLTZ2021103417,
title = {Do you listen to music while studying? A portrait of how people use music to optimize their cognitive performance},
journal = {Acta Psychologica},
volume = {220},
pages = {103417},
year = {2021},
issn = {0001-6918},
doi = {https://doi.org/10.1016/j.actpsy.2021.103417},
url = {https://www.sciencedirect.com/science/article/pii/S0001691821001670},
author = {Franziska Goltz and Makiko Sadakata},
keywords = {Background music, Cognitive performance, Music perception},
abstract = {The effect of background music (BGM) on cognitive task performance is a popular topic. However, the evidence is not converging: experimental studies show mixed results depending on the task, the type of music used and individual characteristics. Here, we explored how people use BGM while optimally performing various cognitive tasks in everyday life, such as reading, writing, memorizing, and critical thinking. Specifically, the frequency of BGM usage, preferred music types, beliefs about the scientific evidence on BGM, and individual characteristics, such as age, extraversion and musical background were investigated. Although the results confirmed highly diverse strategies among individuals regarding when, how often, why and what type of BGM is used, we found several general tendencies: people tend to use less BGM when engaged in more difficult tasks, they become less critical about the type of BGM when engaged in easier tasks, and there is a negative correlation between the frequency of BGM and age, indicating that younger generations tend to use more BGM than older adults. The current and previous evidence are discussed in light of existing theories. Altogether, this study identifies essential variables to consider in future research and further forwards a theory-driven perspective in the field.}
}
@incollection{2004201,
title = {Chapter 6 Alternatives to purely Lagrangian computations},
editor = {Jonas A. Zukas},
series = {Studies in Applied Mechanics},
publisher = {Elsevier},
volume = {49},
pages = {201-250},
year = {2004},
booktitle = {Introduction to Hydrocodes},
issn = {0922-5382},
doi = {https://doi.org/10.1016/S0922-5382(04)80007-2},
url = {https://www.sciencedirect.com/science/article/pii/S0922538204800072},
abstract = {Publisher Summary
Lagrangian techniques deal with problems involving fast and transient loading. Lagrangian methods offer several advantages over the competition. Because Lagrangian codes cannot solve all the problems involving fast short-duration loading, other techniques shoulb be mentioned. The chapter describes the popular alternative methods, Euler codes, coupled Euler-Lagrange codes, arbitrary Lagrange-Euler (ALE) techniques, and meshless methods The advantages of Lagrange codes are offset by grid distortion. With large distortions, the time increment for advancing the computations is forced to approach zero, thus rendering the calculations uneconomical. The use of sliding interfaces and rezoning can extend the range of applicability of Lagrange codes to larger distortions. Similarly, the ability to handle large distortions in Euler codes is offset by the need to account for material transport. Pure Euler techniques are ideal for handling large distortions.}
}
@article{WOLFF2024893,
title = {The mediodorsal thalamus in executive control},
journal = {Neuron},
volume = {112},
number = {6},
pages = {893-908},
year = {2024},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2024.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0896627324000023},
author = {Mathieu Wolff and Michael M. Halassa},
keywords = {thalamus, mediodorsal, prefrontal cortex, cognition, cognitive flexibility, computation, neural architectures},
abstract = {Summary
Executive control, the ability to organize thoughts and action plans in real time, is a defining feature of higher cognition. Classical theories have emphasized cortical contributions to this process, but recent studies have reinvigorated interest in the role of the thalamus. Although it is well established that local thalamic damage diminishes cognitive capacity, such observations have been difficult to inform functional models. Recent progress in experimental techniques is beginning to enrich our understanding of the anatomical, physiological, and computational substrates underlying thalamic engagement in executive control. In this review, we discuss this progress and particularly focus on the mediodorsal thalamus, which regulates the activity within and across frontal cortical areas. We end with a synthesis that highlights frontal thalamocortical interactions in cognitive computations and discusses its functional implications in normal and pathological conditions.}
}
@incollection{RUNCO20141,
title = {Chapter 1 - Cognition and Creativity},
editor = {Mark A. Runco},
booktitle = {Creativity (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {1-38},
year = {2014},
isbn = {978-0-12-410512-6},
doi = {https://doi.org/10.1016/B978-0-12-410512-6.00001-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780124105126000011},
author = {Mark A. Runco},
keywords = {Threshold theory, IQ, Structure of intellect, Associative theory, Problem solving, Problem finding, Incubation, Insight, Intuition, Meta-cognition, Mindfulness, Overinclusive thinking},
abstract = {This chapter discusses various aspects of cognition and creativity. Cognitive theories focus on thinking skills and intellectual processes. The approaches to creative cognition are extremely varied. There are bridges between basic cognitive processes and creative problem solving, as well as connections with intelligence, problem solving, language, and other indications of individual differences. The basic processes are generally nomothetic, meaning that they represent universals. Divergent thinking is employed when an individual is faced with an open-ended task. From this perspective divergent thinking is a kind of problem solving. Divergent thinking is not synonymous with creative thinking, but it does tell something about the cognitive processes that may lead to original ideas and solutions. Many theories of creative cognition look to associative processes. Associative theories focus on how ideas are generated and chained together. Cognitive theories of creativity often focus specifically on the problem-solving process. A problem can be defined as a situation with a goal and an obstacle. The stage models of creative cognition are also elaborated.}
}
@article{MERRITT2024103670,
title = {Igniting kid power: The impact of environmental service-learning on elementary students' awareness of energy problems and solutions},
journal = {Energy Research & Social Science},
volume = {116},
pages = {103670},
year = {2024},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2024.103670},
url = {https://www.sciencedirect.com/science/article/pii/S2214629624002615},
author = {Eileen G. Merritt and Andrea E. Weinberg and Candace Lapan and Sara E. Rimm-Kaufman},
keywords = {Environmental service-learning, Energy literacy, Elementary students, Science education, Randomized controlled trial},
abstract = {Energy concepts are taught in many schools, but children rarely have an opportunity to grapple with energy problems and work on their own solutions. This study explores the impacts of Connect Science, a service-learning (SL) program developed to enhance elementary students' energy literacy in the United States. Program impacts were explored within the context of a randomized controlled trial. Teachers in the SL intervention group were provided with professional development, coaching and curricular materials. Each fourth grade class chose an energy problem to address, and designed projects to test out a solution. Teachers in a waitlist control group taught their typical energy unit. Upon completion of the unit, students were asked to write about a problem related to energy production or use and propose a potential solution. Inductive content analysis was used to code 703 student responses (377 from control group and 326 from SL group). The majority of students expressed concerns about wasting or using too much electricity or the use of nonrenewable energy sources. Solutions focused on energy conservation and the use of renewable or clean resources were mentioned most frequently overall. Students in the SL group were significantly more likely to mention environmental impacts of various energy sources and to suggest energy conservation solutions or educating others. Conversely, the control group student responses more often focused on electric circuits or electrical safety. Results from this study suggest the promise of environmental SL programs to advance energy literacy and promote critical thinking about how to address energy problems.}
}
@article{HARDING2024295,
title = {A new predictive coding model for a more comprehensive account of delusions},
journal = {The Lancet Psychiatry},
volume = {11},
number = {4},
pages = {295-302},
year = {2024},
issn = {2215-0366},
doi = {https://doi.org/10.1016/S2215-0366(23)00411-X},
url = {https://www.sciencedirect.com/science/article/pii/S221503662300411X},
author = {Jessica Niamh Harding and Noham Wolpe and Stefan Peter Brugger and Victor Navarro and Christoph Teufel and Paul Charles Fletcher},
abstract = {Summary
Attempts to understand psychosis—the experience of profoundly altered perceptions and beliefs—raise questions about how the brain models the world. Standard predictive coding approaches suggest that it does so by minimising mismatches between incoming sensory evidence and predictions. By adjusting predictions, we converge iteratively on a best guess of the nature of the reality. Recent arguments have shown that a modified version of this framework—hybrid predictive coding—provides a better model of how healthy agents make inferences about external reality. We suggest that this more comprehensive model gives us a richer understanding of psychosis compared with standard predictive coding accounts. In this Personal View, we briefly describe the hybrid predictive coding model and show how it offers a more comprehensive account of the phenomenology of delusions, thereby providing a potentially powerful new framework for computational psychiatric approaches to psychosis. We also make suggestions for future work that could be important in formalising this novel perspective.}
}
@article{YAN2025109327,
title = {An approach to calculate conceptual distance across multi-granularity based on three-way partial order structure},
journal = {International Journal of Approximate Reasoning},
volume = {177},
pages = {109327},
year = {2025},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2024.109327},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X24002147},
author = {Enliang Yan and Pengfei Zhang and Tianyong Hao and Tao Zhang and Jianping Yu and Yuncheng Jiang and Yuan Yang},
keywords = {Partial order structure, Concept-cognitive learning, Knowledge distance, Three-way decision, Granular computing, Concept graph},
abstract = {The computation of concept distances aids in understanding the interrelations among entities within knowledge graphs and uncovering implicit information. The existing studies predominantly focus on the conceptual distance of specific hierarchical levels without offering a unified framework for comprehensive exploration. To overcome the limitations of unidimensional approaches, this paper proposes a method for calculating concept distances at multiple granularities based on a three-way partial order structure. Specifically: (1) this study introduces a methodology for calculating inter-object similarity based on the three-way attribute partial order structure (APOS); (2) It proposes the application of the similarity matrix to delineate the structure of categories; (3) Based on the similarity matrix describing the three-way APOS of categories, we establish a novel method for calculating inter-category distance. The experiments on eight datasets demonstrate that this approach effectively differentiates various concepts and computes their distances. When applied to classification tasks, it exhibits outstanding performance.}
}
@article{ACISECHE2023109386,
title = {A perspective on the sharing of docking data},
journal = {Data in Brief},
volume = {49},
pages = {109386},
year = {2023},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2023.109386},
url = {https://www.sciencedirect.com/science/article/pii/S2352340923004985},
author = {Samia Aci-Sèche and Stéphane Bourg and Pascal Bonnet and Joseph Rebehmed and Alexandre G. {de Brevern} and Julien Diharce},
keywords = {3D coordinates, Docking, Files, SDF, Sharing, FAIR principles},
abstract = {Computational approaches are nowadays largely applied in drug discovery projects. Among these, molecular docking is the most used for hit identification against a drug target protein. However, many scientists in the field shed light on the lack of availability and reproducibility of the data obtained from such studies to the whole community. Consequently, sustaining and developing the efforts toward a large and fully transparent sharing of those data could be beneficial for all researchers in drug discovery. The purpose of this article is first to propose guidelines and recommendations on the appropriate way to conduct virtual screening experiments and second to depict the current state of sharing molecular docking data. In conclusion, we have explored and proposed several prospects to enhance data sharing from docking experiment that could be developed in the foreseeable future.}
}