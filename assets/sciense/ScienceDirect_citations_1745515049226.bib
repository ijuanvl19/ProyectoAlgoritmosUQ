@incollection{WANG2022238,
title = {1.10 - CyberGIS and Geospatial Data Science for Advancing Geomorphology},
editor = {John (Jack) F. Shroder},
booktitle = {Treatise on Geomorphology (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {238-259},
year = {2022},
isbn = {978-0-12-818235-2},
doi = {https://doi.org/10.1016/B978-0-12-818234-5.00122-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012818234500122X},
author = {Shaowen Wang and Michael P. Bishop and Zhe Zhang and Brennan W. Young and Zewei Xu},
keywords = {Artificial intelligence, CyberGIS, Deep learning, Geomorphology, Geospatial data science, Land cover science, LiDAR, Uncertainty},
abstract = {Theoretical and practical issues in geomorphology have not been adequately addressed due to a lack of formalization and digital representation of spatial and temporal concepts, given the limitations associated with modern-day geographic information systems (GIS). Rapid advancements in geospatial technologies have resulted in new sensors and large volumes of geospatial data that have yet to be fully exploited given a variety of computational issues. Computational limitations involving storage, preprocessing, analysis, and modeling pose significant problems for Earth scientists. Consequently, advanced cyberinfrastructure is required to address geospatial data-science issues involving communication, representation, computation, information production, decision-making, and geovisualization. We identify and discuss important aspects of exploiting advances in cyberinfrastructure that involve computational scalability, artificial intelligence, and uncertainty characterization and analysis for addressing issues in the Earth sciences. Such developments can be termed cyber geographic information science and systems (cyberGIS). We discuss this important topic by addressing the significant overlap of concepts in GIS and geomorphology that can be formalized, digitally represented, implemented, and evaluated with cyberGIS. We then introduce the fundamentals of cyberinfrastructure and cyberGIS, including a discussion of the utilization of artificial intelligence and deep learning. We finally provide one case study demonstrating operational cyberGIS capabilities.}
}
@article{KUO2013510,
title = {Cultural Evolution Algorithm for Global Optimizations and its Applications},
journal = {Journal of Applied Research and Technology},
volume = {11},
number = {4},
pages = {510-522},
year = {2013},
issn = {1665-6423},
doi = {https://doi.org/10.1016/S1665-6423(13)71558-X},
url = {https://www.sciencedirect.com/science/article/pii/S166564231371558X},
author = {H.C. Kuo and C.H. Lin},
keywords = {Cultural Algorithm, Genetic Algorithm, Nelder-Mead’s simplex method, Global optimization},
abstract = {The course of socio-cultural transition can neither be aimless nor arbitrary, instead it requires a clear direction. A common goal of social species’ evolution is to move towards an advanced spiritual and conscious state. This study aims to develop a population-based algorithm on the basis of cultural transition goal. In this paper, the socio-cultural model based on a system thought framework could be used to develop a cultural evolution algorithm (CEA). CEA leverage four strategies, each consists of several search methods with similar thinking. Seven benchmark functions are utilized to validate the search performance of the proposed algorithm. The results show that all of the four strategies of cultural evolution algorithm have better performance when compared with relevant literatures. Finally, the CEA was then applied to optimize two different reliability engineering problems, a Serial-Parallel System design and a Bridge System design. For the Serial-Parallel System design, the CEA achieved the exact solution with ease, and for the Bridge System design, the solution obtained by the CEA is superior to those from other literatures.}
}
@article{RUAN2023100872,
title = {Public perception of electric vehicles on Reddit and Twitter: A cross-platform analysis},
journal = {Transportation Research Interdisciplinary Perspectives},
volume = {21},
pages = {100872},
year = {2023},
issn = {2590-1982},
doi = {https://doi.org/10.1016/j.trip.2023.100872},
url = {https://www.sciencedirect.com/science/article/pii/S2590198223001197},
author = {Tao Ruan and Qin Lv},
keywords = {Cross-platform, Twitter, Reddit, Electric vehicles, Public perception, Topic modeling, Computational social science},
abstract = {Electrified mobility such as electric vehicles (EVs) is a promising solution to reduce carbon emissions in transportation and mitigate global warming. Understanding public perception of EVs can help better support their adoption. A previous study shows that online social networks (OSNs) such as Reddit can be a valuable source for studying public perceptions of EVs and provide different perspectives from traditional methods that leverage surveys, questionnaires, or interviews (Ruan and Lv, 2022). Our work aims to investigate this direction further through the following research question: Given the distinct mechanisms of various OSNs, can we obtain a more comprehensive picture of public perception of EVs by integrating the analysis from different platforms? Specifically, our study is based on EV-related discussions on two popular OSN platforms: Twitter and Reddit. We have collected 3,437,917 Reddit posts (including 274,979 submissions and 3,162,938 comments) and 7,383,327 Tweets between January 2011 and December 2020 and analyzed them from several perspectives. Our analysis shows that users have had different topic and sentiment patterns in EV-related discussions on the two platforms over the past decade. We also leverage the verified account information on Twitter to reveal that the most influential users are politicians and news media; however, the general public has very different conversation patterns with the two types of accounts — politicians seem to be increasingly (over) optimistic about EVs while the public may think differently.}
}
@article{DAS2022104116,
title = {Role of non-motorized transportation and buses in meeting climate targets of urban regions},
journal = {Sustainable Cities and Society},
volume = {86},
pages = {104116},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2022.104116},
url = {https://www.sciencedirect.com/science/article/pii/S2210670722004292},
author = {Deepjyoti Das and Pradip P. Kalbar and Nagendra R. Velaga},
keywords = {Decarbonization, Life cycle thinking, Avoided trip and material, Carbon budget, Climate change, Sustainable transportation},
abstract = {Studies examining the potential of low-carbon modes of passenger transportation for achieving climate goals are limited. The study is one of the first to assess the potential of non-motorized transportation (NMT) and buses to meet regional climate targets representing 2 °C, 1.5 °C, and Intended Nationally Determined Contributions from 2018 to 2050. Also, the approach towards quantifying contribution from avoided trips and materials in holistically understanding the potential of NMT and buses is novel. Data from the transportation model of Mumbai Metropolitan Region's Comprehensive Mobility Plan is used to assess multiple scenarios of upgrading NMT and bus infrastructure to reduce cumulative carbon dioxide emissions (CCE) from passenger transportation. The assessment is based on three push levels, i.e., conservative, moderate, and aggressive. Results show that upgrading bus infrastructure contributes higher to reducing CCE than NMT. As NMT also contributes significantly to decreasing CCE, it is recommended that bus and NMT development should be integrated. However, their combined contribution will not meet the climate targets. Since avoided materials contribute considerably more than avoided trips, high emission materials such as aluminum used in light-weighting should be questioned. The results provide policy guidance to authorities in prioritizing buses and NMT infrastructure development during city planning.}
}
@article{FOO201410,
title = {Evolution of acquired resistance to anti-cancer therapy},
journal = {Journal of Theoretical Biology},
volume = {355},
pages = {10-20},
year = {2014},
issn = {0022-5193},
doi = {https://doi.org/10.1016/j.jtbi.2014.02.025},
url = {https://www.sciencedirect.com/science/article/pii/S0022519314001003},
author = {Jasmine Foo and Franziska Michor},
keywords = {Drug resistance, Cancer, Evolution, Mathematical modeling, Optimal dosing strategies},
abstract = {Acquired drug resistance is a major limitation for the successful treatment of cancer. Resistance can emerge due to a variety of reasons including host environmental factors as well as genetic or epigenetic alterations in the cancer cells. Evolutionary theory has contributed to the understanding of the dynamics of resistance mutations in a cancer cell population, the risk of resistance pre-existing before the initiation of therapy, the composition of drug cocktails necessary to prevent the emergence of resistance, and optimum drug administration schedules for patient populations at risk of evolving acquired resistance. Here we review recent advances towards elucidating the evolutionary dynamics of acquired drug resistance and outline how evolutionary thinking can contribute to outstanding questions in the field.}
}
@incollection{HUNG2017227,
title = {Chapter 12 - Rationality and Escherichia Coli},
editor = {T.-W. Hung and T.J. Lane},
booktitle = {Rationality},
publisher = {Academic Press},
address = {San Diego},
pages = {227-240},
year = {2017},
isbn = {978-0-12-804600-5},
doi = {https://doi.org/10.1016/B978-0-12-804600-5.00012-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012804600500012X},
author = {T.-W. Hung},
keywords = {practical rationality, procedural rationality, , human uniqueness, nonhuman rationality},
abstract = {If rationality is the defining characteristic of the human species, as Aristotle asserts, why is this trait rarely noticed in Eastern traditions? How should we interpret the reasoning ability in problem solving that is increasingly reported in other animals? In this chapter, I focus on descriptive-practical-procedural rationality (one’s action is described as rational if it is determined by internal processes that conform to logical or Bayesian rules). I argue that this rationality can be found in all organisms with adaptive capacity, including unicellular bacteria. To this end, I first review three seemingly true claims and explain why they lead to an inconsistency: (1) Escherichia coli are computational systems in a nontrivial sense, (2) E. coli are not creatures that can be rational or irrational, and (3) rationality is a matter of computational facts in that nontrivial sense, and organisms of the same computation are the type of creatures that can be rational or irrational. I then suggest rejecting claim (2) by examining recent microbiological data on E. coli, explaining the extent to which they satisfy this type of rationality. I also discuss some objections to and implications of this view. Instead of concluding that humans and bacteria both evolve with rationality at the same level, I argue that organisms’ rationality capacities comes in degree.}
}
@incollection{SIEGEL20223,
title = {Chapter 1 - Introduction: Defining the Role of Statistics in Business},
editor = {Andrew F. Siegel and Michael R. Wagner},
booktitle = {Practical Business Statistics (Eighth Edition)},
publisher = {Academic Press},
edition = {Eighth Edition},
pages = {3-18},
year = {2022},
isbn = {978-0-12-820025-4},
doi = {https://doi.org/10.1016/B978-0-12-820025-4.00001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128200254000014},
author = {Andrew F. Siegel and Michael R. Wagner},
abstract = {We begin this chapter with an overview of the competitive advantage provided by a knowledge of statistical methods, followed by some basic facts about statistics and probability and their role in business. Statistical activities can be grouped into five main activities (designing, exploring, modeling, estimating, and hypothesis testing), and one way to clarify statistical thinking is to be able to match the business task at hand with the correct collection of statistical methods. This chapter sets the stage for the rest of the book, which follows up with many important detailed procedures for accomplishing business goals that involve these activities. Next follows an overview of data mining of Big Data (which involves these main activities) and its importance in business. Then we distinguish the field of probability (where, based on assumptions, we reach conclusions about what is likely to happen—a useful exercise in business where nobody knows for sure what will happen) from the field of statistics (where we know from the data what happened, from which we infer conclusions about the system that produced these data) while recognizing that probability and statistics will work well together in future chapters. The chapter concludes with some words of advice on how to integrate statistical thinking with other business viewpoints and activities.}
}
@article{ZHANG2022786,
title = {Research on Graph Neural Network in Stock Market},
journal = {Procedia Computer Science},
volume = {214},
pages = {786-792},
year = {2022},
note = {9th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.242},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922019524},
author = {Wenjun Zhang and Zhensong Chen and Jianyu Miao and Xueyong Liu},
keywords = {Graph neural networks, stocks, forecasts},
abstract = {The stock market is a very important part of the financial field, and the prediction of the stock market has a great relationship with the returns and risk safety of the entire financial field. With the continuous mature application of machine learning and deep learning in other fields, such as image processing and text analysis, people begin to focus on the use of different models so as to predict stock volatility. However, in view of the unique multi-source and heterogeneous characteristics of stock information, the artificial neural network relying on deep learning cannot make a good prediction on it. At this time, the graph neural network that can well analyze the graph structure data is gradually favored by scholars at home and abroad, and the research thinking is also expanding. This dissertation examines the purpose of deeply analyzing the methods of different graph neural network models on stock prediction through an inductive study of amount of relevant literature. In this paper, we not only classify the literature by various graph neural network models, but also describe objectively the models and ideas presented in each paper. By referring to literature, this paper summarizes the previous research results, analyzes the applicability and results of different methods, and lays a foundation for better stock prediction in the future.}
}
@article{ERDMANN202242,
title = {A generative framework for the study of delusions},
journal = {Schizophrenia Research},
volume = {245},
pages = {42-49},
year = {2022},
note = {Computational Approaches to Understanding Psychosis},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2020.11.048},
url = {https://www.sciencedirect.com/science/article/pii/S0920996420306277},
author = {Tore Erdmann and Christoph Mathys},
keywords = {Delusion, Dirichlet process, Hierarchical predictive coding, Auxiliary hypothesis, Epistemic trust},
abstract = {Despite the ubiquity of delusional information processing in psychopathology and everyday life, formal characterizations of such inferences are lacking. In this article, we propose a generative framework that entails a computational mechanism which, when implemented in a virtual agent and given new information, generates belief updates (i.e., inferences about the hidden causes of the information) that resemble those seen in individuals with delusions. We introduce a particular form of Dirichlet process mixture model with a sampling-based Bayesian inference algorithm. This procedure, depending on the setting of a single parameter, preferentially generates highly precise (i.e. over-fitting) explanations, which are compartmentalized and thus can co-exist despite being inconsistent with each other. Especially in ambiguous situations, this can provide the seed for delusional ideation. Further, we show by simulation how the excessive generation of such over-precise explanations leads to new information being integrated in a way that does not lead to a revision of established beliefs. In all configurations, whether delusional or not, the inference generated by our algorithm corresponds to Bayesian inference. Furthermore, the algorithm is fully compatible with hierarchical predictive coding. By virtue of these properties, the proposed model provides a basis for the empirical study and a step toward the characterization of the aberrant inferential processes underlying delusions.}
}
@article{JHA2025103700,
title = {Transitive reasoning: A high-performance computing model for significant pattern discovery in cognitive IoT sensor network},
journal = {Ad Hoc Networks},
volume = {167},
pages = {103700},
year = {2025},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2024.103700},
url = {https://www.sciencedirect.com/science/article/pii/S1570870524003111},
author = {Vidyapati Jha and Priyanka Tripathi},
keywords = {Transitive reasoning, Knowledge discovery, CIoT, Probabilistic clustering, Total variation regularization},
abstract = {Current research on the Internet of Things (IoT) has given rise to a new field of study called cognitive IoT (CIoT), which aims to incorporate cognition into the designs of IoT systems. Consequently, the CIoT inherits specific attributes and challenges from IoT. The CIoT applications generate vast, diverse, constantly changing, and time-dependent data due to the billions of devices involved. The efficient operation of these CIoT systems requires the extraction of valuable insights from vast data sources in a computationally efficient manner. Therefore, this study proposes transitive reasoning to glean significant concepts and patterns from a 21.25-year environmental dataset. To reduce the effects of missing entries, the proposed methodology includes a grouping of data using probabilistic clustering and applying total variance regularization in the alternate direction method of multipliers (ADMM) to regularize the sensory data. As a result, noisy entries will be less conspicuous. Afterward, it calculates the transitional plausibility value for each cluster using the transited value and then turns it into binary data to create concept lattices. In addition, each concept that is formed is assigned a weight, and the concept with the largest transitive strength value is chosen, followed by calculating the mean value. Therefore, this pattern is seen as significant. Experimental results on 21.25-year environmental data show an accuracy of over 99.5%, outperforming competing methods, as shown by cross-validation using multiple metrics.}
}
@article{KRUIJVER2022102748,
title = {The number of alleles in DNA mixtures with related contributors},
journal = {Forensic Science International: Genetics},
volume = {61},
pages = {102748},
year = {2022},
issn = {1872-4973},
doi = {https://doi.org/10.1016/j.fsigen.2022.102748},
url = {https://www.sciencedirect.com/science/article/pii/S1872497322000898},
author = {Maarten Kruijver and James M. Curran},
keywords = {DNA mixtures, Probabilistic genotyping},
abstract = {The maximum allele count (MAC) across loci and the total allele count (TAC) are often used to gauge the number of contributors to a DNA mixture. Computational strategies that predict the total number of alleles in a mixture arising from a certain number of contributors of a given population have been developed. Previous work considered the restricted case where all of the contributors to a mixture are unrelated. We relax this assumption and allow mixture contributors to be related according to a pedigree. We introduce an efficient computational strategy. This strategy based on first determining a probability distribution on the number of independent alleles per locus, and then conditioning on this distribution to compute a distribution of the number of distinct alleles per locus. The distribution of the number of independent alleles per locus is obtained by leveraging the Identical by Descent (IBD) pattern distribution which can be computed from the pedigree. We explain how allelic dropout and a subpopulation correction can be accounted for in the calculations.}
}
@incollection{NEWMAN2020183,
title = {Chapter 7 - Cognitive developmental theories},
editor = {Barbara M. Newman and Philip R. Newman},
booktitle = {Theories of Adolescent Development},
publisher = {Academic Press},
pages = {183-211},
year = {2020},
isbn = {978-0-12-815450-2},
doi = {https://doi.org/10.1016/B978-0-12-815450-2.00007-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128154502000073},
author = {Barbara M. Newman and Philip R. Newman},
keywords = {Problem solving, Equilibrium, Schemes, Adaptation, Egocentrism, Formal operational thought, Moral reasoning, Social reasoning, Metacognition, Education},
abstract = {The chapter focuses on cognitive developmental theories which address the emerging nature of concept formation, reasoning, planning, and problem solving, and the increasingly complex structures that support changing and flexible capacities for thinking about multidimensional problems with probabilistic outcomes. This chapter summarizes the work of Jean Piaget and the extension of his ideas among neo-Piagetian theorists including Deanna Kuhn, Paul Klacziynski, and Robbie Case. The following key concepts are explained: equilibrium, schemes, organization, adaptation, stages of development, and egocentrism. The stage of Formal Operational Reasoning and elaboration of cognitive capacities in adolescence are described in detail. Application of these theories to moral reasoning, social reasoning, metacognition, and educational initiatives are discussed. Experimental approaches and paper and pencil measures of cognitive reasoning are described. The strengths and limitations of cognitive developmental theories are reviewed.}
}
@article{KOCHEN1958267,
title = {The acquisition and utilization of information in problem solving and thinking},
journal = {Information and Control},
volume = {1},
number = {3},
pages = {267-288},
year = {1958},
issn = {0019-9958},
doi = {https://doi.org/10.1016/S0019-9958(58)80005-4},
url = {https://www.sciencedirect.com/science/article/pii/S0019995858800054},
author = {Manfred Kochen and Eugene H. Galanter},
abstract = {Some of the logical consequences of drawing a distinction between the following two aspects of problem-solving behavior are explored: (a) actions directed toward the acquisition of information to guide future actions toward valuable goals; (b) actions directed toward the utilization of accumulated information to attain a valuable goal. An experimental paradigm accomplishing this separation is described for the case of an environment of periodic sequences of binary events. A general way of describing behavioral strategies is developed in terms of: (a) a plan for when to acquire information, to guess an outcome, or to guess at the solution; and (b) a program for how to compute guesses from the information accumulated. The structure of the binary environmental sequences, the structure of these behavioral strategies, and the relations between them are analyzed, and certain strategies which maximize value are suggested. Computing machine interpretations of certain specific strategies for a restricted kind of experiment are displayed, and predictions from these are compared with experimental data from pilot studies performed with human subjects.}
}
@incollection{CANCES20033,
title = {Computational quantum chemistry: A primer},
series = {Handbook of Numerical Analysis},
publisher = {Elsevier},
volume = {10},
pages = {3-270},
year = {2003},
booktitle = {Special Volume, Computational Chemistry},
issn = {1570-8659},
doi = {https://doi.org/10.1016/S1570-8659(03)10003-8},
url = {https://www.sciencedirect.com/science/article/pii/S1570865903100038},
author = {Eric Cancès and Mireille Defranceschi and Werner Kutzelnigg and Claude {Le Bris} and Yvon Maday},
abstract = {Publisher Summary
This chapter discusses basic modeling. The chapter illustrates that quantum chemistry aims at understanding the properties of matter through the modeling of its behavior at a subatomic scale, where matter is described as an assembly of nuclei and electrons. At this scale, the equation that rules the interactions between these constitutive elements is the Schrödinger equation. It can be considered as a universal model for at least three reasons. First, it contains all the physical information of the system under consideration so that any of the properties of this system can be deduced in theory from the Schrödinger equation associated to it. Second, the Schrödinger equation does not involve any empirical parameter, except some fundamental constants of Physics; it can thus be written for any kind of molecular system provided its chemical composition, in terms of natures of nuclei and number of electrons, is known. Third, this model enjoys remarkable predictive capabilities, as confirmed by comparisons with a large amount of experimental data of various types.}
}
@article{DING2025121721,
title = {Endogenous dynamics of rumor spreading and debunking considering the influence of attitude: An agent-based modeling approach},
journal = {Information Sciences},
volume = {694},
pages = {121721},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121721},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524016359},
author = {Haixin Ding},
keywords = {Rumor, Rumor debunking, Innovation diffusion, Social Judgement theory, Balance theory, Agent-based Modelling},
abstract = {Rumor and debunking help create one another, but their endogenous dynamics are still worth further exploration. Relatedly, although attitude is important during the communication process, this indispensable element has been underrepresented for rumors as a specific kind of communication. The study first uses the balance logic to reveal the fundamental influence of attitude on rumor-related interactions at the individual level; Based on the innovation diffusion perspective and social judgment theory, the study constructs a conceptual framework and mathematical models of endogenous dynamics of rumor spreading and debunking considering the influence of attitude. Using an agent-based modeling approach, the study conducts systematic computational experiments. The results show that both attitude distribution and attitude interaction structure influence the endogenous dynamics, and the two types of factors interact with each other. The study explains debunking without official rebuttal endogenously, exposes debunking may not result in the intended outcome, and illustrates focusing only on rumor spreaders would always underestimate the impact of rumors systematically.}
}
@article{BISWAL2024110150,
title = {Unlocking the potential of signature-based drug repurposing for anticancer drug discovery},
journal = {Archives of Biochemistry and Biophysics},
volume = {761},
pages = {110150},
year = {2024},
issn = {0003-9861},
doi = {https://doi.org/10.1016/j.abb.2024.110150},
url = {https://www.sciencedirect.com/science/article/pii/S0003986124002728},
author = {Sruti Biswal and Bibekanand Mallick},
keywords = {Cancer, Anticancer drug, Drug repurposing, Gene signature},
abstract = {Cancer is the leading cause of death worldwide and is often associated with tumor relapse even after chemotherapeutics. This reveals malignancy is a complex process, and high-throughput omics strategies in recent years have contributed significantly in decoding the molecular mechanisms of these complex events in cancer. Further, the omics studies yield a large volume of cancer-specific molecular signatures that promote the discovery of cancer therapy drugs by a method termed signature-based drug repurposing. The drug repurposing method identifies new uses for approved drugs beyond their intended initial therapeutic use, and there are several approaches to it. In this review, we discuss signature-based drug repurposing in cancer, how cancer omics have revolutionized this method of drug discovery, and how one can use the cancer signature data for repurposed drug identification by providing a step-by-step procedural handout. This modern approach maximizes the use of existing therapeutic agents for cancer therapy or combination therapy to overcome chemotherapeutics resistance, making it a pragmatic and efficient alternative to traditional resource-intensive and time-consuming methods.}
}
@article{IBEZIM2024e02226,
title = {Potential dual inhibitors of Hexokinases and mitochondrial complex I discovered through machine learning approach},
journal = {Scientific African},
volume = {24},
pages = {e02226},
year = {2024},
issn = {2468-2276},
doi = {https://doi.org/10.1016/j.sciaf.2024.e02226},
url = {https://www.sciencedirect.com/science/article/pii/S2468227624001728},
author = {Akachukwu Ibezim and Emmanuel Onah and Sochi Chinaemerem Osigwe and Peter Ukwu Okoroafor and Onyeoziri Pius Ukoha and Jair Lage {de Siqueira-Neto} and Fidele Ntie-Kang and Karuppasamy Ramanathan},
keywords = {Hexokinases, Mitochondrial complex I, Cancer, MACCS fingerprints, Boruta algorithms, Machine learning, Metabolic plasticity},
abstract = {Hexokinases (Hks) and mitochondrial complex I (MCI) are involved in the energy metabolism of cells; glycolysis/fermentation and oxidative phosphorylation. Both Hks and MCI are known to play critical roles in either division of metabolic plasticity which enables tumor progression and proliferation in the presence of chemotherapies. Therefore, targeting these enzymes are important in cancer drug resistance. Here, computational models for the prediction of inhibition of Hks were developed based on experimental data and an optimal feature subset that was selected by the Boruta algorithm (a wrapper feature selection algorithm coupled with random forest). Out of the seven models that were explored, a random forest classifier gave the best prediction (GA = 0.84, FNR = 0.12 and AUC = 0.96 for the external dataset). Fragmentation analysis led to the identification of the unique structural scaffolds that characterize hexokinase inhibitors and non-inhibitors. The best Hks inhibition model predicted that 23 molecules out of the 191 dataset of MCI actives (IC50 ≤ 10 µM) that were screened, have more than 60 % probability of exhibiting Hk inhibitory activity. Hence, they are possible dual inhibitors of both targets. Furthermore, the 23 molecules’ core structures are members of the scaffolds that are unique to Hk inhibitors earlier predicted by fragment analysis. The need for dual targeting agents in cancer therapy, particularly in combating cancer drug resistance, highlights the relevance of these findings.}
}
@article{LABO2018185,
title = {Application of low-invasive techniques and incremental seismic rehabilitation to increase the feasibility and cost-effectiveness of seismic interventions},
journal = {Procedia Structural Integrity},
volume = {11},
pages = {185-193},
year = {2018},
note = {XIV INTERNATIONAL CONFERENCE ON BUILDING PATHOLOGY AND CONSTRUCTIONS REPAIR, FLORENCE, ITALY, JUNE 20-22, 2018},
issn = {2452-3216},
doi = {https://doi.org/10.1016/j.prostr.2018.11.025},
url = {https://www.sciencedirect.com/science/article/pii/S2452321618301264},
author = {S. Labò and E. Casprini and C. Passoni and J. Zanni and A. Belleri and A. Marini and P. Riva},
keywords = {Incremental rehabilitation, seismic retrofit, renovation strategy, low-invasive techniques, life cycle thinking, diagrid, school buildings},
abstract = {The high seismic risk connected to the existing construction heritage requires a wide-scale renovation action to ensure structural resilience and avoid future human and economic losses. Given the urgency and the scale of the problem and the lack of available resources, a new strategy for the renovation of the obsolete European building stock should be envisioned, accounting for both safety and environmental, social and economic sustainability. This research aims at exploring new cost-effective seismic retrofit solutions based on the principles of low-invasiveness and incremental seismic rehabilitation, as envisioned by FEMA P-420 (2009). The incremental rehabilitation approach allows to plan repair and retrofit actions along with the maintenance works expected during the building's lifetime, thereby spreading them in time and reducing costs. In addition, low-invasiveness of the solutions is required to reduce the impacts on the functionality of the building, thus cutting the costs connected to downtime. A possible solution is represented by the introduction of an exoskeleton entirely carried out from outside. In this paper, a new sustainable technique is proposed, where the existing structure is connected to a self-supporting exoskeleton adopting demountable dry techniques, which may be assembled and activated in different phases of the building lifetime. As a proof of concept, the approach is then applied to a school building.}
}
@article{ZHANG2025115558,
title = {Opportunities of applying Large Language Models in building energy sector},
journal = {Renewable and Sustainable Energy Reviews},
volume = {214},
pages = {115558},
year = {2025},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2025.115558},
url = {https://www.sciencedirect.com/science/article/pii/S136403212500231X},
author = {Liang Zhang and Zhelun Chen},
keywords = {Large language models, Building energy efficiency, Building decarbonization, Knowledge extraction, Intelligent control systems, Data infrastructure, Education and training},
abstract = {In recent years, the rapid advancement and impressive capabilities of Large Language Models have been evident across various engineering domains. This paper explores the application, implications, and potential of Large Language Models in building energy sectors, especially energy efficiency and decarbonization studies, based on an extensive literature review and a survey from building engineers and scientists. The paper explores how LLMs can enhance intelligent control systems, automate code generation for software and modeling tools, optimize data infrastructure, and refine analysis of technical reports and papers. Additionally, the paper discusses the role of LLMs in improving regulatory compliance, supporting building lifecycle management, and revolutionizing education and training practices within the sector. Despite the promising potential of Large Language Models, challenges including complex and expensive computation, data privacy, security and copyright, complexity in fine-tuned Large Language Models, and self-consistency are discussed. The paper concludes with a call for future research focused on the enhancement of LLMs for domain-specific tasks, multi-modal LLMs, and collaborative research between AI and energy experts.}
}
@article{VALJAK2023191,
title = {Functional modelling through Function Class Method: A case from DfAM domain},
journal = {Alexandria Engineering Journal},
volume = {66},
pages = {191-209},
year = {2023},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2022.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1110016822007852},
author = {Filip Valjak and Nenad Bojčetić},
keywords = {Functional modelling, Function structure, Function class, Design for Additive Manufacturing},
abstract = {Functional modelling is an essential part of systematic design approaches and is often prescribed in engineering design textbooks. However, function models created with current function modelling techniques often lack formal and repeatable representation, limiting their use in computational reasoning. Therefore, this paper presents a new functional modelling method to support function models' creation with formal and repeatable representation. The key element of the proposed method is a Function Class – a function-modelling element that categorises defined functions on a function block level by specifying operating flow, input and output flows, and integrates primary rules for functional modelling such as conservation law. The formalisation on a function block level reduces the number of morphological errors and provides a theoretical framework for future computational processing of function models. This paper proposes a protocol for developing Function Classes and defines a theoretical function modelling framework through Function Class Method. The development and use of the Function Class Method are demonstrated through the development of Function Classes for the Design for Additive Manufacturing domain as the first step toward a universal function modelling approach.}
}
@article{WOLFENGAGEN2016353,
title = {Concordance in the Crowdsourcing Activity},
journal = {Procedia Computer Science},
volume = {88},
pages = {353-358},
year = {2016},
note = {7th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2016, held July 16 to July 19, 2016 in New York City, NY, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.07.448},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916317045},
author = {Viacheslav E. Wolfengagen and Larisa Yu. Ismailova and Sergey Kosikov},
keywords = {crowdsourcing, Big Data, Thick Data, variable domains, cognition model, concordance, computational model},
abstract = {A concordance in cognition activity of possibly interrelated crowdsourcers aimed to property recognition in the voluminous data sources is considered. Data sources are of either usual nature or manually generated with the crowdsourcing. The proposed model is based on the variable domains assumption. A general layout is able to take into account an interaction of crowdsourcers and properties when they are varying with the evolving the events. The cognition model is of stage-by-stage type and has the representable functor. This model as may be shown is faithfully embedded into a category of indexed sets. Using the proposed neighborhood for cognition activity leads to a flexible computing model.}
}
@article{MAO2025102712,
title = {A survey on pragmatic processing techniques},
journal = {Information Fusion},
volume = {114},
pages = {102712},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102712},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524004901},
author = {Rui Mao and Mengshi Ge and Sooji Han and Wei Li and Kai He and Luyao Zhu and Erik Cambria},
keywords = {Pragmatic processing, Metaphor understanding, Sarcasm detection, Personality recognition, Aspect extraction, Sentiment polarity detection},
abstract = {Pragmatics, situated in the domains of linguistics and computational linguistics, explores the influence of context on language interpretation, extending beyond the literal meaning of expressions. It constitutes a fundamental element for natural language understanding in machine intelligence. With the advancement of large language models, the research focus in natural language processing has predominantly shifted toward high-level task processing, inadvertently downplaying the importance of foundational pragmatic processing tasks. Nevertheless, pragmatics serves as a crucial medium for unraveling human language cognition. The exploration of pragmatic processing stands as a pivotal facet in realizing linguistic intelligence. This survey encompasses important pragmatic processing techniques for subjective and emotive tasks, such as personality recognition, sarcasm detection, metaphor understanding, aspect extraction, and sentiment polarity detection. It spans theoretical research, the forefront of pragmatic processing techniques, and downstream applications, aiming to highlight the significance of these low-level tasks in advancing natural language understanding and linguistic intelligence.}
}
@article{DOSSOU2021476,
title = {Development of a decision support tool for sustainable urban logistics optimization},
journal = {Procedia Computer Science},
volume = {184},
pages = {476-483},
year = {2021},
note = {The 12th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 4th International Conference on Emerging Data and Industry 4.0 (EDI40) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.03.060},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921006918},
author = {Paul-Eric Dossou and Axel Vermersch},
keywords = {Type your keywords here, separated by semicolons},
abstract = {Traffic flows are increasing in cities, partially due to congestions provoked by trucks. These congestions cause many problems such as pollution (gasoil, Carbon, etc.), noise, waste of time. Indeed, cities like Paris, Hamburg, Milan, and “Grand Paris Sud” conurbation are thinking about a sustainable alternative solution to road transportation. Then, a research based on co-creation methodology integrating all stakeholders (local authorities, companies, citizens) for elaborating an alternative solution to road transportation has been defined. This paper presents the architecture and the development of a decision aided tool for simulating and optimizing alternative solutions to road transportation}
}
@article{ESHAGHI2024107342,
title = {Methods for enabling real-time analysis in digital twins: A literature review},
journal = {Computers & Structures},
volume = {297},
pages = {107342},
year = {2024},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2024.107342},
url = {https://www.sciencedirect.com/science/article/pii/S0045794924000713},
author = {Mohammad Sadegh Es-haghi and Cosmin Anitescu and Timon Rabczuk},
abstract = {This paper presents a literature review on methods for enabling real-time analysis in digital twins, which are virtual models of physical systems. The advantages of digital twins are numerous, including cost reduction, risk mitigation, efficiency enhancement, and decision-making support. However, their implementation faces challenges such as the need for real-time data analysis, resource limitations, and data uncertainty. The paper focuses on methods for reducing computational demands, which have not been systematically discussed in the literature. The paper reviews and categorizes methods and tools for accelerating the modeling of physical phenomena and reducing the computational needs of digital twins.}
}
@article{TAPIA2019170,
title = {Design of biomass value chains that are synergistic with the food–energy–water nexus: Strategies and opportunities},
journal = {Food and Bioproducts Processing},
volume = {116},
pages = {170-185},
year = {2019},
issn = {0960-3085},
doi = {https://doi.org/10.1016/j.fbp.2019.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0960308519300641},
author = {John Frederick D. Tapia and Sheila Samsatli and Stephen S. Doliente and Elias Martinez-Hernandez and Wan Azlina Binti Wan Ab Karim Ghani and Kean Long Lim and Helmi Zulhaidi Mohd Shafri and Nur Shafira Nisa Binti Shaharum},
keywords = {Biomass value chains (BVCs), Food–energy–water (FEW) nexus, Mathematical modelling, Biomass supply chains, Optimisation, Process systems engineering, Sustainable land use, Bioenergy},
abstract = {Humanity’s future sustainable supply of energy, fuels and materials is aiming towards renewable sources such as biomass. Several studies on biomass value chains (BVCs) have demonstrated the feasibility of biomass in replacing fossil fuels. However, many of the activities along the chain can disrupt the food–energy–water (FEW) nexus given that these resource systems have been ever more interlinked due to increased global population and urbanisation. Essentially, the design of BVCs has to integrate the systems-thinking approach of the FEW nexus; such that, existing concerns on food, water and energy security, as well as the interactions of the BVCs with the nexus, can be incorporated in future policies. To date, there has been little to no literature that captures the synergistic opportunities between BVCs and the FEW nexus. This paper presents the first survey of process systems engineering approaches for the design of BVCs, focusing on whether and how these approaches considered synergies with the FEW nexus. Among the surveyed mathematical models, the approaches include multi-stage supply chain, temporal and spatial integration, multi-objective optimisation and uncertainty-based risk management. Although the majority of current studies are more focused on the economic impacts of BVCs, the mathematical tools can be remarkably useful in addressing critical sustainability issues in BVCs. Thus, future research directions must capture the details of food–energy–water interactions with the BVCs, together with the development of more insightful multi-scale, multi-stage, multi-objective and uncertainty-based approaches.}
}
@article{ULRICH1988309,
title = {Computation and conceptual design},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {4},
number = {3},
pages = {309-315},
year = {1988},
note = {Special Issue Manufacturing Systems and Technology of the Future},
issn = {0736-5845},
doi = {https://doi.org/10.1016/0736-5845(88)90002-6},
url = {https://www.sciencedirect.com/science/article/pii/0736584588900026},
author = {Karl Ulrich and Warren Seering},
abstract = {Design is the transformation between a functional and a structural description of a device. Conceptual design is the initial stage of this transformation. We hypothesize that most new design are derived from knowledge of existing designs. We identify a special case of this process and call it novel combination. By describing an implemented program which designs novel mechanical fasteners, we explain how knowledge of existing devices can be represented and used. We highlight the issues arising from this implementation and propose four areas of future research. This work is important for establishing a fundamental understanding of conceptual design, leading to enhanced design teaching and better design tools.}
}
@article{LEITE20231,
title = {Interval incremental learning of interval data streams and application to vehicle tracking},
journal = {Information Sciences},
volume = {630},
pages = {1-22},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.02.027},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523002165},
author = {Daniel Leite and Igor Škrjanc and Sašo Blažič and Andrej Zdešar and Fernando Gomide},
keywords = {Granular machine learning, Online learning, Granular computing, Interval analysis, Data stream},
abstract = {This paper presents a method called Interval Incremental Learning (IIL) to capture spatial and temporal patterns in uncertain data streams. The patterns are represented by information granules and a granular rule base with the purpose of developing explainable human-centered computational models of virtual and physical systems. Fundamentally, interval data are either included into wider and more meaningful information granules recursively, or used for structural adaptation of the rule base. An Uncertainty-Weighted Recursive-Least-Squares (UW-RLS) method is proposed to update affine local functions associated with the rules. Online recursive procedures that build interval-based models from scratch and guarantee balanced information granularity are described. The procedures assure stable and understandable rule-based modeling. In general, the model can play the role of a predictor, a controller, or a classifier, with online sample-per-sample structural adaptation and parameter estimation done concurrently. The IIL method is aligned with issues and needs of the Internet of Things, Big Data processing, and eXplainable Artificial Intelligence. An application example concerning real-time land-vehicle localization and tracking in an uncertain environment illustrates the usefulness of the method. We also provide the Driving Through Manhattan interval dataset to foster future investigation.}
}
@article{GERSHMAN2020104394,
title = {Origin of perseveration in the trade-off between reward and complexity},
journal = {Cognition},
volume = {204},
pages = {104394},
year = {2020},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2020.104394},
url = {https://www.sciencedirect.com/science/article/pii/S0010027720302134},
author = {Samuel J. Gershman},
keywords = {Decision making, Information theory, Reinforcement learning},
abstract = {When humans and other animals make repeated choices, they tend to repeat previously chosen actions independently of their reward history. This paper locates the origin of perseveration in a trade-off between two computational goals: maximizing rewards and minimizing the complexity of the action policy. We develop an information-theoretic formalization of policy complexity and show how optimizing the trade-off leads to perseveration. Analysis of two data sets reveals that people attain close to optimal trade-offs. Parameter estimation and model comparison supports the claim that perseveration quantitatively agrees with the theoretically predicted functional form (a softmax function with a frequency-dependent action bias).}
}
@article{QAMMAR2023e16230,
title = {Statistical analysis of the university sustainability in the higher education institution a case study from the Khyber Pakhtunkhwa province in Pakistan},
journal = {Heliyon},
volume = {9},
number = {5},
pages = {e16230},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e16230},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023034370},
author = {Naseha Wafa Qammar and Zohaib Ur {Rehman Afridi} and Shamaima Wafa Qammar},
keywords = {Sustainability, Higher education institute, Students and faculty, Pakistan},
abstract = {Educational institutions can incorporate the idea of sustainability at the grass root level for any society. This study is part of an effort to get insight into the campus sustainability in one of the Higher Education Institution (HEI) in the Khyber Pakhtunkhwa region of Pakistan. Aim is to investigation university students' and faculty members insight regarding sustainability. Thus, questionnaire-based survey followed by statistical inference was conducted for the potential outcomes. The questionnaire is comprised of 24 questions, 05 of which are on demographics and the remaining 19 are about sustainability. The sustainability related questions focused mostly on the respondents' knowledge, understanding, and interest in sustainability. A handful of the other questions in the questionnaire were tailored to the university input to achieve sustainability. The dataset is manipulated with basic statistical and computational approaches, and the results are analyzed using mean values. The mean values are further classified into flag values of 0 and 1. Flag value 1 indicates a good marker of the received response, while flag value 0 indicates the least amount of information included in responses. The results show that respondents' knowledge, awareness, interest, and engagement in sustainability are significantly sufficient, as we obtained a flag value of 1 for all questions about sustainability. The study's findings, on the other hand, indicated that the institution is lagging in terms of supporting, disseminating, and implementing campus-wide sustainability-related activities. This study is one of the first initiatives to provide a baseline dataset and substantial information to go a step further in achieving the bottom-line target of being and acting sustainable in the HEI.}
}
@article{199064,
title = {Natural languages: Berwick, R ‘Natural language computational complexity and generative capacity’ Comput. Artif. Intell. Vol 8 No 5 (1989) pp 423–441},
journal = {Knowledge-Based Systems},
volume = {3},
number = {1},
pages = {64},
year = {1990},
issn = {0950-7051},
doi = {https://doi.org/10.1016/0950-7051(90)90091-U},
url = {https://www.sciencedirect.com/science/article/pii/095070519090091U}
}
@article{MARKMAN20181,
title = {Combining the Strengths of Naturalistic and Laboratory Decision-Making Research to Create Integrative Theories of Choice},
journal = {Journal of Applied Research in Memory and Cognition},
volume = {7},
number = {1},
pages = {1-10},
year = {2018},
issn = {2211-3681},
doi = {https://doi.org/10.1016/j.jarmac.2017.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S2211368117301778},
author = {Arthur B. Markman},
keywords = {Decision making, Naturalistic decision making, External validity, Internal validity},
abstract = {Naturalistic decision-making research contrasts with traditional laboratory research along a number of dimensions. It is typically more observational, more focused on expert performance, and more attentive to the context in which decisions are made than laboratory studies. This approach helps to shore up some of the weaknesses of laboratory research by providing incentive to develop integrative theories of choice and examining strong methods of problem solving in a choice domain. This paper contrasts the strengths and weaknesses of laboratory and naturalistic approaches to decision making. Then, it explores strategies for using both of these approaches as well as mathematical and computational modeling to find the optimal tradeoff between internal and external validity for research projects.}
}
@article{LUPION2025112832,
title = {A holistic approach for resource-constrained neural network architecture search},
journal = {Applied Soft Computing},
volume = {172},
pages = {112832},
year = {2025},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2025.112832},
url = {https://www.sciencedirect.com/science/article/pii/S1568494625001437},
author = {M. Lupión and N.C. Cruz and E.M. Ortigosa and P.M. Ortigosa},
keywords = {Artificial neural networks, Neural architecture search, Meta-heuristic, TLBO, Neural network encoding, Performance predictor},
abstract = {The design of Artificial Neural Networks (ANN) is critical for their performance. The research field called Neural Network Search (NAS) investigates automated design strategies. This work proposes a novel NAS stack that stands out in three facets. First, the representation scheme encodes problem-specific ANN as plain vectors of numbers without needing auxiliary conversion models. Second, it is a pioneer in relying on the TLBO meta-heuristic. This optimizer supports large-scale problems and only expects two parameters, contrasting with other meta-heuristics used for NAS. Third, the stack includes a new evaluation predictor that avoids evaluating non-promising architectures. It combines several machine learning methods that train as the optimizer evaluates solutions, which avoids preliminary preparing this component and makes it self-adaptive. The proposal has been tested by using it to build a CIFAR-10 classifier while forcing the architecture to have fewer than 150,000 parameters, assuming that the resulting network must be deployed in a resource-constrained IoT device. The designs found with and without the predictor achieve validation accuracies of 78.68% and 80.65%, respectively. Both outperform a larger model from the recent literature. The predictor slightly constraints the evolution of solutions, but it approximately halves the computational effort. After extending the test to the CIFAR-100 dataset, the proposal achieves a validation accuracy of 65.43% with 478,006 parameters in its fastest configuration, competing with current results in the literature.}
}
@incollection{GORI2024339,
title = {Chapter 6 - Learning with constraints},
editor = {Marco Gori and Alessandro Betti and Stefano Melacci},
booktitle = {Machine Learning (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
pages = {339-442},
year = {2024},
isbn = {978-0-323-89859-1},
doi = {https://doi.org/10.1016/B978-0-32-389859-1.00013-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323898591000131},
author = {Marco Gori and Alessandro Betti and Stefano Melacci},
keywords = {Support constraint machines, Learning from constraints, Lifelong learning, Constraint satisfaction, Penalty functions, Logic constraints, t-norms, Recurrent neural networks, Long short term memory networks (LSTM), Graphical models},
abstract = {This chapter provides a unified view of learning and inference in structured environments that are formally expressed as constraints that involve both data and tasks. A preliminary discussion has been put forward in Section 1.1.5, where we began proposing an abstract interpretation of the ordinary notion of constraint that characterizes human-based learning, reasoning, and decision processes. Here we make an effort to formalize those processes and explore the corresponding computational aspects. A first fundamental remark for the formulation of a sound theory is that most interesting real-world problems correspond with learning environments that are heavily structured, a feature that has been mostly neglected in the previous chapters on linear and kernel machines, as well as on deep networks. So far we have been mostly concerned with machine learning models where the agent takes a decision on patterns represented by x∈Rd, whereas we have mostly neglected the issue of constructing appropriate representations from the environmental information e∈E. The discussion in Section 1.1.5 has already stimulated the need of processing information organized as lists, trees, and graphs. Interestingly, in this chapter, it is shown that computational models, like recurrent neural networks and graph neural networks can also be regarded as a way for expressing appropriate constraints on environmental data by means of diffusion processes. In these cases the distinguishing feature of the computational model is that the focus is on uniform diffusion processes, whereas one can think of constraints that involve both data and tasks in a more general way. Basically, different vertexes of a graph that model the environment can be involved in different relations, thus giving rise to a different treatment. As a result, this yields richer computational mechanisms that involve the meaning attached to the different relations.}
}
@article{ADHIKARI2025,
title = {The use of Monte Carlo simulation techniques in brachytherapy: A comprehensive literature review},
journal = {Brachytherapy},
year = {2025},
issn = {1538-4721},
doi = {https://doi.org/10.1016/j.brachy.2025.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S1538472125000364},
author = {Tirthraj Adhikari and Tomas Montenegro and Jae Won Jung and Courtney Oare and Gabriel Fonseca and Luc Beaulieu and Abdullah Alshreef and Clara Ferreira},
keywords = {Monte Carlo simulation, Dosimetry, Heterogeneity, Brachytherapy},
abstract = {ABSTRACT
Monte Carlo techniques have become crucial in brachytherapy since their introduction in the early 1980s, offering significant improvements in source parameter characterizations, and dose calculations. It provides precise dose distributions by modeling complex radiation interactions and can be determine doses in nonhomogeneous detailed cases. They are not affected by experimental artifacts, unlike traditional detectors, and can distinguish between primary and scatter dose components. However, MC techniques have limitations. They are susceptible to systematic errors and require thorough validation against experimental data, despite generally showing smaller standard deviations. Additionally, MC simulations can be computationally intensive and depend heavily on accurate input data and models. Recent research, including 1433 publications identified up to October 2024, highlights the ongoing development and application of MC techniques in brachytherapy. Of these, 426 articles met the inclusion criteria for relevance. This comprehensive review aims to help brachytherapy researchers to identify the appropriate MC code depending on the application in BT research. Of the forty-five MC codes used in BT, MCNP is noted as the most widely used MC code due to its robust modeling capabilities in various materials and geometries. AAPM TG-186 and TG-372 reports have recommended the use of model base dose calculation algorithms, since it can offer more accurate dose calculations over TG-43 formalism, particularly in heterogeneous tissues. Despite these recommendations, further research is needed to refine dosimetry for various isotopes, geometry and media. In essence, MC techniques have greatly enhanced the accuracy, precision and flexibility of brachytherapy techniques, though challenges such as systematic errors, heterogeneities corrections, and high computational demands remain. Continued research and development of MC codes and algorithms are essential for advancing the field and improving clinical outcomes.}
}
@article{COMPANY2009592,
title = {Computer-aided sketching as a tool to promote innovation in the new product development process},
journal = {Computers in Industry},
volume = {60},
number = {8},
pages = {592-603},
year = {2009},
note = {Computer Aided Innovation},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2009.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S016636150900133X},
author = {Pedro Company and Manuel Contero and Peter Varley and Nuria Aleixos and Ferran Naya},
keywords = {Engineering design and innovation, CAI software, Computer-aided sketching},
abstract = {Sketching is an established part of engineering culture. Sketches assist product designers during the creative stages of design and help them to develop inventions. Paper-and-pencil sketching is highly useful but lacks functionalities, mainly because it is disconnected from the rest of the (computer-aided) design process. However, CAS tools are not yet as usable as paper-and-pencil, although they provide full integration with the subsequent phases of the design processes (CAD, CAE, CAM, etc.) and other interesting functionalities. We desire computer-aided sketching (CAS) tools which furnish users with the sketching environment they require to make full use of their conceptual design and innovation talents, while providing full integration with the subsequent phases of the design processes (CAD, CAE, CAM, etc.). In this paper we discuss the importance of sketching in conceptual design, we review the current situation of engineering sketching, and we then analyze the main characteristics which a successful and fully integrated CAS tool should include. We consider CAS, not as a single problem, but as at least three: thinking, prescriptive and talking sketches require different approaches to functionality. Finally, we present the current state of the art in CAS tools by describing the main features and outstanding problems of our own applications.}
}
@article{BELLANTE2025104341,
title = {Evaluating the potential of quantum machine learning in cybersecurity: A case-study on PCA-based intrusion detection systems},
journal = {Computers & Security},
volume = {154},
pages = {104341},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2025.104341},
url = {https://www.sciencedirect.com/science/article/pii/S0167404825000306},
author = {Armando Bellante and Tommaso Fioravanti and Michele Carminati and Stefano Zanero and Alessandro Luongo},
keywords = {Quantum computing, Quantum machine learning, QML, Evaluation, Framework, Impact, PCA, Principal component analysis, Network intrusion detection, Network security},
abstract = {Quantum computing promises to revolutionize our understanding of the limits of computation, and its implications in cryptography have long been evident. Today, cryptographers are actively devising post-quantum solutions to counter the threats posed by quantum-enabled adversaries. Meanwhile, quantum scientists are innovating quantum protocols to empower defenders. However, the broader impact of quantum computing and quantum machine learning (QML) on other cybersecurity domains still needs to be explored. In this work, we investigate the potential impact of QML on cybersecurity applications of traditional ML. First, we explore the potential advantages of quantum computing in machine learning problems specifically related to cybersecurity. Then, we describe a methodology to quantify the future impact of fault-tolerant QML algorithms on real-world problems. As a case study, we apply our approach to standard methods and datasets in network intrusion detection, one of the most studied applications of machine learning in cybersecurity. Our results provide insight into the conditions for obtaining a quantum advantage and the need for future quantum hardware and software advancements.}
}
@article{YU2024107998,
title = {Bridging the gap: Geometry-centric discriminative manifold distribution alignment for enhanced classification in colorectal cancer imaging},
journal = {Computers in Biology and Medicine},
volume = {170},
pages = {107998},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.107998},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524000829},
author = {Weiwei Yu and Nuo Xu and Nuanhui Huang and Houliang Chen},
keywords = {Medical image analysis, Colorectal cancer detection, Domain adaptation, Transfer learning, Manifold learning, Computational oncology},
abstract = {The early detection of colorectal cancer (CRC) through medical image analysis is a pivotal concern in healthcare, with the potential to significantly reduce mortality rates. Current Domain Adaptation (DA) methods strive to mitigate the discrepancies between different imaging modalities that are critical in identifying CRC, yet they often fall short in addressing the complexity of cancer's presentation within these images. These conventional techniques typically overlook the intricate geometrical structures and the local variations within the data, leading to suboptimal diagnostic performance. This study introduces an innovative application of the Discriminative Manifold Distribution Alignment (DMDA) method, which is specifically engineered to enhance the medical image diagnosis of colorectal cancer. DMDA transcends traditional DA approaches by focusing on both local and global distribution alignments and by intricately learning the intrinsic geometrical characteristics present in manifold space. This is achieved without depending on the potentially misleading pseudo-labels, a common pitfall in existing methodologies. Our implementation of DMDA on three distinct datasets, involving several unique DA tasks, has consistently demonstrated superior classification accuracy and computational efficiency. The method adeptly captures the complex morphological and textural nuances of CRC lesions, leading to a significant leap in domain adaptation technology. DMDA's ability to reconcile global and local distributional disparities, coupled with its manifold-based geometrical structure learning, signals a paradigm shift in medical imaging analysis. The results obtained are not only promising in terms of advancing domain adaptation theory but also in their practical implications, offering the prospect of substantially improved diagnostic accuracy and faster clinical workflows. This heralds a transformative approach in personalized oncology care, aligning with the pressing need for early and accurate CRC detection.}
}
@article{RICAURTE2020102,
title = {Project-based learning as a strategy for multi-level training applied to undergraduate engineering students},
journal = {Education for Chemical Engineers},
volume = {33},
pages = {102-111},
year = {2020},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2020.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1749772820300464},
author = {Marvin Ricaurte and Alfredo Viloria},
keywords = {Project-based learning, Multi-level, Undergraduate students, Process engineering},
abstract = {This study presents a project-based learning methodology whose particularity is the inclusion of training at different levels of undergraduate engineering programs, which allows for the interaction among students from different semesters who work together on a common project. To show the applicability of the proposed methodology, a project for the industrial production of ethanol from sugar cane was considered. Students enrolled in Process Design (9th semester) and Computer-Assisted Technical Design (5th semester), courses included in the engineering programs offered by the Department of Chemical Engineering at Yachay Tech University (Ecuador), jointly developed it. The details of the project were presented to the students of the Introduction to Engineering course (3rd semester) to boost their interest in the engineering as applied science. The activities carried out in each of the courses are described in detail together with a description of how the learning outcomes were achieved thanks to the implementation of a multi-level training strategy. Teamwork and collaborative-integrated learning are the elements highlighted by the students who participated in the project. Some of the innovative aspects of the proposed methodology include professional training and multi-level learning, the development of logical thinking typical of engineers, the knowledge handover associated with the professional activities of process engineers engaged with real-world projects. Additionally, this methodology prizes the industrial experience that professors at the undergraduate level may have by allowing them to contribute with an engineering vision to the training of young people in engineering projects. This study was inspired by the principle of Constructive Alignment and by goal # 4 (quality education) of the 2030 Agenda for Sustainable Development.}
}
@article{MARIN2021100015,
title = {Human macrophage polarization in the response to Mycobacterium leprae genomic DNA},
journal = {Current Research in Microbial Sciences},
volume = {2},
pages = {100015},
year = {2021},
issn = {2666-5174},
doi = {https://doi.org/10.1016/j.crmicr.2020.100015},
url = {https://www.sciencedirect.com/science/article/pii/S2666517420300171},
author = {Alberto Marin and Kristopher {Van Huss} and John Corbett and Sangjin Kim and Jonathon Mohl and Bo-young Hong and Jorge Cervantes},
keywords = {RNAseq, , Leprosy, Macrophage polarization},
abstract = {Infection with Mycobacterium leprae, the causative organism of leprosy, is still endemic in numerous parts of the world including the southwestern United States. The broad variation of symptoms in the leprosy disease spectrum range from the milder tuberculoid leprosy (paucibacillary) to the more severe and disfiguring lepromatous leprosy (multibacillary). The established thinking in the health community is that host response, rather than M. leprae strain variation, is the reason for the range of disease severity. More recent discoveries suggest that macrophage polarization also plays a significant role in the spectrum of leprosy disease but to what degree it contributes is not fully established. In this study, we aimed to analyze if different strains of M. leprae elicit different transcription responses in human macrophages, and to examine the role of macrophage polarization in these responses. Genomic DNA from three different strains of M. leprae DNA (Strains NHDP, Br4923, and Thai-53) were used to stimulate human macrophages under three polarization conditions (M1, M1-activated, and M2). Transcriptome analysis revealed a large number of differentially expressed (DE) genes upon stimulation with DNA from M. leprae strain Thai-53 compared to strains NHDP and Br4923, independent of the macrophage polarization condition. We also found that macrophage polarization affects the responses to M. leprae DNA, with up-regulation of numerous interferon stimulated genes. These findings provide a deeper understanding of the role of macrophage polarization in the recognition of M. leprae DNA, with the potential to improve leprosy treatment strategies.}
}
@article{SREENATH1992121,
title = {A hybrid computation environment for multibody simulation},
journal = {Mathematics and Computers in Simulation},
volume = {34},
number = {2},
pages = {121-140},
year = {1992},
issn = {0378-4754},
doi = {https://doi.org/10.1016/0378-4754(92)90049-M},
url = {https://www.sciencedirect.com/science/article/pii/037847549290049M},
author = {N. Sreenath},
abstract = {A simulation architecture capable of generating the dynamical equations of a multibody system symbolically, automatically creating the computer code to simulate these equations numerically, run the simulation and display the results using animation and graphics is discussed. The power of object-oriented programming is used systematically to manipulate the symbolic, numeric and graphic modules and produce an effective tool for understanding the complicated motions of multibody systems. The architecture has been implemented in OOPSS (Object-Oriented Planar System Simulator) a software package written in a multilanguage (macsyma–fortran–lisp) environment. The package supports user interface capable of interactively modifying system parameters, change runtime initial conditions and introduce feedback control.}
}
@article{CARBONELL2016145,
title = {The role of metaphors in the development of technologies. The case of the artificial intelligence},
journal = {Futures},
volume = {84},
pages = {145-153},
year = {2016},
note = {SI: Metaphors in FS},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2016.03.019},
url = {https://www.sciencedirect.com/science/article/pii/S0016328715300902},
author = {Javier Carbonell and Antonio Sánchez-Esguevillas and Belén Carro},
keywords = {CLA, Metaphors, Artificial intelligence, Lakoff and Johnson},
abstract = {Technology plays a prominent role in configuring the way we live and work. In this paper we go further and think that it is a first level driver in the configuration of our deepest perceptions and has a paramount influence on shaping our worldviews and metaphors, though this aspect goes unnoticed for most of the population. In this paper we analyze how metaphors take action in the characterization of technologies, mainly emerging technologies, and in their evolution, and furthermore the impact of technologies and metaphors on the way we perceive our daily life. We analyze metaphors underlying brain nature and artificial intelligence, raising the connections between them and showing how metaphors in one of these fields impact on the way we understand the other. This fact has important consequences, for instance it conditions the evolution of computational systems, and we propose two scenarios for this evolution. This paper relies on the conceptual model and classification of metaphors proposed by Lakoff and Johnson in “Metaphors we live by”, from the orientational metaphors that show values and mantras, to the deepest structural metaphors that are reconfiguring how life is conceived. It also relies on CLA (Causal Layered Analysis) and to its reference book “CLA 2.0” in order to insert this analysis in a wider and future oriented framework and to analyze scenarios.}
}
@article{FAIRHALL2014ix,
title = {The receptive field is dead. Long live the receptive field?},
journal = {Current Opinion in Neurobiology},
volume = {25},
pages = {ix-xii},
year = {2014},
note = {Theoretical and computational neuroscience},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2014.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0959438814000361},
author = {Adrienne Fairhall},
abstract = {Advances in experimental techniques, including behavioral paradigms using rich stimuli under closed loop conditions and the interfacing of neural systems with external inputs and outputs, reveal complex dynamics in the neural code and require a revisiting of standard concepts of representation. High-throughput recording and imaging methods along with the ability to observe and control neuronal subpopulations allow increasingly detailed access to the neural circuitry that subserves neural representations and the computations they support. How do we harness theory to build biologically grounded models of complex neural function?}
}
@article{OKEREKE2014637,
title = {Virtual testing of advanced composites, cellular materials and biomaterials: A review},
journal = {Composites Part B: Engineering},
volume = {60},
pages = {637-662},
year = {2014},
issn = {1359-8368},
doi = {https://doi.org/10.1016/j.compositesb.2014.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S1359836814000109},
author = {M.I. Okereke and A.I. Akpoyomare and M.S. Bingley},
keywords = {A. Polymer–matrix composites (PMCs), C. Computational modelling, C. Numerical analysis, E. Weaving, Virtual testing},
abstract = {This paper documents the emergence of virtual testing frameworks for prediction of the constitutive responses of engineering materials. A detailed study is presented, of the philosophy underpinning virtual testing schemes: highlighting the structure, challenges and opportunities posed by a virtual testing strategy compared with traditional laboratory experiments. The virtual testing process has been discussed from atomistic to macrostructural length scales of analyses. Several implementations of virtual testing frameworks for diverse categories of materials are also presented, with particular emphasis on composites, cellular materials and biomaterials (collectively described as heterogeneous systems, in this context). The robustness of virtual frameworks for prediction of the constitutive behaviour of these materials is discussed. The paper also considers the current thinking on developing virtual laboratories in relation to availability of computational resources as well as the development of multi-scale material model algorithms. In conclusion, the paper highlights the challenges facing developments of future virtual testing frameworks. This review represents a comprehensive documentation of the state of knowledge on virtual testing from microscale to macroscale length scales for heterogeneous materials across constitutive responses from elastic to damage regimes.}
}
@article{SOPER2022126712,
title = {Quantifying the effect of solvent on the morphology of organic crystals using a statistical thermodynamics approach},
journal = {Journal of Crystal Growth},
volume = {591},
pages = {126712},
year = {2022},
issn = {0022-0248},
doi = {https://doi.org/10.1016/j.jcrysgro.2022.126712},
url = {https://www.sciencedirect.com/science/article/pii/S0022024822002007},
author = {Eleanor M. Soper and Radoslav Y. Penchev and Stephen M. Todd and Frank Eckert and Marc Meunier},
keywords = {A2. Solvent screening, A2. Particle engineering, A1. Surface chemistry, A1. Cosmo-RS, A1. Morphology},
abstract = {A method for predicting the effect of solvent on the morphology of organic crystals is presented, providing an efficient screening tool for identifying ideal crystallization solvents. The solvent effect is estimated by the computation of chemical potentials and activity coefficients of crystal surfaces using a first principles-based statistical thermodynamics approach. Density functional theory and COSMO-RS are utilized to determine the activity coefficients of the crystal growth faces of a selection of active pharmaceutical ingredients (APIs) in solvents across a broad range of polarities. The ability of COSMO-RS to predict and quantify the effects of solvent on crystal growth and morphology is assessed using hierarchical clustering to classify the solvents according to their overall interaction strength with the crystal faces. The COSMO-RS approach allows for a physical interpretation of the predictions in terms of surface polarity and is confirmed by comparison to published experimental data. Herein a methodology is reported for automated computation of the activity coefficients of all solvent-surface pairs directly from the drug crystal structure. The procedure goes beyond the traditional trial-and-error solvent selection process and has the potential to be used as a rapid computational screening tool in pharmaceutical drug development.}
}
@article{BALKHI20051223,
title = {Proteomics of Acute Myeloid Leukemia: Cytogenetic Risk Groups Differ Specifically in Their Proteome, Interactome and Posttranslational Protein Modifications.},
journal = {Blood},
volume = {106},
number = {11},
pages = {1223},
year = {2005},
issn = {0006-4971},
doi = {https://doi.org/10.1182/blood.V106.11.1223.1223},
url = {https://www.sciencedirect.com/science/article/pii/S0006497119761138},
author = {Mumtaz Y. Balkhi and Mulu Geletu and Maximilian Christopeit and Hermann M. Behre and Gerhard Behre},
abstract = {Acute Myeloid Leukemia (AML) is characterized by specific cytogenetic aberrations which are strong determinants of prognostic outcome and therapeutic response. Because the clinical outcome in AML cytogenetic groups differs considerably, we hypothesized that cytogenetic risk groups of AML might differ specifically in their proteome, protein interaction pathways and posttranslational modifications (PTMs). Thus, we determined the proteome of 30 AML patients belonging to various cytogenetic groups based on two-dimensional gel electrophoresis and Nano LC coupled MALDI-TOF-TOF tandem mass spectrometry. We could identify substantial differences in the proteome, protein expression and peak pattern between cytogenetic risk groups of AML. The interactome analysis based on computational bioinformatics using Ingenuity analysis revealed major regulating networks: MAPK8 and MYC for complex aberrant karyotype AML, TP53 for t(8;21)-AML, TP53- MYC- PRKAC for 11q23-AML, JUN and MYC for inv(16)-AML. Most interestingly, peak explorer analysis revealed a modification of O-linked acetyl glucosamine of hnRNPH1 in AML patients with a 11q23 translocation, an acetylation of calreticulin in t(8;21) translocation AML, an increased intensity of dimethylated peptide of hnRNPA2/B1 in AML patients with translocations of t(8;21) and inv(16) in comparison to healthy bone marrow. We show for the first time that cytogenetic risk groups of AML differ specifically both in their proteome, interactome and PTMs. These findings lead to a new thinking about the pathogenesis of AML and has major therapeutic implications because PTMs are the primary drug targets.}
}
@article{KRAUSE20211094,
title = {The challenge of ensuring affordability, sustainability, consistency, and adaptability in the common metrics agenda},
journal = {The Lancet Psychiatry},
volume = {8},
number = {12},
pages = {1094-1102},
year = {2021},
issn = {2215-0366},
doi = {https://doi.org/10.1016/S2215-0366(21)00122-X},
url = {https://www.sciencedirect.com/science/article/pii/S221503662100122X},
author = {Karolin Rose Krause and Sophie Chung and Maria da Luz {Sousa Fialho} and Peter Szatmari and Miranda Wolpert},
abstract = {Summary
Mental health research grapples with research waste and stunted field progression caused by inconsistent outcome measurement across studies and clinical settings, which means there is no common language for considering findings. Although recognising that no gold standard measures exist and that all existing measures are flawed in one way or another, anxiety and depression research is spearheading a common metrics movement to harmonise measurement, with several initiatives over the past 5 years recommending the consistent use of specific scales to allow read-across of measurements between studies. For this approach to flourish, however, common metrics must be acceptable and adaptable to a range of contexts and populations, and global access should be as easy and affordable as possible, including in low-income countries. Within a measurement landscape dominated by fixed proprietary measures and with competing views of what should be measured, achieving this goal poses a range of challenges. In this Personal View, we consider tensions between affordability, sustainability, consistency, and adaptability that, if not addressed, risk undermining the common metrics agenda. We outline a three-pronged way forward that involves funders taking more direct responsibility for measure development and dissemination; a move towards managing measure dissemination and adaptation via open-access measure hubs; and transitioning from fixed questionnaires to item banks. We argue that now is the time to start thinking of mental health metrics as 21st century tools to be co-owned and co-created by the mental health community, with support from dedicated infrastructure, coordinating bodies, and funders.}
}
@article{NAGANANDHINI2019548,
title = {Effective Diagnosis of Alzheimer’s Disease using Modified Decision Tree Classifier},
journal = {Procedia Computer Science},
volume = {165},
pages = {548-555},
year = {2019},
note = {2nd International Conference on Recent Trends in Advanced Computing ICRTAC -DISRUP - TIV INNOVATION , 2019 November 11-12, 2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.01.049},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920300570},
author = {S. Naganandhini and P. Shanmugavadivu},
keywords = {Alzheimer’s Disease, Feature Selection, Classification, Decision Tree, Early Detection, Hyper Parameter Tuning},
abstract = {Alzheimer’s disease (AD) is described as a severe form of the neural disorder that collectively degenerate the essential cognitive activities of a human being (thinking, memory retention, etc.,) in particular among the elderly individuals and eventually results in death. In addition to the adverse ill-health effects on the patients, AD imposes paramount responsibility and burden on the caretakers too. Several genetic and pathological traits and non-invasive diagnostic strategies are being vigorously investigated and explored to discover the early onset of this debilitating disease. The prognosis of AD assumes importance, as the deterioration of health due to its progression may be either contained or controlled. Moreover, early and accurate detection of AD helps medical practitioners to prescribe case-specific medical treatment procedure. Among the popular machine learning algorithms, decision tree technique is widely used for classification/prediction, due to its accuracy and speed.This research article presents a novel decision tree-based classification technique, with optimum hyper parametertuning, that is ideally suitable for AD diagnosis, even at the early stages of development. The performance of this newly proposed Decision Tree Classifier with Hyper Parameters Tuning (DTC-HPT) is validated on the Open Access Imaging Studies Series (OASIS) dataset that contains patients’ data on the different stages of AD. The DTC-HPT is designed with the primary objective to classify the nature of brain abnormality using the most relevantand potentially significant data attributes/parameters. The efficiency of DTC-HPT on AD classification is measured as Accuracy, Precision, Recall, and F1-Score. The correctness of AD classification by DTC-HPTwith an average accuracy of 99.10% endorse that this classification technique can be used for AD detection on the AD clinical datasets.}
}
@article{ROBSON2014287,
title = {When do aquatic systems models provide useful predictions, what is changing, and what is next?},
journal = {Environmental Modelling & Software},
volume = {61},
pages = {287-296},
year = {2014},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2014.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S1364815214000188},
author = {Barbara J. Robson},
keywords = {Modelling philosophy, Biogeochemical modelling, Ecological models, Developments, Progress, Knowledge gaps},
abstract = {This article considers how aquatic systems modelling has changed since 1995 and how it must change in future if we are to continue to advance. A distinction is made between mechanistic and statistical models, and the relative merits of each are considered. The question of “when do aquatic systems models provide accurate and useful predictions?” is addressed, implying some guidelines for model development. It is proposed that, in general, ecological models only provide management-relevant predictions of the behaviour of real systems when there are strong physical (as opposed to chemical or ecological) drivers. Developments over the past 15 years have included changes in technology, changes in the modelling community and changes in the context in which modelling is conducted: the implications of each are briefly discussed. Current trends include increased uptake of best practice guidelines, increasing integration of models, operationalisation, data assimilation, development of improved tools for skill assessment, and application of models to new management questions and in new social contexts. Deeper merging of statistical and mechanistic modelling approaches through such techniques as Bayesian Melding, Bayesian Hierarchical Modelling and surrogate modelling is identified as a key emerging area. Finally, it is suggested that there is a need to systematically identify areas in which our current models are inadequate. We do not yet know for which categories of problems well-implemented aquatic systems models can (or cannot) be expected to accurately predict observational data and system behaviour. This can be addressed through better modelling and publishing practices.}
}
@article{THEISE200417,
title = {Understanding cell lineages as complex adaptive systems},
journal = {Blood Cells, Molecules, and Diseases},
volume = {32},
number = {1},
pages = {17-20},
year = {2004},
note = {Stem Cell Plasticity},
issn = {1079-9796},
doi = {https://doi.org/10.1016/j.bcmd.2003.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S1079979603002523},
author = {Neil D Theise and Mark d'Inverno},
keywords = {Stem cells, Lineage system, Reactive systems},
abstract = {Stem cells may be considered complex reactive systems because of their vast number in a living system, their reactive nature, and the influence of local environmental factors (such as the state of neighboring cells, tissue matrix, stem cell physiological processes) on their behavior. In such systems, emergent global behavior arises through the multitude of local interactions among the cell agents. Approaching hematopoietic and other stem cell lineages from this perspective have critical ramifications on current thinking relating to the plasticity of these lineage systems, the modeling of stem cell systems, and the interpretation of clinical data regarding many diseases within such models.}
}
@article{LILWALL1989268,
title = {Seismological Algorithms, Computational Methods and Computer Programs: Durk J. Doornbos (Editor), Academic Press/Harcourt Brace Jovanovich, 1988, 469 pp., £39.50, ISBN 0-12-220770-X},
journal = {Physics of the Earth and Planetary Interiors},
volume = {58},
number = {2},
pages = {268-269},
year = {1989},
issn = {0031-9201},
doi = {https://doi.org/10.1016/0031-9201(89)90062-9},
url = {https://www.sciencedirect.com/science/article/pii/0031920189900629},
author = {R.C. Lilwall}
}
@article{DESCIOLI2011204,
title = {The omission effect in moral cognition: toward a functional explanation},
journal = {Evolution and Human Behavior},
volume = {32},
number = {3},
pages = {204-215},
year = {2011},
issn = {1090-5138},
doi = {https://doi.org/10.1016/j.evolhumbehav.2011.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S1090513811000055},
author = {Peter DeScioli and Rebecca Bruening and Robert Kurzban},
keywords = {Omission, Transparency, Moral judgment, Moral psychology},
abstract = {Moral judgment involves much more than computations of the expected consequences of behavior. A prime example of the complexity of moral thinking is the frequently replicated finding that violations by omission are judged less morally wrong than violations by commission, holding intentions constant. Here we test a novel hypothesis: Omissions are judged less harshly because they produce little material evidence of wrongdoing. Evidence is crucial because moral accusations are potentially very costly unless supported by others. In our experiments, the omission effect was eliminated when physical evidence showed that an omission was chosen. Perpetrators who “opted out” by pressing a button that would clearly have no causal effects on the victim, rather than rescuing them, were judged as harshly as perpetrators who directly caused death. These results show that, to reduce condemnation, omissions must not only be noncausal, they must also leave little or no material evidence that a choice was made.}
}
@article{NEMETH2024101385,
title = {The interplay between subcortical and prefrontal brain structures in shaping ideological belief formation and updating},
journal = {Current Opinion in Behavioral Sciences},
volume = {57},
pages = {101385},
year = {2024},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2024.101385},
url = {https://www.sciencedirect.com/science/article/pii/S2352154624000366},
author = {Dezső Németh and Teodóra Vékony and Gábor Orosz and Zoltán Sarnyai and Leor Zmigrod},
abstract = {History illustrates that economic crises and other sociopolitical threats often lead to a rise of polarization and radicalism, whereby people become more susceptible to intolerant political messages, including propaganda and ideological rhetoric. Political science, sociology, economics, and psychology have explored many dimensions of this phenomenon, yet a critical piece of the puzzle is still missing: what cognitive and neural mechanisms in the brain mediate between these threats and responsiveness to political messages? To answer this question, here, we present a theory that combines cognitive neuroscience theories, namely stress-induced memory shift and competitive cognitive processes, with political science. Our Threat-based Neural Switch Theory posits that the processing of political information, similarly to other information processing, is shaped by the competitive interaction between goal-directed and habitual processes. Threats, including resource overload or scarcity, can shift neural networks toward receptiveness to oversimplified political messages. This theory sets out a research program aimed at discovering the cognitive and neural underpinning of how situational factors alter brain functions and modify political information processing.}
}
@article{YU2022102230,
title = {Spatial processing rather than logical reasoning was found to be critical for mathematical problem-solving},
journal = {Learning and Individual Differences},
volume = {100},
pages = {102230},
year = {2022},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2022.102230},
url = {https://www.sciencedirect.com/science/article/pii/S1041608022001170},
author = {Mingxin Yu and Jiaxin Cui and Li Wang and Xing Gao and Zhanling Cui and Xinlin Zhou},
keywords = {Logical reasoning, Spatial processing, Mathematical problem-solving},
abstract = {Students' ability to solve mathematical problems is a standard mathematical skill; however, its cognitive correlates are unclear. Thus, this study aimed to examine whether spatial processing (mental rotation, paper folding, and the Corsi blocks test) and logical reasoning (abstract and concrete syllogisms) were correlated with mathematical problem-solving (word problems and geometric proofing) for college students. The regression results showed that after controlling for gender, age, general IQ, language processing, cognitive processing (visual perception, attention, and memory skills), and number sense and arithmetic computation skills, spatial processing skills still predicted mathematical problem-solving and geometry skills in Chinese college students. Contrastingly, logical reasoning measures related to syllogisms did not predict after controlling for these variables. Further, notably, it did not correlate significantly with geometry performance when no control variables were included. Our results suggest that spatial processing is a significant component of math skills involving word and geometry problems (even after controlling for multiple key cognitive factors).}
}
@incollection{HUANG2006586,
title = {Neo-Gricean Pragmatics},
editor = {Keith Brown},
booktitle = {Encyclopedia of Language & Linguistics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {586-590},
year = {2006},
isbn = {978-0-08-044854-1},
doi = {https://doi.org/10.1016/B0-08-044854-2/04529-6},
url = {https://www.sciencedirect.com/science/article/pii/B0080448542045296},
author = {Y. Huang},
keywords = {classical Gricean pragmatics, conversational implicature, division of pragmatic labor, explicature, Grice's circle, Horn-scales, I-impliciture, implicature, interaction between Q-, I, and M-implicatures, maxims, M-implicature, neo-Gricean pragmatics, pragmatic intrusion, pragmatics, presumptive meaning, Q-implicature, R-implicature},
abstract = {Since its inception, Gricean pragmatics has revolutionized pragmatic theorizing and has to date remained one of the cornerstones of contemporary thinking in linguistic pragmatics and the philosophy of language. This article undertakes to present and assess a neo-Gricean pragmatic theory of conversational implicature, focusing on the bipartite model developed by Laurence Horn and the tripartite model advanced by Stephen Levinson.}
}
@article{WU2025200236,
title = {HC-means Clustering Algorithm for Precision Marketing on E-commerce Platforms},
journal = {Systems and Soft Computing},
pages = {200236},
year = {2025},
issn = {2772-9419},
doi = {https://doi.org/10.1016/j.sasc.2025.200236},
url = {https://www.sciencedirect.com/science/article/pii/S2772941925000547},
author = {Dan Wu and Xin Liu},
keywords = {Big data, Cluster analysis, K-means algorithm, Precision marketing, Customer segmentation},
abstract = {Abstracts
With the rapid development of e-commerce industry, precision marketing has become a key means for enterprises to enhance competitiveness and profitability. However, traditional marketing methods often cannot accurately identify the characteristics of customers, leading to the waste of e-commerce resources. In this context, e-commerce enterprises urgently need a more accurate and efficient marketing method to meet the growing business needs. To this end, this study attempts to optimize the traditional K-means algorithm, and fundamentally improve the clustering effect in precision marketing by optimizing the selection of initial clustering centers and similarity measurement methods. Based on this, the research constructs an e-commerce marketing system based on HC-means algorithm to more accurately divide customer groups, identify high-value customers, potential customers and lost customers, and formulate differentiated marketing strategies for different groups. Experiments show that the average accuracy of HC-means algorithm in Glass database is 93.71, which is 15.48-15.79 higher than the highest accuracy of other two kinds of algorithms in the same kind of database. When the cluster number is 8, the Mahalanobis distance of HC-Means is reduced by 2.1 and 1.2 respectively compared with K-means and DBSCAN, which indicates that the clustering results are more reasonable in data distribution. When the cluster number is 3, more than half of the customers' consumption interval days are mainly concentrated between 8-12 days, and about 10% of the customers make purchases every 2 days. These accurate customer behavior insights provide a strong basis for marketing strategy development. To sum up, the HC-Means system constructed by the research has achieved remarkable results in e-commerce precision marketing, greatly improving user satisfaction, and providing a valuable reference scheme for e-commerce enterprises to optimize marketing mode and achieve sustainable development.}
}
@article{STROBL2024104585,
title = {Counterfactual formulation of patient-specific root causes of disease},
journal = {Journal of Biomedical Informatics},
volume = {150},
pages = {104585},
year = {2024},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2024.104585},
url = {https://www.sciencedirect.com/science/article/pii/S1532046424000030},
author = {Eric V. Strobl},
keywords = {Root cause analysis, Causal inference, Precision medicine, Causal discovery, Computational medicine},
abstract = {Objective:
Root causes of disease intuitively correspond to root vertices of a causal model that increase the likelihood of a diagnosis. This description of a root cause nevertheless lacks the rigorous mathematical formulation needed for the development of computer algorithms designed to automatically detect root causes from data. We seek a definition of patient-specific root causes of disease that models the intuitive procedure routinely utilized by physicians to uncover root causes in the clinic.
Methods:
We use structural equation models, interventional counterfactuals and the recently developed mathematical formalization of backtracking counterfactuals to propose a counterfactual formulation of patient-specific root causes of disease matching clinical intuition.
Results:
We introduce a definition of patient-specific root causes of disease that climbs to the third rung of Pearl’s Ladder of Causation and matches clinical intuition given factual patient data and a working causal model. We then show how to assign a root causal contribution score to each variable using Shapley values from explainable artificial intelligence.
Conclusion:
The proposed counterfactual formulation of patient-specific root causes of disease accounts for noisy labels, adapts to disease prevalence and admits fast computation without the need for counterfactual simulation.}
}
@article{MAJUMDAR2023100126,
title = {A novel approach for communicating with patients suffering from completely locked-in-syndrome (CLIS) via thoughts: Brain computer interface system using EEG signals and artificial intelligence},
journal = {Neuroscience Informatics},
volume = {3},
number = {2},
pages = {100126},
year = {2023},
issn = {2772-5286},
doi = {https://doi.org/10.1016/j.neuri.2023.100126},
url = {https://www.sciencedirect.com/science/article/pii/S2772528623000110},
author = {Sharmila Majumdar and Amin Al-Habaibeh and Ahmet Omurtag and Bubaker Shakmak and Maryam Asrar},
keywords = {Brain, CLIS, EEG, Feature extraction, ASPS, ANN, ALS, MND, AI},
abstract = {This paper investigates the development of an intelligent system method to address completely locked-in-syndrome (CLIS) that is caused by some illnesses such as Amyotrophic Lateral Sclerosis (ALS) as the most predominant type of Motor Neuron Disease (MND). In the last stages of ALS and despite the limitations in body movements, patients however will have a fully functional brain and cognitive capabilities and able to feel pain but fail to communicate. This paper aims to address the CLIS problem by utilizing EEG signals that human brain generates when thinking about a specific feeling or imagination as a way to communicate. The aim is to develop a low-cost and affordable system for patients to use to communicate with carers and family members. In this paper, the novel implementation of the ASPS (Automated Sensor and Signal Processing Selection) approach for feature extraction of EEG is presented to select the most suitable Sensory Characteristic Features (SCFs) to detect human thoughts and imaginations. Artificial Neural Networks (ANN) are used to verify the results. The findings show that EEG signals are able to capture imagination information that can be used as a means of communication; and the ASPS approach allows the selection of the most important features for reliable communication. This paper explains the implementation and validation of ASPS approach in brain signal classification for bespoke arrangement. Hence, future work will present the results of relatively high number of volunteers, sensors and signal processing methods.}
}
@article{FRADKIN20231013,
title = {Theory-Driven Analysis of Natural Language Processing Measures of Thought Disorder Using Generative Language Modeling},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {8},
number = {10},
pages = {1013-1023},
year = {2023},
note = {Natural Language Processing in Psychiatry and Clinical Neuroscience Research},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2023.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S2451902223001258},
author = {Isaac Fradkin and Matthew M. Nour and Raymond J. Dolan},
keywords = {Computational psychiatry, GPT-2, Natural language processing, Psychosis, Schizophrenia, Thought disorder},
abstract = {Background
Natural language processing (NLP) holds promise to transform psychiatric research and practice. A pertinent example is the success of NLP in the automatic detection of speech disorganization in formal thought disorder (FTD). However, we lack an understanding of precisely what common NLP metrics measure and how they relate to theoretical accounts of FTD. We propose tackling these questions by using deep generative language models to simulate FTD-like narratives by perturbing computational parameters instantiating theory-based mechanisms of FTD.
Methods
We simulated FTD-like narratives using Generative-Pretrained-Transformer-2 by either increasing word selection stochasticity or limiting the model’s memory span. We then examined the sensitivity of common NLP measures of derailment (semantic distance between consecutive words or sentences) and tangentiality (how quickly meaning drifts away from the topic) in detecting and dissociating the 2 underlying impairments.
Results
Both parameters led to narratives characterized by greater semantic distance between consecutive sentences. Conversely, semantic distance between words was increased by increasing stochasticity, but decreased by limiting memory span. An NLP measure of tangentiality was uniquely predicted by limited memory span. The effects of limited memory span were nonmonotonic in that forgetting the global context resulted in sentences that were semantically closer to their local, intermediate context. Finally, different methods for encoding the meaning of sentences varied dramatically in performance.
Conclusions
This work validates a simulation-based approach as a valuable tool for hypothesis generation and mechanistic analysis of NLP markers in psychiatry. To facilitate dissemination of this approach, we accompany the paper with a hands-on Python tutorial.}
}
@article{TSIGKINOPOULOU2017518,
title = {Respectful Modeling: Addressing Uncertainty in Dynamic System Models for Molecular Biology},
journal = {Trends in Biotechnology},
volume = {35},
number = {6},
pages = {518-529},
year = {2017},
note = {Special Issue: Computation and Modeling},
issn = {0167-7799},
doi = {https://doi.org/10.1016/j.tibtech.2016.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167779916302311},
author = {Areti Tsigkinopoulou and Syed Murtuza Baker and Rainer Breitling},
abstract = {Although there is still some skepticism in the biological community regarding the value and significance of quantitative computational modeling, important steps are continually being taken to enhance its accessibility and predictive power. We view these developments as essential components of an emerging ‘respectful modeling’ framework which has two key aims: (i) respecting the models themselves and facilitating the reproduction and update of modeling results by other scientists, and (ii) respecting the predictions of the models and rigorously quantifying the confidence associated with the modeling results. This respectful attitude will guide the design of higher-quality models and facilitate the use of models in modern applications such as engineering and manipulating microbial metabolism by synthetic biology.}
}
@article{VALLEETOURANGEAU2016195,
title = {Insight with hands and things},
journal = {Acta Psychologica},
volume = {170},
pages = {195-205},
year = {2016},
issn = {0001-6918},
doi = {https://doi.org/10.1016/j.actpsy.2016.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0001691816301755},
author = {Frédéric Vallée-Tourangeau and Sune Vork Steffensen and Gaëlle Vallée-Tourangeau and Miroslav Sirota},
keywords = {Problem solving, Insight, Task ecology, Enactivism, Methodological individualism},
abstract = {Two experiments examined whether different task ecologies influenced insight problem solving. The 17 animals problem was employed, a pure insight problem. Its initial formulation encourages the application of a direct arithmetic solution, but its solution requires the spatial arrangement of sets involving some degree of overlap. Participants were randomly allocated to either a tablet condition where they could use a stylus and an electronic tablet to sketch a solution or a model building condition where participants were given material with which to build enclosures and figurines. In both experiments, participants were much more likely to develop a working solution in the model building condition. The difference in performance elicited by different task ecologies was unrelated to individual differences in working memory, actively open-minded thinking, or need for cognition (Experiment 1), although individual differences in creativity were correlated with problem solving success in Experiment 2. The discussion focuses on the implications of these findings for the prevailing metatheoretical commitment to methodological individualism that places the individual as the ontological locus of cognition.}
}
@incollection{POULINDUBOIS2017653,
title = {Chapter 26 - The Development of Object Categories: What, When, and How?},
editor = {Henri Cohen and Claire Lefebvre},
booktitle = {Handbook of Categorization in Cognitive Science (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {San Diego},
pages = {653-671},
year = {2017},
isbn = {978-0-08-101107-2},
doi = {https://doi.org/10.1016/B978-0-08-101107-2.00027-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780081011072000270},
author = {Diane Poulin-Dubois and Sabina Pauen},
keywords = {Categorization, development, infant, child, categories, perceptual, conceptual},
abstract = {From birth, infants are exposed to a wealth of information from their surroundings. This makes early categorization abilities especially important for infants and children to process information and come to understand the world around them. As a result of several sophisticated experimental paradigms, it is well-established that early categorization abilities become refined over the developmental trajectory. Researchers have identified a global-to-basic shift in early categorical thinking, such that preverbal infants discriminate between global-level categories (i.e., dogs, cats, chairs, tables, etc.) before basic-level categories (i.e., different breeds of cats and dogs). However, differences in the literature regarding the timing of this shift emerge depending on the paradigm used to measure categorization. There is evidence to suggest that infants also use dynamic, causal, and functional information to guide their object categorization and discrimination. This chapter provides a comprehensive review of the research and theory on early categorization and concept development.}
}
@article{BATTAGLIA2025197,
title = {The paradox of the self-studying brain},
journal = {Physics of Life Reviews},
volume = {52},
pages = {197-204},
year = {2025},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2024.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S1571064524001787},
author = {Simone Battaglia and Philippe Servajean and Karl J. Friston},
keywords = {Theoretical neuroscience, Consciousness, Perception, Introspection, Neurophenomenology, Brain},
abstract = {The paradox of a brain trying to study itself presents a conundrum, raising questions about self-reference, consciousness, psychiatric disorders, and the boundaries of scientific inquiry. By which means can this complex organ shift the focus of study towards itself? We aim at unpacking the intricacies of this paradox. Historically, this question has been raised by philosophers under different frameworks. Thanks to the development of novel techniques to study the brain on a functional and structural level - as well as neurostimulation protocols that can modulate its activity in selected areas - we now possess advanced methods to progress this intricate inquiry. Nonetheless, the broader implications of the brain's pursuit of understanding itself remain unclear to this day. Ultimately, the need to employ both perception and introspection has led to different formulations of consciousness. This creates a challenge, as evidence supporting one formulation does not necessarily support the other. By deconstructing the paradoxical nature of self understanding - from a philosophical and neuroscientific point of view - we may gain insights into the human brain, which could lead to improved understanding of self-awareness and consciousness.}
}
@incollection{PATEL2023191,
title = {Chapter 6 - Human-intensive techniques},
editor = {Robert A. Greenes and Guilherme {Del Fiol}},
booktitle = {Clinical Decision Support and Beyond (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Oxford},
pages = {191-215},
year = {2023},
isbn = {978-0-323-91200-6},
doi = {https://doi.org/10.1016/B978-0-323-91200-6.00030-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323912006000309},
author = {Vimla L. Patel and Jane Shellum and Timothy Miksch and Edward H. Shortliffe},
keywords = {Knowledge acquisition, Expert decision making, Learning and expertise, Human problem solving, Institutional standard setting, Interactive transfer of expertise},
abstract = {This chapter focuses on human expertise as a source of knowledge, emphasizing human-intensive techniques for acquiring knowledge from a variety of knowledge sources. Much of such work deals with the acquisition of knowledge for the purpose of encoding it to be used in decision support systems. There are also approaches that deal with the role of experts in creating evidence-based guidelines and institutional protocols. Those who want to capture expert knowledge naturally seek interaction with human beings who are excellent at the same task for which the knowledge product is intended. As a result, we need to understand both the factual knowledge that is required to solve the relevant problems and the judgmental knowledge that characterizes an excellent decision maker. The chapter accordingly focuses on computational methods and cognitive perspectives that can assist with the elicitation of knowledge by interacting with expert human beings.}
}
@article{OMIDI20231,
title = {Molecular dynamic study of perovskite with improved thermal and mechanical stability for solar cells application: Calculation the final strength of the modeled atomic structures and the Young's modulus},
journal = {Engineering Analysis with Boundary Elements},
volume = {156},
pages = {1-7},
year = {2023},
issn = {0955-7997},
doi = {https://doi.org/10.1016/j.enganabound.2023.07.037},
url = {https://www.sciencedirect.com/science/article/pii/S0955799723003922},
author = {Mohammad Omidi and Zahra Karimi and Shirin Rahmani and Ali {Naderi Bakhtiyari} and Mahmood {Karimi Abdolmaleki}},
keywords = {Molecular dynamic, LAMMPS, Mechanical properties, Stress-strain, Solar cell},
abstract = {The Large-scale Atomic/Molecular Massively Parallel Simulator (LAMMPS) software is used to do molecular dynamics simulations, which entail modeling atom behavior over time using interatomic potentials. This approach is used to calculate perovskite structures' mechanical characteristics. For testing purposes, stress-strain curves are completed in the X, Y, and Z directions to represent the material's reaction to applied stress in terms of strain. The simulated structures are deformed inside the computational experiments using the loads and deform approaches command to get the stress-strain curves. The mechanical data of the structures may be retrieved by producing a deformation. These stress-strain curves are then compared in three axes of X, Y, and Z for XSnO3 (X= Cs, Rb, and K) at varied temperature and pressure settings. Finally, we applied this material to solar cell devices to find the performance of perovskite materials and calculated the efficiency.}
}
@incollection{GRANJOU20161,
title = {1 - The Time Beast},
editor = {Céline Granjou},
booktitle = {Environmental Changes},
publisher = {Elsevier},
pages = {1-43},
year = {2016},
isbn = {978-1-78548-026-3},
doi = {https://doi.org/10.1016/B978-1-78548-026-3.50001-5},
url = {https://www.sciencedirect.com/science/article/pii/B9781785480263500015},
author = {Céline Granjou},
keywords = {Analogism, Anthropophagy, Doctrine of the Apocalypse, Environmental change, Environmental humanities, Evolution, Multi-species ethnography, Nature/society partition, Plate tectonics, Sociology},
abstract = {Abstract:
This chapter will give insights into the historical shaping of the very peculiar notion of a nature without any future. We will retrace some of its roots in the secularization of Christian apocalypse, Newtonian physics and Linnean classification – while at the same time, Cartesian, Kantian and Hegelian philosophies perceived humans as subjects of reason, emancipation and civilization. We will revisit the way that, in the 19th Century, the Darwinian theory of evolution and, more recently, the development of geophysics, both contributed in the thinking of nature itself as able to instigate and create new futures.}
}
@incollection{GOMILA20121,
title = {1 - Introduction: Language as the Key Factor to Human Singularity},
editor = {Antoni Gomila},
booktitle = {Verbal Minds},
publisher = {Elsevier},
address = {London},
pages = {1-4},
year = {2012},
isbn = {978-0-12-385200-7},
doi = {https://doi.org/10.1016/B978-0-12-385200-7.00001-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780123852007000011},
author = {Antoni Gomila},
keywords = {Cognitive architecture, verbal minds, cognitive control, flexibility, language},
abstract = {Publisher Summary
This chapter explains language as a distinctive human trait characterized by flexibility and creativity of human brains. Language is the sole symbol of communication and representation of individuality and sociality. The role of language on human thinking is to define the cultural and behavioral novelties of human thoughts. Lately, language is going through pendulum dynamics from past 30 years because the communicative approaches are becoming hegemonic. There has been a lot of new evidence to support the constitutive approach and its becoming mainstream, however, the modern critics of the cognitive view of language react in a paradoxical manner and do not support a cognitive role of language. This chapter aims at providing a defined role of language in human cognition to analyze the different ways, in which the relation between language and cognition has been conceived, to review the evidence amassed in recent years on this relationship, and to conclude multiple ways to conceive of the relationship at its best accounts for the facts.}
}
@article{GOTLIEB2025104126,
title = {Pathology Education for Undergraduate and Graduate Students: It is Not Just for Clinical Trainees},
journal = {Laboratory Investigation},
volume = {105},
number = {5},
pages = {104126},
year = {2025},
issn = {0023-6837},
doi = {https://doi.org/10.1016/j.labinv.2025.104126},
url = {https://www.sciencedirect.com/science/article/pii/S0023683725000364},
author = {Avrum I. Gotlieb and Richard N. Mitchell}
}
@incollection{DEBEUKELAER2018455,
title = {Chapter 21 - Relating movements in aesthetic spaces: Immersing, distancing, and remembering},
editor = {Julia F. Christensen and Antoni Gomila},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {237},
pages = {455-469},
year = {2018},
booktitle = {The Arts and The Brain},
issn = {0079-6123},
doi = {https://doi.org/10.1016/bs.pbr.2018.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0079612318300141},
author = {Sophie {De Beukelaer} and Ruben Azevedo and Manos Tsakiris},
keywords = {Aesthetic experience, Embodied simulation, Associative processing, Constructive memory, Aesthetic spaces},
abstract = {According to Aby Warburg, the aesthetic experience is informed by a pendulum-like movement of the observer's mind that allows him to immerse as well as to take distance from the artwork's composing elements. To account for Warburg's definition, we are proposing embodied simulation and associative processing as constitutive mechanisms of this pendulum-like movement within the aesthetic experience that enable the observer to relate to the displayed artistic material within aesthetic spaces. Furthermore, we suggest that associative processing elicits constructive memory processes that permit the development of a knowledge within which the objects of art become part of memory networks, potentially informing future ways of thinking, feeling, and behaving in real-world situations, as an individual or collectively.}
}
@article{JIANG2023104680,
title = {Using sequence mining to study students’ calculator use, problem solving, and mathematics achievement in the National Assessment of Educational Progress (NAEP)},
journal = {Computers & Education},
volume = {193},
pages = {104680},
year = {2023},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2022.104680},
url = {https://www.sciencedirect.com/science/article/pii/S0360131522002512},
author = {Yang Jiang and Gabrielle A. Cayton-Hodges and Leslie {Nabors Oláh} and Ilona Minchuk},
keywords = {Calculator, Assessment, Problem solving, Sequence mining, Process data, Mathematics},
abstract = {Using appropriate tools strategically to aid in problem solving is a crucial skill identified in K-12 mathematics curriculum standards. As more assessments transition from paper-and-pencil to digital formats, a variety of interactive tools have been made available to test takers in digital testing platforms. Using onscreen calculators as an example, this study illustrates how process data obtained from student interactions with a digitally-based large-scale assessment can be leveraged to explore how and how well test takers use interactive tools and unveil their mathematical problem-solving processes and strategies. Specifically, sequence mining techniques using the longest common subsequence were applied on process data collected from a nationally representative sample who took the National Assessment of Educational Progress (NAEP) mathematics assessment to examine patterns of eighth-grade students’ calculator-use behaviors and the content of calculator input across a series of items. Sequences of keystrokes executed on the onscreen calculator by test takers were compared to reference sequences identified by content experts as proficient and efficient use to infer how well and how consistently the calculator was used. Results indicated that calculator-use behaviors and content differed by item characteristics. Students were more likely to use calculators on calculation-demanding items that involve intensive and complex computations than on items that involve simple or no computation. Using the calculator on more calculation-demanding items and using it in a manner that is more efficient and more similar to reference sequences on these items were related to higher mathematical proficiency. Findings have implications for assessment design and can be used in educational practices to provide educators with actionable process-related information on tool use and problem solving.}
}
@article{HOBBS2019100055,
title = {Estimating peak water demand: Literature review of current standing and research challenges},
journal = {Results in Engineering},
volume = {4},
pages = {100055},
year = {2019},
issn = {2590-1230},
doi = {https://doi.org/10.1016/j.rineng.2019.100055},
url = {https://www.sciencedirect.com/science/article/pii/S2590123019300556},
author = {Ian Hobbs and Martin Anda and Parisa A. Bahri},
keywords = {Fixture unit, Peak water demand, Modified wistort method, Exhaustive enumeration method, Water demand calculator, Loading unit normalisation method},
abstract = {Since the 1940s, the models used to estimate peak water demand has been based largely upon variations and refinements of the probabilistic ‘fixture unit’ model. An approach originally advanced by Hunter (1940) in the United States of America (USA). Seeking an improved approach to the 'fixture unit' model, now widely recognised as outdated, is the key driving force behind the current work. Boosted by the development of computing power, the plumbing industry, researchers, and academics have, over the last decade, developed computational models as a means of estimating peak water demand. This paper builds on computational models embracing the estimation of peak water demand. A brief outline of the fixture unit and its limitations is provided with key developments in computational modeling comprising current developments from the USA and UK. A brief outline of computational models is presented: Modified Wistort Method (MWM); the Exhaustive Enumeration Method (EEM), and the Water Demand Calculator (WDC). Also presented, from the UK, is the Loading Unit Normalisation Assessment method (LUNA) aimed at an improved model to size domestic hot and cold-water systems. The analysis of the computational models suggests the WDC model is conceivably the most compatible with that of the plumbing industry's design requirements. Suggesting this model could easily be adapted to meet the requirements across international borders. Challenges for the international acceptance of the WDC are the field study requirements to determine p (probability of use) and q (fixture flow rate) values for all types of buildings.}
}
@article{AMALINA2023e19539,
title = {Cognitive and socioeconomic factors that influence the mathematical problem-solving skills of students},
journal = {Heliyon},
volume = {9},
number = {9},
pages = {e19539},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e19539},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023067476},
author = {Ijtihadi Kamilia Amalina and Tibor Vidákovich},
keywords = {External factor, Internal factor, Mathematics thinking skills, Middle-school students, Structural equation modeling},
abstract = {Mathematical problem-solving is necessary to encounter professional, 21st-century, and everyday challenges. The relevant context of mathematical problem-solving is related to science, which is presented using natural language. Mathematical problem-solving requires both mathematical skills and nonmathematical skills, e.g., science knowledge and text comprehension skills. Thus, several internal and external factors affect success in mathematical problem-solving. In this study, we investigated the cognitive (i.e., mathematics domain-specific prior knowledge (DSPK), science background knowledge, and text comprehension skills) and socioeconomic status (SES) (i.e., parents' educational level and family income) factors that affect students' mathematical problem-solving skills. The data considered in this study included tests, documents, and a questionnaire from grade seven to nine students (n = 1067). In addition, a theoretical model was constructed using structural equation modeling. We found that this model was close to satisfying the critical values of fit indices. The model was then modified by deleting the nonsignificant paths, and the modified model exhibited a better fit. We found that most of the exploratory variables directly affected mathematical problem-solving skills, with the exception of the parents' educational levels. The strongest factor was mathematics DSPK. Both the father’s and mother’s educational levels indirectly influenced mathematical problem-solving skills through family income. In addition, text comprehension skills indirectly impacted mathematical problem-solving skills with science background knowledge acting as a mediator.}
}
@article{GARBOCZI2001455,
title = {Elastic moduli of a material containing composite inclusions: effective medium theory and finite element computations},
journal = {Mechanics of Materials},
volume = {33},
number = {8},
pages = {455-470},
year = {2001},
issn = {0167-6636},
doi = {https://doi.org/10.1016/S0167-6636(01)00067-9},
url = {https://www.sciencedirect.com/science/article/pii/S0167663601000679},
author = {E.J. Garboczi and J.G. Berryman},
keywords = {Fnite element, Effective medium theory, Concrete, Microstructure, Random elastic},
abstract = {Concrete is a good example of a composite material in which the inclusions (rocks and sand) are surrounded by a thin shell of altered matrix material and embedded in the normal matrix material. Concrete, therefore, may be viewed as consisting of a matrix material containing composite inclusions. Assigning each of these phases different linear elastic moduli results in a complicated effective elastic moduli problem. A new kind of differential effective medium theory (D-EMT) is presented in this paper that is intended to address this problem. The key new idea is that each inclusion particle, surrounded by a shell of another phase, is mapped onto an effective particle of uniform elastic moduli. The resulting simpler composite, with a normal matrix, is then treated in usual D-EMT. Before use, however, the accuracy of this method must be determined, as effective medium theory of any kind is an uncertain approximation. One good way to assess the accuracy of effective medium theory is to compare to exact results for known microstructures and phase moduli. Exact results, however, only exist for certain microstructures (e.g., dilute limit of inclusions) or special choices of the moduli (e.g., equal shear moduli). Recently, a special finite element method has been developed that can compute the linear elastic moduli of an arbitrary digital image in 2D or 3D. If a random microstructure can be represented with enough resolution by a digital image, then its elastic moduli can be readily computed. This method is used, after proper error analysis, to provide stringent tests of the new D-EMT equations, which are found to compare favorably to numerically exact finite element simulations, in both 2D and 3D, with varying composite inclusion particle size distributions.}
}
@article{CASTILLO2018165,
title = {In search of missing time: A review of the study of time in leadership research},
journal = {The Leadership Quarterly},
volume = {29},
number = {1},
pages = {165-178},
year = {2018},
issn = {1048-9843},
doi = {https://doi.org/10.1016/j.leaqua.2017.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1048984317300632},
author = {Elizabeth A. Castillo and Mai P. Trinh},
keywords = {Leadership, Time, Process, Computational science, Agent-based model},
abstract = {Many studies describe leadership as a dynamic process. However, few examine the passage of time as a critical dimension of that dynamism. This article illuminates this knowledge gap by conducting a systematic review of empirical studies on temporal effects of leadership to identify if and how time has been considered as a factor. After synthesizing key findings from the review, the article discusses methodological implications. We propose that a computational science approach, particularly agent-based modeling, is a fruitful path for future leadership research. This article contributes to leadership scholarship by shedding light on a missing variable (time) and offering a novel way to investigate the temporal, dynamic, emergent, and recursive aspects of leadership. We demonstrate the usefulness of agent-based modeling with an example of leader-member exchange relationship development.}
}
@incollection{HALFORD2008298,
title = {Cognitive Developmental Theories},
editor = {Marshall M. Haith and Janette B. Benson},
booktitle = {Encyclopedia of Infant and Early Childhood Development},
publisher = {Academic Press},
address = {San Diego},
pages = {298-308},
year = {2008},
isbn = {978-0-12-370877-9},
doi = {https://doi.org/10.1016/B978-012370877-9.00039-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780123708779000396},
author = {G.S. Halford},
abstract = {Theories of cognitive development are reviewed, beginning with pioneering theories by Piaget and Vygotsky. Neo-Piagetian theories which integrated Piagetian theory with other conceptions of cognition were developed by McLaughlin, Pascual-Leone, Case, Fischer, and Chapman. Complexity theories propose that children become capable of dealing with more complex relations as they develop. Information processing theories, neural net theories, dynamic systems theories, and theories of reasoning processes all provide models of the reasoning processes employed by children at different ages. Microgenetic analysis methods are used to study the processes of transition from one level of thinking to the next.}
}
@article{BOWLER2016117,
title = {Mindful makers: Question prompts to help guide young peoples' critical technical practices in maker spaces in libraries, museums, and community-based youth organizations},
journal = {Library & Information Science Research},
volume = {38},
number = {2},
pages = {117-124},
year = {2016},
issn = {0740-8188},
doi = {https://doi.org/10.1016/j.lisr.2016.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0740818815300840},
author = {Leanne Bowler and Ryan Champagne},
abstract = {This study examines question prompts as a means to scaffold reflection and reflexivity in the design, development, and use of technological artifacts in maker spaces for youth at public libraries, museums, and community-based organizations. Qualitative analysis is applied to data gathered in four focus groups with teens, three semi-structured interviews with adults who facilitate maker spaces, and six observation sessions. Outcomes include a rich description of critical thinking in the context of technology practice, and secondly, a set of eight activation questions that serve as a tool kit to encourage reflection and scaffold mindful and critical practices in community-based maker spaces for youth. Results from this study support the development of nstruments and practices to support mindful making and critical technical practice in maker spaces for youth.}
}
@article{FERGUSON20141885,
title = {Training in Minimally Invasive Lobectomy: Thoracoscopic Versus Robotic Approaches},
journal = {The Annals of Thoracic Surgery},
volume = {97},
number = {6},
pages = {1885-1892},
year = {2014},
issn = {0003-4975},
doi = {https://doi.org/10.1016/j.athoracsur.2014.01.055},
url = {https://www.sciencedirect.com/science/article/pii/S0003497514003403},
author = {Mark K. Ferguson and Konstantin Umanskiy and Cindy Warnes and Amy D. Celauro and Wickii T. Vigneswaran and Vivek N. Prachand},
abstract = {Background
Skills required for thoracoscopic and robotic operations likely differ. The needs and abilities of trainees learning these approaches require assessment.
Methods
Trainees performed initial components of minimally invasive lobectomies using thoracoscopic or robotic approaches. Component difficulty was scored by trainees using the NASA task load index (NASATLX). Performance of each component was graded by trainees and attending surgeons on a 5-point ordinal scale (naïve, beginning learner, advanced learner, competent, master).
Results
Eleven surgical trainees performed 87 replications among three lobectomy components (divide pulmonary ligament; dissect level 7/8/9 nodes; dissect level 4/5 nodes). Before performance NASATLX scores did not differ among components or between surgical approaches. Trainees' after performance NASATLX scores appropriately calibrated task load for the components. After performance NASATLX scores were significantly lower for thoracoscopy than before performance estimates; robotic scores were similar before surgery and after performance. Task load was higher for robotic than for thoracoscopic approaches. Trainees rated their performance higher than did attending surgeons in domains of knowledge and thinking, but ratings for other domains were similarly low. Ratings for performance improved significantly as component performance repetitions increased.
Conclusions
Trainees did not differentiate task load among components or surgical approaches before attempting them. Task load scores differentiated difficulty among initial components of lobectomy, and were greater for robotic than for thoracoscopic approaches. Trainees overestimated their level of cognitive performance compared with attending physician evaluation of trainee performance. The study provides insights into how to customize training for thoracoscopic and robotic lobectomy and identifies tools to assess training effectiveness.}
}
@article{GUZMANURBINA2022109295,
title = {FIEMA, a system of fuzzy inference and emission analytics for sustainability-oriented chemical process design},
journal = {Applied Soft Computing},
volume = {126},
pages = {109295},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.109295},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622004859},
author = {Alexander Guzman-Urbina and Kakeru Ouchi and Hajime Ohno and Yasuhiro Fukushima},
keywords = {Sustainability engineering, Emission analytics, Fuzzy systems, Data clustering},
abstract = {In the quest to achieve sustainable development goals, developments in sustainability-oriented chemical process design are key to innovation in the chemical industry, especially important for processes aiming for sustainable fuels. One of the greatest challenges is the difficulty of modeling the highly complex interactions among the design variables, such as catalyst technology attributes, and greenhouse gas emissions. Most of the computational aids crucial to deal with the complexity of chemical processes require data that is either unavailable or uncertain at an early stage of design. The multistage integrated system for sustainable design proposed in this paper boosts these computational aids by applying data science techniques to allow uncertainty to be handled more efficiently, thereby facilitating the modeling of the interactions between the properties of new materials or processes and sustainability indicators. In this system, current data connectivity methods are used to find paths of correlation among catalysts properties and greenhouse gas emissions. The key feature of the proposed system relies on the integration through multiple stages of Fuzzy Inference systems and a data-driven technique for Emissions Analytics, FIEMA.11FIEMA: Fuzzy Inference and Emission Analytics. The algorithm in FIEMA provides a semi-supervised learning approach to emission analytics: it determines data clusters by a C-means algorithm and subsequently builds fuzzy sets for multiple stages of input–output inference. The proposed FIEMA system was demonstrated in an effort to determine the optimal configurations of the properties of catalysts to minimize the probability of associated greenhouse gas emissions for a methanol production process. The results showed the potential of this approach to reduce the search space of catalyst material designs by suggesting promising configurations for oxygen storage capacity, mechanical strength, lifetime, size, and poisoning level. The research impacts of this study contribute to the development of clean fuels by a computationally-efficient system for early design, and by the determination of catalysts development paths that assure an actual reduction of the life-cycle emissions.}
}
@article{ANGIONE2015102,
title = {Analysis and design of molecular machines},
journal = {Theoretical Computer Science},
volume = {599},
pages = {102-117},
year = {2015},
note = {Advances in Computational Methods in Systems Biology},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2015.01.030},
url = {https://www.sciencedirect.com/science/article/pii/S0304397515000663},
author = {C. Angione and J. Costanza and G. Carapezza and P. Lió and G. Nicosia},
keywords = {Pareto optimality,  modelling, Turing machine, Molecular machine, Biological complexity, Petri nets, Register machines, Von Neumann architectures, Trade-off genetic strategies, Flux-balance analysis},
abstract = {Biologically inspired computation has been recently used with mathematical models towards the design of new synthetic organisms. In this work, we use Pareto optimality to optimize these organisms in a multi-objective fashion. We infer the best knockout strategies to perform specific tasks in bacteria, which involve concurrent maximization/minimization of multiple functions (codomain) and optimization of several decision variables (domain). Furthermore, we propose and exploit a mapping between the metabolism and a register machine. We show that optimized bacteria have computational capability and act as molecular Turing machines programmed using a Pareto optimal solution. Finally, we investigate communication between bacteria as a means to evaluate their computational capability. We report that the density and gradient of the Pareto curve are useful tools to compare models and understand their structure, while modelling organisms as computers proves useful to carry out computation using biological machines with specific input–output conditions, as well as to estimate the bacterial computational effort for specific tasks.}
}
@article{KOSIKOV2021492,
title = {Data Enrichment in the Information Graphs Environment Based on a Specialized Architecture of Information Channels},
journal = {Procedia Computer Science},
volume = {190},
pages = {492-499},
year = {2021},
note = {2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: Eleventh Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921013922},
author = {Sergey Kosikov and Larisa Ismailova and Viacheslav Wolfengagen},
keywords = {data enrichment, information channels, conceptual constructions, informational graph, applicative computations, semantics},
abstract = {The paper considers the possibility of constructing a specialized computing system oriented at the transmission of data through information channels, that are determined taking into account the semantics of the selected data. In the process of computations the data is connected with semantic characteristics that describe the channel of computations, which can be considered as a method of semantic data enrichment. The system of information channels as a whole can be considered as an information graph describing the structuring of the processed data. The information graph supports the data model in the form of a network, the framework of which are objects and the relationships between them. The paper proposes language tools for determining the information graph and interpretation tools that provide practical computations. The set of information channels that make up the information graph can be considered as a low-level tool for data enrichment. The paper studies the possibility of determining tools of higher level. An applicative type language is proposed for defining information graphs, the syntax and semantics of the language are specified. The proposed language can be considered as an intermediate level tool for defining semantics. A procedure is proposed for compiling the language into a low-level construct, preserving the semantics of the language. The supporting system for the proposed computing system includes a low-level language interpreter, as well as an intermediate-level language compiler into a low-level language. The supporting system is implemented in an applicative programming environment. Some elements of the supporting system were tested when developing applied information systems in the field of jurisprudence.}
}
@article{DODERO20221227,
title = {Ship design assessment through virtual prototypes},
journal = {Procedia Computer Science},
volume = {200},
pages = {1227-1236},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.323},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922003325},
author = {Matteo Dodero and Serena Bertagna and Luca Braidotti and Alberto Marinò and Vittorio Bucci},
keywords = {Ship design, early stage design, Virtual Prototype, Product Model Program},
abstract = {The traditional design process has been developed, through time, by trial and error, following an evolutive approach. By following this procedure, the design team focused its attention on only one conceptual design alternative at a time, which is perfected step by step until the expected outcome is obtained. Nevertheless nowadays, due to the high complexity of ships and increasingly stringent operational requirements, this approach appears to be obsolete in a market where cost and time reduction is a fundamental parameter. Indeed, to be competitive in the shipbuilding market, very accurate information should be available since the beginning of the process, to allow the design team a 360-degree exploration of a high number of alternatives and then identify the best design solution in no time. In this paper a new, rational, design process, necessary to raise efficiency and effectiveness of ship design, is presented. By using a multi-purpose design software, the authors were able to create a Virtual Prototype of a case study ship with ease and little training, obtaining, since early-stage design phases, some outputs of interest (such as longitudinally weight distribution of ship structures, preliminary midship section, GZ curves and powering curves) without great computational efforts. The most important benefit of using only one multipurpose software instead of multiple specific ones lies in the elimination of remarking activities for switching from one software to another, reducing loss of data’s risks during the process.}
}
@article{CELLERIER1990159,
title = {Psychology and computation: A response to Bunge},
journal = {New Ideas in Psychology},
volume = {8},
number = {2},
pages = {159-175},
year = {1990},
issn = {0732-118X},
doi = {https://doi.org/10.1016/0732-118X(90)90006-N},
url = {https://www.sciencedirect.com/science/article/pii/0732118X9090006N},
author = {G. Cellerier and J.-J. Ducret}
}
@article{YEE1991249,
title = {Dynamical approach study of spurious steady-state numerical solutions of nonlinear differential equations. I. The dynamics of time discretization and its implications for algorithm development in computational fluid dynamics},
journal = {Journal of Computational Physics},
volume = {97},
number = {2},
pages = {249-310},
year = {1991},
issn = {0021-9991},
doi = {https://doi.org/10.1016/0021-9991(91)90001-2},
url = {https://www.sciencedirect.com/science/article/pii/0021999191900012},
author = {H.C Yee and P.K Sweby and D.F Griffiths},
abstract = {The goal of this paper is to utilize the theory of nonlinear dynamics approach to investigate the possible sources of errors and slow convergence and nonconvergence of steady-state numerical solutions when using the time-dependent approach for nonlinear hyperbolic and parabolic partial differential equations terms. This interdisciplinary research belongs to a subset of a new field of study in numerical analysis sometimes referred to as “ the dynamics of numerics and the numerics of dynamics.” At the present time, this new interdisciplinary topic is still the property of an isolated discipline with all too little effort spent in pointing out an underlying generality that could make it adaptable to diverse fields of applications. This is the first of a series of research papers under the same topic. Our hope is to reach researchers in the fields of computational fluid dynamics (CFD) and, in particular, hypersonic and combustion related CFD. By simple examples (in which the exact solutions of the governing equations are known), the application of the apparently straightforward numerical technique to genuinely nonlinear problems can be shown to lead to incorrect or misleading results. One striking phenomenon is that with the same initial data, the continuum and its discretized counterpart can asymptotically approach different stable solutions. This behavior is especially important for employing a time-dependent approach to the steady state since the initial data are usually not known and a freestream condition or an intelligent guess for the initial conditions is often used. With the unique property of the different dependence of the solution on initial data for the partial differential equation and the discretized counterpart, it is not easy to delineate the true physics from numerical artifacts when numerical methods are the sole source of solution procedure for the continuum. Part I concentrates on the dynamical behavior of time discretization for scalar nonlinear ordinary differential equations in order to motivate this new yet unconventional approach to algorithm development in CFD and to serve as an introduction for parts 11 and III of the same series of research papers.}
}
@article{WU2025111756,
title = {A rapid indoor 3D wind field prediction model based on conditional generative adversarial networks},
journal = {Journal of Building Engineering},
volume = {100},
pages = {111756},
year = {2025},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2024.111756},
url = {https://www.sciencedirect.com/science/article/pii/S2352710224033242},
author = {Yaqi Wu and Xiaoqian Li and Xing Zheng and Chenxi Lei and Ye Yuan and Zhen Han and Gang Liu},
keywords = {3D wind field, Fast prediction, Pix2pix, Image encoding},
abstract = {The prediction of building performance during the early design phase is essential for architects and engineers. Given the complex nature of parameter inputs and the need for time efficiency, surrogate models have become a preferred method for predicting building performance. However, most surrogate models for indoor airflow could not predict the wind flow field information in three-dimensional (3D) space (named 3D wind field). The few advanced 3D data prediction models are often computationally expensive. This paper proposes a surrogate model based on Conditional Generative Adversarial Networks for the prediction of indoor 3D wind fields under natural ventilation. The core innovation lies in compressing indoor 3D wind field information into 2D planes via image encoding and subsequently obtaining wind field maps of arbitrary planes through data post-processing. By taking prefabricated houses as the case study, a database is constructed and the model is trained to predict the wind field at any cross-section within the space. The resulting surrogate model can generate predictions within a 3–5 s timeframe. To evaluate the accuracy of the model prediction, 21 testing planes were selected. Mean Absolute Error (MAE) and Mean Absolute Percentage Error (MAPE) were used to assess the numerical accuracy, and Structure Similarity Index Measure (SSIM) was used to comprehensively evaluate the visualization results of the wind field images. The results indicate that the model exhibits outstanding prediction performance for the planes, with an MAE of 0.1233, a MAPE of 12.20 %, and an SSIM of 0.9492 on the test set. Compared to simulation methods, this approach can improve prediction speed by 350 times-450 times, significantly enhancing the efficiency of obtaining 3D wind fields during the early design stages.}
}
@article{MELNIKOFF2018280,
title = {The Mythical Number Two},
journal = {Trends in Cognitive Sciences},
volume = {22},
number = {4},
pages = {280-293},
year = {2018},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2018.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S136466131830024X},
author = {David E. Melnikoff and John A. Bargh},
keywords = {dual process, dual system, type 1, type 2, automaticity},
abstract = {It is often said that there are two types of psychological processes: one that is intentional, controllable, conscious, and inefficient, and another that is unintentional, uncontrollable, unconscious, and efficient. Yet, there have been persistent and increasing objections to this widely influential dual-process typology. Critics point out that the ‘two types’ framework lacks empirical support, contradicts well-established findings, and is internally incoherent. Moreover, the untested and untenable assumption that psychological phenomena can be partitioned into two types, we argue, has the consequence of systematically thwarting scientific progress. It is time that we as a field come to terms with these issues. In short, the dual-process typology is a convenient and seductive myth, and we think cognitive science can do better.}
}
@article{POLETTI20141803,
title = {Adverse childhood experiences worsen cognitive distortion during adult bipolar depression},
journal = {Comprehensive Psychiatry},
volume = {55},
number = {8},
pages = {1803-1808},
year = {2014},
issn = {0010-440X},
doi = {https://doi.org/10.1016/j.comppsych.2014.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0010440X14001825},
author = {Sara Poletti and Cristina Colombo and Francesco Benedetti},
abstract = {Background
Cognitive distortion is a central feature of depression, encompassing negative thinking, dysfunctional personality styles and dysfunctional attitudes. It has been hypothesized that ACEs could increase the vulnerability to depression by contributing to the development of a stable negative cognitive style. Nevertheless, little research has been carried out on possible associations between adverse childhood experiences (ACEs) and cognitive distortion, and whether any gender differences exist.
Aim
The aim of this study was to examine the association between ACEs and cognitive distortions and possible differences between genders in a sample of patients affected by bipolar disorder.
Method
130 patients with bipolar disorder (BD) (46 men and 84 females), completed the Risky Family Questionnaire to assess ACEs and the Cognition Questionnaire (CQ) to assess cognitive distortions.
Results
A positive association was found between ACE and the CQ total score. Investigating the 5 dimensions assessed through the CQ, only the dimension “generalization across situations” was significantly associated to ACE. An interaction between ACE and gender was found for “generalization across situations”, while no differential effect among females and males was found for CQ total score.
Conclusion
This is the first study to report a relationship between negative past experiences and depressive cognitive distortions in subjects affected by BD. Growing in a family environment affected by harsh parenting seems to a cognitive vulnerability to depression; this effect is especially strong in females.}
}
@article{YUKSEL2025100890,
title = {Transformation of labor: Educational robotics coding in elementary schools for 21st century skills},
journal = {Entertainment Computing},
volume = {52},
pages = {100890},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100890},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124002581},
author = {Akça Okan Yüksel and Bilal Atasoy and Selçuk Özdemir},
keywords = {21st century skills, Educational robotics coding, Elementary school students, Sociocultural, Cognitive, Affective},
abstract = {This study aims to examine the effects of educational robotics activities on students’ 21st century skills. In the study, explanatory mixed method design was used. As a quantitative data collection tool, the 21st century skills scale was utilized [47]. In addition to quantitative data, qualitative data were collected from the teachers and students through semi-structured interviews. Within the scope of the research, students participated in robotic design courses with Arduino over the learning management system for ten weeks and participated in a competition with their products at the end of the activity. The activity was held with the participation of 62 students and 10 teachers. The findings of the study showed that educational robotic activities caused a significant increase in the affective domain of the students. While it did not cause any significant increase in the cognitive and sociocultural domains, the average scores of students increased on post-tests compared to the pretests for these two domains. In addition, students’ 21st century skills did not differ according to gender. Although there is no statistically significant difference in pretest–posttest results due to grade level, an increase was observed in posttest averages at each grade level. The observed increase in the post-tests, although not statistical, reveals the positive effect of educational robotic coding in terms of students’ 21st century skills. To support the results of the quantitative data, the analysis of the qualitative data revealed a consensus of both teachers and students on the contribution of such applications to the advancement of contemporary skills. In conclusion, the results of this study show that educational robotic coding can be used to develop 21st century skills of elementary school students.}
}
@article{KOICHU2015233,
title = {Proving as problem solving: The role of cognitive decoupling},
journal = {The Journal of Mathematical Behavior},
volume = {40},
pages = {233-244},
year = {2015},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2015.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0732312315300067},
author = {Boris Koichu and Uri Leron},
keywords = {Proving, Problem solving, Cognitive decoupling, Cycles in problem solving, Drawings and diagrams, Dual process theory},
abstract = {This paper discusses the process of proving from a novel theoretical perspective, imported from cognitive psychology research. This perspective highlights the role of hypothetical thinking, mental representations and working memory capacity in proving, in particular the effortful mechanism of cognitive decoupling: problem solvers need to form in their working memory two closely related models of the problem situation – the so-called primary and secondary representations – and to keep the two models decoupled, that is, keep the first fixed while performing various transformations on the second, while constantly struggling to protect the primary representation from being “contaminated” by the secondary one. We first illustrate the framework by analyzing a common scenario of introducing complex numbers to college-level students. The main part of the paper consists of re-analyzing, from the perspective of cognitive decoupling, previously published data of students searching for a non-trivial proof of a theorem in geometry. We suggest alternative (or additional) explanations for some well-documented phenomena, such as the appearance of cycles in repeated proving attempts, and the use of multiple drawings.}
}
@article{SMITH2020108208,
title = {Imprecise action selection in substance use disorder: Evidence for active learning impairments when solving the explore-exploit dilemma},
journal = {Drug and Alcohol Dependence},
volume = {215},
pages = {108208},
year = {2020},
issn = {0376-8716},
doi = {https://doi.org/10.1016/j.drugalcdep.2020.108208},
url = {https://www.sciencedirect.com/science/article/pii/S0376871620303732},
author = {Ryan Smith and Philipp Schwartenbeck and Jennifer L. Stewart and Rayus Kuplicki and Hamed Ekhtiari and Martin P. Paulus},
keywords = {Substance use disorders, Computational modeling, Active inference, Learning rate, Explore-exploit dilemma, Directed exploration},
abstract = {Background
Substance use disorders (SUDs) are a major public health risk. However, mechanisms accounting for continued patterns of poor choices in the face of negative life consequences remain poorly understood.
Methods
We use a computational (active inference) modeling approach, combined with multiple regression and hierarchical Bayesian group analyses, to examine how treatment-seeking individuals with one or more SUDs (alcohol, cannabis, sedatives, stimulants, hallucinogens, and/or opioids; N = 147) and healthy controls (HCs; N = 54) make choices to resolve uncertainty within a gambling task. A subset of SUDs (N = 49) and HCs (N = 51) propensity-matched on age, sex, and verbal IQ were also compared to replicate larger group findings.
Results
Results indicate that: (a) SUDs show poorer task performance than HCs (p = 0.03, Cohen’s d = 0.33), with model estimates revealing less precise action selection mechanisms (p = 0.004, d = 0.43), a lower learning rate from losses (p = 0.02, d = 0.36), and a greater learning rate from gains (p = 0.04, d = 0.31); and (b) groups do not differ significantly in goal-directed information seeking.
Conclusions
Findings suggest a pattern of inconsistent behavior in response to positive outcomes in SUDs combined with a tendency to attribute negative outcomes to chance. Specifically, individuals with SUDs fail to settle on a behavior strategy despite sufficient evidence of its success. These learning impairments could help account for difficulties in adjusting behavior and maintaining optimal decision-making during and after treatment.}
}
@article{RUDD2025108517,
title = {Fixational eye movements and edge integration in lightness perception},
journal = {Vision Research},
volume = {227},
pages = {108517},
year = {2025},
issn = {0042-6989},
doi = {https://doi.org/10.1016/j.visres.2024.108517},
url = {https://www.sciencedirect.com/science/article/pii/S0042698924001615},
author = {Michael E. Rudd and Idris Shareef},
keywords = {Lightness perception, ON and OFF cells, Fixational eye movements, Staircase Gelb illusion, Chevreul’s illusion, Fading of stabilized images},
abstract = {A neural theory of human lightness computation is described and computer-simulated. The theory proposes that lightness is derived from transient ON and OFF cell responses in the early visual pathways that have different characteristic neural gains and that are generated by fixational eye movements (FEMs) as the eyes transit luminance edges in the image. The ON and OFF responses are combined with corollary discharge signals that encode the eye movement direction to create directionally selective ON and OFF responses. Cortical neurons with large-scale receptive fields independently integrate the outputs of all of the directional ON or OFF responses whose associated eye movement directions point towards their receptive field centers, with a spatial weighting determined by the receptive field profile. Lightness is computed by subtracting the spatially integrated OFF activity from spatially integrated ON activity and normalizing the difference signal so that the maximum response in the spatial lightness map at any given time equals a fixed activation level corresponding to the percept of white. Two different mechanisms for ON and OFF cells responses are considered and simulated, and both are shown to produce an overall lightness model that explains a host of quantitative and qualitative lightness phenomena, including the Staircase Gelb and related illusions, failures of lightness constancy in the simultaneous contrast illusion, Chevreul’s illusion, lightness filling-in, and perceptual fading of stabilized images. The neural plausibility of the two variants of the theory, as well as its implication for lightness constancy and failures of lightness constancy are discussed.}
}
@article{SEOW2021436,
title = {How Local and Global Metacognition Shape Mental Health},
journal = {Biological Psychiatry},
volume = {90},
number = {7},
pages = {436-446},
year = {2021},
note = {BPS 90/7Pharmacologic Prevention and Treatment of Posttraumatic Stress Disorder},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2021.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0006322321013299},
author = {Tricia X.F. Seow and Marion Rouault and Claire M. Gillan and Stephen M. Fleming},
keywords = {Confidence, Mental health, Metacognition, Self-beliefs, Self-efficacy, Transdiagnostic psychiatry},
abstract = {Metacognition is the ability to reflect on our own cognition and mental states. It is a critical aspect of human subjective experience and operates across many hierarchical levels of abstraction—encompassing local confidence in isolated decisions and global self-beliefs about our abilities and skills. Alterations in metacognition are considered foundational to neurologic and psychiatric disorders, but research has mostly focused on local metacognitive computations, missing out on the role of global aspects of metacognition. Here, we first review current behavioral and neural metrics of local metacognition that lay the foundation for this research. We then address the neurocognitive underpinnings of global metacognition uncovered by recent studies. Finally, we outline a theoretical framework in which higher hierarchical levels of metacognition may help identify the role of maladaptive metacognitive evaluation in mental health conditions, particularly when combined with transdiagnostic methods.}
}
@article{DUAN2023103365,
title = {Mining multigranularity decision rules of concept cognition for knowledge graphs based on three-way decision},
journal = {Information Processing & Management},
volume = {60},
number = {4},
pages = {103365},
year = {2023},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2023.103365},
url = {https://www.sciencedirect.com/science/article/pii/S0306457323001024},
author = {Jiangli Duan and Guoyin Wang and Xin Hu and Deyou Xia and Di Wu},
keywords = {Granular computing, Cognitive intelligence, Concept cognition, Knowledge graph, Three-way decision},
abstract = {Machine understanding and thinking require prior knowledge consisting of explicit and implicit knowledge. The current knowledge base contains various explicit knowledge but not implicit knowledge. As part of implicit knowledge, the typical characteristics of the things referred to by the concept are available by concept cognition for knowledge graphs. Therefore, this paper attempts to realize concept cognition for knowledge graphs from the perspective of mining multigranularity decision rules. Specifically, (1) we propose a novel multigranularity three-way decision model that merges the ideas of multigranularity (i.e., from coarse granularity to fine granularity) and three-way decision (i.e., acceptance, rejection, and deferred decision). (2) Based on the multigranularity three-way decision model, an algorithm for mining multigranularity decision rules is proposed. (3) The monotonicity of positive or negative granule space ensured that the positive (or negative) granule space from coarser granularity does not need to participate in the three-classification process at a finer granularity, which accelerates the process of mining multigranularity decision rules. Moreover, the experimental results show that the multigranularity decision rule is better than the two-way decision rule, frequent decision rule and single granularity decision rule, and the monotonicity of positive or negative granule space can accelerate the process of mining multigranularity decision rules.}
}
@incollection{TVERSKY197817,
title = {2 - Judgment under Uncertainty: Heuristics and Biases: Biases in judgments reveal some heuristics of thinking under uncertainty},
editor = {PETER DIAMOND and MICHAEL ROTHSCHILD},
booktitle = {Uncertainty in Economics},
publisher = {Academic Press},
pages = {17-34},
year = {1978},
isbn = {978-0-12-214850-7},
doi = {https://doi.org/10.1016/B978-0-12-214850-7.50008-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780122148507500085},
author = {Amos Tversky and Daniel Kahneman},
abstract = {Publisher Summary
Many decisions are based on beliefs concerning the likelihood of uncertain events such as the outcome of an election, the guilt of a defendant, or the future value of the dollar. Occasionally, beliefs concerning uncertain events are expressed in numerical form as odds or subjective probabilities. In general, the heuristics are quite useful, but sometimes they lead to severe and systematic errors. The subjective assessment of probability resembles the subjective assessment of physical quantities such as distance or size. These judgments are all based on data of limited validity, which are processed according to heuristic rules. However, the reliance on this rule leads to systematic errors in the estimation of distance. This chapter describes three heuristics that are employed in making judgments under uncertainty. The first is representativeness, which is usually employed when people are asked to judge the probability that an object or event belongs to a class or event. The second is the availability of instances or scenarios, which is often employed when people are asked to assess the frequency of a class or the plausibility of a particular development, and the third is adjustment from an anchor, which is usually employed in numerical prediction when a relevant value is available.}
}
@incollection{MILLER2020205,
title = {Chapter 10 - AI, autonomous machines and human awareness: Towards shared machine-human contexts in medicine},
editor = {William F. Lawless and Ranjeev Mittu and Donald A. Sofge},
booktitle = {Human-Machine Shared Contexts},
publisher = {Academic Press},
pages = {205-220},
year = {2020},
isbn = {978-0-12-820543-3},
doi = {https://doi.org/10.1016/B978-0-12-820543-3.00010-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128205433000109},
author = {D. Douglas Miller and Elena A. Wood},
keywords = {Medicine, Health care, Artificial intelligence, Medical education, Applications, Challenges},
abstract = {Medical curricula trend to integrate clinical skills training and to create efficiencies in preclinical medical sciences, but the rapid emergence big data-intensive health care has led to initiating collaborations among data scientists, computer engineers, and medical educators that might generate novel educational high-technology platforms and innovative AI practice applications. The preprocessing of big data improves neural network feature recognition, improving the speed and accuracy of AI diagnostics and permitting chronic disease predictions. Applications of generative adversarial networks to create virtual patient phenotypes and image sets exposes medical learners to endless illness presentations, improving system-1 critical thinking for differential diagnosis development. AI offers great potential for education data managers working in support of medical educators and learners. These opportunities to build a shared context, in keeping with these themes of this book, include emerging data-driven AI applications for medical education and provider training include individual aptitude-based career advising, early identification of learners with academic difficulties, highly focused e-tutoring interventions, and natural language processing of standardized exam questions.}
}
@article{CHANG2025100364,
title = {Co-designing AI with youth partners: Enabling ideal classroom relationships through a novel AI relational privacy ethical framework},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100364},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100364},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000049},
author = {Michael Alan Chang and Mike Tissenbaum and Thomas M. Philip and Sidney K. D’Mello},
keywords = {Architectures for educational technology systems, Cooperative/collaborative learning, Cultural and social implications, Interdisciplinary studies, participatory design, AI-supported collaboration},
abstract = {In recent years, the design of AI-based tools for educational spaces have been largely driven by researchers who impart their past expertises, experiences, and perspectives in the design process. While this typically leads to technically feasible designs and are often well-grounded in theories of learning, youth agency is typically limited in this process. In this paper, we argue that designers have a significant ethical responsibility to incorporate youth voices – in particular, their dreams and concerns – into the design of AI tools starting from conception. This need is particularly important as new applications for AI, such as AI-supported collaboration, introduce new surveillance vectors into classroom spaces. Drawing from recent scholarship which advances ethics and relationality in participatory co-design with youth, we introduce a co-design methodology in which youth are supported in imagining expansive technical possibilities for K-12 public schools, grounded within affordances, limitations, and tradeoffs of AI and machine learning techniques. This approach is demonstrated through our Learning Futures Workshop, which brought together 30 historically minoritized youth in conversation with experts in both education and technology. Through detailed case study on the enactment of the workshop, including a thematic analysis of the activities the youth engaged in and their outputs, we identified new, expansive relational possibilities for AI, ethical commitments to support the design, and finally, developed a novel AI Relational Privacy ethical framework that supports the design of new collaborative AI platforms. We conclude by connecting these findings and frameworks to the design of newly enacted AI-based applications and underlying data infrastructures.}
}
@article{GRUJIC20243381,
title = {Neurobehavioral meaning of pupil size},
journal = {Neuron},
volume = {112},
number = {20},
pages = {3381-3395},
year = {2024},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2024.05.029},
url = {https://www.sciencedirect.com/science/article/pii/S0896627324004069},
author = {Nikola Grujic and Rafael Polania and Denis Burdakov},
keywords = {pupil, arousal, cognition, noradrenaline, orexin, hypocretin},
abstract = {Summary
Pupil size is a widely used metric of brain state. It is one of the few signals originating from the brain that can be readily monitored with low-cost devices in basic science, clinical, and home settings. It is, therefore, important to investigate and generate well-defined theories related to specific interpretations of this metric. What exactly does it tell us about the brain? Pupils constrict in response to light and dilate during darkness, but the brain also controls pupil size irrespective of luminosity. Pupil size fluctuations resulting from ongoing “brain states” are used as a metric of arousal, but what is pupil-linked arousal and how should it be interpreted in neural, cognitive, and computational terms? Here, we discuss some recent findings related to these issues. We identify open questions and propose how to answer them through a combination of well-defined tasks, neurocomputational models, and neurophysiological probing of the interconnected loops of causes and consequences of pupil size.}
}
@article{KRAJNAK2021132976,
title = {Reactive islands for three degrees-of-freedom Hamiltonian systems},
journal = {Physica D: Nonlinear Phenomena},
volume = {425},
pages = {132976},
year = {2021},
issn = {0167-2789},
doi = {https://doi.org/10.1016/j.physd.2021.132976},
url = {https://www.sciencedirect.com/science/article/pii/S0167278921001330},
author = {Vladimír Krajňák and Víctor J. García-Garrido and Stephen Wiggins},
keywords = {Phase space of Hamiltonian systems, Stable and unstable manifolds, Normally hyperbolic invariant manifolds, Reactive islands, Spherinders, Lagrangian descriptors},
abstract = {We develop the geometrical, analytical, and computational framework for reactive island theory for three degrees-of-freedom time-independent Hamiltonian systems. In this setting, the dynamics occurs in a 5-dimensional energy surface in phase space and is governed by four-dimensional stable and unstable manifolds of a three-dimensional normally hyperbolic invariant sphere. The stable and unstable manifolds have the geometrical structure of spherinders and we provide the means to investigate the ways in which these spherinders and their intersections determine the dynamical evolution of trajectories. This geometrical picture is realized through the computational technique of Lagrangian descriptors. In a set of trajectories, Lagrangian descriptors allow us to identify the ones closest to a stable or unstable manifold. Using an approximation of the manifold on a surface of section we are able to calculate the flux between two regions of the energy surface.}
}
@article{MAYER2024115725,
title = {Site heterogeneity and broad surface-binding isotherms in modern catalysis: Building intuition beyond the Sabatier principle},
journal = {Journal of Catalysis},
volume = {439},
pages = {115725},
year = {2024},
issn = {0021-9517},
doi = {https://doi.org/10.1016/j.jcat.2024.115725},
url = {https://www.sciencedirect.com/science/article/pii/S002195172400438X},
author = {James M. Mayer},
abstract = {Learning the science of heterogeneous catalysis and electrocatalysis always starts with the simple case of a flat, uniform surface with an ideal adsorbate. It has of course been recognized for a century that real catalysts are more complicated. For the increasingly complex catalysts of the 21st century, this Perspective argues that surface heterogeneity and non-ideal binding isotherms are central features, and their implications need to be incorporated in current thinking. A variety of systems are described herein where catalyst complexity leads to broad, non-Langmuirian surface isotherms for the binding of hydrogen atoms – and this occurs even for ideal, flat Pt(111) surfaces. Modern catalysis employs nanoscale materials whose surfaces have substantial step, edge, corner, impurity, and other defect sites, and they increasingly have both metallic and non-metallic elements MnXm, including metal oxides, chalcogenides, pnictides, carbides, doped carbons, etc. The surfaces of such catalysts are often not crystal facets of the bulk phase underneath, and they typically have a variety of potential active sites. Catalytic surfaces in operando are often non-stoichiometric, amorphous, dynamic, and impure, and often vary from one part of the surface to another. Understanding of the issues that arise at such nanoscale, multi-element catalysts is just beginning to emerge. Yet these catalysts are widely discussed using Brønsted/Bell-Evans-Polanyi (BEP) relations, volcano plots, Tafel slopes, the Butler-Volmer equation, and other linear free energy relations (LFERs), which all depend on the implicit assumption that the active sites are “similar” and that surface adsorption is close to ideal. These assumptions underly the ubiquitous intuition based on the Sabatier Principle, that the fastest catalysis will occur when key intermediates have free energies of adsorption that are not too strong nor too weak. Current catalysis research often aims to minimize the complexity of non-ideal isotherms through experimental and computational design (e.g., the use of single crystal surfaces), and these studies are the foundation of the field. In contrast, this Perspective argues that the heterogeneity of binding sites and binding energies is an inherent strength of these catalysts. This diversity makes many nanoscale catalysts inherently a high-throughput screen wrapped in a tiny package. Only by making the heterogeneity part of the foundation of catalysis models, sorting the types of active sites and dissecting non-ideal binding isotherms, will modern catalysis learn to harness the inherent diversity of real catalysts. Controlling and exploiting diversity rather than avoiding it will help to optimize complex modern catalysts and catalytic conditions.}
}
@article{RIVIERE2024637,
title = {Proceedings from the inaugural Artificial Intelligence in Primary Immune Deficiencies (AIPID) conference},
journal = {Journal of Allergy and Clinical Immunology},
volume = {153},
number = {3},
pages = {637-642},
year = {2024},
issn = {0091-6749},
doi = {https://doi.org/10.1016/j.jaci.2024.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0091674924000332},
author = {Jacques G. Rivière and Pere {Soler Palacín} and Manish J. Butte},
keywords = {Artificial intelligence, machine learning, large language models, natural language processing, electronic health records, inborn errors of immunity, diagnosis, ethics},
abstract = {Here, we summarize the proceedings of the inaugural Artificial Intelligence in Primary Immune Deficiencies conference, during which experts and advocates gathered to advance research into the applications of artificial intelligence (AI), machine learning, and other computational tools in the diagnosis and management of inborn errors of immunity (IEIs). The conference focused on the key themes of expediting IEI diagnoses, challenges in data collection, roles of natural language processing and large language models in interpreting electronic health records, and ethical considerations in implementation. Innovative AI-based tools trained on electronic health records and claims databases have discovered new patterns of warning signs for IEIs, facilitating faster diagnoses and enhancing patient outcomes. Challenges in training AIs persist on account of data limitations, especially in cases of rare diseases, overlapping phenotypes, and biases inherent in current data sets. Furthermore, experts highlighted the significance of ethical considerations, data protection, and the necessity for open science principles. The conference delved into regulatory frameworks, equity in access, and the imperative for collaborative efforts to overcome these obstacles and harness the transformative potential of AI. Concerted efforts to successfully integrate AI into daily clinical immunology practice are still needed.}
}
@article{THOMPSON2013256,
title = {The role of answer fluency and perceptual fluency in the monitoring and control of reasoning: Reply to Alter, Oppenheimer, and Epley (2013)},
journal = {Cognition},
volume = {128},
number = {2},
pages = {256-258},
year = {2013},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2013.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0010027713000553},
author = {Valerie A. Thompson and Rakefet Ackerman and Yael Sidi and Linden J. Ball and Gordon Pennycook and Jamie A. {Prowse Turner}},
keywords = {Perceptual fluency, Answer fluency, Dual process theories, Metacognition, Intuition, Analytic thinking},
abstract = {In this reply, we provide an analysis of Alter et al. (2013) response to our earlier paper (Thompson et al., 2013). In that paper, we reported difficulty in replicating Alter, Oppenheimer, Epley, and Eyre’s (2007) main finding, namely that a sense of disfluency produced by making stimuli difficult to perceive, increased accuracy on a variety of reasoning tasks. Alter, Oppenheimer, and Epley (2013) argue that we misunderstood the meaning of accuracy on these tasks, a claim that we reject. We argue and provide evidence that the tasks were not too difficult for our populations (such that no amount of “metacognitive unease” would promote correct responding) and point out that in many cases performance on our tasks was well above chance or on a par with Alter et al.’s (2007) participants. Finally, we reiterate our claim that the distinction between answer fluency (the ease with which an answer comes to mind) and perceptual fluency (the ease with which a problem can be read) is genuine, and argue that Thompson et al. (2013) provided evidence that these are distinct factors that have different downstream effects on cognitive processes.}
}
@article{JONES201795,
title = {An exploratory study on student understandings of derivatives in real-world, non-kinematics contexts},
journal = {The Journal of Mathematical Behavior},
volume = {45},
pages = {95-110},
year = {2017},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2016.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0732312316301791},
author = {Steven R. Jones},
keywords = {Calculus, Derivative, Applications, Real-world, Student understanding},
abstract = {Much research on calculus students’ understanding of applied derivatives has been done in kinematics-based contexts (i.e. position, velocity, acceleration). However, given the wide range of applications in science and engineering that are not based on kinematics, nor even explicitly on time, it is important to know how students understand applied derivatives in non-kinematics contexts. In this study, interviews with six students and surveys with 38 students were used to explore students’ “ways of understanding” and “ways of thinking” regarding applied, non-kinematics derivatives. In particular, six categories of ways of understanding emerged from the data as having been shared by a substantial portion of the students in this study: (1) covariation, (2) invoking time, (3) other symbols as constants, (4) other symbols as implicit functions, (5) implicit differentiation, and (6) output values as amounts instead of rates of change.}
}