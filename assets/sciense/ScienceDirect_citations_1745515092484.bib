@article{LI2021512,
title = {Design and implementation of neural network computing framework on Zynq SoC embedded platform},
journal = {Procedia Computer Science},
volume = {183},
pages = {512-518},
year = {2021},
note = {Proceedings of the 10th International Conference of Information and Communication Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.02.091},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921005676},
author = {Xingying Li and Zhenyu Yin and Fulong Xu and Feiqing Zhang and Guangyuan Xu},
keywords = {Neural network, embedded platform, Zynq SoC, darknet, depthwise separable convolution, MobileNetV2},
abstract = {Limited resources and low computing power of embedded platform make it difficult to apply neural network technology. To overcome this problem, a new neural network computing framework “Zynq-Darknet” was proposed. The framework is based on Darknet, which constructs depthwise separable convolution and a lightweight classiﬁcation model MobileNetV2 and was deployed to Xilinx Zynq-7000 System-on-Chip (SoC) with Linux operating system (OS). In order to verify the performance of the framework and model, experiments were conducted on imagenet-1k dataset using different network structures. The results show that the MobileNetV2 network model based on Zynq-Darknet can effectively perform image classification, and ensure a certain real-time and accuracy while reducing the computational complexity and storage overhead, assuming promising application prospects.}
}
@article{GOMES2019411,
title = {State-of-the-art of transmission expansion planning: A survey from restructuring to renewable and distributed electricity markets},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {111},
pages = {411-424},
year = {2019},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2019.04.035},
url = {https://www.sciencedirect.com/science/article/pii/S014206151831888X},
author = {Phillipe Vilaça Gomes and João Tomé Saraiva},
keywords = {Heuristics, Optimization, Mathematical programming, Metaheuristic, Transmission expansion planning},
abstract = {Transmission Expansion Planning (TEP) problem aims at identifying when and where new equipment as transmission lines, cables and transformers should be inserted on the grid. The transmission upgrade capacity is motivated by several factors as meeting the increasing electricity demand, increasing the reliability of the system and providing non-discriminatory access to cheap generation for consumers. However, TEP problems have been changing over the years as the electrical system evolves. In this way, this paper provides a detailed historical analysis of the evolution of the TEP over the years and the prospects for this challenging task. Furthermore, this study presents an outline review of more than 140 recent articles about TEP problems, literature insights and identified gaps as a critical thinking in how new tools and approaches on TEP can contribute for the new era of renewable and distributed electricity markets.}
}
@incollection{WANG20249,
title = {1.02 - Artificial Intelligence and Bioinformatics Applications in Precision Medicine and Future Implications},
editor = {Kenneth S. Ramos},
booktitle = {Comprehensive Precision Medicine (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {9-24},
year = {2024},
isbn = {978-0-12-824256-8},
doi = {https://doi.org/10.1016/B978-0-12-824010-6.00058-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128240106000587},
author = {Ni Wang and Qiang He},
keywords = {Artificial intelligence, Bioinformatics, Clinical trials, Disease risk assessment, Drug discovery, Early disease detection, Genome sequencing, Oncology, Personalized medicine, Pharmacogenomics, Precision medicine},
abstract = {Artificial intelligence (AI) and bioinformatics have emerged as key technologies for advancing precision medicine. Many tools of AI and bioinformatics have been applied in healthcare that would be of great value to advance the goals of precision medicine. AI is a branch of computer science that deals with the automation of intelligent behavior. Bioinformatics is a field of study that combines biology, computer science, and statistics to analyze and interpret biologic data. The latter involves the development and application of computational methods and tools for storing, organizing, analyzing, and interpreting biologic information, including genomic, proteomic, and metabolomic data. AI and bioinformatics are well-positioned to help tailor medical decisions and treatments at the individual and population levels. Some of those applications and the multidisciplinary implications are presented in this chapter.}
}
@article{FRANZ1994433,
title = {A critical framework for methodological research in architecture},
journal = {Design Studies},
volume = {15},
number = {4},
pages = {433-447},
year = {1994},
issn = {0142-694X},
doi = {https://doi.org/10.1016/0142-694X(94)90006-X},
url = {https://www.sciencedirect.com/science/article/pii/0142694X9490006X},
author = {Jill M. Franz},
keywords = {methodological research, critical framework, architecture, design},
abstract = {This paper reviews a cross-section of methodological studies undertaken in architecture since the Second World War. Despite a variety of orientations, technically, conceptually and philosophically, most studies reflect an understanding of people and objects as discrete entities interacting in an passive and unilateral manner. This dominant dualist understanding is concluded to be the essential cause of the ‘implementation gap' between architectural research and practice. For the gap to close, the development and institution of a critical framework is needed which encourages researchers to acknowledge explicitly the ontological and epistomological issues associated with architectural practice, education and research. Underlying this recommendation is a dialectic appreciation of person-world interaction; one which accepts as a holistic theme for inquiry, the experiential and interpretative quality of human thinking, feeling and action.}
}
@article{2024100676,
title = {Erratum regarding missing declaration of competing interest statements in previously published articles},
journal = {International Journal of Child-Computer Interaction},
volume = {41},
pages = {100676},
year = {2024},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100676},
url = {https://www.sciencedirect.com/science/article/pii/S221286892400045X}
}
@incollection{KATZ202453,
title = {Chapter Three - The role of big data and analytics in utilities innovation},
editor = {Reza Arghandeh and Yuxun Zhou},
booktitle = {Big Data Application in Power Systems (Second Edition)},
publisher = {Elsevier Science},
edition = {Second Edition},
pages = {53-68},
year = {2024},
isbn = {978-0-443-21524-7},
doi = {https://doi.org/10.1016/B978-0-443-21524-7.00003-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780443215247000037},
author = {Jeffrey S. Katz},
keywords = {Big data, Analytics, Data science, Smart grid, Power system simulation, Numerical weather analysis, Smarter energy research},
abstract = {The computational technology known as big data and its subsequent processing, analytics, are driving innovation in electric power system integration of renewable energy, outage prediction, processing of increasing volumes of smart grid data, as well as the velocity of such data. In the age of cybersecurity, the veracity of this data is also a factor. The almost concurrent rise of cognitive computing gives new importance to unstructured data such as drone images and text in maintenance reports. The intelligent connection of real-time numerical data with written and visual data gives rise to even more innovation. The benefits of high-precision weather modeling on power demand, grid damage, and solar- and wind-based generation are also considered.}
}
@article{EBELING2023120768,
title = {A multi-dimensional framework to analyze group behavior based on political polarization},
journal = {Expert Systems with Applications},
volume = {233},
pages = {120768},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120768},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423012708},
author = {Régis Ebeling and Jéferson Nobre and Karin Becker},
keywords = {Analysis framework, Political polarization, Group behavior, Topic modeling, Social network analysis, COVID-19},
abstract = {The recent wave of elections won by right-wing worldwide brings up increased discussions biased by political polarization, including in social media. Social media data enables the investigation of the contexts where political polarization occurs, enabling to derive insights into how it affects human behavior. Related work has shown how computing techniques can be leveraged to understand political polarization in restricted scenarios, but the complexity of this behavior can be better understood when considered from different viewpoints. This article describes a multi-dimensional analysis framework to study the behavior of groups on Twitter in politically polarized scenarios. It can be applied to various themes where groups display stances that can be politically biased, and it aggregates a wide range of computational techniques in an innovative way to provide rich insights. The framework includes guidelines and techniques to: (a) collect data on Twitter to represent the groups; (b) automatically infer the political leaning of users; (c) derive topological properties of the groups’ social network and analyze political influence; (d) identify topics representing concerns at coarse and fine-grained granularity levels using a hybrid topic modeling approach; (e) identify psychological aspects based on linguistic cues, and (f) analyze the sources of information disseminated by the groups. Applying the framework in two case studies related to COVID-19 revealed patterns of behavior common to ideologies. We confirmed that the stances were politically motivated and that both groups, left/right, were subject to the echo chamber effect. Comparatively, the social structure of the right-oriented groups is more connected, and they rely on politicians and social media for information spreading. Left-oriented groups are less connected and more prone to facts. The psychological aspects reveal that both groups are emotionally distressed in arguing about being right, given their beliefs.}
}
@article{2024100677,
title = {Erratum regarding missing Declaration of Competing Interest statements in previously published articles},
journal = {International Journal of Child-Computer Interaction},
volume = {41},
pages = {100677},
year = {2024},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100677},
url = {https://www.sciencedirect.com/science/article/pii/S2212868924000461}
}
@article{BALE2015150,
title = {Energy and complexity: New ways forward},
journal = {Applied Energy},
volume = {138},
pages = {150-159},
year = {2015},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2014.10.057},
url = {https://www.sciencedirect.com/science/article/pii/S0306261914011076},
author = {Catherine S.E. Bale and Liz Varga and Timothy J. Foxon},
keywords = {Complexity science, Energy systems, Modelling, Complex adaptive systems, Agent-based modelling, Energy policy},
abstract = {The purpose of this paper is to review the application of complexity science methods in understanding energy systems and system change. The challenge of moving to sustainable energy systems which provide secure, affordable and low-carbon energy services requires the application of methods which recognise the complexity of energy systems in relation to social, technological, economic and environmental aspects. Energy systems consist of many actors, interacting through networks, leading to emergent properties and adaptive and learning processes. Insights on these type of phenomena have been investigated in other contexts by complex systems theory. However, these insights are only recently beginning to be applied to understanding energy systems and systems transitions. The paper discusses the aspects of energy systems (in terms of technologies, ecosystems, users, institutions, business models) that lend themselves to the application of complexity science and its characteristics of emergence and coevolution. Complex-systems modelling differs from standard (e.g. economic) modelling and offers capabilities beyond those of conventional models, yet these methods are only beginning to realize anything like their full potential to address the most critical energy challenges. In particular there is significant potential for progress in understanding those challenges that reside at the interface of technology and behaviour. Some of the computational methods that are currently available are reviewed: agent-based and network modelling. The advantages and limitations of these modelling techniques are discussed. Finally, the paper considers the emerging themes of transport, energy behaviour and physical infrastructure systems in recent research from complex-systems energy modelling. Although complexity science is not well understood by practitioners in the energy domain (and is often difficult to communicate), models can be used to aid decision-making at multiple levels e.g. national and local, and to aid understanding and allow decision making. The techniques and tools of complexity science, therefore, offer a powerful means of understanding the complex decision-making processes that are needed to realise a low-carbon energy system. We conclude with recommendations for future areas of research and application.}
}
@article{KEE19891479,
title = {A computational model of the structure and extinction of strained, opposed flow, premixed methane-air flames},
journal = {Symposium (International) on Combustion},
volume = {22},
number = {1},
pages = {1479-1494},
year = {1989},
issn = {0082-0784},
doi = {https://doi.org/10.1016/S0082-0784(89)80158-4},
url = {https://www.sciencedirect.com/science/article/pii/S0082078489801584},
author = {Robert J. Kee and James A. Miller and Gregory H. Evans and Graham Dixon-Lewis},
abstract = {The application of laminar flamelet concepts to turbulent flame propagation requires a detailed understanding of strained laminar flames. Here we use numerical methods, including are-length continuation, to simulate the complex chemical kinetic behavior in premixed methane-air flames that are stabilized between two opposed-flow burners. We predict both the detailed structure and the extinction limits for these flames over a range of fuel-air mixtures. In addition to discussing the flame structure, a sensitivity analysis provides further insight on the chemical behavior near extinction. Finally, we discuss the comparison of the predictions with Law's experimental extinction data. An especially important aspect of this comparison is the recognition that fluid mechanical aspects of the traditional strained-flame analysis are deficient in representing experiments such as Law's. We develop and solve a new system of equations that is able to describe the experiments much more accurately.}
}
@article{DESAFERREIRA2013135,
title = {Promoting integrative medicine by computerization of traditional Chinese medicine for scientific research and clinical practice: The SuiteTCM Project},
journal = {Journal of Integrative Medicine},
volume = {11},
number = {2},
pages = {135-139},
year = {2013},
issn = {2095-4964},
doi = {https://doi.org/10.3736/jintegrmed2013013},
url = {https://www.sciencedirect.com/science/article/pii/S2095496414601096},
author = {Arthur {de Sá Ferreira}},
keywords = {traditional Chinese medicine, evidence-based practice, computer-assisted decision making},
abstract = {Background
Chinese and contemporary Western medical practices evolved on different cultures and historical contexts and, therefore, their medical knowledge represents this cultural divergence. Computerization of traditional Chinese medicine (TCM) is being used to promote the integrative medicine to manage, process and integrate the knowledge related to TCM anatomy, physiology, semiology, pathophysiology, and therapy.
Methods
We proposed the development of the SuiteTCM software, a collection of integrated computational models mainly derived from epidemiology and statistical sciences for computerization of Chinese medicine scientific research and clinical practice in all levels of prevention. The software includes components for data management (DataTCM), simulation of cases (SimTCM), analyses and validation of datasets (SciTCM), clinical examination and pattern differentiation (DiagTCM, TongueTCM, and PulseTCM), intervention selection (AcuTCM, HerbsTCM, and DietTCM), management of medical records (ProntTCM), epidemiologic investigation of sampled data (ResearchTCM), and medical education, training, and assessment (StudentTCM).
Discussion
The SuiteTCM project is expected to contribute to the ongoing development of integrative medicine and the applicability of TCM in worldwide scientific research and health care. The SuiteTCM 1.0 runs on Windows XP or later and is freely available for download as an executable application.}
}
@article{HEIRDSFIELD200257,
title = {Flexibility and inflexibility in accurate mental addition and subtraction: two case studies},
journal = {The Journal of Mathematical Behavior},
volume = {21},
number = {1},
pages = {57-74},
year = {2002},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(02)00103-7},
url = {https://www.sciencedirect.com/science/article/pii/S0732312302001037},
author = {Ann M Heirdsfield and Tom J Cooper},
keywords = {Mental computation, Addition, Subtraction, Accuracy},
abstract = {This paper reports on a study of two children’s mental computation in addition and subtraction, and compares their mental architecture. Both students were identified as being accurate, however, one student used a variety of mental strategies (was flexible) while the other student used only one strategy that reflected the written procedure for each of the addition and subtraction algorithms taught in the classroom. Interviews were used to identify both children’s knowledge and ability with respect to number sense (including numeration, number and operations, basic facts, estimation), metacognition and affects. Frameworks were developed to show how these factors interacted to explain the two types of accuracy in mental addition and subtraction. Flexible accuracy was related to the presence of strong number sense knowledge integrated with metacognitive strategies and beliefs and beliefs about self and teaching; while inflexible accuracy was a result of compensation of inadequate knowledge supported by beliefs about self and teaching.}
}
@article{MEHREGAN2012426,
title = {An application of Soft System Methodology},
journal = {Procedia - Social and Behavioral Sciences},
volume = {41},
pages = {426-433},
year = {2012},
note = {The First International Conference on Leadership, Technology and Innovation Management},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2012.04.051},
url = {https://www.sciencedirect.com/science/article/pii/S1877042812009317},
author = {M. Reza Mehregan and Mahnaz Hosseinzadeh and Aliyeh Kazemi},
keywords = {Soft System Methodology (SSM), University course timetabling, rich picture, conceptual model},
abstract = {The typical course timetabling problem is assigning Classes of students to appropriate faculty members, suitable classrooms and available timeslots. Hence, it involves a large number of stakeholders including students, teachers and institutional administrators. Different kinds of Hard Operational Research techniques have been employed over the years to address such problems. Due to the computational difficulties of this NP complete problem as well as the size and the complexity of the real world instances, an efficient optimal solution cannot be found easily.As an alternative strategy, this paper investigates the application of Checkland‘s Soft System Methodology (SSM) to the course timetabling problem. Besides giving an ideal course timetable, even to large and complex real problems, application of SSM, generates debate, learning, and understanding; enables key changes; facilitates negotiating the actions to be taken and makes possible the meaningful collaboration among concerned stakeholders. This paper also provides an appropriate course timetable for the management faculty at University of Tehran to show the potential of this application to real problems.}
}
@article{GERO2001283,
title = {The differences between retrospective and concurrent protocols in revealing the process-oriented aspects of the design process},
journal = {Design Studies},
volume = {22},
number = {3},
pages = {283-295},
year = {2001},
issn = {0142-694X},
doi = {https://doi.org/10.1016/S0142-694X(00)00030-2},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X00000302},
author = {John S. Gero and Hsien-Hui Tang},
keywords = {design cognition, protocol studies, case study},
abstract = {This paper presents the results of studying a single designer using protocol analyses and examines the implications of the results on studies of design thinking. It contrasts two types of protocols: concurrent protocols and retrospective protocols. The results indicate that concurrent and retrospective protocols both produce very similar outcomes in terms of exploring the process-oriented aspects of designing. As a result, it is argued there is no associated interference with the ongoing design process when using concurrent protocols.}
}
@article{MEJIA20113964,
title = {On the complexity of sandpile critical avalanches},
journal = {Theoretical Computer Science},
volume = {412},
number = {30},
pages = {3964-3974},
year = {2011},
note = {Cellular Automata and Discrete Complex Systems},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2011.02.029},
url = {https://www.sciencedirect.com/science/article/pii/S0304397511001496},
author = {Carolina Mejia and J. {Andres Montoya}},
keywords = {Abelian sandpile model, Self-organized criticality,  complete problems, Parallel algorithms},
abstract = {In this work, we study The Abelian Sandpile Model from the point of view of computational complexity. We begin by studying the length distribution of sandpile avalanches triggered by the addition of two critical configurations: we prove that those avalanches are long on average, their length is bounded below by a constant fraction of the length of the longest critical avalanche which is, in most of the cases, superlinear. At the end of the paper we take the point of view of computational complexity, we analyze the algorithmic hardness of the problem consisting in computing the addition of two critical configurations, we prove that this problem is P complete, and we prove that most algorithmic problems related to The Abelian Sandpile Model are NC reducible to it.}
}
@article{NASCIMENTO2023105421,
title = {Core–shell clustering approach for detection and analysis of coastal upwelling},
journal = {Computers & Geosciences},
volume = {179},
pages = {105421},
year = {2023},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2023.105421},
url = {https://www.sciencedirect.com/science/article/pii/S0098300423001255},
author = {Susana Nascimento and Alexandre Martins and Paulo Relvas and Joaquim F. Luís and Boris Mirkin},
keywords = {Spatio-temporal clustering, Time series segmentation, SST images, Coastal upwelling, Core–shell cluster},
abstract = {A comprehensive approach is presented to analyze season’s coastal upwelling represented by weekly sea surface temperature (SST) image grids. The proposed model, core–shell clustering, assumes that the season’s upwelling can be divided into shorter periods of stability, time ranges, consisting of constant core and variable shell parts. A one-by-one core–shell clustering algorithm is provided. The algorithm parameters are automatically derived from the least-squares clustering criterion. The approach applies to SST gridded data for sixteen successive years (2004–2019) of coastal upwelling in the western Iberian coast, the northernmost branch of the Canary Current Upwelling System. Our results show that at each season, there are 3 to 5 time intervals, the ranges, at which the upwelling presents stable core patterns of relatively cold water surrounded by somewhat larger shell areas of warmer waters. Based on other experimental computations performed by our team, we conclude that this pattern is not just a purely local phenomenon but has a more global meaning. Inter-annual time series analysis are consistent among themselves and with existing expert domain knowledge.}
}
@article{PIHLAJAMAKI2020101103,
title = {Subjective cognitive complaints and sickness absence: A prospective cohort study of 7059 employees in primarily knowledge-intensive occupations},
journal = {Preventive Medicine Reports},
volume = {19},
pages = {101103},
year = {2020},
issn = {2211-3355},
doi = {https://doi.org/10.1016/j.pmedr.2020.101103},
url = {https://www.sciencedirect.com/science/article/pii/S2211335520300632},
author = {Minna Pihlajamäki and Heikki Arola and Heini Ahveninen and Jyrki Ollikainen and Mikko Korhonen and Tapio Nummi and Jukka Uitti and Simo Taimela},
keywords = {Subjective cognitive complaints, Screening questionnaire, Occupational healthcare, Self-reported data, Sickness allowance, Register data},
abstract = {Knowledge-intensive work requires capabilities like monitoring multiple sources of information, prioritizing between competing tasks, switching between tasks, and resisting distraction from the primary task(s). We assessed whether subjective cognitive complaints (SCC), presenting as self-rated problems with difficulties of concentration, memory, clear thinking and decision making predict sickness absence (SA) in knowledge-intensive occupations. We combined SCC questionnaire results with reliable registry data on SA of 7743 professional/managerial employees (47% female). We excluded employees who were not active in working life, on long-term SA, and those on a work disability benefit at baseline. The exposure variable was the presence of SCC. Age and SA before the questionnaire as a proxy measure of general health were treated as confounders and the analyses were conducted by gender. The outcome measure was the accumulated SA days during a 12-month follow-up. We used a hurdle model to analyse the SA data. SCC predicted the number of SA days during the 12-month follow-up. The ratio of the means of SA days was higher than 2.8 as compared to the reference group, irrespective of gender, with the lowest limit of 95% confidence interval 2.2. In the Hurdle model, SCC, SA days prior to the questionnaire, and age were additive predictors of the likelihood of SA and accumulated SA days, if any. Subjective cognitive complaints predict sickness absence in knowledge-intensive occupations, irrespective of gender, age, or general health. This finding has implications for supporting work ability (productivity) among employees with cognitively demanding tasks.}
}
@incollection{MAXWELL19961,
title = {Driving forces for innovation in applied catalysis},
editor = {Joe W. Hightower and W. {Nicholas Delgass} and Enrique Iglesia and Alexis T. Bell},
series = {Studies in Surface Science and Catalysis},
publisher = {Elsevier},
volume = {101},
pages = {1-9},
year = {1996},
booktitle = {11th International Congress On Catalysis - 40th Anniversary},
issn = {0167-2991},
doi = {https://doi.org/10.1016/S0167-2991(96)80210-2},
url = {https://www.sciencedirect.com/science/article/pii/S0167299196802102},
author = {Ian E. Maxwell},
abstract = {Publisher Summary
Catalytic environmental technologies such as automobile exhaust catalysts and the selective catalytic reduction (SCR) DeNOx systems in power plants have significantly contributed to the reduction of environmentally harmful emissions into the lower atmosphere. Some studies have identified catalysis as not only being pervasive but also offering significant scope for the innovative development of new and improved technologies for environmentally acceptable processes and products in the future. The spectrum of process industries that are directly impacted by catalysis include, for example, oil refining, natural gas conversion, petrochemicals, fine chemicals, and pharmaceuticals. Environmental catalytic technologies also play an important role in emission control systems for power generation, fossil fuel driven transportation, oil refining, and chemical industries. Catalytic technologies typically embrace a wide range of disciplines, such as heterogeneous and homogeneous catalysis, materials science, process technology, reactor engineering, separation technology, surface science, computational chemistry, and analytic chemistry. Innovation in this field is, therefore, often achieved by lateral thinking across these different disciplines. This chapter attempts to develop this theme by means of examples from recent commercial successes and from this platform provides some guidelines for multi-disciplinary approaches at the academic and industrial interface to enhanced innovation in catalytic technologies in the future.}
}
@article{BALL2024,
title = {Trust but Verify: Lessons Learned for the Application of AI to Case-Based Clinical Decision-Making From Postmarketing Drug Safety Assessment at the US Food and Drug Administration},
journal = {Journal of Medical Internet Research},
volume = {26},
year = {2024},
issn = {1438-8871},
doi = {https://doi.org/10.2196/50274},
url = {https://www.sciencedirect.com/science/article/pii/S1438887124003078},
author = {Robert Ball and Andrew H Talal and Oanh Dang and Monica Muñoz and Marianthi Markatou},
keywords = {drug safety, artificial intelligence, machine learning, natural language processing, causal inference, case-based reasoning, clinical decision support},
abstract = {Adverse drug reactions are a common cause of morbidity in health care. The US Food and Drug Administration (FDA) evaluates individual case safety reports of adverse events (AEs) after submission to the FDA Adverse Event Reporting System as part of its surveillance activities. Over the past decade, the FDA has explored the application of artificial intelligence (AI) to evaluate these reports to improve the efficiency and scientific rigor of the process. However, a gap remains between AI algorithm development and deployment. This viewpoint aims to describe the lessons learned from our experience and research needed to address both general issues in case-based reasoning using AI and specific needs for individual case safety report assessment. Beginning with the recognition that the trustworthiness of the AI algorithm is the main determinant of its acceptance by human experts, we apply the Diffusion of Innovations theory to help explain why certain algorithms for evaluating AEs at the FDA were accepted by safety reviewers and others were not. This analysis reveals that the process by which clinicians decide from case reports whether a drug is likely to cause an AE is not well defined beyond general principles. This makes the development of high performing, transparent, and explainable AI algorithms challenging, leading to a lack of trust by the safety reviewers. Even accounting for the introduction of large language models, the pharmacovigilance community needs an improved understanding of causal inference and of the cognitive framework for determining the causal relationship between a drug and an AE. We describe specific future research directions that underpin facilitating implementation and trust in AI for drug safety applications, including improved methods for measuring and controlling of algorithmic uncertainty, computational reproducibility, and clear articulation of a cognitive framework for causal inference in case-based reasoning.}
}
@article{DILUZIO2023104418,
title = {A randomized deep neural network for emotion recognition with landmarks detection},
journal = {Biomedical Signal Processing and Control},
volume = {81},
pages = {104418},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.104418},
url = {https://www.sciencedirect.com/science/article/pii/S1746809422008722},
author = {Francesco {Di Luzio} and Antonello Rosato and Massimo Panella},
keywords = {Emotion recognition, Randomized neural networks, Facial landmarks, Deep learning, Video sequences},
abstract = {In this paper, we present an innovative deep neural architecture employing parameter randomization in a complex classification model for emotion recognition. Actually, randomized deep neural networks represent an interesting alternative to exploring the efficiency-to-accuracy balance in real-life applications. Moreover, we also introduce the use of input frames composed of 468 facial landmarks coordinates and an innovative sampling procedure avoiding padding. The proposed randomized classifier is trained for emotion recognition on video sequences and the related accuracy is compared with a non-randomized version of the same model and with well-known benchmark architectures, demonstrating the robustness of the proposed approach in terms of classification accuracy and training time.}
}
@article{KARUNATHILAKE2019558,
title = {Renewable energy selection for net-zero energy communities: Life cycle based decision making under uncertainty},
journal = {Renewable Energy},
volume = {130},
pages = {558-573},
year = {2019},
issn = {0960-1481},
doi = {https://doi.org/10.1016/j.renene.2018.06.086},
url = {https://www.sciencedirect.com/science/article/pii/S0960148118307389},
author = {Hirushie Karunathilake and Kasun Hewage and Walter Mérida and Rehan Sadiq},
keywords = {Multi-criteria decision making, Life cycle thinking, Fuzzy techniques, Renewable energy, Community energy system planning},
abstract = {Developing net-zero energy communities powered by renewable energy (RE) resources has become a popular concept. To make the best choices for community-level net-zero energy systems, it is necessary to identify the best energy technologies at local level. Evaluation of RE technologies has to be extended from technical and economic aspects to include environmental and social wellbeing. It is possible to identify the true costs and benefits of energy use by taking a cradle-to-grave life cycle perspective. In this study, a RE screening and multi-stage energy selection framework was developed. A fuzzy multi-criteria decision making approach was used in ranking the technologies to incorporate the conflicting requirements, stakeholder priorities, and uncertainties. Different scenarios were investigated to reflect different decision maker priorities. Under a pro-environment scenario, small hydro, onshore wind, and biomass combustion technologies perform best. Under a pro-economic decision scenario, biomass combustion, small hydro, and landfill gas have the best rankings. Triple bottom line sustainability was combined with technical feasibility through a ruled-based approach to avoid the theoretical pitfalls inherent in energy-related decision making. This assessment is geared towards providing decision makers with flexible tools, and is expected to aid in the pre-project planning stage of RE projects.}
}
@article{LUO2011384,
title = {Application of Improved EAHP on Stability Evaluation of Coal Seam Roof},
journal = {Procedia Earth and Planetary Science},
volume = {3},
pages = {384-393},
year = {2011},
note = {2011 Xi'an International Conference on Fine Exploration and Control of Water & Gas in Coal Mines},
issn = {1878-5220},
doi = {https://doi.org/10.1016/j.proeps.2011.09.110},
url = {https://www.sciencedirect.com/science/article/pii/S1878522011001111},
author = {Donghai Luo and Shunxin Sun and Dunhu Zhang and Yuqing Wan and Guangchao Zhang and Junqiang Niu},
keywords = {Coal Seam Roof, Stability Evaluation, EAHP, Model},
abstract = {Stability of coal seam roof is one of the important factors to ensure safe and efficient coal production. Stability result is the complex interaction subjected to a larger number of geological factors. Only taking comprehensive consideration into evaluation can the result be in line with the actual complex geological environment. Main factors of coal seam roof stability are divided into four major factors and eight secondary factors. Major factors are sedimentary environment, structural feature, rock mechanics property and so on. Secondary factors are the combination of roof rock, lithology difference, bedding changes, and so on. Stability rank is divided into four grades: super stability, stability, basically stability and instability. EAHP model of stability evaluation of coal seam roof and the extension comparison matrix are established by means of the improved EAHP (Extension Analytical Hierarchy Process) method. Using the method based on judgment by possibility degree matrix can get the Sorting order. Evaluation results show that: the stability grade of main coal seam 5# roof of the mine is stable. It is true and credible. The method not only has the merits of “Extension to consider fuzziness of human thinking to judge”, but also eliminates a lot of spreadsheet work in traditional AHP. These studies are useful experiment and explore to study on comprehensive evaluation of coal seam roof stability.}
}
@article{CAI20051145,
title = {BioSim—a biomedical character-based problem solving environment},
journal = {Future Generation Computer Systems},
volume = {21},
number = {7},
pages = {1145-1156},
year = {2005},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2004.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X04000469},
author = {Yang Cai and Ingo Snel and Betty Cheng and B. {Suman Bharathi} and Clementine Klein and Judith Klein-Seetharaman},
keywords = {Scientific visualization, Biological discovery, Game design, Problem solving, Artificial life, Education},
abstract = {Understanding and solving biomedical problems requires insight into the complex interactions between the components of biomedical systems by domain and non-domain experts. This is challenging because of the enormous amount of data and knowledge in this domain. Therefore, non-traditional educational tools have been developed such as a biological storytelling system, animations of biomedical processes and concepts, and interactive virtual laboratories. The next-generation problem solving tools need to be more interactive to include users with any background, while remaining sufficiently flexible to target open research problems at any level of abstraction, from the conformational changes of a protein to the interaction of the various biochemical pathways in our body. Here, we present an interactive and visual problem solving environment for the biomedical domain. We designed a biological world model, in which users can explore biological interactions by role-playing “characters” such as cells and molecules or as an observer in a “shielded vessel”, both with the option of networked collaboration between simultaneous users. The system architecture of these “characters” contains four main components: (1) bio-behavior is modeled using cellular automata; (2) bio-morphing uses vision-based shape tracking techniques to learn from recordings of real biological dynamics; (3) bio-sensing is based on molecular principles of recognition to identify objects, environmental conditions and progression in a process; (4) bio-dynamics implements mathematical models of cell growth and fluid-dynamic properties of biological solutions. The principles are implemented in a simple world model of the human vascular system and a biomedical problem that involves an infection by Neisseria meningitides where the biological characters are white and red blood cells and Neisseria cells. Our case studies show that the problem solving environment can inspire user's strategic, creative and innovative thinking.}
}
@article{DIRKSEN2022102994,
title = {From agent to action: The use of ethnographic social simulation for crime research},
journal = {Futures},
volume = {142},
pages = {102994},
year = {2022},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2022.102994},
url = {https://www.sciencedirect.com/science/article/pii/S0016328722000945},
author = {Vanessa Dirksen and Martin Neumann and Ulf Lotzmann},
keywords = {Agent-based modelling, Complementarity, Computational social science, Simulation, Ethnography, Policing},
abstract = {This paper proposes a methodology for grounding agent-based social simulation in ethnographic data, using the example of crime research. The application of computational tools in crime research typically entails a removal of the “intelligible frame” of criminal behaviour and, hence, of meaningful evidence. Ethnography is a microscopic research tradition geared towards the preservation of contextualized meaning deemed essential for the exploration of the variety of prospective alternative scenarios and, hence, of plausible futures. On the basis of exemplary empirical material from a qualitative study on the transit trade of cocaine in the Netherlands, this paper looks into the complementarity and potential integration of the research traditions of ethnography and agent-based modelling. That is to say, it explores the compatibility of the formal languages of both these domains and the mutual benefit of “stitching together” these at first sight very different methods. The ethnographic approach to social simulation specifies the what-if relations of traditional/conventional ABM modelling into condition-action sequences. As we contend, it is exactly this more microscopic level of condition-action sequences that is needed to facilitate ”thick description” and, in turn, enable the grounding of ABM in meaningful evidence.}
}
@article{FALL2010140,
title = {Artificial states? On the enduring geographical myth of natural borders},
journal = {Political Geography},
volume = {29},
number = {3},
pages = {140-147},
year = {2010},
issn = {0962-6298},
doi = {https://doi.org/10.1016/j.polgeo.2010.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0962629810000533},
author = {Juliet J. Fall},
keywords = {Artificial states, Boundaries, Ethnic homogeneity, Failed states, Nationalism, Natural boundaries, Territorial trap},
abstract = {Alberto Alesina, William Easterly and Janina Matuszeski's paper Artificial States, published as a National Bureau of Economic Research Working Paper in June 2006, suggests a theory linking the nature of country borders to the economic success of countries (Alesina, Easterly, & Matuszeski, 2006). This paper critically examines this suggestion that natural boundaries and ethnic homogeneity are desirable for economic reasons. It takes issue with the understanding of artificial and natural boundaries that they develop, arguing that this ignores two centuries of critical and quantitative geographical scholarship that has mapped, documented and critiqued the obsession of a link between topography and the appropriate shape of states and boundaries. It explores how their argument is linked to a defence of ethnically homogeneous states. The focus is on their teleological and paradoxically ahistorical vision that naturalizes politics by appealing to spatial myths of homogeneity and geometric destiny, grounded in a reactionary understanding of space as container. In so doing, I am mindful of the strong links between such proposals and calls for post-conflict partition, and the corresponding discourses of ethnic and cultural homogenization on which they rely. Instead of thinking of boundaries as geometric objects, squiggly or not, I consider boundaries through the simultaneous processes of reification, naturalization, and fetishization.}
}
@article{WARING2015254,
title = {Managerial and non-technical factors in the development of human-created disasters: A review and research agenda},
journal = {Safety Science},
volume = {79},
pages = {254-267},
year = {2015},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2015.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S0925753515001575},
author = {Alan Waring},
keywords = {Major hazards, Disasters, Safety management, Safety culture, Risk decisions},
abstract = {A number of common underlying factors in the development of human-created disasters, as cited in numerous official inquiry reports, encompass in particular, safety management system defects and weaknesses in an organization’s safety culture. Human factors such as faulty risk cognition, bounded rationality, groupthink, failure of foresight and organizational learning, suspect motivations, reactive attitudes, and inappropriate risk decision-making, are commonly associated characteristics of such shortcomings. This article summarizes and discusses underlying managerial and non-technical factors in human-created major hazard accidents in the light of theories of accident causation, findings from disaster inquiries and published research, and the systemic holism-versus-reductionism debate. Ideally, all site operators would know and understand disaster aetiology and preventive requirements and be motivated to enact them. However, there is sufficient empirical evidence from inquiry reports into major hazard incidents and disasters that idealized enactment rarely occurs and in many cases safety policy and strategy as enacted is distant from espoused safety policy and strategy. Research questions relating to board level thinking and actions on major hazard risks are posited and a proposal for a more holistic and potentially more effective major hazard safety research framework is put forward.}
}
@incollection{REYESGARCIA2025151,
title = {Chapter 7 - Decoding imagined speech for EEG-based BCI},
editor = {Ayman S. El-Baz and Jasjit S. Suri},
booktitle = {Brain-Computer Interfaces},
publisher = {Academic Press},
pages = {151-175},
year = {2025},
series = {Advances in Neural Engineering},
isbn = {978-0-323-95439-6},
doi = {https://doi.org/10.1016/B978-0-323-95439-6.00004-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323954396000041},
author = {Carlos A. Reyes-García and Alejandro A. Torres-García and Tonatiuh Hernández-del-Toro and Jesús S. García-Salinas and Luis Villaseñor-Pineda},
keywords = {Brain–computer interfaces (BCI), Classification, Electroencephalograms (EEG), Imagined speech, Silent speech interfaces (SSI)},
abstract = {Brain–computer interfaces (BCIs) are systems that transform the brain's electrical activity into commands to control a device. To create a BCI, it is necessary to establish the relationship between a certain stimulus, internal or external, and the brain activity it provokes. A common approach in BCIs is motor imagery, which involves imagining limb movement. Unfortunately, this approach allows few commands. As an alternative, this chapter presents another approach, an internal language-related stimulus known as imagined speech, which is the action of imagining the diction of a word without emitting any sound or articulating any movement. This neuroparadigm is more intuitive, less subjective, and ambiguous, which are very relevant advantages; however, the cost to properly process the brain signal is not trivial. This chapter describes the main components of an EEG-based imagined speech BCI, along with key works, emerging trends, and challenges in this research area. Regarding the challenges, we present four of them in the pursuit of decoding imagined speech. The first challenge involves accurately recognizing isolated words. The second one is the automatic selection of a subset of EEG channels aiming to reduce computational cost and provide evidence of promising locations for studying imagined speech. The third challenge introduces an innovative approach to addressing scenarios where a new word needs to be added to the vocabulary after the computational model has been trained. Lastly, the fourth challenge concerns the online recognition of words from continuous EEG signals. Despite advances in the area, there is still much work to be done. Important initial steps have been taken in terms of the application of novel techniques for preprocessing, artifact removal, feature extraction, and classification which are the stages to be taken to process the collected signal. Additionally, the community has shared datasets and organized evaluation forums to accelerate the search for solutions.}
}
@article{BURCH2023101039,
title = {Investigating two teachers’ development of combinatorial meaning for algebraic structure},
journal = {The Journal of Mathematical Behavior},
volume = {70},
pages = {101039},
year = {2023},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2023.101039},
url = {https://www.sciencedirect.com/science/article/pii/S0732312323000093},
author = {Lori J. Burch},
keywords = {Mathematical meanings, Combinatorial reasoning, Algebraic reasoning, Polynomial operations, Secondary teachers},
abstract = {This paper reports on the results of a four-day teaching experiment that supported two algebra teachers to develop a combinatorial meaning for algebraic structure. The purpose of the teaching episodes was to support the teachers (a) to establish a combinatorial understanding for algebraic structure (Tillema & Burch, 2022) by generalizing the cubic identity, a+b3=a3+3a2b+3ab2+b3, as a symbolization of quantitative and combinatorial relationships out of a contextualized problem (Tillema & Gatza, 2016) and (b) to develop a combinatorial meaning as a mobilization of their understanding through a series of algebraic tasks (cf. Thompson et al., 2014). The findings from this study contribute to research literature on teachers’ mathematical meanings within secondary algebra by investigating how teachers’ combinatorial meanings developed and how differences in their combinatorial meanings impacted their algebraic reasoning. The findings demonstrate a combinatorial pathway for supporting the development of expanding and factoring as reversible polynomial operations (cf. Sangwin & Jones, 2017).}
}
@article{JAYAWARDENA2025118097,
title = {Marine specialized metabolites: Unveiling Nature's chemical treasures from the deep blue},
journal = {TrAC Trends in Analytical Chemistry},
volume = {183},
pages = {118097},
year = {2025},
issn = {0165-9936},
doi = {https://doi.org/10.1016/j.trac.2024.118097},
url = {https://www.sciencedirect.com/science/article/pii/S0165993624005806},
author = {Thilina U. Jayawardena and Natacha Merindol and Nuwan Sameera Liyanage and Fatima Awwad and Isabel Desgagné-Penix},
keywords = {Natural products, Conservation, Bioprospecting, metabolomics, Biotechnology, Biosynthesis, Isolation and spectroscopic characterization},
abstract = {Marine specialized metabolites (MSM) represent a fascinating realm of chemical diversity with multifaceted functions across the spectrum of life on Earth. These metabolites serve as weapons, metal transporters, regulatory agents, and more. The conservation of genes responsible for their production over extensive evolutionary timescales underscores their selective advantage. Recent decades have witnessed an upsurge in MSM studies, driven by advancements in analytical techniques and the ever-growing accessibility of the aquatic environment. Marine macro and microorganisms offer a rich tapestry of specialized metabolites, some exhibiting potent activities in diverse domains, including medicine. The study of MSM presents several challenges, reflecting the need to separate complex mixtures into individual bioactive metabolites and utilize state-of-the-art extraction methods. Comprehensive structural analysis relies on advanced spectroscopic approaches, including nuclear magnetic resonance and mass spectrometry. These tools are instrumental in unravelling the chemical diversity of MSM and understanding their potential applications. While bioprospecting offers enormous potential, it raises critical challenges concerning sustainability, conservation, and equitable benefit-sharing. International protocols like the Nagoya Protocol seeks to regulate access to and share benefits from genetic resources, with considerable implications for marine bioprospecting. The convergence of advanced metabolomics, metagenomics, and synthetic biology offers promising avenues for accelerating the discovery and sustainable production of MSM, shaping the future of this field. This comprehensive review provides a deep dive into the challenges, methodologies, and emerging trends in studying marine-derived natural products, underscoring the immense potential of MSM for advancing chemical sciences and their transformative applications in diverse areas such as food, medicine, biotechnology, and environmental conservation. By bridging multiple disciplines, the continued exploration and sustainable utilization of these metabolites hold the promise of unlocking new innovations for society's benefit.}
}
@article{KEAN2025109125,
title = {Intuitive physical reasoning is not mediated by linguistic nor exclusively domain-general abstract representations},
journal = {Neuropsychologia},
pages = {109125},
year = {2025},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2025.109125},
url = {https://www.sciencedirect.com/science/article/pii/S0028393225000600},
author = {Hope H. Kean and Alexander Fung and R.T. Pramod and Jessica Chomik-Morales and Nancy Kanwisher and Evelina Fedorenko},
abstract = {The ability to reason about the physical world is a critical tool in the human cognitive toolbox, but the nature of the representations that mediate physical reasoning remains debated. Here, we use fMRI to illuminate this question by investigating the relationship between the physical-reasoning system and two well-characterized systems: a) the domain-general Multiple Demand (MD) system, which supports abstract reasoning, including mathematical and logical reasoning, and b) the language system, which supports linguistic computations and has been hypothesized to mediate some forms of thought. We replicate prior findings of a network of frontal and parietal areas that are robustly engaged by physical reasoning and identify an additional physical-reasoning area in the left frontal cortex, which also houses components of the MD and language systems. Critically, direct comparisons with tasks that target the MD and the language systems reveal that the physical-reasoning system overlaps with the MD system, but is dissociable from it in fine-grained activation patterns, which replicates prior work. Moreover, the physical-reasoning system does not overlap with the language system. These results suggest that physical reasoning does not rely on linguistic representations, nor exclusively on the domain-general abstract reasoning that the MD system supports.}
}
@article{LORIMER2009152,
title = {Empathic accuracy in coach–athlete dyads who participate in team and individual sports},
journal = {Psychology of Sport and Exercise},
volume = {10},
number = {1},
pages = {152-158},
year = {2009},
issn = {1469-0292},
doi = {https://doi.org/10.1016/j.psychsport.2008.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1469029208000526},
author = {Ross Lorimer and Sophia Jowett},
keywords = {Empathy, Understanding, Interaction, Coach–athlete dyads},
abstract = {Objective
The purpose of the present study was to investigate the empathic accuracy of coach–athlete dyads participating in team and individual sports.
Method
An adaptation of Ickes's [2001. Measuring empathic accuracy. In J. A. Hall & F. J. Bernieri (Eds.), Interpersonal sensitivity (pp. 219–242). Mahwah, NJ: Lawrence Erlbaum Associates] unstructured dyadic interaction paradigm was used to assess the empathic accuracy of 40 coach–athlete dyads. Accordingly, each dyad was filmed during a training session. The dyad members viewed selected video footage that displayed discrete interactions that had naturally occurred during that session. Dyad members reported what they remembered thinking/feeling while making inferences about what their partner's thought/felt at each point. Empathic accuracy was estimated by comparing self-reports and inferences.
Results
The results indicted that accuracy for coaches in individual sports was higher than coaches in team sports. Shared cognitive focus also differed between team and individual sports, and fully mediated the effect of sport-type on coach empathic accuracy. Moreover, coaches whose training sessions were longer demonstrated increased empathic accuracy. No differences were found for athletes.
Conclusions
The results suggest that the dynamics of the interaction between a coach and an athlete play a key role in how accurately they perceive each other.}
}
@article{WALHA20252959,
title = {Deep Learning and Machine Learning Architectures for Dementia Detection from Speech in Women},
journal = {CMES - Computer Modeling in Engineering and Sciences},
volume = {142},
number = {3},
pages = {2959-3001},
year = {2025},
issn = {1526-1492},
doi = {https://doi.org/10.32604/cmes.2025.060545},
url = {https://www.sciencedirect.com/science/article/pii/S1526149225000396},
author = {Ahlem Walha and Amel Ksibi and Mohammed Zakariah and Manel Ayadi and Tagrid Alshalali and Oumaima Saidani and Leila Jamel and Nouf Abdullah Almujally},
keywords = {Dementia detection in women, Alzheimer’s disease, deep learning, machine learning, support vector machine, voting classifier},
abstract = {Dementia is a neurological disorder that affects the brain and its functioning, and women experience its effects more than men do. Preventive care often requires non-invasive and rapid tests, yet conventional diagnostic techniques are time-consuming and invasive. One of the most effective ways to diagnose dementia is by analyzing a patient’s speech, which is cheap and does not require surgery. This research aims to determine the effectiveness of deep learning (DL) and machine learning (ML) structures in diagnosing dementia based on women’s speech patterns. The study analyzes data drawn from the Pitt Corpus, which contains 298 dementia files and 238 control files from the Dementia Bank database. Deep learning models and SVM classifiers were used to analyze the available audio samples in the dataset. Our methodology used two methods: a DL-ML model and a single DL model for the classification of diabetics and a single DL model. The deep learning model achieved an astronomic level of accuracy of 99.99% with an F1 score of 0.9998, Precision of 0.9997, and recall of 0.9998. The proposed DL-ML fusion model was equally impressive, with an accuracy of 99.99%, F1 score of 0.9995, Precision of 0.9998, and recall of 0.9997. Also, the study reveals how to apply deep learning and machine learning models for dementia detection from speech with high accuracy and low computational complexity. This research work, therefore, concludes by showing the possibility of using speech-based dementia detection as a possibly helpful early diagnosis mode. For even further enhanced model performance and better generalization, future studies may explore real-time applications and the inclusion of other components of speech.}
}
@article{ALAGEEL20152003,
title = {Human Factors in the Design and Evaluation of Bioinformatics Tools},
journal = {Procedia Manufacturing},
volume = {3},
pages = {2003-2010},
year = {2015},
note = {6th International Conference on Applied Human Factors and Ergonomics (AHFE 2015) and the Affiliated Conferences, AHFE 2015},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2015.07.247},
url = {https://www.sciencedirect.com/science/article/pii/S2351978915002486},
author = {Naelah Al-Ageel and Areej Al-Wabil and Ghada Badr and Noura AlOmar},
keywords = {Bioinformatics tools, Human factors, Usability metrics, Heuristics evaluation},
abstract = {Human factors contribute significantly to the information visualization design considerations and usability evaluation process, and have been shown to play an important role in the design, development and quality assurance of bioinformatics tools. Despite the technological advances in bioinformatics computational methods, humans are an indispensable part of the data mining and decision making process. The complexity of biology data visualization can make perception and analysis a complex cognitive activity for professionals in the bioinformatics domain. Information Visualization (InfoVis) can provide valuable assistance for data analysis in bioinformatics by visually depicting sequences, genomes, alignments, and macromolecular structures. InfoVis coupled with interaction modalities of bioinformatics tools also impact the efficiency and effectiveness of decision-making tasks in applied bioinformatics computing. However, the way people perceive and interact with bioinformatics tools can strongly influence their understanding of the complex data as well as the perceived usability and accessibility of these systems. In this paper, we present a synthesis of research studies and initiatives that have recently examined human factors in interaction and visualization for bioinformatics tools, particularly in perception-based design. Although bioinformatics’ visualization and interaction design research that involves human factors is considered in its infancy, a plethora of potentially promising areas have yet to be explored. The aims of this paper are to review current human factors research in interaction, usability and visualization within bioinformatics tools to provide a basis for future investigations in systems and software engineering of bioinformatics tools, and to identify promising areas for future research directions in interaction design of bioinformatics tools.}
}
@incollection{RAPAPORT1994225,
title = {CHAPTER 10 - Syntactic Semantics: Foundations of Computational Natural-Language Understanding},
editor = {Eric Dietrich},
booktitle = {Thinking Computers and Virtual Persons},
publisher = {Academic Press},
pages = {225-273},
year = {1994},
isbn = {978-0-12-215495-9},
doi = {https://doi.org/10.1016/B978-0-12-215495-9.50015-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780122154959500156},
author = {William J. Rapaport},
abstract = {Publisher Summary
This chapter discusses the way by which it is possible to understand natural language and whether a computer could do so. It presents the argument that although a certain kind of semantic interpretation is needed for understanding natural language, it is a kind that only involves syntactic symbol manipulation of precisely the sort of which computers are capable, so that it is possible, in principle, for computers to understand natural language. The chapter highlights the recent arguments by John R. Searle and by Fred Dretske to the effect that computers cannot understand natural language. A program is like the script of a play. The computer is like the actors, sets, and a process is like an actual production of the play—the play in the process of being performed. The computer must be able to do things like: to convince someone, to imitate a human, that is, it must not merely be a cognitive agent, but also an acting one. In particular, to imitate a human, the computer needs to be able to reason about what another cognitive agent, such as a human, believes.}
}
@incollection{GARDNER202451,
title = {Chapter 3 - Designing and prototyping smarter urban spaces},
editor = {Nicole Gardner},
booktitle = {Scaling the Smart City},
publisher = {Elsevier},
pages = {51-74},
year = {2024},
series = {Smart Cities},
isbn = {978-0-443-18452-9},
doi = {https://doi.org/10.1016/B978-0-443-18452-9.00009-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443184529000094},
author = {Nicole Gardner},
keywords = {Cyber-physical system, Design, Design education, Design pedagogy, Design process, Human-computer interaction, Interaction, Interaction design, IoT, Physical computing, Prototyping, Smart city, Urban technology},
abstract = {This chapter outlines how the concept of the smart city is explored and challenged in an undergraduate design course that adopts a cross-scale framework to design and prototype urban technology projects. It sets out an integrated and interdisciplinary approach to urban technology design that combines context-oriented spatial design methods with physical computing and interaction principles. It construes the design and prototyping of urban technology projects as sociotechnical thought experiments that can materialize ethical concerns and explore alternate ways that urban life can be lived with technology. The chapter concludes by outlining the themes that organize selected existing and speculative urban technology projects in the following chapters of the book.}
}
@article{HUDA2024122380,
title = {Experts and intelligent systems for smart homes’ Transformation to Sustainable Smart Cities: A comprehensive review},
journal = {Expert Systems with Applications},
volume = {238},
pages = {122380},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122380},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423028828},
author = {Noor Ul Huda and Ijaz Ahmed and Muhammad Adnan and Mansoor Ali and Faisal Naeem},
keywords = {Artificial intelligence, Automation, Block chain, Energy management, Expert intelligent systems, Management systems, Smart cities},
abstract = {In this constantly evolving landscape of urbanization, the relationship between technology and automation, in regards to sustainability, holds immense significance. The intricate strands of human intelligence are seamlessly interwoven with the fabric of technological progress, giving rise to exquisite patterns of synergy and collaborative innovation. Automation is just another step in this process which started with the industrial revolution and now has paved way towards urbanization. Smart homes or home automation is a subset of Internet of Things (IoT) based automation that has added into the comfort, ease, and quality of our living standards and is now being integrated to form the concept of Smart Cities. In the past decade, various techniques and processes of smart home automation have been proposed and implemented. To extend and translate the existing methods into new one, the understanding of the former is imperative to the research procedure. This review stands as a comprehensive exploration, diving into the pivotal role of intelligent systems and expert knowledge in driving the transformation of smart homes into sustainable smart cities. By meticulously analyzing and aggregating an array of contemporary techniques used in smart homes, this paper offers profound contributions to the intersection of urban evolution and technological innovation. The review’s holistic approach not only facilitates a deep understanding of smart homes’ contributions but also charts a course for innovative strategies in city planning, infrastructure, and technological integration. In bridging the gap between technology and sustainable urban development, this exploration underscores the transformative power of leveraging smart home techniques to lay the foundation for harmonious and forward-thinking smart cities. The technologies cover a wide range of methodologies and intelligent systems used for communication, security and management in an urban infrastructure. The paper focuses on analysis of the technology to provide an outlook into achieving the goal of sustainable smart cities and deal with challenges like scalability and big data computation. Our comprehensive analysis yields a holistic set of technology comparisons and illuminates the promising future prospects within this domain. The information is highly insightful in creating a bigger picture for adopting state of the art technologies like Federated Learning (FL), Digital Twin and Embedded Edge computing in better planning and infrastructure management in smart cities. These findings offer reliable and potent methods to chart not only the course of research but also to enhance these technologies for the betterment of mankind’s convenience and advancement.}
}
@article{AMMAR2018116,
title = {MPEG-4 AVC stream-based saliency detection. Application to robust watermarking},
journal = {Signal Processing: Image Communication},
volume = {60},
pages = {116-130},
year = {2018},
issn = {0923-5965},
doi = {https://doi.org/10.1016/j.image.2017.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S0923596517301674},
author = {Marwa Ammar and Mihai Mitrea and Marwen Hasnaoui and Patrick {Le Callet}},
keywords = {Saliency map, MPEG-4 AVC stream, Density fixation map, Saccade locations, Robust watermarking},
abstract = {By bridging uncompressed-domain saliency detection and MPEG-4 AVC compression principles, the present paper advances a methodological framework for extracting the saliency maps directly from the stream syntax elements. In this respect, inside each GOP, the intensity, color, orientation and motion elementary saliency maps are related to the energy of the luma coefficients, to the energy of chroma coefficients, to the gradient of the prediction modes and to the amplitude of the motion vectors, respectively. The three spatial saliency maps are pooled according to an average formula, while the static-temporal fusion is achieved by six different formulas. The experiments consider both ground-truth and applicative evaluations. The ground-truth benchmarking investigates the relation between the predicted MPEG-4 AVC saliency map and the actual human saliency, captured by eye-tracking devices. It is based on two corpora (representing density fixation maps and saccade locations), two objective criteria (related to the closeness between the predicted and the real saliency maps and to the difference between the behavior of the predicted saliency map in fixation and random locations), two objective measures (KLD – the Kullback Leibler Divergence and AUC – the Area Under the ROC Curve) and 5 state-of-the-art saliency models (3 acting in spatial domain and 2 acting in compressed domain). The applicative validation is carried out by integrating the MPEG-4 AVC saliency map into a robust watermarking application. As an overall conclusion, the paper demonstrates that although the MPEG-4 AVC standard does not explicitly relies on any visual saliency principle, its stream syntax elements preserve this property. Four main benefits for the MPEG-4 AVC based saliency extraction are thus brought to light: (1) it outperforms (or, at least, is as good as) state-of-the-art uncompressed domain methods, (2) it allows significant gains to be obtained in watermarking transparency (for prescribed data payload and robustness), (3) it is less sensitive to the randomness in the processed visual content, and (4) it has a linear computational complexity. For instance, the ground truth results exhibit absolute relative gains between 60% and 164% in KLD, between 17% and 21% in AUC, and relative gains in KLD sensitivity between 1.18 and 6.12 and in AUC sensitivity between 1.06 and 33.7; the applicative validation brings to light transparency gains up to 10 dB in PSNR.}
}
@article{JACOB2022470,
title = {Algorithmic Approaches to Classify Autism Spectrum Disorders: A Research Perspective},
journal = {Procedia Computer Science},
volume = {201},
pages = {470-477},
year = {2022},
note = {The 13th International Conference on Ambient Systems, Networks and Technologies (ANT) / The 5th International Conference on Emerging Data and Industry 4.0 (EDI40)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.03.061},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922004744},
author = {Shomona Gracia Jacob and Majdi Mohammed {Bait Ali Sulaiman} and Bensujin Bennet},
keywords = {Machine learning, Supervised learning, Pattern discovery, Autism disorder, Data Mining},
abstract = {Autism Spectrum Disorder (ASD) is a neurodevelopmental disability that exhibits sluggish progress in vocal development, restricted interest in normal activity and repetitive disoriented behavior. This syndrome, has gained a lot of attention due to its prevalence among children across all countries and from different economic backgrounds. However, ASD detection and treatment yet remains in its infancy due to the lack of awareness among parents, limited screening of proper developmental milestones and a dearth of diagnostic tools to classify this syndrome with convincing accuracy. Recent studies report that scalable biomarkers for early detection have made little progress in research due to the erraticism of this disorder. Moreover, the study on developing tools or applications for parents, teachers, and healthcare workers to identify children who exhibit any form of autism is still a work in progress. The research work undertaken in this paper presents an analysis of supervised machine learning algorithms on mining interesting details that link the diverse nature of ASD and the possibility of computationally detecting markers for the syndrome. The preliminary findings on the performance of traditional machine learning algorithms in ASD classification is reported with the possibility of integrating deep learning architectures for ASD detection and therapy.}
}
@incollection{FLOTHER202583,
title = {Chapter 6 - Early quantum computing applications on the path towards precision medicine},
editor = {Laura Kelly and William P. Stanford},
booktitle = {Implementation of Personalized Precision Medicine},
publisher = {Academic Press},
pages = {83-96},
year = {2025},
isbn = {978-0-323-98808-7},
doi = {https://doi.org/10.1016/B978-0-323-98808-7.00001-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323988087000011},
author = {Frederik F. Flöther},
keywords = {Precision medicine, Quantum computers, Artificial intelligence/machine learning, EHRs},
abstract = {The last few years have seen rapid progress in transitioning quantum computing from lab to industry. In healthcare and life sciences, more than 40 proof-of-concept experiments and studies have been conducted; an increasing number of these are even run on real quantum hardware. Major investments have been made with hundreds of millions of dollars already allocated towards quantum applications and hardware in medicine. In addition to pharmaceutical and life sciences uses, clinical and medical applications are now increasingly coming into the picture. This chapter focuses on three key use case areas associated with (precision) medicine, including genomics and clinical research, diagnostics, and treatments and interventions. Examples of organizations and the use cases they have been researching are given; ideas how the development of practical quantum computing applications can be further accelerated are described.}
}
@article{JANSEN2022100020,
title = {The illusion of data validity: Why numbers about people are likely wrong},
journal = {Data and Information Management},
volume = {6},
number = {4},
pages = {100020},
year = {2022},
issn = {2543-9251},
doi = {https://doi.org/10.1016/j.dim.2022.100020},
url = {https://www.sciencedirect.com/science/article/pii/S2543925122001188},
author = {Bernard J. Jansen and Joni Salminen and Soon-gyo Jung and Hind Almerekhi},
keywords = {People data, Measurement, Quantitative paradigm, Statistics},
abstract = {This reflection article addresses a difficulty faced by scholars and practitioners working with numbers about people, which is that those who study people want numerical data about these people. Unfortunately, time and time again, this numerical data about people is wrong. Addressing the potential causes of this wrongness, we present examples of analyzing people numbers, i.e., numbers derived from digital data by or about people, and discuss the comforting illusion of data validity. We first lay a foundation by highlighting potential inaccuracies in collecting people data, such as selection bias. Then, we discuss inaccuracies in analyzing people data, such as the flaw of averages, followed by a discussion of errors that are made when trying to make sense of people data through techniques such as posterior labeling. Finally, we discuss a root cause of people data often being wrong – the conceptual conundrum of thinking the numbers are counts when they are actually measures. Practical solutions to address this illusion of data validity are proposed. The implications for theories derived from people data are also highlighted, namely that these people theories are generally wrong as they are often derived from people numbers that are wrong.}
}
@article{RENNA2023104420,
title = {A randomized controlled trial comparing two doses of emotion regulation therapy: Preliminary evidence that gains in attentional and metacognitive regulation reduce worry, rumination, and distress},
journal = {Behaviour Research and Therapy},
volume = {170},
pages = {104420},
year = {2023},
issn = {0005-7967},
doi = {https://doi.org/10.1016/j.brat.2023.104420},
url = {https://www.sciencedirect.com/science/article/pii/S0005796723001687},
author = {Megan E. Renna and Phillip E. Spaeth and Jean M. Quintero and Mia S. O'Toole and Christina F. Sandman and David M. Fresco and Douglas S. Mennin},
keywords = {Emotion regulation, Randomized controlled trial, Distress, Anxiety, Depression},
abstract = {Background
Emotion regulation therapy (ERT) promotes resilience in distress disorders by strengthening attentional and metacognitive capacities. Regulation skills are presented with the goal of ameliorating the perseverative negative thinking (PNT) that characterizes these disorders. This study tested ERT in a randomized controlled trial comparing the effectiveness of 16-session (ERT16) versus 8-session (ERT8) doses.
Method
Patients (N = 72) endorsing elevated worry and/or rumination and meeting diagnostic criteria for a distress disorder were randomized to ERT8 or ERT16. PNT, anxiety/depressive symptoms, functioning/quality of life, and treatment mechanisms (attention shifting, attention focusing, decentering, reappraisal) were measured at pre, mid, and post treatment. Clinical symptom severity was also assigned via diagnostic interview at each timepoint.
Results
ERT produced significant improvements across outcomes. ERT16 showed an advantage over ERT8 for distress disorder severity, worry, rumination, and attention shifting from pre-post treatment. Changes in ERT treatment mechanisms mediated changes in clinical improvement.
Conclusion
These findings provide evidence of the effectiveness of two doses of ERT in reducing PNT and distress through improvements in regulation skills.
Clinicaltrials.gov identifier
NCT04060940.}
}
@article{HESTER2019187,
title = {Simulation of integrative physiology for medical education},
journal = {Morphologie},
volume = {103},
number = {343},
pages = {187-193},
year = {2019},
issn = {1286-0115},
doi = {https://doi.org/10.1016/j.morpho.2019.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S1286011519300554},
author = {R.L. Hester and W. Pruett and J. Clemmer and A. Ruckdeschel},
keywords = {VPH, Simulation, Physiology, Healthcare, Electronic health record},
abstract = {Summary
Medical education is founded on the understanding of physiology. While lecture materials and reading contribute to the learning of physiology, the richness and complexity of the subject suggest that more active learning methods may provide a richer introduction to the science as it applies to the practice of medicine. Simulation has been previously used in basic science to better understand the interaction of physiological systems. In the current context, simulation generally refers to interactive case studies performed with a manikin or anatomic device. More recently, simulation has grown to encompass computational simulation: virtual models of physiology and pathophysiology where students can see in a mechanistic setting how tissues and organs interact with one another to respond to changes in their environment. In this manuscript, we discuss how simulation fits into the overall history of medical education, and detail two computational simulation products designed for medical education. The first of these is an acute simulator, JustPhysiology, which reduces the scope of a large model, HumMod, down to a more focused interface. The second is Sycamore, an electronic health record-delivered, real time simulator of patients designed to teach chronic patient care to students. These products represent a new type of tool for medical and allied health students to encourage active learning and integration of basic science knowledge into clinical situations.
Résumé
L’étude de la médecine est fondée entre autres sur la compréhension de la physiologie. Bien que l’apprentissage de la physiologie puisse se faire au moyen de cours magistraux et la lecture de contenus spécialisés, la richesse et la complexité du sujet laissent supposer que des méthodes d’apprentissage plus interactives puissent susciter une initiation plus élaborée de cette science et de son application à la pratique de la médecine. La simulation a précédemment été appliquée aux sciences fondamentales afin de mieux comprendre l’interaction entre systèmes physiologiques. Dans le contexte actuel, la simulation réfère en général à des études de cas interactives réalisées à l’aide d’un mannequin ou tout autre modèle anatomique. Plus récemment, la simulation s’est étendue à la simulation informatique incluant des modèles virtuels de physiologie et de physiopathologie à partir desquels les étudiants peuvent apprécier dans un contexte mécanistique comment les tissus et organes interagissent dans leur réponse à tout changement environnemental. Dans cet article nous présentons comment la simulation s’intègre dans l’histoire de l’éducation de la médecine et détaillons deux modèles de simulation informatique adaptés à l’éducation médicale. Le premier modèle, JustPhysiology, est un simulateur de courte durée qui réduit le champ d’action d’un simulateur plus complexe, HumMod, à une interface plus spécialisée. Le second outil est Sycamore, un dossier de santé électronique généré en temps réel et conçu pour un apprentissage de la pratique de soins médicaux en continu. Ces simulateurs informatiques représentent un nouvel outil pour les étudiants en médecine et autres professions de santé afin d’encourager un apprentissage actif et l’intégration de concepts scientifiques fondamentaux aux conditions cliniques.}
}
@article{BASOV2024101869,
title = {Professional patios, emotional studios: Locating social ties in European art residences},
journal = {Poetics},
volume = {102},
pages = {101869},
year = {2024},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2024.101869},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X24000081},
author = {Nikita Basov and Dafne Muntanyola-Saura and Sergi Méndez and Oleksandra Nenko},
keywords = {Material space, Social network, Socio-material network analysis, Mixed method, Statistical modeling of ethnographic data, Artistic residence},
abstract = {To foster creativity through sociality, residences put artists together. At the same time, in their quest for originality, artists often opt for individualism. Little is known on how physical collocation in residences affects artistic sociality. Addressing this gap, we draw on a combination of interviews, observations, and surveys, analysed with an innovative mixture of abductive coding, computational space analysis, and statistical network modeling. This allows us to unveil how room sharing and object usage relate to friendships and collaborations between residents. Along with explicit individualism of artists, we spot plenty of social ties between them. And these ties are positively related to joint material embeddedness. Simultaneously, the two main types of residential zones – working studios and leisure areas – appear to encourage the types of social ties inverse to our expectations. Our findings inform the practice of artistic residence organising and the proposed approach enables explanatory analysis of the relation between material space and sociality in various settings.}
}
@article{DURSTEWITZ2008739,
title = {The Dual-State Theory of Prefrontal Cortex Dopamine Function with Relevance to Catechol-O-Methyltransferase Genotypes and Schizophrenia},
journal = {Biological Psychiatry},
volume = {64},
number = {9},
pages = {739-749},
year = {2008},
note = {Neurodevelopment and the Transition from Schizophrenia Prodrome to Schizophrenia},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2008.05.015},
url = {https://www.sciencedirect.com/science/article/pii/S000632230800646X},
author = {Daniel Durstewitz and Jeremy K. Seamans},
keywords = {Attractor dynamics, computational model, dopamine, GABA currents, NMDA currents, prefrontal cortex, schizophrenia},
abstract = {There is now general consensus that at least some of the cognitive deficits in schizophrenia are related to dysfunctions in the prefrontal cortex (PFC) dopamine (DA) system. At the cellular and synaptic level, the effects of DA in PFC via D1- and D2-class receptors are highly complex, often apparently opposing, and hence difficult to understand with regard to their functional implications. Biophysically realistic computational models have provided valuable insights into how the effects of DA on PFC neurons and synaptic currents as measured in vitro link up to the neural network and cognitive levels. They suggest the existence of two discrete dynamical regimes, a D1-dominated state characterized by a high energy barrier among different network patterns that favors robust online maintenance of information and a D2-dominated state characterized by a low energy barrier that is beneficial for flexible and fast switching among representational states. These predictions are consistent with a variety of electrophysiological, neuroimaging, and behavioral results in humans and nonhuman species. Moreover, these biophysically based models predict that imbalanced D1:D2 receptor activation causing extremely low or extremely high energy barriers among activity states could lead to the emergence of cognitive, positive, and negative symptoms observed in schizophrenia. Thus, combined experimental and computational approaches hold the promise of allowing a detailed mechanistic understanding of how DA alters information processing in normal and pathological conditions, thereby potentially providing new routes for the development of pharmacological treatments for schizophrenia.}
}
@article{KARSTEN2020104512,
title = {Closing the gap: Merging engineering and anthropology in holistic fire safety assessments in the maritime and offshore industries},
journal = {Safety Science},
volume = {122},
pages = {104512},
year = {2020},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2019.104512},
url = {https://www.sciencedirect.com/science/article/pii/S0925753518316175},
author = {Mette Marie Vad Karsten and Aqqalu Thorbjørn Ruge and Thomas Hulin},
keywords = {Anthropology, Fire safety engineering, Transdisciplinary, Risk, Maritime, Offshore},
abstract = {This article reports on the endeavor to merge the fields of anthropology and fire safety engineering in holistic fire safety assessments within the maritime and offshore industries. The article suggests a combination of the two disciplines to transition from an interdisciplinary approach towards transdisciplinarity. The approach has been developed and adjusted during three cases of risk analyses and prevention strategies on fire safety. The article presents two methodological insights illustrating the necessary attitude of interdisciplinarity as a foundation towards transdisciplinarity. It advocates for the need of willingness in organizations and project teams to consider both disciplines as equally valid, integrate them in research definition, and create a base for common understanding. Subsequently, it is proposed that transdisciplinary work requires the creation of a group of core members acting as guarantors of transdisciplinarity, thus becoming themselves transdisciplinary humans working in a joined framework of thinking and methods. The article also presents two operational findings integrating the two disciplines within the area of fire safety. The first finding concerns including ‘daily operations’ in fire safety design, as daily practices and perceptions among crew can have a high impact on fire safety. The second finding concerns ‘reclassification of space and place’. It highlights mixing and shifting between work- and leisure-related practices within the same physical space, leading to the identification of new fire scenarios. It also explores the shifts between work, leisure, and emergency places, and their link to the shifts in professional roles of crew.}
}
@incollection{BAKER2023209,
title = {From capacity to ability to automation: “Western” conceptions of the figure of man and ableist subjectivities},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {209-218},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.12005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305120056},
author = {Bernadette Baker},
keywords = {Ableist subjectivities, Wynter, Colonialism, Humanism, Locke, Gall, Artificial intelligence (AI)},
abstract = {This chapter analyzes the shifting construct of ableist subjectivities that lie at the heart of a modernity-colonialism-racialization vortex and that were entangled with humanism's rise. Drawing on and building on the work of Sylvia Wynter, it examines how a “figure of Man” in three different but related iterations helped shape ontological hierarchies and violent inclusions/exclusions. It illustrates these moves via whitestream scholarship that came to dominate certain geopolitical locales from the 1600s, 1800s, and 2000s and that subsequently spread. From discourses of capacity (tutoring children), to ability (compulsory schooling), to automation (systems theory and computational superintelligence), the figure of Man not only overrepresents for “the human” but also puts in jeopardy all lives and ways of being beyond its exclusive borders. The questions that remain for education exceed standard policy debates about school reform, inclusive schooling, or evaluation, pointing instead toward a wider planetary context, existential issues, and power relations that sit within the tensions between “Man's” exclusivity, “the human's” idiosyncracies and primacy, and the potential rewriting/erasure of both via new technologies.}
}
@article{QI2021338821,
title = {Accurate diagnosis of lung tissues for 2D Raman spectrogram by deep learning based on short-time Fourier transform},
journal = {Analytica Chimica Acta},
volume = {1179},
pages = {338821},
year = {2021},
issn = {0003-2670},
doi = {https://doi.org/10.1016/j.aca.2021.338821},
url = {https://www.sciencedirect.com/science/article/pii/S0003267021006474},
author = {Yafeng Qi and Lin Yang and Bangxu Liu and Li Liu and Yuhong Liu and Qingfeng Zheng and Dameng Liu and Jianbin Luo},
keywords = {Raman spectrogram, Lung cancer, Short-time Fourier transform, Deep learning},
abstract = {Multivariate statistical analysis methods have an important role in spectrochemical analyses to rapidly identify and diagnose cancer and the subtype. However, utilizing these methods to analyze lager amount spectral data is challenging, and poses a major bottleneck toward achieving high accuracy. Here, a new convolutional neural networks (CNN) method based on short-time Fourier transform (STFT) to diagnose lung tissues via Raman spectra readily is proposed. The models yield that the accuracies of the new method are higher than the conventional methods (principal components analysis -linear discriminant analysis and support vector machine) for validation group (95.2% vs 85.5%, 94.4%) and test group (96.5% vs 90.4%, 93.9%) after cross-validation. The results illustrate that the new method which converts one-dimensional Raman data into two-dimensional Raman spectrograms improve the discriminatory ability of lung tissues and can achieve automatically accurate diagnosis of lung tissues.}
}
@article{NAKHLE2024100411,
title = {Shrinking the giants: Paving the way for TinyAI},
journal = {Device},
volume = {2},
number = {8},
pages = {100411},
year = {2024},
issn = {2666-9986},
doi = {https://doi.org/10.1016/j.device.2024.100411},
url = {https://www.sciencedirect.com/science/article/pii/S2666998624002473},
author = {Farid Nakhle},
keywords = {accelerated models, compressed models, miniaturized intelligence, tiny artificial intelligence, tiny machine learning},
abstract = {Summary
In the current era of technological advancement, the quest for more efficient and accessible artificial intelligence (AI) is driving the investigation of the predictive potential of small architecture-based, compressed, and accelerated AI models (TinyAI) and the benefits of running those on small-scale digital edge computing devices. This perspective delves into the expanding world of TinyAI, envisioning a future in which powerful machine intelligence can be encapsulated within pocket-sized devices, and discusses the technological challenges and opportunities associated with it. In addition, some of the myriad applications and benefits that can arise from their deployment will be discussed.}
}
@article{TUPPURAINEN2024108835,
title = {Conceptual design of furfural extraction, oxidative upgrading and product recovery: COSMO-RS-based process-level solvent screening},
journal = {Computers & Chemical Engineering},
volume = {191},
pages = {108835},
year = {2024},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2024.108835},
url = {https://www.sciencedirect.com/science/article/pii/S0098135424002539},
author = {Ville Tuppurainen and Lorenz Fleitmann and Jani Kangas and Kai Leonhard and Juha Tanskanen},
keywords = {Furfural oxidation, Hydrogen peroxide, Conceptual process design, COSMO-RS predictive thermodynamics, Solvent screening},
abstract = {Liquid phase oxidation of furfural using hydrogen peroxide offers a promising route for bio-based C4 furanones and diacids; however, only dilute water-based process designs have been previously suggested that have limited techno-economic potential. In this study, a conceptual process design is presented, where aqueous furfural is extracted using an organic solvent, coupled with peroxide oxidation and product recovery in the presence of the solvent. To address the problem of solvent selection, the COSMO-RS-based solvent screening framework is applied, where quantum mechanics-based thermodynamics are utilized in pinch-based process models. About 2500 solvent candidates were identified as feasible. Focusing on a set of 400 solvent candidates revealed energy consumption values (Qreb,tot/ṁprod recov) between approximately 2 MWh/tonne and 33 MWh/tonne, signifying the potential of the solvent-based process in outperforming the reference aqueous process (49.4 MWh/tonne). The study provides potential solvent candidates and future directions to consider in more costly computational and experimental efforts.}
}
@article{YUTING2023e15851,
title = {Current status of digital humanities research in Taiwan},
journal = {Heliyon},
volume = {9},
number = {5},
pages = {e15851},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e15851},
url = {https://www.sciencedirect.com/science/article/pii/S240584402303058X},
author = {Pan Yuting and Jiang Yinfeng and Zhang Jingli},
keywords = {Digital humanities, Text mining, Social network analysis, GIS},
abstract = {Purpose
Review the current research status of the theory, techniques, and practice of digital humanities in Taiwan.
Methods
Select the 8 issues of the Journal of Digital Archives and Digital Humanities from its inception in 2018–2021, and the papers of the 5-year International Conference of Digital Archives and Digital Humanities from 2017 to 2021 as the research data, and conduct text analysis of the collected 252 articles.
Results
From the statistical analysis results, the number of practical articles is the largest, followed by tools and techniques, and the least number of theoretical articles. Text tools and literature research are the most concentrated aspects of digital humanities research in Taiwan.
Limitations
It still needs to be further compared with the current research status of digital humanities in Mainland China.
Conclusions
Digital humanities in Taiwan focuses on the development of tools and techniques, and practical applications of literature and history, and focuses on Taiwan's native culture to form its own digital humanities research characteristics.}
}
@incollection{SWARNALINGAM202535,
title = {Chapter 3 - Electroencephalographic evaluation of epileptogenicity: traditional versus novel biomarkers to guide surgery},
editor = {Aria Fallah and George M. Ibrahim and Alexander G. Weil},
booktitle = {Pediatric Epilepsy Surgery Techniques},
publisher = {Academic Press},
pages = {35-55},
year = {2025},
isbn = {978-0-323-95981-0},
doi = {https://doi.org/10.1016/B978-0-323-95981-0.00006-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323959810000060},
author = {Eroshini Swarnalingam and Julia Jacobs},
keywords = {Medically refractory epilepsy, drug resistant epilepsy, epilepsy surgery, epileptogenic zone, invasive EEG, Stereo EEG, high-frequency oscillations, infra-slow activity, epileptogenicity index},
abstract = {Objectives
Precise localization of the epileptogenic zone (EZ) is a crucial step prior to any planned surgical intervention for medication-refractory epilepsy. However, in reality, a single gold standard biomarker of the EZ does not exist, emphasizing the need for novel biomarkers. The objectives of this chapter are to discuss some of these novel biomarkers utilized to guide epilepsy surgery and discuss their utility and evidence.
Methods
We discuss available evidence to support the use of some of the novel biomarkers of epileptogenicity and compare these with the use of more traditional biomarkers.
Results
There are two main types of developments in the biomarker for epileptic activity. Studies that expand the conventional frequency spectrum of cortical activity such as high-frequency oscillations or those that use computational analysis to assess epileptogenic networks such as the epileptogenicity index. Clinical evidence in most of the new biomarkers is limited to retrospective data analysis and most novel biomarkers require clinical trials prior to incorporating them into day-to-day pre surgical evaluation.
Conclusion
A better understating of the epileptogenic networks as a whole, rather than conceptualizing the EZ as a single focus, will lead to improved surgical outcomes. Currently no single biomarker can be considered a gold standard in outlining the epileptogenic network. Therefore, a combination of complementary investigative methods currently is the best approach to decide on brain areas that need to be removed for seizure free outcomes.}
}
@incollection{SCHILLER2018246,
title = {Some Thermodynamics and Electrostatics With a View to Electrochemistry},
editor = {Klaus Wandelt},
booktitle = {Encyclopedia of Interfacial Chemistry},
publisher = {Elsevier},
address = {Oxford},
pages = {246-257},
year = {2018},
isbn = {978-0-12-809894-3},
doi = {https://doi.org/10.1016/B978-0-12-409547-2.13605-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095472136054},
author = {R. Schiller},
keywords = {Activity coefficient, Chemical potential, Cole–Cole plot, Conservation laws, Dielectric relaxation time, Dipole moment, Electric dipole, Entropy of mixing, Gauss law of electrostatics, Kramers–Kronig relations, Osmotic coefficient, Poisson equation, Polarization, Relative permittivity, Solvation energy},
abstract = {This article tries to offer an overview of some basic laws of thermodynamics and electrostatics which are considered to be part of the foundations of electrochemical thinking. Equilibrium thermodynamics is introduced in terms of conservation laws paying particular attention to the notion of chemical potential. After discussing the forces, potentials, and energetics of charges in vacuum the same problems are dealt with in continuous dielectric media. Here polarization, formation and role of dipole moments, their relation to relative permittivity (dielectric constant) are discussed both in macroscopic and atomic/molecular terms. Finally the kinetics of the response of relative permittivity to the variation of polarizing fields and charges are described. Solvation processes are referred to in connection with both thermodynamic and electrostatic considerations.}
}
@article{OZTOP2022106240,
title = {Analysis of melting of phase change material block inserted to an open cavity},
journal = {International Communications in Heat and Mass Transfer},
volume = {137},
pages = {106240},
year = {2022},
issn = {0735-1933},
doi = {https://doi.org/10.1016/j.icheatmasstransfer.2022.106240},
url = {https://www.sciencedirect.com/science/article/pii/S0735193322003621},
author = {Hakan F. Öztop and Hakan Coşanay and Fatih Selimefendigil and Nidal Abu-Hamdeh},
keywords = {Partially open cavity, PCM, Melting, Computational, Finned heater},
abstract = {A numerical work has been conducted to explore the effects of opening parameters on melting of phase change material (PCM) during natural convection in a partially open enclosure. A finned heater is located on bottom wall while the remaining parts are insulated. Paraffin wax is used as PCM and two-dimensional time dependent analysis is performed by using the finite volume method for the parameters of location of opening and temperature difference. The governing parameters for the study are chosen for the range of Ra = 1.45 × 108 ≤ Ra ≤ Ra = 1.97 × 108, 0.25 ≤ w/H ≤ 0.75 and 0.25 ≤ c/H ≤ 0.75. It is found that both opening ratio and opening length are effective parameter on melting time and these can be used as control parameters for improving the energy efficiency. Also, heat transfer can be controlled by using PCM inserted block and opening parameters. Among different cases of opening ratios and locations of opening, the most favorable configuration is obtained at Ra = 1.97 × 108, w/H = 0.25, c/H = 0.25 while average heat transfer enhancement by about 60% is achieved. At the lowest and highest value of Rayleigh numbers, the most favorable location of the opening is obtained at c/H = 0.25 in order to have the highest reduction amount of phase completion time.}
}
@article{BEJINES2023108405,
title = {Counting semicopulas on finite structures},
journal = {Fuzzy Sets and Systems},
volume = {462},
pages = {108405},
year = {2023},
note = {Aggregation operations (186 p.)},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2022.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0165011422004122},
author = {C. Bejines and M. Ojeda-Hernández},
keywords = {Semicopula, Fuzzy Logic, Finite plane partition, Discrete Mathematics},
abstract = {Semicopulas are the operators chosen to model conjunction in the fuzzy/many-valued logics. In fact, a special kind of semicopula, called t-norm, is widely used in many applications of logic to engineering, computer science and fuzzy systems. The main result of this paper is the computation of the exact number of semicopulas that can be defined on a finite chain in terms of its length. The final formula is achieved via relating semicopulas with finite plane partitions.}
}
@article{YANG2024234071,
title = {Application and development of the Lattice Boltzmann modeling in pore-scale electrodes of solid oxide fuel cells},
journal = {Journal of Power Sources},
volume = {599},
pages = {234071},
year = {2024},
issn = {0378-7753},
doi = {https://doi.org/10.1016/j.jpowsour.2024.234071},
url = {https://www.sciencedirect.com/science/article/pii/S0378775324000223},
author = {Xiaoxing Yang and Guogang Yang and Shian Li and Qiuwan Shen and He Miao and Jinliang Yuan},
keywords = {Lattice Boltzmann method modeling, Pore-scale simulation, Reactive transport, Solid oxide fuel cells simulation},
abstract = {The lattice Boltzmann method (LBM) plays an important role in the study of the internal flow behavior at the pore-scale inside the electrodes of solid oxide fuel cells (SOFCs). Porosity, tortuosity, and particle size have a remarkable effect on gas transport and electrocatalytic processes, determining the performance of cells when SOFCs are applied in electric power generation, energy storage systems, and industrial production in recent years. However, these pore-scale transport progresses are not well characterized in the numerical studies of conventional computational fluid dynamics (CFD), thus modeling with LBM at the pore-scale is an effective tool for simulating gas transport and electrochemical reactions in electrodes. It overcomes the drawbacks of experimental techniques that do not characterize these processes accurately enough and detail the distribution of important variables. In this review, the methodology and process of electrode pore-scale modeling are presented, along with the application and current studies of LBM for diffusion, electrochemical reactions, and ion migration in SOFC porous electrodes. Important results are discussed. Finally, future perspectives on pore-scale studies of porous electrodes are given. This in-depth review intends to provide ideas for the development and further application of LBM in porous SOFC electrodes.}
}
@article{BETTINGER2023105459,
title = {Conceptual foundations of physiological regulation incorporating the free energy principle and self-organized criticality},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {155},
pages = {105459},
year = {2023},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2023.105459},
url = {https://www.sciencedirect.com/science/article/pii/S0149763423004281},
author = {Jesse S. Bettinger and Karl J. Friston},
keywords = {Physiological regulation, Homeostasis, Allostasis, Variational systems, Free energy principle, Criticality, Griffiths region, Complex adaptive systems, Dynamic stability, Metastability, Control theory, Neuro-immunology, Computational psychiatry, Resilience},
abstract = {Bettinger, J. S., K. J. Friston. Conceptual Foundations of Physiological Regulation incorporating the Free Energy Principle & Self-Organized Criticality. NEUROSCI BIOBEHAV REV 23(x) 144-XXX, 2022. Since the late nineteen-nineties, the concept of homeostasis has been contextualized within a broader class of "allostatic" dynamics characterized by a wider-berth of causal factors including social, psychological and environmental entailments; the fundamental nature of integrated brain-body dynamics; plus the role of anticipatory, top-down constraints supplied by intrinsic regulatory models. Many of these evidentiary factors are integral in original descriptions of homeostasis; subsequently integrated; and/or cite more-general operating principles of self-organization. As a result, the concept of allostasis may be generalized to a larger category of variational systems in biology, engineering and physics in terms of advances in complex systems, statistical mechanics and dynamics involving heterogenous (hierarchical/heterarchical, modular) systems like brain-networks and the internal milieu. This paper offers a three-part treatment. 1) interpret "allostasis" to emphasize a variational and relational foundation of physiological stability; 2) adapt the role of allostasis as "stability through change" to include a "return to stability" and 3) reframe the model of homeostasis with a conceptual model of criticality that licenses the upgrade to variational dynamics.}
}
@article{POTTS2025104968,
title = {Explaining institutional technology},
journal = {European Economic Review},
volume = {173},
pages = {104968},
year = {2025},
issn = {0014-2921},
doi = {https://doi.org/10.1016/j.euroecorev.2025.104968},
url = {https://www.sciencedirect.com/science/article/pii/S0014292125000182},
author = {Jason Potts and Kurt Dopfer and Bill Tulloh},
keywords = {Technology, Economic evolution, Knowledge, Information theory, User innovation},
abstract = {This paper offers a review and several refinements and extensions of Explaining Technology, by Koppl et al. (2023), which develops a combinatorial theory of the evolution of technology. First, we suggest that the mechanism of tinkering can be formulated in the theory of user innovation. Second, we propose that a useful refinement is to focus on institutional technologies. This offers a better explanation of emergent levels of evolutionary selection, or major transitions, and also adapts their framework to better explain the nature of a digital economy. Third, we propose a more ambitious line of generalisation from explaining technology to explaining knowledge. We suggest this is possible from the rubric of the new ‘physics of information’ in constructor theory, assembly theory and Bayesian mechanics.}
}
@article{OXMAN2000337,
title = {Design media for the cognitive designer1This paper is based on the keynote speech on `The Challenge of Design Computation' given by the author at ECAADE '97 in Vienna.1},
journal = {Automation in Construction},
volume = {9},
number = {4},
pages = {337-346},
year = {2000},
issn = {0926-5805},
doi = {https://doi.org/10.1016/S0926-5805(99)00017-5},
url = {https://www.sciencedirect.com/science/article/pii/S0926580599000175},
author = {Rivka Oxman},
keywords = {Generic design knowledge, Design collaboration, Re-representation, Typology, },
abstract = {Work on media for design which are responsive to the cognitive processes of the human designer are introduced as a paradigm for research and development. Design media are intended to support the cognitive nature of design and, particularly, the exploitation of design knowledge in computational environments. Basic theoretical assumptions are presented which underlie the development of design media. A central assumption is that designers share common forms of design knowledge which can be formalized, represented, and employed in computational environments. Generic knowledge is proposed as one such seminal form of design knowledge. We then develop a cognitive model which relates to the internal mental representations, strategies and mechanisms of generic design. The paper emphasizes the theoretical foundations of design media. This theoretical discussion is then exemplified through case studies presenting current research for the support of visual cognition in design. We introduce an approach to design schema as a visual form of generic design knowledge. Secondly we present a conceptual framework for the support of schema emergence in visual reasoning in design media. Finally, some implications of schema emergence in design collaboration are presented and discussed.}
}
@article{MALOMO2024106108,
title = {Discontinuum models for the structural and seismic assessment of unreinforced masonry structures: a critical appraisal},
journal = {Structures},
volume = {62},
pages = {106108},
year = {2024},
issn = {2352-0124},
doi = {https://doi.org/10.1016/j.istruc.2024.106108},
url = {https://www.sciencedirect.com/science/article/pii/S2352012424002601},
author = {D. Malomo and B. Pulatsu},
keywords = {Unreinforced masonry, Structural analysis, Seismic analysis, Discontinuum analysis, Discrete Element Methods, DEM, AEM, NSCD, Computational modelling, Numerical modelling},
abstract = {In the last few decades, discontinuum (or discrete, discontinuous) numerical modelling strategies – i.e. those capable of representing the motion of multiple, intersecting discontinuities explicitly – have become increasingly popular for the structural and seismic assessment of unreinforced masonry (URM) structures. The automatic recognition of new contact points and prediction of large deformations up to complete separation are unique features of discontinuum-based models, making them particularly suitable for unit-by-unit simulations. The adaptation of discrete computational models, primarily used for analyzing rock mechanics and geomechanics problems, to the conservation, structural and earthquake engineering evaluation of URM assemblies is still ongoing, and recent advances in computer-aided technologies are accelerating significantly their adoption. Researchers have now developed fracture energy-based contact models tailored to unreinforced masonry mechanics, explored discontinuum analysis from the mortar joint- to the 3D building-level, combined discrete modelling strategies with analytical or continuum approaches, integrated the latest structural health monitoring and image-based developments into discontinuum-based analysis framework. Concurrently, new and still unsolved issues have also arisen, including the selection of appropriate damping schemes, degree of idealization and discretization strategies, identification of appropriate lab or onsite tests to infer meaningful equivalent mechanical input parameters. This paper offers to the research and industry communities an updated critical appraisal and practical guidelines on the use of discontinuum-based structural and seismic assessment strategies for URM structures, providing opportunities to uncover future key research paths. First, masonry mechanics and discontinuum-based idealization options are discussed by considering micro-, meso- and macro-scale modelling strategies. Pragmatic suggestions are provided to select appropriate input parameters essential to model masonry composite and its constituents at different scales. Then, discontinuum approaches are classified based on their formulation, focusing on the Distinct Element Method (DEM), Applied Element Method (AEM) and Non-Smooth Contact Dynamics (NSCD), and an overview of primary differences, capabilities, pros and cons are thoroughly discussed. Finally, previous discontinuum-based analyses of URM small-scale specimens, isolated planar or curved components, assemblies or complex structures are critically reviewed and compared in terms of adopted strategies and relevant outcomes. This paper presents to new and experienced analysts an in-depth summary of what modern discontinuum-based tools can provide to the structural and earthquake engineering fields, practical guidelines on implementing robust and meaningful modelling strategies at various scales, and potential future research directions.}
}
@incollection{MANIKTALA2008247,
title = {Chapter 12 - Discussion Forums, Datasheets, and Other Real-World Issues},
editor = {Sanjaya Maniktala},
booktitle = {Troubleshooting Switching Power Converters},
publisher = {Newnes},
address = {Burlington},
pages = {247-289},
year = {2008},
isbn = {978-0-7506-8421-7},
doi = {https://doi.org/10.1016/B978-075068421-7.50014-X},
url = {https://www.sciencedirect.com/science/article/pii/B978075068421750014X},
author = {Sanjaya Maniktala},
abstract = {Publisher Summary
This chapter explains a few concepts such as thinking is the key, one needs to cross check everything, and product liability concerns. The chapter describes that the company's online tools can be used to discover design problems and correct them as long as thinking is applied as well. But what about “errors” in the online tools themselves? The chapter deals in and highlights that if the thinking process is done assiduously, sometimes one might arrive at the opposite conclusion that one initially foresaw. Anyone can even suddenly realize that he/she can be a part of the very problem that they are trying to fix; it could be in his/her own backyard. The chapter thinks about the customers and highlights that the very idea of a company starting a forum such as this one is essentially brilliant and thoroughly laudable. It also imparts a perception of transparency to their operations from the get-go. One should not outrightly believe anything and put in front of everyone, even if it is on semiglossy paper or in high-definition video or Flash HTML format. As engineers, one is required to put pen to paper, and at least do a sanity check.}
}
@article{GOLDSCHMIDT2006549,
title = {Variances in the impact of visual stimuli on design problem solving performance},
journal = {Design Studies},
volume = {27},
number = {5},
pages = {549-569},
year = {2006},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2006.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X06000172},
author = {Gabriela Goldschmidt and Maria Smolkov},
keywords = {creativity, design problems, problem solving, visual stimuli},
abstract = {Research in cognitive psychology and in design thinking has shown that the generation of inner representations in imagery and external representations via sketching are instrumental in design problem solving. In this paper we focus on another facet of visual representation in design: the ‘consumption’ of external visual representations, regarded as stimuli, when those are present in the designer's work environment. An empirical study revealed that the presence of visual stimuli of different kinds can affect performance, measured in terms of practicality, originality and creativity scores attained by designs developed by subjects under different conditions. The findings suggest that the effect of stimuli is contingent on the type of the design problem that is being solved.}
}
@article{MOHAMED2023108104,
title = {A discrete-based multi-scale modeling approach for the propagation of seismic waves in soils},
journal = {Soil Dynamics and Earthquake Engineering},
volume = {173},
pages = {108104},
year = {2023},
issn = {0267-7261},
doi = {https://doi.org/10.1016/j.soildyn.2023.108104},
url = {https://www.sciencedirect.com/science/article/pii/S0267726123003494},
author = {Tarek Mohamed and Jérôme Duriez and Guillaume Veylon and Laurent Peyras and Patrick Soulat},
keywords = {Multi-scale, DEM, Toyoura sand, Seismic waves propagation, Bounding surface plasticity, Inertial effect},
abstract = {A three-dimensional multi-scale discrete–continuum model (Finite Volume Method × Discrete Element Method, FVM × DEM) is developed for a discrete-based description of the mechanical behavior of granular soils in boundary value problems (BVPs). In such a scheme, the constitutive response of the material is derived through direct DEM computations on a representative volume element attached to each mesh element. The developed multi-scale approach includes the inertial effect in the stress homogenization formulation and serves to study the mechanism of propagation of seismic waves, in comparison with a more classical BVP simulation that adopts an advanced bounding surface plasticity model “P2PSand”. We start with a detailed and fair calibration and validation of these two models against laboratory tests for Toyoura sand under monotonic and cyclic loading. Then, the performance of the two approaches is compared for the case of a seismic wave loading passing through a saturated soil column with different relative densities, revealing several differences between the results of the two models.}
}
@article{LORENZODUS202015,
title = {The communicative modus operandi of online child sexual groomers: Recurring patterns in their language use},
journal = {Journal of Pragmatics},
volume = {155},
pages = {15-27},
year = {2020},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2019.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S0378216619306162},
author = {Nuria Lorenzo-Dus and Anina Kinzel and Matteo {Di Cristofaro}},
keywords = {Child sexual abuse, Online grooming, Linguistic patterns, Corpus assisted discourse studies},
abstract = {Online child sexual groomers manipulate their targets into partaking in sexual activity online and, in some cases, offline. To do so they use language (and other semiotic means, such as images) strategically. This study uses a Corpus-Assisted Discourse Studies methodology to identify recurring patterns in online groomers' language use, mapping them to the specific grooming goal that their use in context fulfils. The analysis of the groomers' language (c. 3.3 million words) within 622 conversations from the Perverted Justice website newly identifies 70 such recurring linguistic patterns (three-word collocations), as well as their relative strength of association to one or more grooming goals. The results can be used to inform computational models for detecting online child sexual grooming language. They can also support the development of training resources that raise awareness of typical language structures that characterise online sexual groomers’ communicative modus operandi.}
}
@article{ANAZKHAN2023,
title = {Metal additive manufacturing of alloy structures in architecture: A review on achievements and challenges},
journal = {Materials Today: Proceedings},
year = {2023},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2023.05.192},
url = {https://www.sciencedirect.com/science/article/pii/S2214785323028183},
author = {Muhammed {Anaz Khan} and Aysha Latheef},
keywords = {Architecture, Additive manufacturing, Facades, Construction industry, Structural engineering},
abstract = {There is a growing trend in modern architecture towards asymmetrical building layouts. This development can be attributed to the proliferation of cutting-edge production methods and structurally novel approaches. With the advent of computational design and digital manufacturing techniques, formerly static designs may now be modified to create one-of-a-kind, high-performance prototypes. Metal additive manufacturing (MAM) is a cutting-edge production method that paves the way for new architectural designs, construction processes, and materials. Interesting possibilities for optimising structural elements are made possible by MAM because of its ability to deposit material just where it is structurally necessary. The construction sector places a premium on directed energy deposition additive manufacturing (DED AM) and wire arc additive manufacturing (WAAM). The adaptability of these two technologies in the construction industry and their possible future applications are explored in this literature review.}
}
@article{WANG2024119836,
title = {Novel score function and standard coefficient-based single-valued neutrosophic MCDM for live streaming sales},
journal = {Information Sciences},
volume = {654},
pages = {119836},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.119836},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523014214},
author = {Fei Wang},
keywords = {Single-valued neutrosophic set, Score function, Standard coefficient, Multi-criteria decision making, Live streaming sales},
abstract = {Single-valued neutrosophic sets (SVNS) provide a comprehensive approach to express uncertainty in decision scenarios, surpassing the utility of fuzzy sets (FS) and intuitionistic fuzzy sets (IFS). Yet, current SVNS score function concepts stem from FS and IFS construction methods, showing inconsistencies. Thus, we introduce a novel SVNS score function based on inherent uncertainty essence. Additionally, we devise a standard coefficient to gauge SVNS standardization akin to fuzzy sets. Addressing SVNS researchability and limitations in fundamental concepts, especially the score function, we propose an SVNS-based multi-criteria decision-making (MCDM) model. This leverages the new score function and standard coefficient. We demonstrate its effectiveness on two decisions: “software engineer recruitment” with known weights and “investment selection” with unknown weights. Ultimately, we successfully applied the model to the field of live streaming sales to solve the actual MCDM problem. By comparing with existing methods, we affirm the model's validity and practicality. Compared to prior approaches, the new method exhibits: (1) Enhanced stability and credibility in result values and rankings, promoting robust optimal solutions. (2) Reduced computational steps and workload, enhancing usability and practicality.}
}
@article{CERONGARCIA20221,
title = {Jigsaw cooperative learning of multistage counter-current liquid-liquid extraction using Mathcad®},
journal = {Education for Chemical Engineers},
volume = {38},
pages = {1-13},
year = {2022},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2021.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S1749772821000488},
author = {Mª Carmen Cerón-García and Lorenzo López-Rosales and Juan José Gallardo-Rodríguez and Elvira Navarro-López and Asterio Sánchez-Mirón and Francisco García-Camacho},
keywords = {Liquid-liquid extraction, Mass transfer, Jigsaw cooperative learning, Computational tools, Mathcad®},
abstract = {This work shows the improvement in comprehending counter-current liquid-liquid extraction by applying jigsaw-type cooperative learning and the engineering math software Mathcad® in a chemical engineering course, part of the Chemistry degree. This study was performed on two different groups at the University of Almería (Spain) over three academic years. The students were divided into two groups: one half of the class followed a non-cooperative learning methodology (the control) while the other half were spread among the jigsaw cooperative groups following the methodology known as “Jigsaw Experts Groups”. A main template made with Mathcad® of a multistage counter-current liquid-liquid extraction was supplied to both the jigsaw and non-jigsaw groups. The assessment of this educational experience in the course revealed that the jigsaw group outperformed the control group. The use of Mathcad® proved to be very intuitive and effective in explaining these relatively complex problems and utilising it is highly recommended; we suggest it is used rather than classical and less intuitive graphical methods.}
}
@article{FOLLEY2003467,
title = {Psychoses and creativity: is the missing link a biological mechanism related to phospholipids turnover?},
journal = {Prostaglandins, Leukotrienes and Essential Fatty Acids},
volume = {69},
number = {6},
pages = {467-476},
year = {2003},
note = {Recent Advances of Membran e Pathology in Schizophrenia},
issn = {0952-3278},
doi = {https://doi.org/10.1016/j.plefa.2003.08.019},
url = {https://www.sciencedirect.com/science/article/pii/S0952327803001716},
author = {Bradley S Folley and Mikisha L Doop and Sohee Park},
keywords = {Creativity, Norepinephrine, Fatty acids, Schizophrenia, Psychoses},
abstract = {Recent evidence suggests that genetic and biochemical factors associated with psychoses may also provide an increased propensity to think creatively. The evolutionary theories linking brain growth and diet to the appearance of creative endeavors have been made recently, but they lack a direct link to research on the biological correlates of divergent and creative thought. Expanding upon Horrobin's theory that changes in brain size and in neural microconnectivity came about as a result of changes in dietary fat and phospholipid incorporation of highly unsaturated fatty acids, we propose a theory relating phospholipase A2 (PLA2) activity to the neuromodulatory effects of the noradrenergic system. This theory offers probable links between attention, divergent thinking, and arousal through a mechanism that emphasizes optimal individual functioning of the PLA2 and NE systems as they interact with structural and biochemical states of the brain. We hope that this theory will stimulate new research in the neural basis of creativity and its connection to psychoses.}
}
@article{PITKOW2017943,
title = {Inference in the Brain: Statistics Flowing in Redundant Population Codes},
journal = {Neuron},
volume = {94},
number = {5},
pages = {943-953},
year = {2017},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2017.05.028},
url = {https://www.sciencedirect.com/science/article/pii/S089662731730466X},
author = {Xaq Pitkow and Dora E. Angelaki},
keywords = {brain, inference, theory, population code, message-passing, redundant, coding, nuisance, nonlinear},
abstract = {It is widely believed that the brain performs approximate probabilistic inference to estimate causal variables in the world from ambiguous sensory data. To understand these computations, we need to analyze how information is represented and transformed by the actions of nonlinear recurrent neural networks. We propose that these probabilistic computations function by a message-passing algorithm operating at the level of redundant neural populations. To explain this framework, we review its underlying concepts, including graphical models, sufficient statistics, and message-passing, and then describe how these concepts could be implemented by recurrently connected probabilistic population codes. The relevant information flow in these networks will be most interpretable at the population level, particularly for redundant neural codes. We therefore outline a general approach to identify the essential features of a neural message-passing algorithm. Finally, we argue that to reveal the most important aspects of these neural computations, we must study large-scale activity patterns during moderately complex, naturalistic behaviors.}
}
@article{TEY2021153,
title = {The Impact of Concession Patterns on Negotiations: When and Why Decreasing Concessions Lead to a Distributive Disadvantage},
journal = {Organizational Behavior and Human Decision Processes},
volume = {165},
pages = {153-166},
year = {2021},
issn = {0749-5978},
doi = {https://doi.org/10.1016/j.obhdp.2021.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0749597821000613},
author = {Kian Siong Tey and Michael Schaerer and Nikhil Madan and Roderick I. Swaab},
keywords = {Negotiations, Concessions, Reservation price, Offers, Signaling, Distributive},
abstract = {We propose that making a series of decreasing concessions (e.g., $1,500–1,210–1,180–1,170) signals that negotiators are reaching their limit and that this results in a negotiation disadvantage for offer recipients. Although we find that most negotiators do not use this strategy naturally, seven studies (N = 2,311) demonstrate that decreasing concessions causes recipients to make less ambitious counteroffers (Studies 1–5) and reach worse deals (Study 2) in distributive negotiations. We find that this disadvantage occurs because decreasing concessions shape recipients’ expectations of the subsequent offers that will be made, which results in inflated perceptions of the counterparts’ reservation price relative to the other concession strategies (Study 3). In addition, we find that this disadvantage is particularly large when concessions decrease at a moderate rate (Study 4a) and when decreasing concessions takes place over more (vs. fewer) rounds (Study 4b). Finally, we find that recipients can protect themselves against the deleterious effects of decreasing concession by thinking of a target before they enter the negotiation (Study 5).}
}
@article{LI2013262,
title = {Improved Particle Filter for Target Tracing Application based on ChinaGrid},
journal = {AASRI Procedia},
volume = {5},
pages = {262-267},
year = {2013},
note = {2013 AASRI Conference on Parallel and Distributed Computing and Systems},
issn = {2212-6716},
doi = {https://doi.org/10.1016/j.aasri.2013.10.087},
url = {https://www.sciencedirect.com/science/article/pii/S2212671613000887},
author = {Yuqiang Li and Xixu He and Haitao Jia},
keywords = {Target tracing, Particle filter, ChinaGrid},
abstract = {Most practical target tracking are usually maneuvering, while most target tracking algorithm are linear filter. More estimation error is introduced from linear filter. Nowadays more and more researchers pay their attention in Maneuvering Target Tracking algorithm. Particle filter has been developed for estimation of nonlinear system states. This paper presents an improved particle filter, which can apply the maneuvering target tracking problem. In practice, the particle filter would take abundant computation for estimate the maneuvering target tracking. The ChinaGrid system use the agile and distributed federations to reduce the computing time, which achieve to fast resolution for particle filter computation of target tracing application. Lastly the simulation proves it.}
}
@article{NEUDERT2024100092,
journal = {Journal of Responsible Technology},
volume = {20},
pages = {100092},
year = {2024},
issn = {2666-6596},
doi = {https://doi.org/10.1016/j.jrt.2024.100092},
url = {https://www.sciencedirect.com/science/article/pii/S2666659624000180},
author = {Philipp Neudert and Mareike Smolka and Britta Acksel and Yana Boeva}
}
@article{HARTMAN2025100398,
title = {Recommendations for the Development of Artificial Intelligence Applications for the Retail Level},
journal = {Journal of Food Protection},
volume = {88},
number = {1},
pages = {100398},
year = {2025},
issn = {0362-028X},
doi = {https://doi.org/10.1016/j.jfp.2024.100398},
url = {https://www.sciencedirect.com/science/article/pii/S0362028X24001820},
author = {Jim Hartman},
keywords = {AI applications based on cognitive models, Development of HACCP plans, Food safety root cause analyses, Foodborne illness outbreak investigations},
abstract = {Some of the early applications of artificial intelligence (AI) for food safety appear to be intended for use at the level of manufacturing and distribution. Artificial intelligence applications to facilitate foodborne illness outbreak investigations, development of HACCP plans, and food safety root cause analyses at the retail level are needed. For example, the interview form in the International Association for Food Protection booklet, Procedures to Investigate Foodborne Illness, could be filled out by humans, but much of the rest of the forms could be completed by artificial intelligence applications. Humans would still have to do the environmental assessments. Most AI applications to date have consisted of pattern identification. Pattern recognition applications may not be capable of assisting in all the proposed retail applications, but it would not be helpful to propose these retail applications without offering a possible path forward. Progress in the proposed directions may require the development of more robust artificial intelligence based on cognitive models. Because this paradigm shift is less familiar to food safety professionals, a comparison between pattern recognition algorithms and cognitive models is offered. An explanation of cognitive models is included to raise awareness of this approach.}
}
@article{TIAN2016363,
title = {Identifying informative energy data in Bayesian calibration of building energy models},
journal = {Energy and Buildings},
volume = {119},
pages = {363-376},
year = {2016},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2016.03.042},
url = {https://www.sciencedirect.com/science/article/pii/S0378778816301967},
author = {Wei Tian and Song Yang and Zhanyong Li and Shen Wei and Wei Pan and Yunliang Liu},
keywords = {Bayesian computation, Cluster analysis, Model calibration, Building energy},
abstract = {Bayesian computation has received increasing attention in calibrating building energy models due to its flexibility and accuracy. However, there has been little research on how to determine informative energy data in Bayesian calibration in building energy models. Therefore, this study aims to determine and choose informative energy data using correlation analysis and hierarchical clustering method. A case study of retail building is used to demonstrate the proposed methods to infer four unknown input parameters using EnergyPlus program. The results indicate that the different combinations of energy data can provide various levels of accuracy in estimating unknown input variables in model calibration. This suggests that Bayesian computation is suitable for inferring the parameters when there are missing energy data that can be treated as uninformative output data. The proposed method can be also used to find the redundant information on energy data in order to improve computational efficiency in Bayesian calibration.}
}
@article{BRINK201339,
title = {Computing with networks of spiking neurons on a biophysically motivated floating-gate based neuromorphic integrated circuit},
journal = {Neural Networks},
volume = {45},
pages = {39-49},
year = {2013},
note = {Neuromorphic Engineering: From Neural Systems to Brain-Like Engineered Systems},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2013.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S089360801300052X},
author = {S. Brink and S. Nease and P. Hasler},
keywords = {Neuromorphic VLSI, Floating-gate transistor, Single transistor learning synapse, Spiking winner-take-all, Synfire chain},
abstract = {Results are presented from several spiking network experiments performed on a novel neuromorphic integrated circuit. The networks are discussed in terms of their computational significance, which includes applications such as arbitrary spatiotemporal pattern generation and recognition, winner-take-all competition, stable generation of rhythmic outputs, and volatile memory. Analogies to the behavior of real biological neural systems are also noted. The alternatives for implementing the same computations are discussed and compared from a computational efficiency standpoint, with the conclusion that implementing neural networks on neuromorphic hardware is significantly more power efficient than numerical integration of model equations on traditional digital hardware.}
}
@article{WANG201715,
title = {An overview on the roles of fuzzy set techniques in big data processing: Trends, challenges and opportunities},
journal = {Knowledge-Based Systems},
volume = {118},
pages = {15-30},
year = {2017},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2016.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950705116304452},
author = {Hai Wang and Zeshui Xu and Witold Pedrycz},
keywords = {Big data, Data-intensive science, Fuzzy sets, Fuzzy logic, Granular computing},
abstract = {In the era of big data, we are facing with an immense volume and high velocity of data with complex structures. Data can be produced by online and offline transactions, social networks, sensors and through our daily life activities. A proper processing of big data can result in informative, intelligent and relevant decision making completed in various areas, such as medical and healthcare, business, management and government. To handle big data more efficiently, new research paradigm has been engaged but the ways of thinking about big data call for further long-term innovative pursuits. Fuzzy sets have been employed for big data processing due to their abilities to represent and quantify aspects of uncertainty. Several innovative approaches within the framework of Granular Computing have been proposed. To summarize the current contributions and present an outlook of further developments, this overview addresses three aspects: (1) We review the recent studies from two distinct views. The first point of view focuses on what types of fuzzy set techniques have been adopted. It identifies clear trends as to the usage of fuzzy sets in big data processing. Another viewpoint focuses on the explanation of the benefits of fuzzy sets in big data problems. We analyze when and why fuzzy sets work in these problems. (2) We present a critical review of the existing problems and discuss the current challenges of big data, which could be potentially and partially solved in the framework of fuzzy sets. (3) Based on some principles, we infer the possible trends of using fuzzy sets in big data processing. We stress that some more sophisticated augmentations of fuzzy sets and their integrations with other tools could offer a novel promising processing environment.}
}
@article{CLARKE2009460,
title = {The mediating effects of coping strategies in the relationship between automatic negative thoughts and depression in a clinical sample of diabetes patients},
journal = {Personality and Individual Differences},
volume = {46},
number = {4},
pages = {460-464},
year = {2009},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2008.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S0191886908004285},
author = {Dave Clarke and Tanya Goosen},
keywords = {Automatic thoughts, Cognitive behaviour therapy, Coping, Depression, Diabetes},
abstract = {High levels of depression have been found among diabetes patients, but few studies have examined the influence of coping strategies on the relationship between diabetics’ negative thoughts and their depression. The purpose of the study was to investigate the effects of coping strategies as mediators in the path from automatic negative thoughts to depression. A questionnaire containing the Automatic Thoughts Questionnaire, the Ways of Coping Checklist, a depression inventory and demographic questions was completed by 57 male and 57 female New Zealand diabetic patients, aged 28–88 years (median=60.5, mean=59.3, SD=14.6). Automatic negative thoughts, emotion-focused coping and depression, but not problem-focused coping, were significantly correlated, after controlling for relevant demographic and diabetes variables. Hierarchical linear regression analysis of data showed that emotion-focused coping functioned as a partial mediator between negative thoughts and depression. Cognitive therapy was suggested to control both automatic negative thoughts and emotion-focused coping behaviours of self-blame, wishful thinking and avoidance.}
}
@article{CRESPO201916,
title = {General solution procedures to compute the stored energy density of conservative solids directly from experimental data},
journal = {International Journal of Engineering Science},
volume = {141},
pages = {16-34},
year = {2019},
issn = {0020-7225},
doi = {https://doi.org/10.1016/j.ijengsci.2019.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0020722517327635},
author = {José Crespo and Francisco J. Montáns},
keywords = {Hyperelasticity, Soft materials, Classical invariants, Data-driven constitutive modelling},
abstract = {Energy-conservative, hyperelastic solids assume the existence of a stored energy density which relates stresses and strains for any deformation state. The usual approach to model such materials is to impose an analytical expression of the stored energy function as a function of some invariants and material parameters. These material parameters are best-fitted to available experimental data. This approach is good for analytical derivations but less optimal for data-driven computational approaches and for accurate and efficient finite element analyses. We show in this paper that the stored energy solution of a solid may be accurately obtained in a general case from suitable numerical procedures, regardless of the invariants being use, without using material parameters nor fitting any assumed analytical form. We explain two general, simple, computational procedures to solve the problem. The numerically computed stored energies may be used in general-purpose finite element programs, yielding more general procedures that have an efficiency equivalent to that of the classical approach, which uses pre-defined analytical “models” and fitting parameters.}
}
@article{BRIERLEY2021107870,
title = {The dark art of interpretation in geomorphology},
journal = {Geomorphology},
volume = {390},
pages = {107870},
year = {2021},
issn = {0169-555X},
doi = {https://doi.org/10.1016/j.geomorph.2021.107870},
url = {https://www.sciencedirect.com/science/article/pii/S0169555X21002786},
author = {Gary Brierley and Kirstie Fryirs and Helen Reid and Richard Williams},
keywords = {Landform, Landscape, Explanation, Prediction, Big Data, Fieldwork, Modelling},
abstract = {The process of interpretation, and the ways in which knowledge builds upon interpretations, has profound implications in scientific and managerial terms. Despite the significance of these issues, geomorphologists typically give scant regard to such deliberations. Geomorphology is not a linear, cause-and-effect science. Inherent complexities and uncertainties prompt perceptions of the process of interpretation in geomorphology as a frustrating form of witchcraft or wizardry — a dark art. Alternatively, acknowledging such challenges recognises the fun to be had in puzzle-solving encounters that apply abductive reasoning to make sense of physical landscapes, seeking to generate knowledge with a reliable evidence base. Carefully crafted approaches to interpretation relate generalised understandings derived from analysis of remotely sensed data with field observations/measurements and local knowledge to support appropriately contextualised place-based applications. In this paper we develop a cognitive approach (Describe-Explain-Predict) to interpret landscapes. Explanation builds upon meaningful description, thereby supporting reliable predictions, in a multiple lines of evidence approach. Interpretation transforms data into knowledge to provide evidence that supports a particular argument. Examples from fluvial geomorphology demonstrate the data-interpretation-knowledge sequence used to analyse river character, behaviour and evolution. Although Big Data and machine learning applications present enormous potential to transform geomorphology into a data-rich, increasingly predictive science, we outline inherent dangers in allowing prescriptive and synthetic tools to do the thinking, as interpreting local differences is an important element of geomorphic enquiry.}
}
@article{CHEN1998541,
title = {Toward a better understanding of idea processors},
journal = {Information and Software Technology},
volume = {40},
number = {10},
pages = {541-553},
year = {1998},
issn = {0950-5849},
doi = {https://doi.org/10.1016/S0950-5849(98)00080-9},
url = {https://www.sciencedirect.com/science/article/pii/S0950584998000809},
author = {Z. Chen},
keywords = {Artificial intelligence, Brainstorming, Computational creativity, Idea processors, Creativity support systems},
abstract = {Idea processors, as a kind of software widely used in the business world, have not received much attention from academia. In this paper we provide an overview of the current status of idea processors. We start from the foundations of idea processors, pointing out their roots in brainstorming techniques. By examining several experimental systems and commercial products, we further discuss how idea processors work, their nature, and their typical architecture. We also summarize some research work related to idea processors, as well as relationships between idea processors and studies of computational creativity in artificial intelligence. Other related issues, such as group decision support systems and evaluation methods, are also briefly examined.}
}
@article{LI2024933,
title = {A Novel Quantifiable Weight Model for the Digital Twin Technology in Enterprise},
journal = {Procedia Computer Science},
volume = {247},
pages = {933-942},
year = {2024},
note = {The 11th International Conference on Applications and Techniques in Cyber Intelligence},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.113},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924029132},
author = {Yanchao Li},
keywords = {Quantifiable weight model, digital twin technology, main core},
abstract = {The performance evaluation involves a variety of composite factors, and most factors are subjectively determined by the people, so the fuzziness of this evaluation is inevitable. This paper firstly focuses on this issue based on the dynamic influence of digital twin technology input for different kinds of digital twin technology activities. Then this paper carries out a detailed analysis and research on the main core and criterion of digital twin technology. Thirdly this paper puts forward the following suggestions so as to give full play to the role of digital twin technology input of three different kinds in activities. Lastly this paper constructs a novel quantifiable weight model of performance evaluation by applying fuzzy mathematics theory. The calculation results indicate that this model can keep an eye on the evolution rule of digital twin technology to timely change the system are the optimal plans and measures based on weight analysis, and their weights are 0.336, 0.334 and 0.330 respectively, and the rationality of quantifiable weight behavior has a significant impact on the technology development.}
}
@article{COWLEY2021101364,
title = {Reading: skilled linguistic action},
journal = {Language Sciences},
volume = {84},
pages = {101364},
year = {2021},
note = {A Dialogue between Distributed Language and Reading Disciplines},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2021.101364},
url = {https://www.sciencedirect.com/science/article/pii/S0388000121000103},
author = {Stephen J. Cowley},
keywords = {Reading, Distributed language, Embodied cognitive science, Languaging, Literacy, Radical embodied cognitive science},
abstract = {The paper links critique of ‘inner process’ to a perspective that treats language as activity that is accomplished by living beings. The view traces reading to human ways of coordinating with ‘the seen’. Having contrasted this distributed view with organism-first alternatives, I use a case study of reports to sketch how readers engage with written materials to both select details and project an imagined ‘source’ (e.g. a meaning, author or intention). Far from using inner process (‘decoding’) readers coordinate with a field of patternings. Where skilled, they use recollecting to link looking, silent thinking, expectations and strategic moves. Using judgements, they transform what they observe by setting off experience. I thus build on Wittgenstein's critique of inner process while also endorsing Trybulec’s (2019) radicalization of his view. To avoid treating the sense of ‘written words’ as subjective, the material aspect of patternings is taken to index outward criteria (roughly, standards of judgement). In seeking to replace theories that presuppose ‘text’, I stress how patternings invite directed sensorimotor activity by an intelligent person. Indeed, since persons learn to see wordings (or take a language stance) arrangements of patternings act as marks, ‘symbols’ and aggregations that set off recollection, judgements and iterated action. Skilled readers can use re-reading, the already read etc. to modulate ways of attending. Readers link the said, hints, recollections and ways of actualizing movements to grant reading experience a specific sense. By considering how outer criteria are evoked, reading is traced back to skilled linguistic action.}
}
@incollection{CAULLER1992199,
title = {Chapter 8 - Functions of Very Distal Dendrites: Experimental and Computational Studies of Layer I Synapses on Neocortical Pyramidal Cells},
editor = {THOMAS MCKENNA and JOEL DAVIS and STEVEN F. ZORNETZER},
booktitle = {Single Neuron Computation},
publisher = {Academic Press},
address = {San Diego},
pages = {199-229},
year = {1992},
series = {Neural Networks: Foundations to Applications},
isbn = {978-0-12-484815-3},
doi = {https://doi.org/10.1016/B978-0-12-484815-3.50014-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780124848153500141},
author = {LARRY J. CAULLER and BARRY W. CONNORS},
abstract = {Publisher Summary
This chapter reviews an approach that combines quantitative morphology, physiology, and computational analysis to understandthe functions of a complex synaptic–neuronal interaction in the cortex. A variation of the in vitro slice of rat somatosensory neocortex examines the effectiveness of layer 1 inputs to pyramidal cells whose bodies lie 0.5–1 mm deeper, in layers 3 or 5. The horizontal fibers in layer 1 (HL1) were isolated by disconnecting all deeper horizontal fibers with a cut perpendicular to the surface, extending from just below layer 1 downward through subcortical white matter and Layer 1 was stimulated on one side of the cut and the response mediated by HL1 fibers passing to the other side was recorded extracellularly and intracellularly. Backward cortico–cortical projections, which end largely on distal apical dendrites in layer 1, are important for higher cortical functions. By isolating horizontal afferents to layer 1 in an in vitro neocortical slice, layer 1 synapses can strongly excite pyramidal cells as deep as layer 5b.}
}
@article{KAMEUGNE2025505,
title = {Quadratic horizontally elastic not-first/not-last filtering algorithm for cumulative constraint},
journal = {European Journal of Operational Research},
volume = {320},
number = {3},
pages = {505-515},
year = {2025},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2024.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0377221724006945},
author = {Roger Kameugne and Sévérine Fetgo Betmbe and Thierry Noulamo},
keywords = {Not-first/not-last algorithm, Profile data structure, TimeTable data structure, Cumulative scheduling, Constraint programming, Horizontally elastic scheduling, RCPSP},
abstract = {The not-first/not-last rule is a pendant of the edge finding rule, generally embedded in the cumulative constraint during constraint-based scheduling. It is combined with other filtering rules for more pruning of the tree search. In this paper, the Profile data structure in which tasks are scheduled in a horizontally elastic way is used to strengthen the classic not-first/not-last rule. Potential not-first task intervals are selected using criteria (specified later in the paper), and the Profile data structure is applied to selected task intervals. We prove that this new rule subsumes the classic not-first rule. A quadratic filtering algorithm is proposed for the new rule, thus improving the complexity of the horizontally elastic not-first/not-last algorithm from O(n3) to O(n2). The fixed part of external tasks that overlap with the selected task intervals is considered during the computation of the earliest completion time of task intervals. This improvement increases the filtering power of the algorithm while remaining quadratic. Experimental results, on a well-known suite of benchmark instances of Resource-Constrained Project Scheduling Problems (RCPSPs), show that the propounded algorithms are competitive with the state-of-the-art not-first algorithms in terms of tree search and running time reduction.}
}
@article{SUZEN2020726,
title = {Automatic short answer grading and feedback using text mining methods},
journal = {Procedia Computer Science},
volume = {169},
pages = {726-743},
year = {2020},
note = {Postproceedings of the 10th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2019 (Tenth Annual Meeting of the BICA Society), held August 15-19, 2019 in Seattle, Washington, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.171},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920302945},
author = {Neslihan Süzen and Alexander N. Gorban and Jeremy Levesley and Evgeny M. Mirkes},
keywords = {Natural Language Processing, Information Extraction, Automatic Grading, Machine Learning, Text Mining, Similarity, Clustering, k-means},
abstract = {Automatic grading is not a new approach but the need to adapt the latest technology to automatic grading has become very important. As the technology has rapidly became more powerful on scoring exams and essays, especially from the 1990s onwards, partially or wholly automated grading systems using computational methods have evolved and have become a major area of research. In particular, the demand of scoring of natural language responses has created a need for tools that can be applied to automatically grade these responses. In this paper, we focus on the concept of automatic grading of short answer questions such as are typical in the UK GCSE system, and providing useful feedback on their answers to students. We present experimental results on a dataset provided from the introductory computer science class in the University of North Texas. We first apply standard data mining techniques to the corpus of student answers for the purpose of measuring similarity between the student answers and the model answer. This is based on the number of common words. We then evaluate the relation between these similarities and marks awarded by scorers. We consider an approach that groups student answers into clusters. Each cluster would be awarded the same mark, and the same feedback given to each answer in a cluster. In this manner, we demonstrate that clusters indicate the groups of students who are awarded the same or the similar scores. Words in each cluster are compared to show that clusters are constructed based on how many and which words of the model answer have been used. The main novelty in this paper is that we design a model to predict marks based on the similarities between the student answers and the model answer. We argue that computational methods be used to enhance the reliability of human scoring, and not replace it. Humans are required to calibrate the system, and to deal with situations that are challenging. Computational methods can provide insight into which student answers will be found challenging and thus be a place human judgement is required.}
}
@article{BAKHTAVAR2020122886,
title = {Assessment of renewable energy-based strategies for net-zero energy communities: A planning model using multi-objective goal programming},
journal = {Journal of Cleaner Production},
volume = {272},
pages = {122886},
year = {2020},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2020.122886},
url = {https://www.sciencedirect.com/science/article/pii/S0959652620329310},
author = {Ezzeddin Bakhtavar and Tharindu Prabatha and Hirushie Karunathilake and Rehan Sadiq and Kasun Hewage},
keywords = {Community energy planning, Renewable energy, Life cycle assessment, Multi-objective optimi, z, ation, Grey numbers, Goal programming},
abstract = {Planning decentralised community-level hybrid energy systems has emerged as a solution to the various environmental and economic issues associated with conventional centralised energy supply systems. However, the optimal planning of community energy systems is a challenging issue due to the complexities, uncertainties, conflicting objectives, and high computational times in analysis. This study introduces a new multi-objective model based on weighted goal programming and grey pairwise comparison to assess renewable energy-based strategies in the case of net-zero energy communities. The problem was formulated to determine the optimal energy mix based on minimization of life cycle impacts and costs and maximization of renewable contributions and operational energy savings. To this end, binary integer and continues variables were applied on the code developed in a CPLEX environment. A pairwise comparison based on grey numbers was used to find the impacts of the goals in the objective function of the model under uncertainty. In addition to the grey-based weighting scenario, different weighting scenarios were employed to consider the importance of all goals on the system. These weighting scenarios were used to investigate the effects of changing decision priorities on the outcomes and on stakeholder interests at different levels. The developed goal-programming model was applied to the data of a case example and solved based on the weighting scenarios. Results indicated that the model is capable of finding the best possible strategies with the lowest total undesirable deviations from the desired levels of the goals compared to the literature of the decision-making techniques. The integration of maximum renewable energy (RE) supply in the energy mix with the locally available energy resources can deliver considerable benefits in terms of energy supply cost reduction as well as in mitigating life cycle environmental impacts. When environmental goals are prioritized, integrating low emissions RE as much as possible and excluding waste-to-energy technologies makes best sense, while under a pro-economic perspective, solar integration is comparatively discouraged. The findings of the study are expected to assist community developers and other decision makers involved in regional energy planning. The developed method will also be of use for those who are interested in the use of goal programming to solve complex planning issues involving numerous uncertain parameters.}
}
@article{BLEMKER2023111745,
title = {In vivo imaging of skeletal muscle form and function: 50 years of insight},
journal = {Journal of Biomechanics},
volume = {158},
pages = {111745},
year = {2023},
issn = {0021-9290},
doi = {https://doi.org/10.1016/j.jbiomech.2023.111745},
url = {https://www.sciencedirect.com/science/article/pii/S0021929023003159},
author = {Silvia S. Blemker},
keywords = {Skeletal muscle, Imaging, In vivo},
abstract = {Skeletal muscle form and function has fascinated scientists for centuries. Our understanding of muscle function has long been driven by advancements in imaging techniques. For example, the sliding filament theory of muscle, which is now widely leveraged in biomechanics research, stemmed from observations made possible by scanning electron microscopy. Over the last 50 years, advancing in medical imaging, combined with ingenuity and creativity of biomechanists, have provide a wealth of new and important insights into in vivo human muscle function. Incorporation of in vivo imaging has also advanced computational modeling and allowed our research to have an impact in many clinical populations. While this review does not provide a comprehensive or meta-analysis of the all the in vivo muscle imaging work over the last five decades, it provides a narrative about the past, present, and future of in vivo muscle imaging.}
}
@article{MILLER200021,
title = {Representational Tools and Conceptual Change: The Young Scientist's Tool Kit},
journal = {Journal of Applied Developmental Psychology},
volume = {21},
number = {1},
pages = {21-25},
year = {2000},
issn = {0193-3973},
doi = {https://doi.org/10.1016/S0193-3973(99)00047-7},
url = {https://www.sciencedirect.com/science/article/pii/S0193397399000477},
author = {Kevin F Miller},
abstract = {We interpret the world and its regularities through representations and procedures that are a complex mélange of formal experience, rules of thumb, and naive concepts that precede formal education. These representational tools give us the language in which we can think about science. Three propositions are argued: (a) that such tools are fundamental to scientific reasoning and science education; (b) that cognitive science has a great deal to say about how cognitive tools affect thinking and conceptual change, particularly how the representations intrinsic to ordinary language relate to the symbol systems of formal science and mathematics; and finally, (c) that cognitive science may play a role in developing representational tools that make scientific information more accessible.}
}
@article{MCGEE201440,
title = {The pragmatics of paragraphing English argumentative text},
journal = {Journal of Pragmatics},
volume = {68},
pages = {40-72},
year = {2014},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2014.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S0378216614000770},
author = {Iain McGee},
keywords = {Paragraphing, Lexical cohesion, Argumentative text, Textual colligation, Foregrounding, Discourse signaling, Rhetorical devices, Computational Linguistics},
abstract = {Computational linguistic work into the paragraph and paragraphing has highlighted the significant role that intra-paragraph lexical cohesion plays in ‘marking off’ one paragraph unit from another. The goal of the research reported on in this paper is to consider, in some detail, the relationship that exists between the lexical repetition patterns in an argumentative text (as identified by a computational procedure), the genre moves within it, the actual paragraphing of the texts, and the textual colligation features of the paragraphs. The Link Set Median procedure (Berber-Sardinha, 1997, Berber-Sardinha, 2001, Berber-Sardinha, 2002) is used to document exact, inflectional and derivational lexical repetition usage across 10 short English argumentative texts, and to predict where segmentations originally occurred in the texts. The resulting data are then analyzed in the light of diverse research interests into the paragraph, and classified accordingly. A comparison of these results is made with data where there is either a marginal or no difference in the link set medians of adjacent sentences across paragraph junctures within the same texts. It is suggested that this novel approach of analyzing computational data from multiple paragraph-specific research interests results in a clearer picture of paragraphing practice emerging.}
}
@article{ABRAMSKY200637,
title = {What are the Fundamental Structures of Concurrency?: We still don't know!},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {162},
pages = {37-41},
year = {2006},
note = {Proceedings of the Workshop "Essays on Algebraic Process Calculi" (APC 25)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2005.12.075},
url = {https://www.sciencedirect.com/science/article/pii/S1571066106004105},
author = {Samson Abramsky},
keywords = {Concurrency, process algebra, Petri nets, geometry, quantum information and computation},
abstract = {Process algebra has been successful in many ways; but we don't yet see the lineaments of a fundamental theory. Some fleeting glimpses are sought from Petri Nets, physics and geometry.}
}
@article{BUESODEBARRIO2025101019,
title = {Executable contracts for Elixir},
journal = {Journal of Logical and Algebraic Methods in Programming},
volume = {142},
pages = {101019},
year = {2025},
issn = {2352-2208},
doi = {https://doi.org/10.1016/j.jlamp.2024.101019},
url = {https://www.sciencedirect.com/science/article/pii/S2352220824000737},
author = {Luis Eduardo {Bueso de Barrio} and Lars-Åke Fredlund and Ángel Herranz and Julio Mariño and Clara {Benac Earle}},
abstract = {This article presents the design of a library for attaching and checking executable contracts to code written in the Elixir programming language. In addition to classical contract constructs such as preconditions and postconditions, the library allows specifying exceptional behaviour (i.e., which exceptions are thrown and under which conditions), detecting non-termination issues in recursive functions by specifying a strictly decreasing order in function arguments, and associating timers with function calls to detect slow computations. The library also focuses on language-specific features, enabling the association of contracts with the reception of messages sent by processes and the attachment of constraints to variable names (useful due to variable shadowing in Elixir). Moreover, stateful contracts (i.e., with a model state) permit specifying the behaviour of stateful APIs whose operations can be linearized. Using the stateful contracts, a monitor can be employed to check that the observed state can be explained in terms of possible linearizations.}
}
@article{KNUUTTILA201476,
title = {Varieties of noise: Analogical reasoning in synthetic biology},
journal = {Studies in History and Philosophy of Science Part A},
volume = {48},
pages = {76-88},
year = {2014},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2014.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0039368114000612},
author = {Tarja Knuuttila and Andrea Loettgers},
keywords = {Synthetic biology, Interdisciplinarity, Analogical reasoning, Engineering sciences, Complex systems, Noise},
abstract = {The picture of synthetic biology as a kind of engineering science has largely created the public understanding of this novel field, covering both its promises and risks. In this paper, we will argue that the actual situation is more nuanced and complex. Synthetic biology is a highly interdisciplinary field of research located at the interface of physics, chemistry, biology, and computational science. All of these fields provide concepts, metaphors, mathematical tools, and models, which are typically utilized by synthetic biologists by drawing analogies between the different fields of inquiry. We will study analogical reasoning in synthetic biology through the emergence of the functional meaning of noise, which marks an important shift in how engineering concepts are employed in this field. The notion of noise serves also to highlight the differences between the two branches of synthetic biology: the basic science-oriented branch and the engineering-oriented branch, which differ from each other in the way they draw analogies to various other fields of study. Moreover, we show that fixing the mapping between a source domain and the target domain seems not to be the goal of analogical reasoning in actual scientific practice.}
}
@article{CARAMIA2022100040,
title = {Sustainable two stage supply chain management: A quadratic optimization approach with a quadratic constraint},
journal = {EURO Journal on Computational Optimization},
volume = {10},
pages = {100040},
year = {2022},
issn = {2192-4406},
doi = {https://doi.org/10.1016/j.ejco.2022.100040},
url = {https://www.sciencedirect.com/science/article/pii/S2192440622000168},
author = {Massimiliano Caramia and Giuseppe Stecca},
keywords = {Supply chain optimization, Green management, Successive linear approximations},
abstract = {Designing a supply chain to comply with environmental policy requires awareness of how work and/or production methods impact the environment and what needs to be done to reduce those environmental impacts and make the company more sustainable. This is a dynamic process that occurs at both the strategic and operational levels. However, being environmentally friendly does not necessarily mean improving the efficiency of the system at the same time. Therefore, when allocating a production budget in a supply chain that implements the green paradigm, it is necessary to figure out how to properly recover costs in order to improve both sustainability and routine operations, offsetting the negative environmental impact of logistics and production without compromising the efficiency of the processes to be executed. In this paper, we study the latter problem in detail, focusing on the CO2 emissions generated by the transportation from suppliers to production sites, and by the production activities carried out in each plant. We do this using a novel mathematical model that has a quadratic objective function and all linear constraints except one, which is also quadratic, and models the constraint on the budget that can be used for green investments caused by the increasing internal complexity created by large production flows in the production nodes of the supply network. To solve this model, we propose a multistart algorithm based on successive linear approximations. Computational results show the effectiveness of our proposal.}
}
@article{SCHOEN2025102018,
title = {Improving the teaching and learning of statistics},
journal = {Learning and Instruction},
volume = {95},
pages = {102018},
year = {2025},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2024.102018},
url = {https://www.sciencedirect.com/science/article/pii/S0959475224001452},
author = {Robert C. Schoen and Christopher Rhoads and Alexandra Perez and Tim Jacobbe and Lanrong Li},
keywords = {Curriculum, Teacher professional development, Statistics education, Many-facet rasch, Hierarchical linear modeling},
abstract = {Structured Abstract
Background
Statistical literacy is more important now than ever. Mathematics teachers are often expected to teach statistics, but statistics and mathematics differ in important ways. The mathematics teaching workforce needs more opportunities to learn statistics and how to teach it accurately and effectively.
Aims
This study was designed to estimate the effects of an intervention. The intervention consisted of a combination of an inquiry-oriented curriculum replacement unit and teacher learning opportunities in statistics and probability. Primary outcomes of interest were instructional practice and student understanding of statistics and probability.
Sample
The study sample included seventh-grade teachers and their students (age 13) in a single, urban school district in the southeastern United States. There were 74 classrooms represented in the analytic sample for the instructional outcome and 2,283 students in the analytic sample for the student outcome.
Methods
Schools were randomly assigned to the treatment or control conditions with equal probability of assignment to condition. Treatment-condition teachers participated in four days of professional learning workshops focused on teaching a 20-day curriculum unit. The Instructional Quality Assessment was used to measure instructional practice. The Levels of Conceptual Understanding in Statistics assessment instrument was used to measure student learning outcomes. Data analysis used hierarchical linear modeling.
Results
Positive, statistically significant effects on both instructional practice (ES = .99) and student understanding of statistics (ES = .25) were found.
Conclusions
The study results indicate that the inquiry-oriented lessons in the curriculum—with the support of teacher-learning opportunities—can improve instruction and increase student learning in statistics.}
}
@article{MORIN1992371,
title = {From the concept of system to the paradigm of complexity},
journal = {Journal of Social and Evolutionary Systems},
volume = {15},
number = {4},
pages = {371-385},
year = {1992},
issn = {1061-7361},
doi = {https://doi.org/10.1016/1061-7361(92)90024-8},
url = {https://www.sciencedirect.com/science/article/pii/1061736192900248},
author = {Edgar Morin},
abstract = {This paper is an overview of the author's ongoing reflections on the need for a new paradigm of complexity capable of informing all theories, whatever their field of application or the phenomena in question. Beginning with a critique of General System Theory and the principle of holism with which it is associated, the author suggests that contemporary advances in our knowledge of organization call for a radical reformation in our organization of knowledge. This reformation involves the mobilization of recursive thinking, which is to say a manner of thinking capable of establishing a dynamic and generative feedback loop between terms or concepts (such as whole and part, order and disorder, observer and observed, system and ecosystem, etc.) that remain both complementary and antagonistic. The paradigm of complexity thus stands as a bold challenge to the fragmentary and reductionistic spirit that continues to dominate the scientific enterprise.}
}
@article{CHEN2019381,
title = {How Big Data and High-performance Computing Drive Brain Science},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {17},
number = {4},
pages = {381-392},
year = {2019},
note = {Big Data in Brain Science},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2019.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S1672022919301561},
author = {Shanyu Chen and Zhipeng He and Xinyin Han and Xiaoyu He and Ruilin Li and Haidong Zhu and Dan Zhao and Chuangchuang Dai and Yu Zhang and Zhonghua Lu and Xuebin Chi and Beifang Niu},
keywords = {Brain science, Big data, High-performance computing, Brain connectomes, Deep learning},
abstract = {Brain science accelerates the study of intelligence and behavior, contributes fundamental insights into human cognition, and offers prospective treatments for brain disease. Faced with the challenges posed by imaging technologies and deep learning computational models, big data and high-performance computing (HPC) play essential roles in studying brain function, brain diseases, and large-scale brain models or connectomes. We review the driving forces behind big data and HPC methods applied to brain science, including deep learning, powerful data analysis capabilities, and computational performance solutions, each of which can be used to improve diagnostic accuracy and research output. This work reinforces predictions that big data and HPC will continue to improve brain science by making ultrahigh-performance analysis possible, by improving data standardization and sharing, and by providing new neuromorphic insights.}
}
@article{MOHAMMED2025110933,
title = {Artificial intelligence approaches in predicting the mechanical properties of natural fiber-reinforced concrete: A comprehensive review},
journal = {Engineering Applications of Artificial Intelligence},
volume = {153},
pages = {110933},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.110933},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625009339},
author = {Mohammed Mohammed and Jawad K. Oleiwi and Aeshah M. Mohammed and Azlin F. Osman and Tijjani Adam and Bashir O. Betar and Subash C.B. Gopinath},
keywords = {Implemented artificial intelligence, Natural fiber-reinforced concrete, Mechanical properties, Applications artificial intelligence, Sustainability},
abstract = {Implementing artificial intelligence (AI) techniques in predicting the mechanical properties of natural fiber-reinforced concrete (NFRC) has emerged as a transformative approach, offering significant advancements over traditional modeling methods. Construction materials science increasingly leverages advanced technologies to enhance composites like natural fiber-reinforced concrete (NFRC). However, the varied nature of natural fibers and their interactions within concrete matrices present significant challenges in accurately predicting the NFRC's mechanical properties. This comprehensive review highlights the innovative use of artificial intelligence (AI) techniques to address challenges in predicting material reliability. It explores the application of various AI methodologies, including machine learning (ML) techniques such as artificial neural networks (ANN) and support vector machines (SVM), along with pattern recognition (PR) and deep learning (DL). These approaches are utilized to tackle the challenges posed by the variability of natural fibers and their interactions within concrete matrices. These techniques have proven highly effective in accurately predicting the mechanical behavior of NFRC, marking significant advancements in the field. This review paper aims to summarize techniques for applying the AI methods mentioned in the NFRC. Firstly, we present a general introduction to AI and NFRC, highlighting the significance of AI in predicting the mechanical properties of NFRC. After that, a comparison between ML, PR, and DL in the field is discussed, and a review of recent applications of AI in the field is provided. Further, the advantages of employing such algorithmic methods are discussed in detail. Finally, future directions for employing ML, PR, and DL are presented, and their limitations are discussed.}
}
@article{AJIMATI2025112300,
title = {Adoption of low-code and no-code development: A systematic literature review and future research agenda},
journal = {Journal of Systems and Software},
volume = {222},
pages = {112300},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112300},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224003443},
author = {Matthew Oladeji Ajimati and Noel Carroll and Mary Maher},
keywords = {Citizen development, Low-code, No-code, Digital transformation, Systematic literature review},
abstract = {Context
Low-code/no-code (LCNC) is an emerging technology trend that extends software development beyond professionalsoftware engineers, making it accessible to individuals throughout organizations and society.
Objective
We aim to provide a systematic review of the current research on the adoption of LCNC technologies within citizen development (CD) practices for digital transformation (DT), and to propose a research agenda for this field.
Method
This review is primarily conducted using a multi-phase systematic literature review of publications from the past five years, i.e., between 2017 and 2023.
Results
We identified 40 primary studies that describes the application of LCNC development and CD practices, the theoretical lenses/frameworks used, and the associated benefits and challenges.
Conclusion
In this study, we present three key contributions. First, we provide a comprehensive review of the benefits, challenges, theoretical perspectives, and methods used to explore LCNC and CD adoption. Second, we introduce a framework designed to guide managers in effectively adopting LCNC and CD practices. Finally, our systematic review uncovers gaps in existing research and identifies opportunities for further exploration, which paves the way for a future research agenda.}
}
@article{ANTONIETTI20082172,
title = {Undergraduates’ metacognitive knowledge about the psychological effects of different kinds of computer-supported instructional tools},
journal = {Computers in Human Behavior},
volume = {24},
number = {5},
pages = {2172-2198},
year = {2008},
note = {Including the Special Issue: Internet Empowerment},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2007.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0747563207001598},
author = {Alessandro Antonietti and Barbara Colombo and Yuri Lozotsev},
keywords = {Metacognition, Belief, Learning, Education, Computer},
abstract = {Literature about metacognition suggests that learners develop personal beliefs about the educational technologies that they are asked to employ and that such beliefs can influence learning outcomes. In this perspective, opinions about the psychological effects of computer-supported instructional tools were analysed by means of a questionnaire which included items about the motivational and emotional aspects of learning, the behaviour to have during the learning process, the mental abilities and the style of thinking required, and the cognitive benefits. Items were presented five times: each time they made reference to a different kind of tool (online courses, hypertexts, Web forums, multimedia presentations, and virtual simulations). The questionnaire was filled out by 99 undergraduates attending engineering courses. Results showed that students ranked the psychological effects of the computer-supported tools in a relative different order according to the kind of tool and attributed distinctive effects to each tool. Gender and expertise played a minor role in modulating undergraduates’ beliefs. Implications for instruction were discussed.}
}
@article{CHRISLEY2008119,
title = {Philosophical foundations of artificial consciousness},
journal = {Artificial Intelligence in Medicine},
volume = {44},
number = {2},
pages = {119-137},
year = {2008},
note = {Artificial Consciousness},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2008.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0933365708001000},
author = {Ron Chrisley},
keywords = {Artificial consciousness, Machine consciousness, Prosthetic artificial intelligence, Synthetic phenomenology, Interactive empiricism, Heterophenomenology},
abstract = {Summary
Objective
Consciousness is often thought to be that aspect of mind that is least amenable to being understood or replicated by artificial intelligence (AI). The first-personal, subjective, what-it-is-like-to-be-something nature of consciousness is thought to be untouchable by the computations, algorithms, processing and functions of AI method. Since AI is the most promising avenue toward artificial consciousness (AC), the conclusion many draw is that AC is even more doomed than AI supposedly is. The objective of this paper is to evaluate the soundness of this inference.
Methods
The results are achieved by means of conceptual analysis and argumentation.
Results and conclusions
It is shown that pessimism concerning the theoretical possibility of artificial consciousness is unfounded, based as it is on misunderstandings of AI, and a lack of awareness of the possible roles AI might play in accounting for or reproducing consciousness. This is done by making some foundational distinctions relevant to AC, and using them to show that some common reasons given for AC scepticism do not touch some of the (usually neglected) possibilities for AC, such as prosthetic, discriminative, practically necessary, and lagom (necessary-but-not-sufficient) AC. Along the way three strands of the author's work in AC – interactive empiricism, synthetic phenomenology, and ontologically conservative heterophenomenology – are used to illustrate and motivate the distinctions and the defences of AC they make possible.}
}
@article{AUBIN2014204,
title = {“Principles of Mechanics that are Susceptible of Application to Society”: An unpublished notebook of Adolphe Quetelet at the root of his social physics},
journal = {Historia Mathematica},
volume = {41},
number = {2},
pages = {204-223},
year = {2014},
issn = {0315-0860},
doi = {https://doi.org/10.1016/j.hm.2014.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0315086014000020},
author = {David Aubin},
keywords = {Mechanics, Sociology, Adolphe Quetelet, Astronomy, Social physics, Average man, Applications of mathematics, Analogical thinking},
abstract = {Founder of the Brussels Observatory, Adolphe Quetelet (1796–1874) is especially well known for his theory of the average man. Like the average position of a star obtained through a large quantity of observed data, the average man was, according to Quetelet, subject to fixed causal laws. Published in 1835, his book On Man: Essay of Social Physics is one of the founding works of sociology and mathematical statistics. The sources of the analogy between astronomy and social physics have been debated by historians. To shed light on this question and the conditions of application of mathematics in the 19th century, we publish for the first time a manuscript that is kept in Quetelet's papers at the Royal Academy of Belgium, and give an English translation of it.}
}