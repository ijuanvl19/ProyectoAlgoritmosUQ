@article{BEJAR2005117,
title = {Sensor networks and distributed CSP: communication, computation and complexity},
journal = {Artificial Intelligence},
volume = {161},
number = {1},
pages = {117-147},
year = {2005},
note = {Distributed Constraint Satisfaction},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2004.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S000437020400150X},
author = {Ramón Béjar and Carmel Domshlak and Cèsar Fernández and Carla Gomes and Bhaskar Krishnamachari and Bart Selman and Magda Valls},
keywords = {Distributed CSP benchmark, Phase transitions, Randomized combinatorial search, Communication network delays, NP-completeness},
abstract = {We introduce SensorDCSP, a naturally distributed benchmark based on a real-world application that arises in the context of networked distributed systems. In order to study the performance of Distributed CSP (DisCSP) algorithms in a truly distributed setting, we use a discrete-event network simulator, which allows us to model the impact of different network traffic conditions on the performance of the algorithms. We consider two complete DisCSP algorithms: asynchronous backtracking (ABT) and asynchronous weak commitment search (AWC), and perform performance comparison for these algorithms on both satisfiable and unsatisfiable instances of SensorDCSP. We found that random delays (due to network traffic or in some cases actively introduced by the agents) combined with a dynamic decentralized restart strategy can improve the performance of DisCSP algorithms. In addition, we introduce GSensorDCSP, a plain-embedded version of SensorDCSP that is closely related to various real-life dynamic tracking systems. We perform both analytical and empirical study of this benchmark domain. In particular, this benchmark allows us to study the attractiveness of solution repairing for solving a sequence of DisCSPs that represent the dynamic tracking of a set of moving objects.}
}
@article{IANDOLI2014298,
title = {Socially augmented argumentation tools: Rationale, design and evaluation of a debate dashboard},
journal = {International Journal of Human-Computer Studies},
volume = {72},
number = {3},
pages = {298-319},
year = {2014},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2013.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S1071581913001043},
author = {Luca Iandoli and Ivana Quinto and Anna {De Liddo} and Simon {Buckingham Shum}},
keywords = {Computer-supported argument visualization, Grounding process, Common ground, Debate dashboard, Collective deliberation, Visual feedback},
abstract = {Collaborative Computer-Supported Argument Visualization (CCSAV) is a technical methodology that offers support for online collective deliberation over complex dilemmas. As compared with more traditional conversational technologies, like wikis and forums, CCSAV is designed to promote more critical thinking and evidence-based reasoning, by using representations that highlight conceptual relationships between contributions, and through computational analytics that assess the structural integrity of the network. However, to date, CCSAV tools have achieved adoption primarily in small-scale educational contexts, and only to a limited degree in real world applications. We hypothesise that by reifying conversations as logical maps to address the shortcomings of chronological streams, CCSAV tools underestimate the importance of participation and interaction in enhancing collaborative knowledge-building. We argue, therefore, that CCSAV platforms should be socially augmented in order to improve their mediation capability. Drawing on Clark and Brennan influential Common Ground theory, we designed a Debate Dashboard, which augmented a CCSAV tool with a set of widgets that deliver meta-information about participants and the interaction process. An empirical study simulating a moderately sized collective deliberation scenario provides evidence that this experimental version outperformed the control version on a range of indicators, including usability, mutual understanding, quality of perceived collaboration, and accuracy of individual decisions. No evidence was found that the addition of the Debate Dashboard impeded the quality of the argumentation or the richness of content.}
}
@article{TURKSON2020110464,
title = {Sustainability assessment of energy production: A critical review of methods, measures and issues},
journal = {Journal of Environmental Management},
volume = {264},
pages = {110464},
year = {2020},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2020.110464},
url = {https://www.sciencedirect.com/science/article/pii/S0301479720303984},
author = {Charles Turkson and Adolf Acquaye and Wenbin Liu and Thanos Papadopoulos},
keywords = {Sustainability, Energy production, Systematic review, Systems thinking, Energy policy, Sustainability assessment},
abstract = {Sustainable operations of energy production systems have become an increasingly important policy agenda globally because of the massive pressure placed on energy resources needed to support economic development and population growth. Due to the increasing research interest in examining the operational impacts of energy production systems on the society and the environment, this paper critically reviews the academic literature on the clean, affordable and secure supply of energy focussing on methods of assessments, measures of sustainability and emerging issues in the literature. While there have been some surveys on the sustainability of energy production systems they have either tended to focus on one assessment approach or one type of energy generation technology. This study builds on previous studies by providing a broader and comprehensive examination of the literature across generation technologies and assessment methods. A systematic review of 128 scholarly articles covering a 20-year period, ending 2018, and gathered from ProQuest, Scopus, and manual search is conducted. Synthesis and critical evaluation of the reviewed papers highlight a number of research gaps that exist within the sustainable energy production systems research domain. In addition, using mapping and cluster analyses, the paper visually highlights the network of dominant research issues, which emerged from the review.}
}
@article{GERPOTT2024101783,
title = {New ways of seeing: Four ways you have not thought about Registered Reports yet},
journal = {The Leadership Quarterly},
volume = {35},
number = {2},
pages = {101783},
year = {2024},
issn = {1048-9843},
doi = {https://doi.org/10.1016/j.leaqua.2024.101783},
url = {https://www.sciencedirect.com/science/article/pii/S1048984324000122},
author = {Fabiola H. Gerpott and Roman Briker and George Banks},
keywords = {Registered Reports, Open Science, Transparency, Quantitative, Qualitative, Leadership},
abstract = {The Leadership Quarterly has helped as a pioneer in accepting Registered Reports (RRs), a submission format where authors provide the introduction, theory section, and methods of their paper for peer review before data collection. Proud but never satisfied, we aim to further boost the number of suitable RR submissions due to our firm belief in their potential for fostering transparent, high-impact research. To inspire authors to explore diverse data collection strategies and methods beyond experiments and survey-based (replication) studies, this work presents four distinct but equally suitable research formats for RRs: meta-analyses, qualitative research, computational approaches, and field intervention studies. Expanding prior research that has explored and promoted general practices and methodological standards for RRs, we offer unique recommendations for preparing an adequate RR proposal along each of these four RR avenues. Additionally, we provide a table of summary resources for authors, reviewers, and editors looking to engage more with RR. In conclusion, we envision a future where other top-tier journals and funding agencies follow The Leadership Quarterly by embracing the incorporation of RRs as a critical component of their strategic approach.}
}
@article{PARKER20161,
title = {Coastal planning should be based on proven sea level data},
journal = {Ocean & Coastal Management},
volume = {124},
pages = {1-9},
year = {2016},
issn = {0964-5691},
doi = {https://doi.org/10.1016/j.ocecoaman.2016.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0964569116300205},
author = {A. Parker and C.D. Ollier},
keywords = {Sea level, Measurements, Computations, Tide gauges, Coastal management},
abstract = {There are two related measures of sea level, the absolute sea level, which is the increase in the sea level in an absolute reference frame, and relative sea level, which is the increase in sea level recorded by tide gauges. The first measure is a rather abstract computation, far from being reliable, and is preferred by activists and politicians for no scientific reason. For local and global problems it is better to use local tide gauge data. Proper coastal management should be based on proved measurements of sea level. Tide gauges provide the most reliable measurements, and best data to assess the rate of change. We show as the naïve averaging of all the tide gauges included in the PSMSL surveys show “relative” rates of rise about +1.04 mm/year (570 tide gauges of any length). If we consider only 100 tide gauges with more than 80 years of recording the rise is only +0.25 mm/year. This naïve averaging has been stable and shows that the sea levels are slowly rising but not accelerating. We also show as the additional information provided by GPS and satellite altimetry is of very little help. Computations of “absolute” sea levels suffer from inaccuracies with errors larger than the estimated trends. The GPS is more reliable than satellite altimetry, but the accuracy of the estimation of the vertical velocity at GPS domes is still well above ±1 mm/year and the relative motion of tide gauges vs. GPS domes is mostly unassessed. The satellite altimetry returns a noisy signal so that a +3.2 mm/year trend is only achieved by arbitrary “corrections”. We conclude that if the sea levels are only oscillating about constant trends everywhere as suggested by the tide gauges, then the effects of climate change are negligible, and the local patterns may be used for local coastal planning without any need of purely speculative global trends based on emission scenarios. Ocean and coastal management should acknowledge all these facts. As the relative rates of rises are stable worldwide, coastal protection should be introduced only where the rate of rise of sea levels as determined from historical data show a tangible short term threat. As the first signs the sea levels will rise catastrophically within few years are nowhere to be seen, people should start really thinking about the warnings not to demolish everything for a case nobody knows will indeed happen.}
}
@article{WISTEN199777,
title = {Distributed computation of dynamic traffic equilibria},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {5},
number = {2},
pages = {77-93},
year = {1997},
note = {Parallel Computing in Transport Research},
issn = {0968-090X},
doi = {https://doi.org/10.1016/S0968-090X(97)00003-X},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X9700003X},
author = {M.B. Wisten and M.J. Smith},
abstract = {The dynamic traffic assignment problem is formulated in the space of splitting rates rather than link and route flows. A distributed algorithm for computation of dynamic user-equilibria is specified. The algorithm has been implemented on a Meiko Computing Surface with 32 T800 processors and some numerical results are given. We do not yet have a general proof of convergence for the algorithm but we have been able to demonstrate convergence with all test networks used.}
}
@article{ZAKI2024100188,
title = {A data-driven framework to inform sustainable management of animal manure in rural agricultural regions using emerging resource recovery technologies},
journal = {Cleaner Environmental Systems},
volume = {13},
pages = {100188},
year = {2024},
issn = {2666-7894},
doi = {https://doi.org/10.1016/j.cesys.2024.100188},
url = {https://www.sciencedirect.com/science/article/pii/S2666789424000266},
author = {Mohammed T. Zaki and Lewis S. Rowles and Jeff Hallowell and Kevin D. Orner},
keywords = {Machine learning, Life cycle assessment, Techno-economic analysis, Pyrolysis, Hydrothermal carbonization, Carbon dioxide removal},
abstract = {Thermochemical conversion technologies are emerging as preferred resource recovery practices for managing animal manure in agricultural regions. Although the implementation of such technologies has been previously studied, difficulties exist in maintaining balance between high rate of resource recovery and low environmental, economic, and social impacts, particularly in rural regions with limited resources. We developed a data-driven framework by integrating machine learning with life cycle thinking that can be used as an open-source tool to help overcome these barriers. The framework was applied to compare two emerging technologies: pyrolysis versus hydrothermal carbonization for managing the excess poultry litter in a rural agricultural region. Among different machine learning models, random forest regression was the most successful to predict resource recovery of both technologies. Next, sustainability analysis indicated that the environmental (global warming), economic (annual worth), and social (system intrusiveness) impacts of pyrolysis was lower than hydrothermal carbonization. Finally, the framework revealed that implementation of pyrolysis at 600 °C for 1 h with the heating rate of 20 °C/min would result in the highest rate of resource recovery that corresponded to the lowest impacts. These results can be helpful in providing operational conditions for implementing emerging resource recovery technologies in rural agricultural regions.}
}
@article{MARTINS20183890,
title = {2MBio, a novel tool to encourage creative participatory conceptual design of bioenergy systems – The case of wood fuel energy systems in south Mozambique},
journal = {Journal of Cleaner Production},
volume = {172},
pages = {3890-3906},
year = {2018},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2017.05.062},
url = {https://www.sciencedirect.com/science/article/pii/S0959652617309873},
author = {Ricardo Martins and Judith A. Cherni and Nuno Videira},
keywords = {Design thinking, Systems thinking, Mozambique, Participatory design tools, Wood fuel energy systems, Bioenergy},
abstract = {This paper proposes a new conceptual design tool for bioenergy systems, the 2MBio, and its implementation on the case of wood fuel energy systems (WES) in South Mozambique. Dependence on wood fuel characterises most Sub-Saharan countries and WES are complex socio-ecological systems dynamically linked to crucial development issues, e.g., deforestation and poverty. In Mozambique WES supply over 70% of the national energy needs through an informal business network worth around one million euros each year. In contrast with the 2MBio, currently available tools often aim at supporting decision-making on WES with off-the-shelf expert solutions and optimisation of WES efficiency, supply chains and resource management. While relevant and useful, such approaches are frequently unsuitable to engage the knowledge and creativity of a wide range of crucial actors. The 2MBio addresses this gap providing a simple, visual platform on paper that supports from illiterate to professional users, to stimulate creative ideas and apply current knowledge while designing their own WES. The results of implementation in real settings in South Mozambique produced relevant design breakthroughs. Compared with the absence of any other support tool, and faced with same design challenges, the 2MBio participatory design workshops in south Mozambique resulted in comprehensive analysis of wood fuel energy systems, and innovative integrated WES solutions design. The proposed approach raised participants’ awareness about opportunities and constrains linked to their WES while also facilitating information sharing new learning dynamics and enhance creativity.}
}
@article{BIRO2015876,
title = {Measuring the Level of Algorithmic Skills at the End of Secondary Education in Hungary},
journal = {Procedia - Social and Behavioral Sciences},
volume = {176},
pages = {876-883},
year = {2015},
note = {International Educational Technology Conference, IETC 2014, 3-5 September 2014, Chicago, IL, USA},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2015.01.553},
url = {https://www.sciencedirect.com/science/article/pii/S187704281500590X},
author = {Piroska Biró and Mária Csernoch and János Máth and Kálmán Abari},
keywords = {level of digital thinking, algorithmic skills, school leaving exams in Informatics and Mathematics},
abstract = {Students starting their tertiary studies in Informatics are found to have a low level of algorithmic skills and understanding of programming, which leads to the high number of drop out students and failed semesters during their studies. The students’ low level of programming skills contrasts with their excellent results in the school leaving exams. To find out the reasons for this we have launched the TAaAS project (Testing Algorithmic and Application Skills), which focuses on the students’ algorithmic skills and programming ability in traditional and non-traditional programming environments. Our analyses proved that school leaving exams are not able to measure these abilities of the students, and beyond that, are not able to distinguish between the different levels of the students. Students are accepted into the universities and start their studies based on the misleading results of the school leaving exams.}
}
@article{KOK2016342,
title = {Crowd behavior analysis: A review where physics meets biology},
journal = {Neurocomputing},
volume = {177},
pages = {342-362},
year = {2016},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2015.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S0925231215017403},
author = {Ven Jyn Kok and Mei Kuan Lim and Chee Seng Chan},
keywords = {Crowd behavior analysis, Biologically inspired, Physics-inspired, Computer vision, Survey},
abstract = {Although the traits emerged in a mass gathering are often non-deliberative, the act of mass impulse may lead to irrevocable crowd disasters. The two-fold increase of carnage in crowd since the past two decades has spurred significant advances in the field of computer vision, towards effective and proactive crowd surveillance. Computer vision studies related to crowd are observed to resonate with the understanding of the emergent behavior in physics (complex systems) and biology (animal swarm). These studies, which are inspired by biology and physics, share surprisingly common insights, and interesting contradictions. However, this aspect of discussion has not been fully explored. Therefore, this survey provides the readers with a review of the state-of-the-art methods in crowd behavior analysis from the physics and biologically inspired perspectives. We provide insights and comprehensive discussions for a broader understanding of the underlying prospect of blending physics and biology studies in computer vision.}
}
@article{SHIRALKAR2023100115,
title = {An intelligent method for supply chain finance selection using supplier segmentation: A payment risk portfolio approach},
journal = {Cleaner Logistics and Supply Chain},
volume = {8},
pages = {100115},
year = {2023},
issn = {2772-3909},
doi = {https://doi.org/10.1016/j.clscn.2023.100115},
url = {https://www.sciencedirect.com/science/article/pii/S2772390923000240},
author = {Kedar Shiralkar and Arunkumar Bongale and Satish Kumar and Anupkumar M. Bongale},
keywords = {Supply chain finance (SCF), Supplier segmentation, Supplier categorization, Risk portfolio model, Supply chain sustainability, Supplier relationship management, Modern portfolio theory, Trade credit, Factoring, Dynamic discounting},
abstract = {The COVID-19 pandemic-driven financial crisis grew significant interest among firms to adopt supply chain finance (SCF) to optimize working capital for the financial stability of the supply chain. However, it is impractical for firms with a diverse and extensive supplier base to strategize the SCF solutions for individual suppliers by assessing their financial risk. Hence, this study conceptualizes an intelligent method to demonstrate how supplier segmentation based on suppliers’ payment risk portfolios helps supply chain practitioners to assess suppliers’ financial risk and strategize manageable supply chain finance solutions for them. This method employs a stochastic optimization model to compute suppliers’ optimum payment risk portfolios and generate a supplier segmentation matrix to offer supply chain practitioners the cognitive ability to select appropriate SCF solutions for their suppliers. The proposed method can be implemented into an AI-driven explainable recommendation system to aid supply chain practitioners in applying smart strategic thinking in supply chain finance decision-making.}
}
@article{PITOWSKY1996161,
title = {Laplace's demon consults an oracle: The computational complexity of prediction},
journal = {Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics},
volume = {27},
number = {2},
pages = {161-180},
year = {1996},
issn = {1355-2198},
doi = {https://doi.org/10.1016/1355-2198(96)85115-X},
url = {https://www.sciencedirect.com/science/article/pii/135521989685115X},
author = {Itamar Pitowsky}
}
@article{SILVA2017137,
title = {Evaluating the usefulness of the structural accessibility layer for planning practice – Planning practitioners’ perception},
journal = {Transportation Research Part A: Policy and Practice},
volume = {104},
pages = {137-149},
year = {2017},
issn = {0965-8564},
doi = {https://doi.org/10.1016/j.tra.2017.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S0965856417304755},
author = {Cecília Silva and Tiago Patatas and Ana Amante},
keywords = {Accessibility instrument, Implementation gap, Planning practice, Usefulness in practice},
abstract = {There has been a growing attention on accessibility concepts from both planning practice and research recognising their relevance in understanding the evolution of urban areas. However, despite the large number of accessibility measures available in the literature, they are not widely used to support urban planning practices. Much has been said about the implementation gap of Planning Support Systems with a significant attention paid to usability and more recently to the usefulness of Accessibility Instruments. The paper aims to assess the usefulness of a specific accessibility instrument – the Structural Accessibility Layer (SAL) – and by doing so exploring the strengths of accessibility instruments holding similar characteristics. To this end, we follow a multidimensional assessment framework under development in the Planning Support System literature. This paper explores the main findings of a workshop bringing together local planning practitioners and the developers of the SAL in an experiment using the SAL. The assessment of usefulness of SAL identified the instrument’s strengths with regard to insight into participants’ assumptions, communication, commitment and development of shared language. Regardless, the low fit between planning concerns of participants (in this case study context) and of the SAL seemed to limit its potential use in practice and as such undermines the strengths identified in the usefulness assessment. The assessment developed here only partially confirmed objectives and purposes defined for the SAL. Results confirm the usefulness of the SAL as diagnosis tool, however, the ability of the SAL to contribute to a joint thinking of land use and transport constraints on mobility was not confirmed. Finally, this research raises questions on the role of PSS in changing strategic thinking in planning and how this might conflict with the current PSS research concern in improving usefulness of tools.}
}
@article{OZENCIRA2023101273,
title = {Mapping research on musical creativity: A bibliometric review of the literature from 1990 to 2022},
journal = {Thinking Skills and Creativity},
volume = {48},
pages = {101273},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101273},
url = {https://www.sciencedirect.com/science/article/pii/S1871187123000433},
author = {Gözde Ozenc-Ira},
keywords = {Musical creativity, Creativity, Bibliometric review, Science mapping, VOSviewer},
abstract = {This study aims to map the research literature on musical creativity that was published from 1990 to 2022 by using metadata extracted from 1,177 Web of Science-indexed publications in terms of trends in publications and citations data, leading journals, authors, institutions/organizations, and countries, collaborative networks between authors, institutions, and countries, and trends in keyword frequencies and co-occurrences. The main findings of this study are that (1) research on musical creativity has undergone an incipient phase and has had a growing scientific interest since the mid-2000s, (2) musical creativity is a relatively more specific research field compared to general creativity research that has been represented by more specific sub-fields, e.g., music psychology and ethnomusicology, (3) a small number of scholars – especially from the USA, England, Russia, Spain, Australia, and some countries from South Europe – have made the more impactful contribution as regards musical creativity, (4) there is a small number of research collaborations among scholars, yet the collaborative networks among countries and institutions occur intercontinentally, (5) musical creativity research is growing with cross-disciplinary links with several branches of psychology, neurosciences, cognitive sciences, education, sociology, arts and humanities, and computer sciences, and (6) eight main topical foci have been founded in the literature from 1990 to date – i.e., computational creativity, processes of improvisation, improvisation teaching and learning, interactions/collaboration during improvisation, effects of improvisation practice, innovative music technology, esthetic aspect of everyday creativity, and music therapy. Further research on musical creativity could map the literature by focusing on contextual themes.}
}
@article{GRAGERT199711,
title = {Differential geometric computations and computer algebra},
journal = {Mathematical and Computer Modelling},
volume = {25},
number = {8},
pages = {11-24},
year = {1997},
issn = {0895-7177},
doi = {https://doi.org/10.1016/S0895-7177(97)00055-1},
url = {https://www.sciencedirect.com/science/article/pii/S0895717797000551},
author = {P.K.H Gragert and P.H.M Kersten},
keywords = {Computer algebra, Differential geometry, Literate programming, Supersymmetry},
abstract = {The use of computer algebra in the field of differential geometry and its applications to geometric structures of partial differential equations is discussed. The differential geometric setting is shortly described; a number of programs are slightly touched, some examples given, and an application to the construction of supersymmetric extensions of the Korteweg-de Vries equation is demonstrated.}
}
@article{ZHANG201499,
title = {Profiles of psychiatric symptoms among amphetamine type stimulant and ketamine using inpatients in Wuhan, China},
journal = {Journal of Psychiatric Research},
volume = {53},
pages = {99-102},
year = {2014},
issn = {0022-3956},
doi = {https://doi.org/10.1016/j.jpsychires.2014.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S0022395614000508},
author = {Yao Zhang and Zaifeng Xu and Sheng Zhang and Alethea Desrosiers and Richard S. Schottenfeld and Marek C. Chawarski},
keywords = {Amphetamine type stimulants (ATS), Ketamine, Psychiatric symptoms},
abstract = {Amphetamine type stimulants (ATS) and ketamine have emerged as major drug problems in China, and chronic extensive exposure to these substances frequently co-occurs with psychiatric symptoms. This study compares the psychiatric symptoms of patients reporting ATS use only, ATS and ketamine use, or ketamine use only who were admitted to an inpatient psychiatry ward in Wuhan, China between 2010 and 2011. Data on 375 study participants collected during their ward admission and extracted from their clinical records included their socio-demographics, scores on the Brief Psychiatric Rating Scale (BPRS), and urine toxicology screens.
Results
The ketamine-only group had significantly lower total BPRS scores and significantly lower scores on Thinking Disorder, Activity, and Hostility-Suspicion BPRS subscales than the ATS-only and ATS + ketamine groups (p < 0.001 for all comparisons). The ketamine-only group also had significantly higher scores on the subscales of Anxiety-Depression and Anergia. The ATS-only group had significantly higher scores on subscales of Thinking Disorder, Activity, and Hostility-Suspicion and significantly lower scores on Anxiety-Depression and Anergia subscales than the ketamine-only and ATS + ketamine groups (p < 0.001 for all comparisons). A K-means cluster method identified three distinct clusters of patients based on the similarities of their BPRS subscale profiles, and the identified clusters differed markedly on the proportions of participants reporting different primary drugs of abuse. The study findings suggest that ketamine and ATS users present with different profiles of psychiatric symptoms at admission to inpatient treatment.}
}
@article{MUEHLENSIEPEN2022,
title = {Factors Associated With Telemedicine Use Among German General Practitioners and Rheumatologists: Secondary Analysis of Data From a Nationwide Survey},
journal = {Journal of Medical Internet Research},
volume = {24},
number = {11},
year = {2022},
issn = {1438-8871},
doi = {https://doi.org/10.2196/40304},
url = {https://www.sciencedirect.com/science/article/pii/S1438887122007373},
author = {Felix Muehlensiepen and Pascal Petit and Johannes Knitza and Martin Welcker and Nicolas Vuillerme},
keywords = {telemedicine, rheumatology, primary care, secondary analysis, health services research},
abstract = {Background
Previous studies have demonstrated telemedicine (TM) to be an effective tool to complement rheumatology care and address workforce shortage. With the outbreak of the SARS-CoV-2 pandemic, TM experienced a massive upswing. However, in rheumatology care, the use of TM stagnated again shortly thereafter. Consequently, the factors associated with physicians’ willingness to use TM (TM willingness) and actual use of TM (TM use) need to be thoroughly investigated.
Objective
This study aimed to identify the factors that determine TM use and TM willingness among German general practitioners and rheumatologists.
Methods
We conducted a secondary analysis of data from a German nationwide cross-sectional survey with general practitioners and rheumatologists. Bayesian univariate and multivariate logistic regression analyses were applied to the data to determine which factors were associated with TM use and TM willingness. The predictor variables (covariates) that were studied individually included sociodemographic factors (eg, age and sex), work characteristics (eg, practice location and medical specialty), and self-assessed knowledge of TM. All the variables positively and negatively associated with TM use and TM willingness in the univariate analysis were then considered for Bayesian model averaging analysis after a selection based on the variance inflation factor (≤2.5). All analyses were stratified by sex.
Results
Univariate analysis revealed that out of 83 variables, 36 (43%) and 34 (41%) variables were positively or negatively associated (region of practical equivalence≤5%) with TM use and TM willingness, respectively. The Bayesian model averaging analysis allowed us to identify 13 and 17 factors of TM use and TM willingness, respectively. Among these factors, being female, having very poor knowledge of TM, treating <500 patients per quarter, and not being willing to use TM were negatively associated with TM use, whereas having good knowledge of TM and treating >1000 patients per quarter were positively associated with TM use. In addition, being aged 51 to 60 years, thinking that TM is not important for current and future work, and not currently using TM were negatively associated with TM willingness, whereas owning a smart device and working in an urban area were positively associated with TM willingness.
Conclusions
The results point to the close connection between health care professionals’ knowledge of TM and actual TM use. These results lend support to the integration of digital competencies into medical education as well as hands-on training for health care professionals. Incentive programs for physicians aged >50 years and practicing in rural areas could further encourage TM willingness.}
}
@article{STORAASLI1993349,
title = {Computational mechanics analysis tools for parallel-vector supercomputers},
journal = {Computing Systems in Engineering},
volume = {4},
number = {4},
pages = {349-354},
year = {1993},
note = {Parallel Computational Methods for Large-Scale Structural Analysis and Design},
issn = {0956-0521},
doi = {https://doi.org/10.1016/0956-0521(93)90002-E},
url = {https://www.sciencedirect.com/science/article/pii/095605219390002E},
author = {O.O. Storaasli and D.T. Nguyen and M.A. Baddourah and J. Qin},
abstract = {Computational algorithms for structural analysis on parallel-vector supercomputers are reviewed. These parallel algorithms, developed by the authors, are for the assembly of structural equations, “out-of-core” strategies for linear equation solution, massively distributed-memory equation solution, unsymmetric equation solution, general eigen-solution, geometrically nonlinear finite element analysis, design sensitivity analysis for structural dynamics, optimization algorithm and domain decomposition. The source code for many of these algorithms is available from NASA Langley.}
}
@article{SAJID2023103174,
title = {Thermal case classification of solar-powered cars for binary tetra hybridity nanofluid using Cash and Carp method with Hamilton-Crosser model},
journal = {Case Studies in Thermal Engineering},
volume = {49},
pages = {103174},
year = {2023},
issn = {2214-157X},
doi = {https://doi.org/10.1016/j.csite.2023.103174},
url = {https://www.sciencedirect.com/science/article/pii/S2214157X2300480X},
author = {Tanveer Sajid and Wasim Jamshed and Nek Muhammad Katbar and Mohamed R. Eid and Assmaa Abd-Elmonem and Nesreen Sirelkhtam Elmki Abdalla and Sayed M. {El Din} and Gilder Cieza Altamirano},
keywords = {Solar sports car, Solar sheet, Reiner-Philippoff tetrhybrid nanofluid, Thermal radiation, Heat generation},
abstract = {Solar energy is the most important source of thermal energy that comes from the sun. This kind of energy has enormous potential applications in fields of technology such as photovoltaic panels, renewable power, solar light poles, and solar pumps used for water extraction. The era in which we are living is all about the applications of solar energy in industrial sectors most importantly in solar sports car manufacturing. This article presents a new way of thinking about the heat transport analyses of photovoltaic hybrid vehicles, by factoring Casson-Sutterby liquid with the inclusion of various effects like variable thermal conduction, thermal radiation, heat generation, and tetrahybrid nanoparticles. To solve the modelled equations in regards to both momentum and energy, another well-computational approach known as the Cash and Carp method was used. The effects of a wide variety of factors on temperature, shear stress, and velocity fields, as well as the surface drag coefficient and Nusselt number, are briefly described and illustrated in the form of tables and figures. It then found that the thermal radiation, heat production, and thermal conductivity parameters and insertion of agglomerative tetrhybrid nanoparticles in the base fluid amplify heat transfer rate, it has been shown that the performance of the solar car increases in terms of heat transition. In comparison to standard nanofluid, tetrahybrid nanofluid is the most effective medium for the transmission of heat. From the regression analysis, it is observed that the error in terms of Nusselt number is smaller 0.0151 for the case ε=1.5, and increases to 0.0151 in the case of ε=2.5. Relative percentage error is smaller 4.62% in the case of heat generation Q=0.7 but a maximum of 15.8% in the case of thermal radiation Rd=2.}
}
@article{ZHANG2024100479,
title = {Open source implementations of numerical algorithms for computing the complete elliptic integral of the first kind},
journal = {Results in Applied Mathematics},
volume = {23},
pages = {100479},
year = {2024},
issn = {2590-0374},
doi = {https://doi.org/10.1016/j.rinam.2024.100479},
url = {https://www.sciencedirect.com/science/article/pii/S2590037424000499},
author = {Hong-Yan Zhang and Wen-Juan Jiang},
keywords = {Complete elliptic integral of the first kind (CEI-1), Algorithm design, Orthogonal polynomials, Verification-validation-testing (VVT), STEM education, Computer programming},
abstract = {The complete elliptic integral of the first kind (CEI-1) plays a significant role in mathematics, physics and engineering. There is no simple formula for its computation, thus numerical algorithms are essential for coping with the practical problems involved. The commercial implementations for the numerical solutions, such as the functions ellipticK and EllipticK provided by MATLAB and Mathematica respectively, are based on Kcs(m) instead of the usual form K(k) such that Kcs(k2)=K(k) and m=k2. It is necessary to develop open source implementations for the computation of the CEI-1 in order to avoid potential risks of using commercial software and possible limitations due to the unknown factors. In this paper, the infinite series method, arithmetic-geometric mean (AGM) method, Gauss–Chebyshev method and Gauss–Legendre methods are discussed in details with a top-down strategy. The four key algorithms for computing the CEI-1 are designed, verified, validated and tested, which can be utilized in R& D and be reused properly. Numerical results show that our open source implementations based on K(k) are equivalent to the commercial implementation based on Kcs(m). The general algorithms for computing orthogonal polynomials developed are valuable for the STEM education and scientific computation.}
}
@article{PIERONI2016412,
title = {Transforming a Traditional Product Offer into PSS: A Practical Application},
journal = {Procedia CIRP},
volume = {47},
pages = {412-417},
year = {2016},
note = {Product-Service Systems across Life Cycle},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2016.03.036},
url = {https://www.sciencedirect.com/science/article/pii/S2212827116300051},
author = {Marina Pieroni and Caio Marques and Carina Campese and Daniel Guzzo and Glauco Mendes and Janaína Costa and Maiara Rosa and Maicon Gouveia de Oliveira and Victor Macul and Henrique Rozenfeld},
keywords = {product-service system, servitization, business model, design thinking, practical application, action research},
abstract = {In the last decades, companies have shifted from traditional business models based on selling products to product-service systems (PSS). Despite this tendency, there is a paucity of complete methodologies and tools to guide companies on how the transition should occur. To address this issue, the goal of this research is to present a complete framework to support manufacturing companies in the servitization journey. This novel proposal involves the application of design thinking to define the value proposition integrated with a PSS oriented business model creation, that goes beyond generic methods normally applied; and the specification of business process architecture to support PSS implementation. This research followed a prescriptive approach by means of action research technique. Key findings of the framework application are presented.}
}
@article{ZAHEDI2024103730,
title = {How hypnotic suggestions work – A systematic review of prominent theories of hypnosis},
journal = {Consciousness and Cognition},
volume = {123},
pages = {103730},
year = {2024},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2024.103730},
url = {https://www.sciencedirect.com/science/article/pii/S1053810024000977},
author = {Anoushiravan Zahedi and Steven {Jay Lynn} and Werner Sommer},
keywords = {Hypnosis, Theory, Suggestibility, Hypnotizability, Hypnotic Suggestions (HS), Posthypnotic Suggestions (PHS), Direct Verbal Suggestions},
abstract = {In recent decades, hypnosis has increasingly moved into the mainstream of scientific inquiry. Hypnotic suggestions are frequently implemented in behavioral, neurocognitive, and clinical investigations and interventions. Despite abundant reports about the effectiveness of suggestions in altering behavior, perception, cognition, and agency, no consensus exists regarding the mechanisms driving these changes. This article reviews competing theoretical accounts that address the genesis of subjective, behavioral, and neurophysiological responses to hypnotic suggestions. We systematically analyze the broad landscape of hypnosis theories that best represent our estimation of the current status and future avenues of scientific thinking. We start with procedural descriptions of hypnosis, suggestions, and hypnotizability, followed by a comparative analysis of systematically selected theories. Considering that prominent theoretical perspectives emphasize different aspects of hypnosis, our review reveals that each perspective possesses salient strengths, limitations, and heuristic values. We highlight the necessity of revisiting extant theories and formulating novel evidence-based accounts of hypnosis.}
}
@article{BLISS19921,
title = {Reasoning supported by computational tools},
journal = {Computers & Education},
volume = {18},
number = {1},
pages = {1-9},
year = {1992},
issn = {0360-1315},
doi = {https://doi.org/10.1016/0360-1315(92)90030-9},
url = {https://www.sciencedirect.com/science/article/pii/0360131592900309},
author = {Joan Bliss and Jon Ogborn and Richard Boohan and Jonathan Briggs and Tim Brosnan and Derek Brough and Harvey Mellar and Rob Miller and Caroline Nash and Cathy Rodgers and Babis Sakonidis},
abstract = {This paper sets out the work of the Tools for Exploratory Learning Programme within the ESRC Initiative Information Technology in Education. The research examines young secondary children's reasoning with computational tools. We distinguish between exploratory and expressive modes of learning, that is, interaction with another's model and creation of one's own model, respectively. The research focuses on reasoning, rather than learning, along three dimensions: quantitative, qualitative, and semi-quantitative. It provides a 3 × 2 classification of tasks according to modes of learning and types of reasoning. Modelling tools were developed for the study and descriptions of these are given. The research examined children's reasoning with tools in all three dimensions looking more exhaustively at the semi-quantitative. Pupils worked either in an exploratory mode or an expressive mode on one of the following topics: Traffic, Health and Diet, and Shops and Profits. They spent 3–4 h individually with a researcher over 2 weeks, carrying out four different activities: reasoning without the computer; learning to manipulate first the computer then later the tool and finally carrying out a task with the modelling tool. Pupils were between 12 and 14 yr. Research questions both about children's reasoning when working with or creating models and about the nature of the tools used are discussed. Finally an analytic scheme is set out which describes the nature of the causal and non-causal reasoning observed together with some tentative results.}
}
@article{WASKAN2003259,
title = {Intrinsic cognitive models},
journal = {Cognitive Science},
volume = {27},
number = {2},
pages = {259-283},
year = {2003},
issn = {0364-0213},
doi = {https://doi.org/10.1016/S0364-0213(02)00119-2},
url = {https://www.sciencedirect.com/science/article/pii/S0364021302001192},
author = {Jonathan A Waskan},
keywords = {Philosophy, Artificial intelligence, Psychology, Representation, Philosophy of mind, Philosophy of computation, Causal reasoning, Knowledge representation, Computer simulation},
abstract = {Theories concerning the structure, or format, of mental representation should (1) be formulated in mechanistic, rather than metaphorical terms; (2) do justice to several philosophical intuitions about mental representation; and (3) explain the human capacity to predict the consequences of worldly alterations (i.e., to think before we act). The hypothesis that thinking involves the application of syntax-sensitive inference rules to syntactically structured mental representations has been said to satisfy all three conditions. An alternative hypothesis is that thinking requires the construction and manipulation of the cognitive equivalent of scale models. A reading of this hypothesis is provided that satisfies condition (1) and which, even though it may not fully satisfy condition (2), turns out (in light of the frame problem) to be the only known way to satisfy condition (3).}
}
@article{ERKELENS19982999,
title = {A computational model of depth perception based on headcentric disparity},
journal = {Vision Research},
volume = {38},
number = {19},
pages = {2999-3018},
year = {1998},
issn = {0042-6989},
doi = {https://doi.org/10.1016/S0042-6989(98)00084-4},
url = {https://www.sciencedirect.com/science/article/pii/S0042698998000844},
author = {Casper J. Erkelens and Raymond {van Ee}},
keywords = {Binocular vision, Stereopsis, Disparity, Binocular saccades},
abstract = {It is now well established that depth is coded by local horizontal disparity and global vertical disparity. We present a computational model which explains how depth is extracted from these two types of disparities. The model uses the two (one for each eye) headcentric directions of binocular targets, derived from retinal signals and oculomotor signals. Headcentric disparity is defined as the difference between headcentric directions of corresponding features in the left and right eye’s images. Using Helmholtz’s coordinate systems we decompose headcentric disparity into azimuthal and elevational disparity. Elevational disparities of real objects are zero if the signals which contribute to headcentric disparity do not contain any errors. Azimuthal headcentric disparity is a 1D quantity from which an exact equation relating distance and disparity can be derived. The equation is valid for all headcentric directions and for all binocular fixation positions. Such an equation does not exist if disparity is expressed in retinal coordinates. Possible types of errors in oculomotor signals (six) produce global elevational disparity fields which are characterised by different gradients in the azimuthal and elevational directions. Computations show that the elevational disparity fields uniquely characterise both the type and size of the errors in oculomotor signals. Our model uses a measure of the global elevational disparity field together with local azimuthal disparity to accurately derive headcentric distance throughout the visual field. The model explains existing data on whole-field disparity transformations as well as hitherto unexplained aspects of stereoscopic depth perception.}
}
@article{ZHAI2023101373,
title = {Can reflective interventions improve students’ academic achievement? A meta-analysis},
journal = {Thinking Skills and Creativity},
volume = {49},
pages = {101373},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101373},
url = {https://www.sciencedirect.com/science/article/pii/S1871187123001414},
author = {Na Zhai and Yong Huang and Xiaomei Ma and Jingchun Chen},
keywords = {Reflection, Reflective intervention, Academic achievement, Meta-analysis},
abstract = {Reflection is widely acknowledged as a crucial skill for successful learning and decision-making. Recent evidence has shown that reflection can enhance motivation for in-depth learning, improve cognitive and metacognitive strategies, and promote self-regulated learning. While some studies have reported the positive effects of reflective interventions on student academic outcomes, conflicting findings exist. To provide a comprehensive understanding of the effectiveness of reflective interventions on academic achievement, this meta-analysis synthesized data from 25 quantitative studies (comprising 29 effect sizes) conducted between 2012 and 2022, with a total of 2,111 participants. The results revealed a significant overall effect of reflective interventions on academic achievement (g = 0.793, p < 0.001). Further moderator analyses indicated that the effectiveness of reflective interventions was influenced by factors such as learning mode, intervention duration, the role of reflective writing, and culture. However, education level, discipline, teacher or expert feedback, peer interaction, and technological scaffolding did not significantly affect the impact of reflective interventions across studies. These findings highlight the importance of fostering reflective thinking and refining the detailed design of reflective interventions to enhance students’ academic achievement.}
}
@article{DELEON2003507,
title = {On the computation of the Lichnerowicz–Jacobi cohomology},
journal = {Journal of Geometry and Physics},
volume = {44},
number = {4},
pages = {507-522},
year = {2003},
issn = {0393-0440},
doi = {https://doi.org/10.1016/S0393-0440(02)00056-6},
url = {https://www.sciencedirect.com/science/article/pii/S0393044002000566},
author = {Manuel {de León} and Belén López and Juan C. Marrero and Edith Padrón},
keywords = {Jacobi manifolds, Poisson manifolds, Lie algebroids, Lichnerowicz–Jacobi cohomology, Contact manifolds, Locally conformal symplectic manifolds},
abstract = {Lichnerowicz–Jacobi cohomology of Jacobi manifolds is reviewed. The use of the associated Lie algebroid allows to prove that the Lichnerowicz–Jacobi cohomology is invariant under conformal changes of the Jacobi structure. We also compute the Lichnerowicz–Jacobi cohomology for a large variety of examples.}
}
@article{VANSANTEN19902001,
title = {Computational advances in catalyst modelling.},
journal = {Chemical Engineering Science},
volume = {45},
number = {8},
pages = {2001-2011},
year = {1990},
issn = {0009-2509},
doi = {https://doi.org/10.1016/0009-2509(90)80073-N},
url = {https://www.sciencedirect.com/science/article/pii/000925099080073N},
author = {R.A. {van Santen}},
keywords = {Molecular Catalysis, Theoretical Chemistry, Catalyst Modelling, Zeolite Stability, Theoretical Kinitics.},
abstract = {Fruitful theoretical approaches to predict catalyst stability, to simulate transition states or assist catalyst characterization become available due to the computational possibilities generated by supercomputers. Advances in theoretical chemistry and catalysis provide the conceptual framework that enables application in catalyst modelling. Especially in zeolite catalysis computational techniques are increasingly applied. Because of their well-defined structures they are very suitable for the application of graphics approaches. Techniques have been developed to determine interaction-potentials on the basis of quantumchemical cluster-calculations and to verify them by comparison with experimental and spectroscopic data. Stimulated by quantum chemical studies in chemisorption as well as organometallic chemistry, computational studies of reaction intermediates in homogeneous as well as heterogeneous catalytic reactions have been undertaken. The development of potential energy surface parametrization schemes is of importance to enable the application of molecular dynamics studies to catalyst stability and reactivity}
}
@article{AKBAR2025111281,
title = {Unlocking the potential of EEG in Alzheimer's disease research: Current status and pathways to precision detection},
journal = {Brain Research Bulletin},
volume = {223},
pages = {111281},
year = {2025},
issn = {0361-9230},
doi = {https://doi.org/10.1016/j.brainresbull.2025.111281},
url = {https://www.sciencedirect.com/science/article/pii/S0361923025000930},
author = {Frnaz Akbar and Imran Taj and Syed Muhammad Usman and Ali Shariq Imran and Shehzad Khalid and Imran Ihsan and Ammara Ali and Amanullah Yasin},
keywords = {Electroencephalogram, Alzheimer’s disease, EEG, Mild cognitive impairment, Frontal temporal dementia, Neuro-degenerative},
abstract = {Alzheimer’s disease (AD) affects millions of individuals worldwide and is considered a serious global health issue due to its gradual neuro-degenerative effects on cognitive abilities such as memory, thinking, and behavior. There is no cure for this disease but early detection along with a supportive care plan may aid in improving the quality of life for patients. Automated detection of AD is challenging because its symptoms vary in patients due to genetic, environmental, or other co-existing health conditions. In recent years, multiple researchers have proposed automated detection methods for AD using MRI and fMRI. These approaches are expensive, have poor temporal resolution, do not offer real-time insights, and have not proven to be very accurate. In contrast, only a limited number of studies have explored the potential of Electroencephalogram (EEG) signals for AD detection. In contrast, Electroencephalogram (EEG) signals present a cost-effective, non-invasive, and high-temporal-resolution alternative for AD detection. Despite their potential, the application of EEG signals in AD research remains under-explored. This study reviews publicly available EEG datasets, the variety of machine learning models developed for automated AD detection, and the performance metrics achieved by these methods. It provides a critical analysis of existing approaches, highlights challenges, and identifies key areas requiring further investigation. Key findings include a detailed evaluation of current methodologies, prevailing trends, and potential gaps in the field. What sets this work apart is its in-depth analysis of EEG signals for Alzheimer’s Disease detection, providing a stronger and more reliable foundation for understanding the potential role of EEG in this area.}
}
@article{THIBODEAU2017852,
title = {How Linguistic Metaphor Scaffolds Reasoning},
journal = {Trends in Cognitive Sciences},
volume = {21},
number = {11},
pages = {852-863},
year = {2017},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2017.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1364661317301535},
author = {Paul H. Thibodeau and Rose K. Hendricks and Lera Boroditsky},
keywords = {analogy, decision making, framing, language and thought, metaphor, reasoning},
abstract = {Language helps people communicate and think. Precise and accurate language would seem best suited to achieve these goals. But a close look at the way people actually talk reveals an abundance of apparent imprecision in the form of metaphor: ideas are ‘light bulbs’, crime is a ‘virus’, and cancer is an ‘enemy’ in a ‘war’. In this article, we review recent evidence that metaphoric language can facilitate communication and shape thinking even though it is literally false. We first discuss recent experiments showing that linguistic metaphor can guide thought and behavior. Then we explore the conditions under which metaphors are most influential. Throughout, we highlight theoretical and practical implications, as well as key challenges and opportunities for future research.}
}
@article{RASMUSSEN2007195,
title = {Reinventing solutions to systems of linear differential equations: A case of emergent models involving analytic expressions},
journal = {The Journal of Mathematical Behavior},
volume = {26},
number = {3},
pages = {195-210},
year = {2007},
note = {An Inquiry Oriented Approach to Differential Equations},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2007.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732312307000338},
author = {Chris Rasmussen and Howard Blumenfeld},
keywords = {Modeling, Undergraduate mathematics, Realistic mathematics education, Student thinking, Proportional reasoning},
abstract = {An enduring challenge in mathematics education is to create learning environments in which students generate, refine, and extend their intuitive and informal ways of reasoning to more sophisticated and formal ways of reasoning. Pressing concerns for research, therefore, are to detail students’ progressively sophisticated ways of reasoning and instructional design heuristics that can facilitate this process. In this article we analyze the case of student reasoning with analytic expressions as they reinvent solutions to systems of two differential equations. The significance of this work is twofold: it includes an elaboration of the Realistic Mathematics Education instructional design heuristic of emergent models to the undergraduate setting in which symbolic expressions play a prominent role, and it offers teachers insight into student thinking by highlighting qualitatively different ways that students reason proportionally in relation to this instructional design heuristic.}
}
@article{LIU2023109530,
title = {Quantum computing for power systems: Tutorial, review, challenges, and prospects},
journal = {Electric Power Systems Research},
volume = {223},
pages = {109530},
year = {2023},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2023.109530},
url = {https://www.sciencedirect.com/science/article/pii/S0378779623004194},
author = {Hualong Liu and Wenyuan Tang},
keywords = {Quantum computing, Optimization, Power systems, Renewable energy, Climate neutrality},
abstract = {As a large number of renewable energy resources are connected to power systems, the operation, planning, and optimization of power systems have been becoming more and more complex. Power flow calculation, unit commitment, economic dispatch, energy pricing, and power system planning are essentially computation problems. A lot of computing resources are required for these problems, which are non-trivial, especially for large-scale power systems with the high penetration of renewable energy. Traditionally, the calculation and optimization of power systems are completed by classical computers based on the classical computing theory and the von Neumann architecture. However, with Moore’s law getting closer and closer to the limit, the importance of quantum computing has become increasingly prominent. Quantum computing has been applied to some fields to a certain extent, yet the applications of quantum computing in power systems are rare. As the power industry is the foundation of the national economy, introducing quantum computing into the power system has far-reaching and crucial significance, such as improving the penetration of renewable energy, enhancing the computing efficiency, and helping in achieving the goal of net zero and climate neutrality by 2050. This paper first introduces the core concepts, essential ideas and theories of quantum computing, and then reviews the existing literature on the applications of quantum computing in power systems, and puts forward our critical thinking about the applications of quantum computing in power systems. In brief, this paper is dedicated to a tutorial on quantum computing targeting power system professionals and a review of its applications in power systems. The main contributions of this paper are: (1) introduce quantum computing into the field of power engineering in a thoroughly detailed way and delineate the analysis methodologies of quantum circuits systematically without losing mathematical rigor; (2) based on Dirac’s notation, the related formulae are derived meticulously with sophisticated schematic diagrams; (3) elaborate and derive some critical quantum algorithms in depth, which play an important role in the applications of quantum computing in power systems; (4) critically summarize and comment on the existing literature on the applications of quantum computing in power systems; (5) the future applications and challenges of quantum computing in power systems are prospected and remarked.}
}
@article{TOOBY2025106687,
title = {The evolution of war and its cognitive foundations},
journal = {Evolution and Human Behavior},
volume = {46},
number = {3},
pages = {106687},
year = {2025},
issn = {1090-5138},
doi = {https://doi.org/10.1016/j.evolhumbehav.2025.106687},
url = {https://www.sciencedirect.com/science/article/pii/S1090513825000364},
author = {John Tooby and Leda Cosmides},
keywords = {War, Coalitional aggression, Cooperation, Cognition, Conflict},
abstract = {Coalitional aggression evolved because it allowed the participants to promote their fitness by gaining access to disputed, reproduction-enhancing resources that would otherwise be denied to them. Few species engage in coalitional aggression, even though the social conditions that would favor its evolution seem to be widespread. Why? Forming coalitions to exploit these opportunities requires individuals to solve highly complex and specialized information processing problems involving cooperation, coordination, and social exchange. The difficulty of evolving cognitive mechanisms capable of solving these problems—especially when the individuals involved are not kin—may explain why multi-individual coalitions are phylogenetically rare. We propose that humans and a few other cognitively pre-adapted species have evolved specialized cognitive programs that govern coalitional behavior, which constitute a distinctive coalitional psychology. To derive a preliminary map of this psychology, we started with a task analysis of the adaptive information-processing problems that arise during coalitional aggression. This exercise can shine light on our evolved psychology because algorithms that motivate and organize coalitional aggression would need design features that solve these problems well to be favored by selection. These problems include decisions about when to form a coalition or join one, when to initiate an attack, and how to allocate the costs and benefits that result from coalitional action. The risk contract of war identifies circumstances under which natural selection would favor decisions to initiate an attack. When the conditions of this model are met, mortality rates will not negatively impact the fitness of males in the winning coalition. This outcome has implications for the design of computational systems that motivate coalitional attacks; it may explain why warfare is so favored an activity among men, despite its risks to the participating individuals' welfare.}
}
@article{ZHOU1997497,
title = {Three-dimensional computations of solution hydrodynamics during the growth of potassium dihydrogen phosphate I. Spin up and steady rotation},
journal = {Journal of Crystal Growth},
volume = {180},
number = {3},
pages = {497-509},
year = {1997},
note = {Modelling in Crystal Growth},
issn = {0022-0248},
doi = {https://doi.org/10.1016/S0022-0248(97)00251-0},
url = {https://www.sciencedirect.com/science/article/pii/S0022024897002510},
author = {Yuming Zhou and Jeffrey J. Derby},
keywords = {Solution growth, Three-dimensional modeling, Fluid flow},
abstract = {A novel, massively parallel implementation of the Galerkin finite element method is used to study three-dimensional, time-dependent flows which occur during the rapid growth of potassium dihydrogen phosphate crystals from solution in a system employed by researchers at Lawrence Livermore National Laboratory. Computations for the hydrodynamics of system spin up and steady rotation indicate the importance of time-dependent flow phenomena and emphasize the significant role played by the support and crystal geometry in forming the complicated flows in this system. Predicted flow structures correlate well with experimental observations of inclusion formation.}
}
@article{LI2023101752,
title = {The role of inhibition in overcoming arithmetic natural number bias in the Chinese context: Evidence from behavioral and ERP experiments},
journal = {Learning and Instruction},
volume = {86},
pages = {101752},
year = {2023},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2023.101752},
url = {https://www.sciencedirect.com/science/article/pii/S095947522300021X},
author = {Xiaodong Li and Ping Xu and Ronghuan Jiang and Shuang Chen},
keywords = {Inhibitory control, Negative priming, Natural number bias, Arithmetic operation, Event-related potential},
abstract = {The natural number bias (NNB) in arithmetic operations refers to the application of natural number properties to reasoning about rational numbers. Previous studies found the NNB interferes with students’ problem-solving. However, few studies have examined it in the Chinese context or the underlying mechanism by which it can be overcome. Addressing these gaps, in Experiments 1a (n = 31) and 1b (n = 30), we found that Chinese students demonstrate the NNB despite linguistic differences between Chinese and western languages. Experiment 2 (n = 38) adopted a negative priming paradigm and found that inhibitory control was necessary to overcome the NNB. Experiment 3 (n = 34) employed the event-related potential technique; we observed increased P2 amplitude when students solved congruent problems, and increased N2 and decreased P3 amplitude when they solved incongruent problems. These results indicated that the NNB is rooted in intuitive thinking, and overcoming this bias relies on inhibition.}
}
@article{POWELL2016147,
title = {Deconstructing intellectual curiosity},
journal = {Personality and Individual Differences},
volume = {95},
pages = {147-151},
year = {2016},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2016.02.037},
url = {https://www.sciencedirect.com/science/article/pii/S0191886916300927},
author = {Christopher Powell and Ted Nettelbeck and Nicholas R. Burns},
keywords = {Curiosity, Intellectual curiosity, Epistemic Curiosity, Need for Cognition, Typical Intellectual Engagement, Intellect},
abstract = {Scales of Need for Cognition (NFC), Typical Intellectual Engagement (TIE), and Epistemic Curiosity (EC) measure intellectual curiosity (IC). These scales correlate strongly and have been factor-analyzed individually but not together. Here N=396 (143 males) undergraduates completed measures of NFC, TIE, and EC. Six factors, labeled Intellectual Avoidance, Deprivation, Problem Solving, Abstract Thinking, Reading, and Wide Interest, were identified. TIE is the broadest scale, measuring all factors except Deprivation; NFC measures Intellectual Avoidance and Problem Solving, plus Abstract Thinking and Deprivation to a lesser degree; and EC largely measures Deprivation. Moreover, Reading may not fit in the IC domain; higher-order factor analysis indicated that, whereas items measuring Reading loaded more strongly on their first-order factor, items measuring the other factors strongly loaded on a general factor of IC. These results are significant for understanding the contents of these scales, and for future scale development.}
}
@article{HAYES201739,
title = {Regression-based statistical mediation and moderation analysis in clinical research: Observations, recommendations, and implementation},
journal = {Behaviour Research and Therapy},
volume = {98},
pages = {39-57},
year = {2017},
note = {Best Practice Guidelines for Modern Statistical Methods in Applied Clinical Research},
issn = {0005-7967},
doi = {https://doi.org/10.1016/j.brat.2016.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0005796716301887},
author = {Andrew F. Hayes and Nicholas J. Rockwood},
keywords = {Mediation analysis, Moderation, Interaction, Regression analysis, Mechanisms},
abstract = {There have been numerous treatments in the clinical research literature about various design, analysis, and interpretation considerations when testing hypotheses about mechanisms and contingencies of effects, popularly known as mediation and moderation analysis. In this paper we address the practice of mediation and moderation analysis using linear regression in the pages of Behaviour Research and Therapy and offer some observations and recommendations, debunk some popular myths, describe some new advances, and provide an example of mediation, moderation, and their integration as conditional process analysis using the PROCESS macro for SPSS and SAS. Our goal is to nudge clinical researchers away from historically significant but increasingly old school approaches toward modifications, revisions, and extensions that characterize more modern thinking about the analysis of the mechanisms and contingencies of effects.}
}
@article{HESAMI20251,
title = {Trends in production, consumption, trade, and research of dry beans across the globe and Canada},
journal = {Canadian Journal of Plant Science},
volume = {105},
pages = {1-14},
year = {2025},
issn = {0008-4220},
doi = {https://doi.org/10.1139/cjps-2024-0185},
url = {https://www.sciencedirect.com/science/article/pii/S0008422025000223},
author = {Mohsen Hesami and Mohsen Yoosefzadeh-Najafabadi},
keywords = {agri-food system, computational biology, food security, , plant breeding, yield},
abstract = {Dry beans (Phaseolus vulgaris L.) are known as a significant component of global agri-food systems, in regions such as Southern Asia, Eastern Africa, and South America, where they also serve as a valuable source of feed. Over the past few decades, global production has grown significantly, driven by rising demand, technological advancements, improved yields, and expanded cultivation areas. Canada, in particular, has become a significant player in the dry bean industry, leveraging its rich agricultural landscape and advanced agricultural technologies. Canadian research initiatives, financially supported by both governmental and private funding, have concentrated on developing new bean varieties with higher yields, resistance to pests and diseases, better adaptation to local growing conditions, and improved nutritional profiles. This study reviews trends in dry bean production, consumption, and international trade over the past decades, emphasizing the implications for research on both global and Canadian scales. Collaborative efforts between Canadian institutions and international research organizations have facilitated the exchange of genetic resources and agronomic techniques, thereby enhancing productivity and sustainability. By investing in these innovative endeavors, Canada not only bolsters its strengthened agricultural sector but also contributes significantly to global food security and the achievement of sustainable development goals.}
}
@article{DAS2024100104,
title = {AI and data-driven urbanism: The Singapore experience},
journal = {Digital Geography and Society},
volume = {7},
pages = {100104},
year = {2024},
issn = {2666-3783},
doi = {https://doi.org/10.1016/j.diggeo.2024.100104},
url = {https://www.sciencedirect.com/science/article/pii/S2666378324000266},
author = {Diganta Das and Berwyn Kwek},
keywords = {Singapore, Smart cities, Smart nation, Artificial intelligence (AI), Digital urbanism},
abstract = {This paper presents a deep and critical analysis of Singapore's new wave of state-built digital tools and services and how it connects to its larger smart urbanism project, also known as Smart Nation. The COVID-19 pandemic, and particularly Singapore's response, served as a real-world testing ground for smart urbanist strategies. In particular, we analysed the logic that emanates from these novel digital interventions, how they operate on the complex urban built environment and the population, and their effects on urban and citizenry morphologies. Next, we examined a series of state-led technological implementations that have emerged since the Covid-19 pandemic, providing digital solutions that assist citizens with the changing rhythms of everyday living, data-capturing sensors and gantries to aid authorities in contract tracing efforts and enforce vaccination differentiation measures, geospatial digital mapping of demographic data, in withal robotics for automated policing and cleaning activities; and the use of AI and automated data-driven tools in public health to improve service delivery and care to patients. While we are unable to exhaust every piece of technology for the purpose of this paper, these developments, along with their design thinking and operations, we argue, are helpful in revealing the contemporary conjectures of Singaporean digital urban idealism and the governing strategies of the state. By examining Singapore's response, this study aims to contribute to the ongoing discourse on smart urbanism, offering insights into how cities can leverage technology effectively while balancing technological innovation with privacy and public trust.}
}
@article{BARILE2022467,
title = {Platform-based innovation ecosystems: Entering new markets through holographic strategies},
journal = {Industrial Marketing Management},
volume = {105},
pages = {467-477},
year = {2022},
issn = {0019-8501},
doi = {https://doi.org/10.1016/j.indmarman.2022.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0019850122001614},
author = {Sergio Barile and Cristina Simone and Francesca Iandolo and Antonio Laudando},
keywords = {Platforms, Innovation ecosystems, Platform-based innovation ecosystems, Holographic strategies, Digital algorithms, Platform envelopment},
abstract = {The platformization seems to be a demiurgic force, increasingly (re)shaping this millennium and its socio-economic, technological and physical structures, institutions, and human lives. Innovation ecosystems are experiencing this platformization, leading to the rise of platform-based innovation ecosystems. However, the industrial and managerial literature still lacks a shared definition, a consistent theoretical and strategic framework to explain how platform-based innovation ecosystems emerge and replicate from market to market. This conceptual work attempts to fill those gaps by integrating the extant literature on innovation ecosystems in two ways. First, moving from the literature on innovation ecosystems and industry platforms, using systems thinking framing, it explains the platformization of innovation ecosystems through the double lens structure-system. Second, it identifies the holographic strategy as one of the typical patterns featuring platform-based innovation ecosystem envelopment beyond extant market boundaries. These conceptualizations have insightful theoretical, managerial, and policy implications. In particular, the work discusses the ecosystem as a valid unit of analysis for understanding such an unprecedented shaped-by-platform landscape. Then, it describes the growth strategies of the platform-based innovation ecosystem supporting the platform sponsor in mastering multipoint competition. Eventually, the study pinpoints crucial issues for policymakers in regulating the impact that platformization is having on society.}
}
@article{KELTNER2021216,
title = {A taxonomy of positive emotions},
journal = {Current Opinion in Behavioral Sciences},
volume = {39},
pages = {216-221},
year = {2021},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2021.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S2352154621000991},
author = {Dacher Keltner and Alan Cowen},
abstract = {Within social functionalist theory (SFT), emotions structure attachment relations, cooperative alliances, hierarchies, and collectives. Within this line of thinking, a rich array of positive emotions enable the formation and negotiation of these relationships. Guided by these arguments, we synthesize how top-down confirmatory studies and data-driven, computational studies converge on evidence for 11 positive emotions with distinct experience, expression, and physiology. This taxonomy includes amusement, awe, compassion, contentment, desire, love, joy, interest, pride, relief, and triumph. We conclude by considering how recent taxonomic efforts will advance emotion science in mapping the distinct forms and functions of the positive emotions.}
}
@article{ZORAN2025101135,
title = {Digital gastronomy 2.0: A 15-year transformative journey in culinary-tech evolution and interaction},
journal = {International Journal of Gastronomy and Food Science},
volume = {39},
pages = {101135},
year = {2025},
issn = {1878-450X},
doi = {https://doi.org/10.1016/j.ijgfs.2025.101135},
url = {https://www.sciencedirect.com/science/article/pii/S1878450X25000368},
author = {Amit Raphael Zoran},
abstract = {This paper reviews 15 years of exploration and development in Digital Gastronomy (DG), tracing its progression from foundational frameworks to AI-integrated culinary systems. The journey begins with integrating computational tools like laser cooking, 3D printing, CNC milling, and modular molds, which expand the possibilities of creativity and precision in the kitchen. Building on these technologies, the Meta-Recipe (MR) framework introduces a structured approach to recipe design, allowing chefs to adapt dishes dynamically while maintaining culinary coherence. The concept of “Digital Alchemy” extends this foundation, blending AI-driven methods with traditional healing and sustainable practices to emphasize well-being and environmental consciousness. These advancements culminate in the vision of an AI-augmented kitchen, conceptualized as a collaborative and adaptive space that bridges culinary artistry with algorithmic precision. This research highlights DG's potential as an evolving interdisciplinary field, offering new gastronomy, creativity, and sustainability directions.}
}
@article{MAHONY2020104668,
title = {New ideas for non-animal approaches to predict repeated-dose systemic toxicity: Report from an EPAA Blue Sky Workshop},
journal = {Regulatory Toxicology and Pharmacology},
volume = {114},
pages = {104668},
year = {2020},
issn = {0273-2300},
doi = {https://doi.org/10.1016/j.yrtph.2020.104668},
url = {https://www.sciencedirect.com/science/article/pii/S0273230020300945},
author = {Catherine Mahony and Randolph S. Ashton and Barbara Birk and Alan R. Boobis and Tom Cull and George P. Daston and Lorna Ewart and Thomas B. Knudsen and Irene Manou and Sebastian Maurer-Stroh and Luigi Margiotta-Casaluci and Boris P. Müller and Pär Nordlund and Ruth A. Roberts and Thomas Steger-Hartmann and Evita Vandenbossche and Mark R. Viant and Mathieu Vinken and Maurice Whelan and Zvonar Zvonimir and Mark T.D. Cronin},
keywords = {Repeated dose toxicity testing, Alternatives, Safety assessment, Chemical legislation, , , Read-across, },
abstract = {The European Partnership for Alternative Approaches to Animal Testing (EPAA) convened a ‘Blue Sky Workshop’ on new ideas for non-animal approaches to predict repeated-dose systemic toxicity. The aim of the Workshop was to formulate strategic ideas to improve and increase the applicability, implementation and acceptance of modern non-animal methods to determine systemic toxicity. The Workshop concluded that good progress is being made to assess repeated dose toxicity without animals taking advantage of existing knowledge in toxicology, thresholds of toxicological concern, adverse outcome pathways and read-across workflows. These approaches can be supported by New Approach Methodologies (NAMs) utilising modern molecular technologies and computational methods. Recommendations from the Workshop were based around the needs for better chemical safety assessment: how to strengthen the evidence base for decision making; to develop, standardise and harmonise NAMs for human toxicity; and the improvement in the applicability and acceptance of novel techniques. “Disruptive thinking” is required to reconsider chemical legislation, validation of NAMs and the opportunities to move away from reliance on animal tests. Case study practices and data sharing, ensuring reproducibility of NAMs, were viewed as crucial to the improvement of non-animal test approaches for systemic toxicity.}
}
@article{GARGALO2024108504,
title = {A process systems engineering view of environmental impact assessment in renewable and sustainable energy production: Status and perspectives},
journal = {Computers & Chemical Engineering},
volume = {180},
pages = {108504},
year = {2024},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2023.108504},
url = {https://www.sciencedirect.com/science/article/pii/S0098135423003745},
author = {Carina L. Gargalo and Haoshui Yu and Nikolaus Vollmer and Ahmad Arabkoohsar and Krist V. Gernaey and Gürkan Sin},
keywords = {Renewable and sustainable energy systems, Environmental impact assessment, Process systems engineering, Life cycle assessment, Sustainability},
abstract = {With the increasing concern for climate change, renewable and sustainable energy production has attracted considerable attention from the scientific community, industrial practitioners, and policy and decision-makers. There are many technological alternatives for each sub-category of complex sustainable energy systems. Life cycle assessment (LCA) can be an effective tool to compare the environmental impacts of each pathway and identify the most promising alternatives from an environmental impact perspective. This contribution first reviews the environmental assessment methods and tools developed over the years. Secondly, a comprehensive review of the contribution of the PSE community to the environmental impact analysis of renewable energy systems is performed. It is observed that while LCA is the preferred method, these studies differed widely concerning the choice of impact assessment method used, the level of details shared concerning the underlying LCA calculations, and whether or not sensitivity and uncertainty analyses were carried out, among many others. This makes the comparison of results from different studies difficult and often impossible. It is clear that the PSE community, with its emphasis on systems thinking and holistic approaches, plays a critical role in the design, integration, and operation of complex sustainable energy systems. However, the thorough calculations necessary to ensure a robust and transparent LCA analysis require a shared methodology and a detailed description of the rules. Such explicit, systematic, and transparent methods will set the bar for a minimum requirement for thorough LCA calculations, ensuring fair comparison and discussions of different technical solutions developed in the wider PSE community for sustainable renewables.}
}
@article{KAVLOCK2005265,
title = {Computational Toxicology: Framework, Partnerships, and Program Development: September 29–30, 2003, Research Triangle Park, North Carolina},
journal = {Reproductive Toxicology},
volume = {19},
number = {3},
pages = {265-280},
year = {2005},
note = {Systems Biology/Computational Toxicology},
issn = {0890-6238},
doi = {https://doi.org/10.1016/j.reprotox.2004.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S0890623804000747},
author = {Robert Kavlock and Gerald T. Ankley and Tim Collette and Elaine Francis and Karen Hammerstrom and Jack Fowle and Hugh Tilson and Greg Toth and Patricia Schmieder and Gilman D. Veith and Eric Weber and Douglas C. Wolf and Doug Young}
}
@article{ABEYSEKERA2024100213,
title = {ChatGPT and academia on accounting assessments},
journal = {Journal of Open Innovation: Technology, Market, and Complexity},
volume = {10},
number = {1},
pages = {100213},
year = {2024},
issn = {2199-8531},
doi = {https://doi.org/10.1016/j.joitmc.2024.100213},
url = {https://www.sciencedirect.com/science/article/pii/S2199853124000076},
author = {Indra Abeysekera},
keywords = {Academia, Accounting, Assessments, ChatGPT, Multiple Choice Questions, Sustainable Development Goals of the United Nations},
abstract = {ChatGPT is considered a risk and an opportunity for academia. An area of threat in contemporary settings is whether it can become a student agent for assessments in academia. This study determines how ChatGPT can become a human agent for students on two financial accounting course units, multiple choice question assessments. The study provided five numerical-based and five narrative-based multiple choice questions. There were ten questions for the Introductory Financial Accounting and 10 for the Advanced Financial Accounting course units. ChatGPT received one question at a time requesting a solution. In the Introductory Financial Accounting section, ChatGPT produced incorrect answers because it incorrectly assumed the underlying assumptions contained in those questions. In Advanced Financial Accounting, ChatGPT presented incorrect answers because of the complexity of the task contained in those questions. ChatGPT demonstrated similar competencies in providing solutions to numerical-based and narrative-based questions. ChatGPT obtained the correct answers to sit in the 80th percentile in the Introductory Financial Accounting course unit assessment and the 50th percentile in the Advanced Financial course unit assessment. ChatGPT4 showed improved performance, with the 90th percentile for Introductory Financial Accounting and the 70th percentile for Advanced Financial Accounting. The findings indicate that the knowledge construct requires reflective thinking with ChatGPT in the ecosystem, and what is assumed and assessable knowledge must be revisited.}
}
@article{LI2023113687,
title = {Twins transformer: Cross-attention based two-branch transformer network for rotating bearing fault diagnosis},
journal = {Measurement},
volume = {223},
pages = {113687},
year = {2023},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2023.113687},
url = {https://www.sciencedirect.com/science/article/pii/S0263224123012514},
author = {Jie Li and Yu Bao and WenXin Liu and PengXiang Ji and LeKang Wang and Zhongbing Wang},
keywords = {Attention mechanisms, Cross-attention, Fault diagnosis, Transformer},
abstract = {Due to the inherent shortcomings of traditional depth models, the Transformer model based on the self-attention mechanism has become popular in the field of fault diagnosis. The current Transformer's self-attentive mechanism provides an alternative way of thinking, which can make direct association between each signal. However, it can only focus on the association information within a sequence, and it is difficult to understand the information gap between samples. Therefore, this paper proposes the two-branch Twins attention, which for the first time uses cross-attention to focus on information associations between samples. Twins attention uses cross-attention to learn information associations between samples in addition to retaining the information associations within sequences learned by self-attention. The performance of the proposed model was validated on four popular bearing datasets. Compared to the original transformer structure, the average accuracy of each dataset improved by 1.73% to 99.42%, leading the noise experiments.}
}
@article{SELVAKKUMARAN2020111053,
title = {Review of the use of system dynamics (SD) in scrutinizing local energy transitions},
journal = {Journal of Environmental Management},
volume = {272},
pages = {111053},
year = {2020},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2020.111053},
url = {https://www.sciencedirect.com/science/article/pii/S0301479720309816},
author = {Sujeetha Selvakkumaran and Erik O. Ahlgren},
keywords = {System dynamics, Modelling, Local, Energy transitions, Multi-level perspective},
abstract = {Local energy transition processes are complex socio-technical transitions requiring careful study. The use of System Dynamics (SD) in modelling and analyzing local energy transitions is especially suitable given the characteristics of SD. Our aim is to systematically categorize the different ways SD is used and useful to scrutinize local energy transitions, and to see if we can discern any common themes that can be useful to researchers looking to scrutinize local energy transitions, using SD. The study is exploratory in nature, with peer-reviewed journal and conference articles analyzed using content analysis. The six categories on which the articles are analyzed are: the sector the article studies; the transition that is studied in the article; the modelling depth in the article; the objective of the article; the justification for using SD provided in the article and the levels of interaction with ‘local’. Our findings show most of the local energy transitions have been studied using simulatable Stock and Flow Diagrams in SD methodology. The important sectors in the energy field are represented in terms of SD modelling of local energy transitions, including electricity, transport, district heating etc. Most of the local energy transitions scrutinized by SD in the articles have descriptive objectives, with some prescriptive, and just one evaluative objective. In terms of justification for using SD provided by the articles analyzed in this study, we found four major themes along which the justifications that were provided. They are dynamics, feedbacks, delays and complexity, systematic thinking, bridging disciplines and actor interactions and behaviour. The ‘dynamics, feedbacks, delays and complexity’ theme is the most cited justification for the use of SD in scrutinizing local energy transitions, followed by systematic thinking.}
}
@incollection{SALIMI201883,
title = {Chapter 2 - Fundamentals of Systemic Approach},
editor = {Fabienne Salimi and Frederic Salimi},
booktitle = {A Systems Approach to Managing the Complexities of Process Industries},
publisher = {Elsevier},
pages = {83-180},
year = {2018},
isbn = {978-0-12-804213-7},
doi = {https://doi.org/10.1016/B978-0-12-804213-7.00002-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128042137000025},
author = {Fabienne Salimi and Frederic Salimi},
keywords = {Systems engineering, systems thinking, critical thinking, Safety Critical Element (SCE), Project Management, Complexity, Emergence, SE Competency, Type of Systems, IIoT, Big Data},
abstract = {System thinking, system engineering, and complexity management are the back bone of any operational excellence and process safety management system. This chapter aims to give a solid but concise background for the fundamentals of system engineering, system thinking, and complexity management for process industry. Different type of processes, requirement engineering and management, safety critical systems, critical thinking, and SE competency framework are discussed. It also addresses issues that pertain to human judgment and how people employ rules of thumb and heuristics to problem-solving situations. Various modes of engineering are discussed along with the complexities and concerns within each: cognitive systems engineering, control engineering, software engineering, industrial engineering, performance engineering, and several others. A distinction is also made between technical performance measures and key performance parameters. A list of leading indicators, insights, and requirements are then delineated among the various aspects of system engineering. Finally, an overall analysis of systems thinking, which concerns the process of understanding how various systems are implemented, is provided.}
}
@article{GANAPATHY20158064,
title = {Optimum steepest descent higher level learning radial basis function network},
journal = {Expert Systems with Applications},
volume = {42},
number = {21},
pages = {8064-8077},
year = {2015},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2015.06.036},
url = {https://www.sciencedirect.com/science/article/pii/S0957417415004388},
author = {Kirupa Ganapathy and V. Vaidehi and Jesintha B. Chandrasekar},
keywords = {Neural network, Radial basis function, Dynamic learning, Optimum steepest descent, Higher level components, Healthcare},
abstract = {Dynamically changing real world applications, demands for rapid and accurate machine learning algorithm. In neural network based machine learning algorithms, radial basis function (RBF) network is a simple supervised learning feed forward network. With its simplicity, this network is highly suitable to model and control the nonlinear systems. Existing RBF networks in literature are applied to static applications and also faces challenges such as increased model size, neuron removal, improper center selection etc leading to erroneous output. To overcome the challenges and handle complex real world problems, this paper proposes a new optimum steepest descent based higher level learning radial basis function network (OSDHL-RBFN). The proposed OSDHL-RBFN implements major components inspired from the human brain for efficient learning, adaptive structure and accurate classification. Higher level learning and thinking components of the proposed network are sample deletion, neuron addition, neuron migration, sample navigation and neuroplasticity. These components helps the classifier to think before learning the samples and regulates the learning strategy. The knowledge gained from the trained samples are used by the network to identify the incomplete sample, optimal center and bond strength of hidden & output neurons. Adaptive network structure is employed to minimize classification error. The proposed work also uses optimum steepest descent method for weight parameter update to minimize the sum square error. OSDHL-RBFN is tested and evaluated in both static and dynamic environments on nine benchmark classification (binary and multiclass) problems for balanced, unbalanced, small, large, low dimensional and high dimensional datasets. The overall and class wise efficiency of OSDHL-RBFN is improved when compared to other RBFN’s in the literature. The performance results clearly show that the proposed OSDHL-RBFN reduces the architecture complexity and computation time compared to other RBFN’s. Overall, the proposed OSDHL-RBFN is efficient and suitable for dynamic real world applications in terms of detection time and accuracy. As a case study, OSDHL-RBFN is implemented in real time remote health monitoring application for classifying the various abnormality levels in vital parameters.}
}
@incollection{TILLAS2017101,
title = {Chapter 7 - On the Redundancies of “Social Agency”},
editor = {Jon Leefmann and Elisabeth Hildt},
booktitle = {The Human Sciences after the Decade of the Brain},
publisher = {Academic Press},
address = {San Diego},
pages = {101-120},
year = {2017},
isbn = {978-0-12-804205-2},
doi = {https://doi.org/10.1016/B978-0-12-804205-2.00007-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128042052000070},
author = {A. Tillas},
keywords = {Structure, agency, concepts, intuitions, decision-making, actions},
abstract = {This chapter presents a philosophical argument about the “structure vs agency” debate—one of the central debates in social sciences. I do not argue for the primacy of either of the two but suggest an empirically vindicated view about the nature of thinking, in light of which the traditional debate as well as the notion of “social agency,” is redundant. I argue that thinking is contingent on the weightings of the synaptic connections between neuronal groups grounding it. In turn, socialization is a process of adjusting or conditioning the appropriate synaptic connection weightings. Both conscious (reasoning) and unconscious (intuitions) determinants of sociologically nontrivial actions derive from perceptual encounters with our sociophysical environment. In turn, agents—as social scientists use the term—simply do not exist. Finally, I appeal to neuroscientific evidence and show that we still qualify as agents, if only with regards to sociologically trivial actions.}
}
@article{SIMON1993431,
title = {Experience in using SIMD and MIMD parallelism for computational fluid dynamics},
journal = {Applied Numerical Mathematics},
volume = {12},
number = {5},
pages = {431-442},
year = {1993},
issn = {0168-9274},
doi = {https://doi.org/10.1016/0168-9274(93)90103-X},
url = {https://www.sciencedirect.com/science/article/pii/016892749390103X},
author = {Horst D. Simon and Leonardo Dagum},
keywords = {Parallel architectures, MIMD, SIMD, computational fluid dynamics.},
abstract = {One of the key objectives of the Applied Research Branch in the Numerical Aerodynamic Simulation (NAS) Systems Division at NASA Ames Research Center is the accelerated introduction of highly parallel machines into a fully operational environment. In this report we summarize some of the experiences with the parallel testbed machines at the NAS Applied Research Branch. We discuss the performance results obtained from the implementation of two computational fluid dynamics (CFD) applications, an unstructured grid solver and a particle simulation, on the Connection Machine CM-2 and the Intel iPSC/860.}
}
@article{RANGEL2012970,
title = {Value normalization in decision making: theory and evidence},
journal = {Current Opinion in Neurobiology},
volume = {22},
number = {6},
pages = {970-981},
year = {2012},
note = {Decision making},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2012.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0959438812001201},
author = {Antonio Rangel and John A Clithero},
abstract = {A sizable body of evidence has shown that the brain computes several types of value-related signals to guide decision making, such as stimulus values, outcome values, and prediction errors. A critical question for understanding decision-making mechanisms is whether these value signals are computed using an absolute or a normalized code. Under an absolute code, the neural response used to represent the value of a given stimulus does not depend on what other values might have been encountered. By contrast, under a normalized code, the neural response associated with a given value depends on its relative position in the distribution of values. This review provides a simple framework for thinking about value normalization, and uses it to evaluate the existing experimental evidence.}
}
@article{TAKANO201922,
title = {Difficulty in updating positive beliefs about negative cognition is associated with increased depressed mood},
journal = {Journal of Behavior Therapy and Experimental Psychiatry},
volume = {64},
pages = {22-30},
year = {2019},
issn = {0005-7916},
doi = {https://doi.org/10.1016/j.jbtep.2019.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0005791617302926},
author = {Keisuke Takano and Julie {Van Grieken} and Filip Raes},
keywords = {depression, Rumination, Memory, Reinforcement learning, Q-learning},
abstract = {Background and objectives
Depressed people hold positive beliefs about negative cognition (e.g., rumination is useful to find a solution), which may motivate those individuals to engage in sustained negative thinking. However, in reality, rumination often leads to unfavorable outcomes. Thus, such beliefs create a large discrepancy between one's expectations and the actual outcome. Therefore, we hypothesized that this prediction error would be associated with increased depressed mood.
Methods
We observed how people update their positive beliefs about negative cognition within a volatile environment, in which negative cognition does not always result in a beneficial outcome. Forty-six participants were offered two response options (retrieving a negative or positive personal memory) and subsequently provided either an economic reward or punishment. Retrieving a negative (rather than positive) memory was initially reinforced, although this action-outcome contingency was reversed during the task. In the control condition, positive memory retrieval was initially reinforced, although a contingency reversal was employed to encourage negative memory retrieval.
Results
Model-based computational modeling revealed that participants who showed a delay in switching from negative to positive (but not from positive to negative) responses experienced increased levels of depressed mood. This delay in switching was also found to be associated with depressive symptoms and trait rumination.
Limitations
The non-clinical nature of the sample may limit the clinical implications of the results.
Conclusions
Difficulty in updating positive beliefs (or outcome predictions) for negative cognition may play an important role in depressive symptomatology.}
}
@article{CHING201765,
title = {Children's understanding of the commutativity and complement principles: A latent profile analysis},
journal = {Learning and Instruction},
volume = {47},
pages = {65-79},
year = {2017},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2016.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0959475216301906},
author = {Boby Ho-Hong Ching and Terezinha Nunes},
keywords = {Additive reasoning, Commutativity principle, Complement principle, Latent profile analysis},
abstract = {This study examined patterns of individual differences in the acquisition of the knowledge of the commutativity and complement principles in 115 five-to six-year-old children and explored the role of concrete materials in helping children understand the prinicples. On the basis of latent profile analysis, four groups of children were identified: The first group succeeded in commutativity tasks with concrete materials but in no other tasks; the second succeeded in commutativity tasks in both concrete and abstract conditions, but not in complement tasks; the third group succeeded in all commutativity tasks and in complement tasks with concrete materials, and the final group succeeded in all the tasks. The four groups of children suggest a developmental trend – (1) Knowledge of the commutativity and of the complement principles seems to develop from thinking in the context of specific quantities to thinking about more abstract symbols; (2) There may be an order of understanding of the principles – from the commutativity to the complement principle; (3) Children may acquire the knowledge of the commutativity principle in the more abstract tasks before they start to acquire the knowledge of the complement principle. This study contributes to the literature by showing that assessing additive reasoning in different ways and identifying profiles with classification analyses may be useful for educators to understand more about the developmental stage where each child is placed. It appears that a more fine-grained assessment of additive reasoning can be achieved by incorporating both concrete materials and relatively abstract symbols in the assessment.}
}
@article{LANGE2024101191,
title = {What are explanatory proofs in mathematics and how can they contribute to teaching and learning?},
journal = {The Journal of Mathematical Behavior},
volume = {76},
pages = {101191},
year = {2024},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2024.101191},
url = {https://www.sciencedirect.com/science/article/pii/S0732312324000683},
author = {Marc Lange},
keywords = {Explanation, Proof, Generalization, Pedagogy, Unification, Coincidence},
abstract = {This paper will examine several simple examples (drawn from the mathematics literature) where there are multiple proofs of the same theorem, but only some of these proofs are widely regarded by mathematicians as explanatory. These examples will motivate an account of explanatory proofs in mathematics. Along the way, the paper will discuss why deus ex machina proofs are not explanatory, what a mathematical coincidence is, and how a theorem's proper setting reflects the naturalness of various mathematical kinds. The paper will also investigate how context influences which features of a theorem are salient and consequently which proofs are explanatory. The paper will discuss several ways in which explanatory proofs can contribute to teaching and learning, including how shifts in context (and hence in a proof’s explanatory power) can be exploited in a classroom setting, leading students to dig more deeply into why some theorem holds. More generally, the paper will examine how “Why?” questions operate in mathematical thinking, teaching, and learning.}
}
@incollection{PESCE2024123,
title = {Chapter Seven - Creativity and consciousness in motion: The roundtrip of “mindful” and “mindless” processes in embodied creativity},
editor = {Tal Dotan Ben-Soussan and Joseph Glicksohn and Narayanan Srinivasan},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {287},
pages = {123-151},
year = {2024},
booktitle = {The Neurophysiology of Silence (C): Creativity, Aesthetic Experience and Time},
issn = {0079-6123},
doi = {https://doi.org/10.1016/bs.pbr.2024.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0079612324000785},
author = {Caterina Pesce and Nicoletta Tocci},
keywords = {Creative thinking, Motor creativity, Embodiment, Hypofrontality, Flow, Incubation, Mind wandering, Nature, Green exercise, Mindful movements},
abstract = {In this opinion paper, we make a journey across different accounts of creativity that emphasize either the mindful, conscious and cognitive expression of creativity, or its mindless, unconscious and sensorimotor expression. We try to go beyond dichotomy, putting creativity in motion and outlining its embodied and enactive features. Based on the assumption that no creative act is purely conscious or purely unconscious, our discussion on creativity relies on the distinction of three types of creativity that complementarily contribute to the creative process through shifts in the activation of their substrates in the brain: the deliberate, spontaneous and flow types of creativity. The latter is a hybrid and embodied type, in which movement and physical activity meet creativity. We then focus on the most fascinating contribution of unconscious processes and mind wandering to spontaneous and flow modes of creativity, exploring what happens when the individual apparently takes a break from a deliberate and effortful search for solutions and the creative process progresses through an incubation phase. This phase and the overall creative process can be facilitated by physical activity which, depending on its features and context, can disengage the cognitive control network and free the mind from filters that constrain cognitive processes or, conversely, can engage attentional control on sensorimotor and cognitive task components in a mindful way. Lastly, we focus on the unique features of the outer natural environment of physical activity and of the inner environment during mindful movements that can restore capacities and boost creativity.}
}
@article{COOPER2022100755,
title = {Balboa security v. M&M systems: Forensic accounting for determining commercial damages},
journal = {Journal of Accounting Education},
volume = {58},
pages = {100755},
year = {2022},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2021.100755},
url = {https://www.sciencedirect.com/science/article/pii/S0748575121000427},
author = {John R. Cooper and Brett S. Kawada},
keywords = {Forensic accounting, Commercial damages, Litigation, Supplier-customer relationship},
abstract = {The ability of accounting students to apply skills beyond traditional accounting in a thoughtful and analytical way is becoming increasingly important, especially in fraud detection and forensic accounting. This case provides an opportunity for students to use critical thinking and problem-solving skills in applying accounting knowledge to a supplier-customer commercial damages litigation matter. Students are provided with a fact pattern of a supplier-customer relationship where they analyze issues related to commercial damages stemming from sources common in real world forensic accounting cases. Students evaluate the facts, which include not only financial data but also interviews with key personnel of parties to the legal action, and demonstrate an understanding of the issues involved in the case through responses of questions regarding overriding forensic accounting and professional practice issues. Students will also prepare a written commercial damages report demonstrating the ability to effectively communicate their analyses.}
}
@article{ZHURAVLEV2023104934,
title = {Three levels of information processing in the brain},
journal = {Biosystems},
volume = {229},
pages = {104934},
year = {2023},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2023.104934},
url = {https://www.sciencedirect.com/science/article/pii/S0303264723001090},
author = {Aleksandr V. Zhuravlev},
keywords = {Brain, Information, Consciousness, The hard problem of consciousness, Qualia, Entropy},
abstract = {Information, the measure of order in a complex system, is the opposite of entropy, the measure of chaos and disorder. We can distinguish several levels at which information is processed in the brain. The first one is the level of serial molecular genetic processes, similar in some aspects to digital computations (DC). At the same time, higher cognitive activity is probably based on parallel neural network computations (NNC). The advantage of neural networks is their intrinsic ability to learn, adapting their parameters to specific tasks and to external data. However, there seems to be a third level of information processing as well, which involves subjective consciousness and its units, so called qualia. They are difficult to study experimentally, and the very fact of their existence is hard to explain within the framework of modern physical theory. Here I propose a way to consider consciousness as the extension of basic physical laws – namely, total entropy dissipation leading to a system simplification. At the level of subjective consciousness, the brain seems to convert information embodied by neural activity to a more simple and compact form, internally observed as qualia. Whereas physical implementations of both DC and NNC are essentially approximate and probabilistic, qualia-associated computations (QAC) make the brain capable of recognizing general laws and relationships. While elaborating a behavioral program, the conscious brain does not act blindly or gropingly but according to the very meaning of such general laws, which gives it an advantage compared to any artificial intelligence system.}
}
@article{LANDINO2025,
title = {Neighbor cells restrain furrowing during Xenopus epithelial cytokinesis},
journal = {Developmental Cell},
year = {2025},
issn = {1534-5807},
doi = {https://doi.org/10.1016/j.devcel.2025.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S1534580725001571},
author = {Jennifer Landino and Eileen Misterovich and Lotte {van den Goor} and Babli Adhikary and Shahana Chumki and Lance A. Davidson and Ann L. Miller},
keywords = {cytokinesis, epithelium, Rho GTPase, actin, myosin II, ɑ-actinin, vinculin, , optogenetics, computational modeling},
abstract = {Summary
Cytokinesis challenges epithelial tissue homeostasis by generating forces that pull on neighboring cells. Junction reinforcement at the furrow in Xenopus epithelia regulates the speed of furrowing, suggesting that cytokinesis is subject to resistive forces from epithelial neighbors. We show that contractility factors accumulate near the furrow in neighboring cells, and increasing neighbor cell stiffness slows furrowing. Optogenetically increasing contractility in one or both neighbor cells slows furrowing or induces cytokinetic failure. Uncoupling mechanotransduction between dividing cells and their neighbors increases the furrow ingression rate, alters topological cell packing following cytokinesis, and impairs barrier function at the furrow. Computational modeling validates our findings and provides additional insights about epithelial mechanics during cytokinesis. We conclude that forces from the cytokinetic array must be carefully balanced with restraining forces generated by neighbor cells to regulate the speed and success of cytokinesis and maintain epithelial homeostasis.}
}
@article{XIANG2025102682,
title = {JCANet: Multi-domain federated lightweight self-attention CSI feedback network},
journal = {Physical Communication},
volume = {71},
pages = {102682},
year = {2025},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2025.102682},
url = {https://www.sciencedirect.com/science/article/pii/S1874490725000850},
author = {Jianhong Xiang and Zilu Li and Wei Liu},
keywords = {CSI feedback, Deep learning, FDD, Massive MIMO, Self-attention mechanism},
abstract = {In frequency division duplex (FDD) massive MIMO systems, as the number of antennas increases, the amount of downlink channel state information (CSI) data fed back from the user’s end increases significantly, many deep learning (DL)-based CSI compression feedback methods show their potential. Existing networks mostly extract channel features in the angle-delay domain through complex convolutional structures, neglecting the frequency correlation among subcarriers, which makes it difficult to fully capture global features with long-distance dependencies. Moreover, these approaches suffer from high complexity. To address these issues, we propose a multi-domain joint lightweight self-attention feedback network (JCANet). First, a multi-domain joint strategy is proposed at the encoder side. On the basis of designing angular-delay domain convolution to extract local features of channel information, a frequency domain convolution (FCv) branch is used to span multiple subcarriers to capture the global features of the channel, achieving multi-domain extraction of channel information features. Then, a lightweight multi-scale cross-layer self-attention (LMSCA) module is proposed on the decoder side, which utilizes the multi-scale information of the CSI matrix to establish correlations and long-range dependencies between input sequences under low complexity. Simulation results show that JCANet achieves higher performance with lower computational complexity compared to other lightweight networks.}
}
@article{ZHENG20034147,
title = {A novel approach of three-dimensional hybrid grid methodology: Part 1. Grid generation},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {192},
number = {37},
pages = {4147-4171},
year = {2003},
issn = {0045-7825},
doi = {https://doi.org/10.1016/S0045-7825(03)00385-2},
url = {https://www.sciencedirect.com/science/article/pii/S0045782503003852},
author = {Yao Zheng and Meng-Sing Liou},
keywords = {Computational fluid dynamics, Grid generation, Hybrid grid},
abstract = {We propose a novel approach of three-dimensional hybrid grid methodology, the DRAGON grid method in the three-dimensional space. The DRAGON grid is created by means of a Direct Replacement of Arbitrary Grid Overlapping by Nonstructured grid, and is structured-grid dominated with unstructured grids in small regions. The DRAGON grid scheme is an adaptation to the Chimera thinking. It is capable of preserving the advantageous features of both the structured and unstructured grids, and eliminates/minimizes their shortcomings. In the present paper, we describe essential and programming aspects, and challenges of the three-dimensional DRAGON grid method, with respect to grid generation. We demonstrate the capability of generating computational grids for multi-components complex configurations.}
}
@article{BARROUILLET2011151,
title = {Dual-process theories of reasoning: The test of development},
journal = {Developmental Review},
volume = {31},
number = {2},
pages = {151-179},
year = {2011},
note = {Special Issue: Dual-Process Theories of Cognitive Development},
issn = {0273-2297},
doi = {https://doi.org/10.1016/j.dr.2011.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0273229711000177},
author = {Pierre Barrouillet},
keywords = {Dual-process theories, Cognitive development, Conditional reasoning},
abstract = {Dual-process theories have become increasingly influential in the psychology of reasoning. Though the distinction they introduced between intuitive and reflective thinking should have strong developmental implications, the developmental approach has rarely been used to refine or test these theories. In this article, I review several contemporary dual-process accounts of conditional reasoning that theorize the distinction between the two systems of reasoning as a contrast between heuristic and analytic processes, probabilistic and mental model reasoning, or emphasize the role of metacognitive processes in reflective reasoning. These theories are evaluated in the light of the main developmental findings. It is argued that a proper account of developmental phenomena requires the integration of the main strengths of these three approaches. I propose such an integrative theory of conditional understanding and argue that the modern dual-process framework could benefit from earlier contributions that made the same distinction between intuition and reflective thinking, such as Piaget’s theory.}
}
@article{INDLEKOFER20021035,
title = {Number theory—probabilistic, heuristic, and computational approaches},
journal = {Computers & Mathematics with Applications},
volume = {43},
number = {8},
pages = {1035-1061},
year = {2002},
issn = {0898-1221},
doi = {https://doi.org/10.1016/S0898-1221(02)80012-8},
url = {https://www.sciencedirect.com/science/article/pii/S0898122102800128},
author = {K.-H Indlekofer},
keywords = {Probabilistic number theory, Asymptotic results on arithmetic function, Computational number theory, Stone-Cech compactification, Measure and integration on },
abstract = {After the description of the models of Kubilius, Novoselov and Schwarz, and Spilker, respectively, a probability theory for finitely additive probability measures is developed by use of the Stone-Cech compactification of N. The new model is applied to the result of Erdős and Wintner about the limit distribution of additive functions and to the famous result of Szemerédi in combinatorial number theory. Further, it is explained how conjectures on prime values of irreducible polynomials are used in the search for large prime twins and Sophie Germain primes.}
}
@article{DUKHANOV2016449,
title = {Big Data and Artificial Intelligence for Digital Humanities: An International Master Program via Trans-Eurasian Universities Network},
journal = {Procedia Computer Science},
volume = {101},
pages = {449-451},
year = {2016},
note = {5th International Young Scientist Conference on Computational Science, YSC 2016, 26-28 October 2016, Krakow, Poland},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.11.052},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916327211},
author = {Alexey Dukhanov and Alexander Boukhanovsky and Tatyana Sidorova and Natalya Spitsyna},
keywords = {trans-Eurasian universities’ network, international Master's program, digital humanities, skills of contemporary professional},
abstract = {This paper presents an intention of two Russian universities located at opposite sides of Russia to build with partners – leading world educational centers (in the Top-100 Universities of Times Higher Education) – a trans-Eurasian international network with Master's program “Big Data and Artificial Intelligence for Digital Humanities.” This program significantly extends the area of fostering students’ talent. In addition, it allows students to develop valuable global skills of a contemporary professional: domain expertise, soft skills including creative and system thinking, self-development, working in an international and intercultural team on a research project, etc. After graduation, the alumni will have a wide choice of opportunities to continue their academic career or to get a well-paid job in developing and developed countries around the World.}
}
@article{METTLER2024101932,
title = {Same same but different: How policies frame societal-level digital transformation},
journal = {Government Information Quarterly},
volume = {41},
number = {2},
pages = {101932},
year = {2024},
issn = {0740-624X},
doi = {https://doi.org/10.1016/j.giq.2024.101932},
url = {https://www.sciencedirect.com/science/article/pii/S0740624X24000248},
author = {Tobias Mettler and Gianluca Miscione and Claus D. Jacobs and Ali A. Guenduez},
keywords = {Digital transformation, IS policy research, Computational content analysis, Narratives},
abstract = {The digital transformation (DT) is not only forcing companies to rethink their business models but is also challenging governments to address the question of how information technology will change society today and in the future. By setting the legal boundaries and acting as an investor and promoter of the domestic digital economy, governments actively influence in which ways this transformational process takes place. The vision and objectives how DT should be realized on state level is portrayed in well-crafted DT policies. Yet, little is known how governments, as strategic actors, see their role in the DT and how they frame these documents. In this paper, we argue that policymaking about DT is isomorphic in the global context, rather than a differentiator for countries to gain a competitive edge. Using machine learning to analyze a vast text corpus of policy documents, we identify the common repertoire of narratives used by governments from all around the globe to picture their vision of the DT and show that DT policies appear to be almost context-free due to their high similarity.}
}
@article{MANCHO200655,
title = {A tutorial on dynamical systems concepts applied to Lagrangian transport in oceanic flows defined as finite time data sets: Theoretical and computational issues},
journal = {Physics Reports},
volume = {437},
number = {3},
pages = {55-124},
year = {2006},
issn = {0370-1573},
doi = {https://doi.org/10.1016/j.physrep.2006.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0370157306003401},
author = {Ana M. Mancho and Des Small and Stephen Wiggins},
keywords = {Lagrangian transport, Geophysical fluid flows, Finite time hyperbolicity, Finite time Lyapunov exponents, Stable and unstable manifolds, Transport barriers},
abstract = {In the past 15 years the framework and ideas from dynamical systems theory have been applied to a variety of transport and mixing problems in oceanic flows. The motivation for this approach comes directly from advances in observational capabilities in oceanography (e.g., drifter deployments, remote sensing capabilities, satellite imagery, etc.) which reveal space–time structures that are highly suggestive of the structures one visualizes in the global, geometrical study of dynamical systems theory. In this tutorial, we motivate this approach by showing the relationship between fluid transport in two-dimensional time-periodic incompressible flows and the geometrical structures that exist for two-dimensional area-preserving maps, such as hyperbolic periodic orbits, their stable and unstable manifolds and KAM (Kolmogorov–Arnold–Moser) tori. This serves to set the stage for the attempt to “transfer” this approach to more realistic flows modelling the ocean. However, in order to accomplish this several difficulties must be overcome. The first difficulty that confronts us that any attempt to carry out a dynamical systems approach to transport requires us to obtain the appropriate “dynamical system”, which is the velocity field describing the fluid flow. In general, adequate model velocity fields are obtained by numerical solution of appropriate partial differential equations describing the dynamical evolution of the velocity field. Numerical solution of the partial differential equations can only be done for a finite time interval, and since the ocean is generally not time-periodic, this leads to a new type of dynamical system: a finite-time, aperiodically time-dependent velocity field defined as a data set on a space–time grid. The global, geometrical analysis of transport in such dynamical systems requires both new concepts and new analytical and computational tools, as well as the necessity to discard some of the standard ideas and results from dynamical systems theory. The purpose of this tutorial is to describe these new concepts and analytical tools first using simple dynamical systems where quantities can be computed exactly. We then discuss their computational implications and implementation in the context of a model geophysical flow: a turbulent wind-driven double-gyre in the quasigeostrophic approximation.}
}
@article{JIANG2025100010,
title = {AI4Materials: Transforming the landscape of materials science and enigneering},
journal = {Review of Materials Research},
volume = {1},
number = {1},
pages = {100010},
year = {2025},
issn = {3050-9130},
doi = {https://doi.org/10.1016/j.revmat.2025.100010},
url = {https://www.sciencedirect.com/science/article/pii/S3050913025000105},
author = {Xue Jiang and Dezhen Xue and Yang Bai and William Yi Wang and Jianjun Liu and Mingli Yang and Yanjing Su},
keywords = {Intelligent computation, Machine learning, Materials data infrastructure, Autonomous experiment, Intelligent manufacture},
abstract = {New materials, crucial for economic and technological progress, are prioritized globally with strategies to accelerate their advancement through big data and AI. AI for Materials (AI4Mater) serves as an overall framework for integrating AI into Materials Science and Engineering, which is structured around three main elements: materials data infrastructure, AI4Mater techniques, and applications. This article reviews the development procedure and recent innovations in materials data infrastructure, machine learning in materials, autonomous experiment, intelligent computation, and intelligent manufacture. These efforts aim to foster open access to AI resources and enhance the collective advancement of materials science, ultimately accelerating breakthroughs and elevating the engineering application of new materials in a sustainable manner.}
}
@article{DURSO2015336,
title = {The Threat-Strategy Interview},
journal = {Applied Ergonomics},
volume = {47},
pages = {336-344},
year = {2015},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2014.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0003687014001409},
author = {Francis T. Durso and Sadaf Kazi and Ashley N. Ferguson},
keywords = {Strategies, Knowledge elicitation, Threat and error management},
abstract = {Operators in dynamic work environments use strategies to manage threats in order to achieve task goals. We introduce a structured interview method, the Threat-Strategy Interview (TSI), and an accompanying qualitative analysis to induce operator-level threats, strategies, and the cues that give rise to them. The TSI can be used to elicit knowledge from operators who are on the front line of managing threats to provide an understanding of strategic thinking, which in turn can be applied toward a variety of problems.}
}
@article{GUPTA19971,
title = {Future Challenges for Fuzzy-Neural Computing Systems},
journal = {IFAC Proceedings Volumes},
volume = {30},
number = {25},
pages = {1-6},
year = {1997},
note = {IFAC Symposium on Artificial Intelligence in Real Time Control (AIRTC'97), Kuala Lumpur, Malaysia, 22-25 September 1997},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)41292-4},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017412924},
author = {Madan M. Gupta},
keywords = {Neural Systems, Fuzzy Systems, Fuzzy Logic, Neural Fuzzy Computing},
abstract = {Recently, several significant advances have been made in two distinct theoretical areas. These theoretical advances have created an innovative field of theoretical and applied interest: fuzzy neural systems. Researchers have provided a theoretical basis in the field while industry has used this theoretical basis to create a new class of machines using the innovative technology of fuzzy neural networks. The theory of fuzzy logic provides a mathematical framework for capturing the uncertainties associated with human cognitive processes, such as thinking and reasoning. It also provides a mathematical morphology for emulating certain perceptual and linguistic attributes associated with human cognition. On the other hand, computational neural network paradigms have evolved in the process of understanding the incredible learning and adaptive features of neuronal mechanisms inherent in certain biological species. The integration of these two fields, fuzzy logic and neural networks, has the potential for combining the benefits of these two fascinating fields into a single capsule. The intent of this paper is to describe the basic notions of biological and computational neuronal morphologies, and to describe the principles and architectures of fuzzy neural networks.}
}
@article{XIA2023100730,
title = {Understanding common human driving semantics for autonomous vehicles},
journal = {Patterns},
volume = {4},
number = {7},
pages = {100730},
year = {2023},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2023.100730},
url = {https://www.sciencedirect.com/science/article/pii/S2666389923000703},
author = {Yingji Xia and Maosi Geng and Yong Chen and Sudan Sun and Chenlei Liao and Zheng Zhu and Zhihui Li and Washington Yotto Ochieng and Panagiotis Angeloudis and Mireille Elhajj and Lei Zhang and Zhenyu Zeng and Bing Zhang and Ziyou Gao and Xiqun (Michael) Chen},
keywords = {human-machine interaction, neuroscience, hierarchical understanding abstraction, electroencephalography, neural-informed model, driving behavior perception, driving semantics, autonomous vehicle},
abstract = {Summary
Autonomous vehicles will share roads with human-driven vehicles until the transition to fully autonomous transport systems is complete. The critical challenge of improving mutual understanding between both vehicle types cannot be addressed only by feeding extensive driving data into data-driven models but by enabling autonomous vehicles to understand and apply common driving behaviors analogous to human drivers. Therefore, we designed and conducted two electroencephalography experiments for comparing the cerebral activities of human linguistics and driving understanding. The results showed that driving activates hierarchical neural functions in the auditory cortex, which is analogous to abstraction in linguistic understanding. Subsequently, we proposed a neural-informed, semantics-driven framework to understand common human driving behavior in a brain-inspired manner. This study highlights the pathway of fusing neuroscience into complex human behavior understanding tasks and provides a computational neural model to understand human driving behaviors, which will enable autonomous vehicles to perceive and think like human drivers.}
}
@article{REN20231643,
title = {An Edge Computing Algorithm Based on Multi-Level Star Sensor Cloud},
journal = {CMES - Computer Modeling in Engineering and Sciences},
volume = {136},
number = {2},
pages = {1643-1659},
year = {2023},
issn = {1526-1492},
doi = {https://doi.org/10.32604/cmes.2023.025248},
url = {https://www.sciencedirect.com/science/article/pii/S1526149223002813},
author = {Siyu Ren and Shi Qiu and Keyang Cheng},
keywords = {Star-sensing, sensor cloud, fuzzy set, edge computing, mapping},
abstract = {Star sensors are an important means of autonomous navigation and access to space information for satellites. They have been widely deployed in the aerospace field. To satisfy the requirements for high resolution, timeliness, and confidentiality of star images, we propose an edge computing algorithm based on the star sensor cloud. Multiple sensors cooperate with each other to form a sensor cloud, which in turn extends the performance of a single sensor. The research on the data obtained by the star sensor has very important research and application values. First, a star point extraction model is proposed based on the fuzzy set model by analyzing the star image composition, which can reduce the amount of data computation. Then, a mapping model between content and space is constructed to achieve low-rank image representation and efficient computation. Finally, the data collected by the wireless sensor is delivered to the edge server, and a different method is used to achieve privacy protection. Only a small amount of core data is stored in edge servers and local servers, and other data is transmitted to the cloud. Experiments show that the proposed algorithm can effectively reduce the cost of communication and storage, and has strong privacy.}
}
@article{SUEHR2025109636,
title = {Multi-sphere Rigid-Body Particles in a Parallelized LEBC with LIGGGHTS},
journal = {Computer Physics Communications},
pages = {109636},
year = {2025},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2025.109636},
url = {https://www.sciencedirect.com/science/article/pii/S0010465525001389},
author = {Elizabeth Suehr and Manuel Gale and Ramon Lopez and Raymond L. Fontenot and Peter Liever and Jennifer S Curtis},
keywords = {DEM (Discrete Element Method), LEBC (Lees-Edwards Boundary Condition), LIGGGHTS (LAMMPS improved for general granular and granular heat transfer simulations), Parallelization, Multi-sphere},
abstract = {A method for the Message Passing Interface (MPI) parallelization of the Lees-Edwards boundary condition (LEBC) within the LIGGGHTS framework for multi-sphere rigid particles was created, allowing for the simulation of very detailed complex shapes. Double-send and double-receive communication was added to LIGGGHTS to allow for shared information across disjointed processor domains along the shearing boundary of the LEBC. The verification of this method is performed via 3D shearing simulations of single spheres and sphere clumps and rods with aspect ratios 2, 4, and 6. The predicted shear stress employing the new parallelized LEBC method matches stress values from granular kinetic theory and previously published simulation results. No LEBC simulations for DEM or multi-sphere rigid particles are known to be parallelized, allowing for computationally difficult LEBC multi-sphere simulations to be performed for the first time.}
}
@article{THOMPSON2011107,
title = {Intuition, reason, and metacognition},
journal = {Cognitive Psychology},
volume = {63},
number = {3},
pages = {107-140},
year = {2011},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2011.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0010028511000454},
author = {Valerie A. Thompson and Jamie A. {Prowse Turner} and Gordon Pennycook},
keywords = {Metacognition, Reasoning, Dual Process Theories, Intuition, Analytic thinking, Retrospective confidence},
abstract = {Dual Process Theories (DPT) of reasoning posit that judgments are mediated by both fast, automatic processes and more deliberate, analytic ones. A critical, but unanswered question concerns the issue of monitoring and control: When do reasoners rely on the first, intuitive output and when do they engage more effortful thinking? We hypothesised that initial, intuitive answers are accompanied by a metacognitive experience, called the Feeling of Rightness (FOR), which can signal when additional analysis is needed. In separate experiments, reasoners completed one of four tasks: conditional reasoning (N=60), a three-term variant of conditional reasoning (N=48), problems used to measure base rate neglect (N=128), or a syllogistic reasoning task (N=64). For each task, participants were instructed to provide an initial, intuitive response to the problem along with an assessment of the rightness of that answer (FOR). They were then allowed as much time as needed to reconsider their initial answer and provide a final answer. In each experiment, we observed a robust relationship between the FOR and two measures of analytic thinking: low FOR was associated with longer rethinking times and an increased probability of answer change. In turn, FOR judgments were consistently predicted by the fluency with which the initial answer was produced, providing a link to the wider literature on metamemory. These data support a model in which a metacognitive judgment about a first, initial model determines the extent of analytic engagement.}
}
@article{JING2020644,
title = {A Learner Model Integrating Cognitive and Metacognitive And Its Application on Scratch Programming Projects},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {5},
pages = {644-649},
year = {2020},
note = {3rd IFAC Workshop on Cyber-Physical & Human Systems CPHS 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.04.154},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321002913},
author = {Sifeng Jing and Ying Tang and Xiwei Liu and Xiaoyan Gong},
keywords = {learner model, cognitive state, metacognitive ability, individualized teaching},
abstract = {learner’s cognitive and metacognitive are key personal profile for individualized teaching. To evaluate learner’s comprehensive characteristics, existing learner model were reviewed. Two challenges of constructing an accurate and comprehensive learner model integrating cognitive and metacognitive were summarized. A plan of constructing a comprehensive learner model was made based on analysis of existing massive online learning environment, sensor information technology and educational data-mining. As a case study, a method of how to map learning data onto learners’ cognitive and metacognitive was proposed based on an analysis of a number of pupils’ Scratch projects. Three mapping table were established. Pupil’s cognitive skill could be evaluated from technology shown from Scratch project, namely, data structure, algorithm, computational practices and overall evaluation. Content shown from Scratch project were used to infer pupil’s cognitive style. Meta-cognitive ability can be measured from computational practices and behavior in programming process.}
}
@article{DAVID2019646,
title = {Development of Escape Room Game using VR Technology},
journal = {Procedia Computer Science},
volume = {157},
pages = {646-652},
year = {2019},
note = {The 4th International Conference on Computer Science and Computational Intelligence (ICCSCI 2019) : Enabling Collaboration to Escalate Impact of Research Results for Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.08.223},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919311421},
author = {David David and  Edwin and Edward Arman and  Hikari and Natalia Chandra and Nadia Nadia},
keywords = {Virtual Reality, Presence, Prototype, Unity, Samsung Gear VR},
abstract = {Escape room is one of the media games that can improve the logic of thinking. Puzzles in the escape room traditionally have disadvantages because the type of puzzle that is made requires a lot of material. The purpose of this research is to produce a game with Escape Room as the basic theme with Virtual Reality technology. Virtual Reality technology is used to develop presence in users, attendance is about the intimacy of users with the gaming world. By using Virtual Reality, the puzzle elements that are created can be replaced regularly without the need to change the building’s skeleton. The development method used is a prototype model using Unity game machines. The research method was carried out using a questionnaire for user analysis. The application generated from this research is the Escape Room VR game that can be played on an Android smartphone that is compatible with Samsung Gear VR. The application can be used as an additional means for traditional Escape Room games.}
}
@article{STANCIU2015312,
title = {Embodied Creativity: A Critical Analysis of an Underdeveloped Subject},
journal = {Procedia - Social and Behavioral Sciences},
volume = {187},
pages = {312-317},
year = {2015},
note = {INTERNATIONAL CONFERENCE PSIWORLD 2014 - 5th edition},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2015.03.058},
url = {https://www.sciencedirect.com/science/article/pii/S1877042815018510},
author = {Marius M. Stanciu},
keywords = {Embodied, Creativity, Cognition, Research, Review},
abstract = {While the idea that cognition is embodied appeared in the literature more than four decades ago, studies concerned with how and to what degree might the body and the environment influence creative thinking represent a relatively recent scientific endeavor. In this paper we wish to provide a critical examination of the core ideas of this new field, suggesting new experimental paradigms for testing the more radical and often ignored assertions of the embodied cognition program. We conclude that given the extremely small number of papers that are produced on this subject, as well as its obscurity within the scientific community, future research will have to expand its theoretical considerations greatly if the field is to survive and flourish.}
}
@article{MARINI201828,
title = {Life cycle perspective in RC building integrated renovation},
journal = {Procedia Structural Integrity},
volume = {11},
pages = {28-35},
year = {2018},
note = {XIV INTERNATIONAL CONFERENCE ON BUILDING PATHOLOGY AND CONSTRUCTIONS REPAIR, FLORENCE, ITALY, JUNE 20-22, 2018},
issn = {2452-3216},
doi = {https://doi.org/10.1016/j.prostr.2018.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S2452321618301069},
author = {A. Marini and C. Passoni and A. Belleri},
keywords = {Life Cycle thinking, Deep renovation, Integrated retrofit, Resilience, Sustainability},
abstract = {Enormous resources are invested in Europe for the transition into a sustainable, low carbon, and resilient society. In the construction sector, these concepts are slowly being applied to the renovation of the existing building stock by enforcing their deep and holistic renovation targeting sustainability, safety and resilience. Effectiveness of such an approach to the renovation with respect to traditional retrofit actions emerges when broadening the time frame of the analyses, shifting from the construction time to a life cycle perspective. In this case, the potential of the holistic approach becomes clear in reducing costs, impacts on the inhabitants and impacts on the environment over the building life cycle. Within such a new perspective, new technology options are needed to innovatively combine structural retrofit, architectural restyling and energy efficiency measures. Furthermore, a new design approach conjugating the principles of sustainability, safety and resilience over the building life cycle is required. In such a transition, synergistic and cooperative work of researchers, design professionals, and all the stakeholders in the construction sector is required. In this paper, the basic features of an expanded Life Cycle Thinking (eLCT) approach will be presented, which not only entails the use of recyclable/reusable materials, but also encourages interventions carried out from the outside the buildings to reduce building downtime and avoid inhabitant relocation. In addition, such an expanded LCT fosters the adoption of reparable, easy maintainable, adaptable and fully demountable solutions, such as those featuring dry, demountable and pre-fabricated components. Finally, it addresses the need to account for the End of Life scenario from the initial design stages to guarantee selective dismantling and reuse or recycle to reduce construction waste. Finally, a discussion on the main barriers and challenges in the transition towards this new approach to the renovation of existing building stock is briefly presented.}
}
@article{ANURADHA2022100429,
title = {A RNN based offloading scheme to reduce latency and preserve energy using RNNBOS},
journal = {Measurement: Sensors},
volume = {24},
pages = {100429},
year = {2022},
issn = {2665-9174},
doi = {https://doi.org/10.1016/j.measen.2022.100429},
url = {https://www.sciencedirect.com/science/article/pii/S2665917422000630},
author = {C. Anuradha and M. Ponnavaikko},
keywords = {Computational offloading, Mobile edge computing, Deep neural network, Energy consumption and mobile cloud computing},
abstract = {Mobile cloud computing is currently evolving quickly in today's trend and it provides infinite number of applications to the people those who are using regularly.MCC means the mobile gadgets are strongly tied up with cloud technology to execute various application for attaining many tasks. Mobile devices contain different application according to its own capacity to hold each application. In which many applications are in need of connecting with cloud storage. A new proposed technique named RNNBOS (Recurrent Neural Network Based Offloading scheme) is used to compute calculations in terms of energy source of mobile device along with active conditions of network, Load computations, delay possibility of request from device and quantitative amount of data being transferred for this purpose. We have simulated the above technique using python tool and observed RNN based offloading scheme is good in execution of application using MCC.}
}
@article{LIU202257,
title = {Hierarchical neighborhood entropy based multi-granularity attribute reduction with application to gene prioritization},
journal = {International Journal of Approximate Reasoning},
volume = {148},
pages = {57-67},
year = {2022},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2022.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X22000809},
author = {Keyu Liu and Tianrui Li and Xibei Yang and Hengrong Ju and Xin Yang and Dun Liu},
keywords = {Gene selection, Granular computing, Multi-granularity attribute reduction, Neighborhood rough set, Trilevel thinking},
abstract = {As a prominent model of granular computing, neighborhood rough set provides clear granularity organization and expression in terms of inherent parameter (neighborhood radius). Such characteristic is widely captured in a plenitude of attribute reduction procedures, while igniting a tricky issue of tuning parameters. In this study, we therefore propose a parameter-free multi-granularity attribute reduction scheme. Fundamentally, our scheme applies three-way decision as thinking in threes. First, data-aware multi-granularity structure is automatically induced from self-contained distance space instead of manually edited or appointed granularities. Second, a novel multi-granularity feature evaluation criterion named hierarchical neighborhood entropy is defined to measure the feature significance. Finally, a sequential forward searching algorithm is designed to find the optimal reduct. With application to gene prioritization, our method performed on microarray data is experimentally demonstrated to be more effective and efficient in differentially expressed genes discovery as compared with other well-established attribute reduction algorithms.}
}
@article{VASSILIADIS2024380,
title = {Reloading Process Systems Engineering within Chemical Engineering},
journal = {Chemical Engineering Research and Design},
volume = {209},
pages = {380-398},
year = {2024},
issn = {0263-8762},
doi = {https://doi.org/10.1016/j.cherd.2024.07.066},
url = {https://www.sciencedirect.com/science/article/pii/S0263876224004568},
author = {Vassilios S. Vassiliadis and Vasileios Mappas and Thomas A. Espaas and Bogdan Dorneanu and Adeniyi Isafiade and Klaus Möller and Harvey Arellano-Garcia},
keywords = {Chemical Engineering, Process Systems Engineering, Process model construction and deployment, Digital Twinning, Machine Learning},
abstract = {Established as a sub-discipline of Chemical Engineering in the 1960s by the late Professor R.W.H. Sargent at Imperial College London, Process Systems Engineering (PSE) has played a significant role in advancing the field, positioning it as a leading engineering discipline in the contemporary technological landscape. Rooted in Applied Mathematics and Computing, PSE aligns with the key components driving advancements in our modern, information-centric era. Sargent’s visionary foresight anticipated the evolution of early computational tools into fundamental elements for future technological and scientific breakthroughs, all while maintaining a central focus on Chemical Engineering. This paper aims to present concise and concrete ideas for propelling PSE into a new era of progress. The objective is twofold: to preserve PSE’s extensive and diverse knowledge base and to reposition it more prominently within modern Chemical Engineering, while also establishing robust connections with other data-driven engineering and applied science domains that play important roles in industrial and technological advancements. Rather than merely reacting to contemporary challenges, this article seeks to proactively create opportunities to lead the future of Chemical Engineering across its vital contributions in education, research, technology transfer, and business creation, fully leveraging its inherent multidisciplinarity and versatile character.}
}
@article{NDUNGO2020,
title = {mSphere of Influence: Learning from Nature—Antibody Profiles Important for Protection of Young Infants},
journal = {mSphere},
volume = {5},
number = {5},
year = {2020},
issn = {2379-5042},
doi = {https://doi.org/10.1128/msphere.01021-20},
url = {https://www.sciencedirect.com/science/article/pii/S2379504220001356},
author = {Esther Ndungo},
keywords = {antibody profiles, enteric pathogens, maternal-infant immunity, systems serology},
abstract = {Esther Ndungo works in the field of maternal-infant immunity against enteric pathogens. In this mSphere of Influence article, she reflects on how the paper “Fc glycan-mediated regulation of placental antibody transfer” by Jennewein et al. (M. F. Jennewein, I. Goldfarb, S. Dolatshahi, C. Cosgrove, et al., Cell 178:202–215.e14, 2019, https://doi.org/10.1016/j.cell.2019.05.044) impressed upon her the value of thinking “outside the box” and looking to nature to guide her research.
ABSTRACT
Esther Ndungo works in the field of maternal-infant immunity against enteric pathogens. In this mSphere of Influence article, she reflects on how the paper “Fc glycan-mediated regulation of placental antibody transfer” by Jennewein et al. (M. F. Jennewein, I. Goldfarb, S. Dolatshahi, C. Cosgrove, et al., Cell 178:202–215.e14, 2019, https://doi.org/10.1016/j.cell.2019.05.044) impressed upon her the value of thinking “outside the box” and looking to nature to guide her research.}
}
@article{KRYSSANOV2001329,
title = {Understanding design fundamentals: how synthesis and analysis drive creativity, resulting in emergence},
journal = {Artificial Intelligence in Engineering},
volume = {15},
number = {4},
pages = {329-342},
year = {2001},
note = {Methodology of Emergent Sythesis},
issn = {0954-1810},
doi = {https://doi.org/10.1016/S0954-1810(01)00023-1},
url = {https://www.sciencedirect.com/science/article/pii/S0954181001000231},
author = {V.V Kryssanov and H Tamaki and S Kitamura},
keywords = {Engineering design, Creativity, Semiotics, Emergence},
abstract = {This paper presents results of an ongoing interdisciplinary study to develop a computational theory of creativity for engineering design. Human design activities are surveyed, and popular computer-aided design methodologies are examined. It is argued that semiotics has the potential to merge and unite various design approaches into one fundamental theory that is naturally interpretable and so comprehensible in terms of computer use. Reviewing related work in philosophy, psychology, and cognitive science provides a general and encompassing vision of the creativity phenomenon. Basic notions of algebraic semiotics are given and explained in terms of design. This is to define a model of the design creative process, which is seen as a process of semiosis, where concepts and their attributes represented as signs organized into systems are evolved, blended, and analyzed, resulting in the development of new concepts. The model allows us to formally describe and investigate essential properties of the design process, namely its dynamics and non-determinism inherent in creative thinking. A stable pattern of creative thought — analogical and metaphorical reasoning — is specified to demonstrate the expressive power of the modeling approach; illustrative examples are given. The developed theory is applied to clarify the nature of emergence in design: it is shown that while emergent properties of a product may influence its creative value, emergence can simply be seen as a by-product of the creative process. Concluding remarks summarize the research, point to some unresolved issues, and outline directions for future work.}
}
@article{CIPRIANI2024102277,
title = {Personality traits and climate change denial, concern, and proactivity: A systematic review and meta-analysis},
journal = {Journal of Environmental Psychology},
volume = {95},
pages = {102277},
year = {2024},
issn = {0272-4944},
doi = {https://doi.org/10.1016/j.jenvp.2024.102277},
url = {https://www.sciencedirect.com/science/article/pii/S0272494424000501},
author = {Enrico Cipriani and Sergio Frumento and Angelo Gemignani and Danilo Menicucci},
keywords = {Climate change, Personality, Communication, Big five, Climate change denial, Climate change concern},
abstract = {Climate Change is a global issue which touches the lives of all human beings, each of whom have their own unique outlooks and motivations. Hence, the high degree of complexity which emerges from the involvement of such a large number of people might be better understood through the lenses of their individual differences. We performed a systematic review and meta-analysis following PRISMA guidelines. We searched keywords on Web of Science™ and Scopus®, and included peer-reviewed articles which quantitatively examined correlations between personality and climate attitudes. After screening, 74 papers were included in our review. From these articles, k = 100 samples were extracted and included in meta-analysis models. Our results show that Climate Change Denial is positively correlated with Social Dominance Orientation (r = 0.39) and Right-Wing Authoritarianism (r = 0.42), and negatively with Openness (r = −0.14), Conscientiousness (r = −0.05), Agreeableness (r = −0.11), Consideration of Future Consequences (r = −0.38), and Actively Open-Minded Thinking (r = −0.38). Concern for Climate Change correlates with Openness (r = 0.10), Neuroticism (r = 0.12), Consideration of Future Consequences (r = 0.34), and negatively with Social Dominance Orientation (r = -0.36) and Right-Wing Authoritarianism (r = −0.22). Finally, Proactivity towards Climate Change correlates positively with Openness (r = 0.17), Extraversion (r = 0.09), Agreeableness (r = 0.05), Neuroticism (r = 0.10), Consideration of Future Consequences (r = 0.39), and negatively with Social Dominance Orientation (r = -0.25) and Right-Wing Authoritarianism (r = -0.31). Moderation analysis shows geographical variations in the Social Dominance Orientation and Climate Denial relationship. We conclude that some personality traits – such as Openness – transversally affect climate change attitudes. Moreover, meta-analytic data suggest that the personality involvement in Climate Change may be dependent on the socio-political context of different countries. Future research, policies, and communication campaigns should take these peculiarities into account.}
}
@incollection{OLIVEIRA200793,
title = {3 - Fundamentals of Quantum Computation and Quantum Information},
editor = {Ivan S. Oliveira and Tito J. Bonagamba and Roberto S. Sarthour and Jair C.C. Freitas and Eduardo R. deAzevedo},
booktitle = {NMR Quantum Information Processing},
publisher = {Elsevier Science B.V.},
address = {Amsterdam},
pages = {93-136},
year = {2007},
isbn = {978-0-444-52782-0},
doi = {https://doi.org/10.1016/B978-044452782-0/50005-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780444527820500051},
author = {Ivan S. Oliveira and Tito J. Bonagamba and Roberto S. Sarthour and Jair C.C. Freitas and Eduardo R. deAzevedo}
}
@article{LIU2018164,
title = {Neural and genetic determinants of creativity},
journal = {NeuroImage},
volume = {174},
pages = {164-176},
year = {2018},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2018.02.067},
url = {https://www.sciencedirect.com/science/article/pii/S1053811918301745},
author = {Zhaowen Liu and Jie Zhang and Xiaohua Xie and Edmund T. Rolls and Jiangzhou Sun and Kai Zhang and Zeyu Jiao and Qunlin Chen and Junying Zhang and Jiang Qiu and Jianfeng Feng},
abstract = {Creative thinking plays a vital role in almost all aspects of human life. However, little is known about the neural and genetic mechanisms underlying creative thinking. Based on a cross-validation based predictive framework, we searched from the whole-brain connectome (34,716 functional connectivities) and whole genome data (309,996 SNPs) in two datasets (all collected by Southwest University, Chongqing) consisting of altogether 236 subjects, for a better understanding of the brain and genetic underpinning of creativity. Using the Torrance Tests of Creative Thinking score, we found that high figural creativity is mainly related to high functional connectivity between the executive control, attention, and memory retrieval networks (strong top-down effects); and to low functional connectivity between the default mode network, the ventral attention network, and the subcortical and primary sensory networks (weak bottom-up processing) in the first dataset (consisting of 138 subjects). High creativity also correlates significantly with mutations of genes coding for both excitatory and inhibitory neurotransmitters. Combining the brain connectome and the genomic data we can predict individuals' creativity scores with an accuracy of 78.4%, which is significantly better than prediction using single modality data (gene or functional connectivity), indicating the importance of combining multi-modality data. Our neuroimaging prediction model built upon the first dataset was cross-validated by a completely new dataset of 98 subjects (r = 0.267, p = 0.0078) with an accuracy of 64.6%. In addition, the creativity–related functional connectivity network we identified in the first dataset was still significantly correlated with the creativity score in the new dataset (p<10−3). In summary, our research demonstrates that strong top-down control versus weak bottom-up processes underlie creativity, which is modulated by competition between the glutamate and GABA neurotransmitter systems. Our work provides the first insights into both the neural and the genetic bases of creativity.}
}
@article{FARHAT199361,
title = {Two-dimensional viscous flow computations on the Connecti on Machine: Unstructured meshes, upwind schemes and massively parallel computations},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {102},
number = {1},
pages = {61-88},
year = {1993},
issn = {0045-7825},
doi = {https://doi.org/10.1016/0045-7825(93)90141-J},
url = {https://www.sciencedirect.com/science/article/pii/004578259390141J},
author = {Charbel Farhat and Loula Fezoui and Stéphane Lanteri},
abstract = {Here we report on our effort in simulating two-dimensional viscous flows on the Connection Machine, using a second-order accurate monotomic upwind scheme for conservation laws (MUSCL) on fully unstructured grids. The spatial approximation combines an upwind finite volume method for the discretization of the convective fluxes with a classical Galerkin finite element method for the discretization of the diffusive fluxes. The resulting semi-discrete equations are time integrated with a second-order low-storage explicit Runge-Kutta method. A communication efficient strategy for mapping thousands of processors onto an arbitrary mesh is presented and proposed as an alternative to the fast north-east-west-south (NEWS) communication mechanism, which is restricted to structured grids. Measured performance results for the simulation of low Reynolds number chaotic flows indicate that an 8K CM-2 (8192 processors) with single precision floating point arithmetic is at least as fast as one CRAY-2 processor.}
}
@incollection{LEE2016135,
title = {Chapter 7 - Identifying and Tracking Emotional and Cognitive Mathematical Processes of Middle School Students in an Online Discussion Group},
editor = {Sharon Y. Tettegah and Michael P. McCreery},
booktitle = {Emotions, Technology, and Learning},
publisher = {Academic Press},
address = {San Diego},
pages = {135-153},
year = {2016},
series = {Emotions and Technology},
isbn = {978-0-12-800649-8},
doi = {https://doi.org/10.1016/B978-0-12-800649-8.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012800649800002X},
author = {Amos Lee and Sharon Tettegah},
keywords = {Online discourse, Math discussions, Math learning, Systemic functional linguistics, Identification analysis},
abstract = {Math discussions are important when learning math. Explaining one’s thinking, listening to other’s thoughts, and reflecting are but a few of the benefits derived from discussions held in class. However, with the growth of online courses, how do math discussions change when in an online setting? While much research exists about math discussions in classrooms, there is not much research on math discussions held online. Due to the important role of discussions in learning math, along with the growing trend of online classes, this study begins to take a look at how students make sense of and keep track of each other’s comments in an online discussion. In these online discussions, turn taking is not as intuitive as face-to-face interactions. Making sense of the discussion sequence and theme can also be challenging. In this study, I found that students used terms that represented mathematical operations to better explain their thought processes and also kept track of how their peers used these terms as well. These findings suggest that, for these students, when in an online discussion, the terms used were of importance when trying to make their thinking clear to their classmates. Also, in these groups, the mathematical terms were commonly used and re-used by more than one individual in trying to gain a consensus in their group thinking. These findings are important when thinking about how to best foster math discussion and learning in an online environment and for designing online classes that institutions use to supplement or support students.}
}
@article{DIETRICH200722,
title = {Who’s afraid of a cognitive neuroscience of creativity?},
journal = {Methods},
volume = {42},
number = {1},
pages = {22-27},
year = {2007},
note = {Neurocognitive Mechanisms of Creativity: A Toolkit},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2006.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S1046202306003100},
author = {Arne Dietrich},
keywords = {Consciousness, Insight, Prefrontal cortex, Right brain, Divergent thinking, Neuroimaging, Attention},
abstract = {This article has two goals. First, the ideas outlined here can be seen as a sustained and disciplined demolition project aimed at sanitizing our bad habits of thinking about creativity. Apart from the enormous amount of fluff out there, the study of creativity is, quite unfortunately, still dominated by a number of rather dated ideas that are either so simplistic that nothing good can possibly come out of them or, given what we know about the brain, factually mistaken. As cognitive neuroscience is making more serious contact with the knowledge base of creativity, we must, from the outset, clear the ground of these pernicious fossil traces from a bygone era. The best neuroimaging techniques help little if we don’t know what to look for. Second, as an antidote to these theoretical duds, the article offers fresh ideas on possible mechanisms of creativity. Given that they are grounded in current understanding of cognitive and neural processes, it is hoped that these ideas represent steps broadly pointing in the right direction. In the end, the fundamental question we must ask ourselves is what, exactly, are the mental processes—or their critical elements—that yield creative thoughts.}
}
@article{STEPHEN2021103085,
title = {Automated essay scoring (AES) of constructed responses in nursing examinations: An evaluation},
journal = {Nurse Education in Practice},
volume = {54},
pages = {103085},
year = {2021},
issn = {1471-5953},
doi = {https://doi.org/10.1016/j.nepr.2021.103085},
url = {https://www.sciencedirect.com/science/article/pii/S1471595321001219},
author = {Tracey C. Stephen and Mark C. Gierl and Sharla King},
keywords = {Automated essay scoring, Constructed-response examinations, Nursing education assessment, Reliability measures},
abstract = {Nursing students’ higher-level thinking skills are ideally assessed through constructed-response items. At the baccalaureate level in North America, however, this exam format has largely fallen into disuse owing to the labor-intensive process of scoring written exam papers. The authors sought to determine if automated essay scoring (AES) would be an efficient and reliable alternative to human scoring. Four constructed-response exam items were administered to an initial cohort of 359 undergraduate nursing students in 2016 and to a second cohort of 40 students in 2018. The items were graded by two human raters (HR1 & HR2) and an AES software platform. AES approximated or surpassed agreement and reliability measures achieved by the HR1 and HR2 with each other, and AES surpassed both human raters in efficiency. A list of answer keywords was created to increase the efficiency and reliability of AES. Low agreement between human raters may be explained by rater drift and fatigue, and shortcomings in the development of Item 1 may have reduced its overall agreement and reliability measures. It can be concluded that AES is a reliable and cost-effective means of scoring constructed-response nursing examinations, but further studies employing greater sample sizes are needed to establish this definitively.}
}
@article{WEISSMAN2011516,
title = {A computational framework for authoring and searching product design specifications},
journal = {Advanced Engineering Informatics},
volume = {25},
number = {3},
pages = {516-534},
year = {2011},
note = {Special Section: Engineering informatics in port operations and logistics},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2011.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1474034611000061},
author = {Alexander Weissman and Martin Petrov and Satyandra K. Gupta},
keywords = {Product design specifications, Engineering design, Requirements engineering},
abstract = {The development of product design specifications (PDS) is an important part of the product development process. Incompleteness, ambiguity, or inconsistency in the PDS can lead to problems during the design process and may require unnecessary design iterations. This generally results in increased design time and cost. Currently, in many organizations, PDS are written using word processors. Since documents written by different authors can be inconsistent in style and word choice, it is difficult to automatically search for specific requirements. Moreover, this approach does not allow the possibility of automated design verification and validation against the design requirements and specifications. In this paper, we present a computational framework and a software tool based on this framework for writing, annotating, and searching computer-interpretable PDS. Our approach allows authors to write requirement statements in natural language to be consistent with the existing authoring practice. However, using mathematical expressions, keywords from predefined taxonomies, and other metadata the author of PDS can then annotate different parts of the requirement statements. This approach provides unambiguous meaning to the information contained in PDS, and helps to eliminate mistakes later in the process when designers must interpret requirements. Our approach also enables users to construct a new PDS document from the results of the search for requirements of similar devices and in similar contexts. This capability speeds up the process of creating PDS and helps authors write more detailed documents by utilizing previous, well written PDS documents. Our approach also enables checking for internal inconsistencies in the requirement statements.}
}
@article{PRONK202443,
title = {Qualitative systems mapping in promoting physical activity and cardiorespiratory fitness: Perspectives and recommendations},
journal = {Progress in Cardiovascular Diseases},
volume = {83},
pages = {43-48},
year = {2024},
note = {Cardiorespiratory Fitness and Physical Activity: An Update of Evidence, Global Status and Recommendations},
issn = {0033-0620},
doi = {https://doi.org/10.1016/j.pcad.2024.02.013},
url = {https://www.sciencedirect.com/science/article/pii/S0033062024000355},
author = {Nicolaas P. Pronk and Bruce Y. Lee},
keywords = {Systems mapping, Causal loop diagram, Physical activity, Cardiorespiratory fitness, Complexity},
abstract = {The purpose of this report is to provide a perspective on the use of qualitative systems mapping, provide examples of physical activity (PA) systems maps, discuss the role of PA systems mapping in the context of iterative learning to derive breakthrough interventions, and provide actionable recommendations for future work. Systems mapping methods and applications for PA are emerging in the scientific literature in the study of complex health issues and can be used as a prelude to mathematical/computational modeling where important factors and relationships can be elucidated, data needs can be prioritized and guided, interventions can be tested and (co)designed, and metrics and evaluations can be developed. Examples are discussed that describe systems mapping based on Group Model Building or literature reviews. Systems maps are highly informative, illustrate multiple components to address PA and physical inactivity issues, and make compelling arguments against single intervention action. No studies were identified in the literature scan that considered cardiorespiratory fitness the focal point of a systems maps. Recommendations for future research and education are presented and it is concluded that systems mapping represents a valuable yet underutilized tool for visualizing the complexity of PA promotion.}
}
@article{ANDERSON1998214,
title = {Stereovision: beyond disparity computations},
journal = {Trends in Cognitive Sciences},
volume = {2},
number = {6},
pages = {214-222},
year = {1998},
issn = {1364-6613},
doi = {https://doi.org/10.1016/S1364-6613(98)01180-2},
url = {https://www.sciencedirect.com/science/article/pii/S1364661398011802},
author = {Barton L Anderson},
keywords = {sterovision, disparity, 3-D sterograms, perceptual grouping, occlusion},
abstract = {One of the most powerful sources of information about three-dimensional (3-D) structure is provided by stereovision (or stereopsis). For over a century, theoretical and empirical investigations into this ability have focused on the role of binocular disparity in generating percepts of 3-D structure. Recent work in image segmentation demonstrates that stereovision can cause large changes in perceptual organization that cannot be understood on the basis of binocular disparity alone. It is argued that these phenomena reveal the need for theoretical tools beyond those that have dominated the study of visual perception over the past three decades.}
}
@article{MOLINA2025115592,
title = {Exploring the neurophysiological basis of misinformation: A behavioral and neural complexity analysis},
journal = {Behavioural Brain Research},
volume = {487},
pages = {115592},
year = {2025},
issn = {0166-4328},
doi = {https://doi.org/10.1016/j.bbr.2025.115592},
url = {https://www.sciencedirect.com/science/article/pii/S0166432825001780},
author = {R. Molina and Y. Crespo and J.R. Árbol and A.V. Arias-Orduña and A.J. Ibáñez-Molina and S. Iglesias-Parro},
keywords = {Misinformation, Disinformation, Social media, Counter-disinformation, Electroencephalography, Sample entropy, Neural complexity},
abstract = {The proliferation of misinformation on social media platforms poses significant challenges to public health, political discourse, and social cohesion. This study investigates the efficacy of a World Health Organization (WHO) infodemic intervention in mitigating the spread of misinformation and explores the underlying neural mechanisms involved in information processing. A sample of 77 university students was randomly assigned to an experimental group, which was exposed to the WHO's infodemic intervention, or a control group, which received a campaign on healthy lifestyle habits. Participants viewed a series of manipulated and non-manipulated tweets before and after the intervention, rating their likelihood to share, verify, and perceive the truthfulness of the information. Electroencephalogram (EEG) data were collected throughout the experiment to assess neural complexity using Sample Entropy (SampEn) measures. Results revealed that the experimental group significantly reduced their intention to share information and perceived truthfulness of both manipulated and non-manipulated items post-intervention. The control group showed no significant changes. EEG analysis demonstrated higher SampEn scores in the frontal and temporal regions for the experimental group post-intervention, indicating increased neural complexity and more homogeneous activation patterns. These findings suggest that the WHO intervention effectively enhanced participants' critical evaluation of information, reflected in both behavioral and neurophysiological changes. This study contributes to the growing body of research on misinformation interventions by providing evidence for the effectiveness of passive, less demanding campaigns in fostering critical thinking and information discernment. Moreover, it offers novel insights into the neural correlates of information processing following such interventions, highlighting the potential of combining behavioral and neurophysiological measures in misinformation research. These findings have important implications for developing targeted strategies to combat misinformation, enhance digital literacy, and inform future public health and policy initiatives in the digital era.}
}
@article{FAN2025126317,
title = {Deep dive into clarity: Leveraging signal-to-noise ratio awareness and knowledge distillation for underwater image enhancement},
journal = {Expert Systems with Applications},
volume = {269},
pages = {126317},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.126317},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424031841},
author = {Guodong Fan and Jingchun Zhou and Chengpei Xu and Zheng Cheng},
keywords = {Underwater image enhancement, SNR-Aware Transformer, Knowledge distillation},
abstract = {This paper presents an innovative dual-branch solution designed for underwater image enhancement (UIE), leveraging the synergistic combination of Signal-to-Noise Ratio (SNR) aware transformers and convolutional models. SNR-Net dynamically enhances pixel quality through spatial-varying operations. While transformers excel in capturing long-range dependencies, they face challenges in weak local relation learning. To address this, we introduce a SNR prior to guide transformer learning, incorporating a novel self-attention mechanism that avoids tokens from regions with very low SNR. Conversely, CNNs, optimized for exploiting local patterns, suffer from limited receptive fields and weak diversity representation. To overcome this limitation, we enhance the receptive field and multi-scale perception of CNNs by introducing a MR-ResNet module. Additionally, we incorporate a Selective Kernel Merging Module (SKMM), an attention-based feature merging module. These enhancements empower our approach to learn an enriched set of features that selectively combine contextual information from both branches while preserving high-quality spatial details. Finally, through knowledge distillation and contrastive learning, SNR-KD significantly reduces the number of parameters and computations of SNR-Net with minimal impact on performance. Extensive experiments validate the effectiveness of our methods, namely SNR-Net and SNR-KD, demonstrating their state-of-the-art performance compared to other recent UIE methods. The code of our model is publicly available at: https://github.com/Alexande-rChan/SNR-UIE.}
}
@article{CHANG20114075,
title = {Dynamic multi-criteria evaluation of co-evolution strategies for solving stock trading problems},
journal = {Applied Mathematics and Computation},
volume = {218},
number = {8},
pages = {4075-4089},
year = {2011},
issn = {0096-3003},
doi = {https://doi.org/10.1016/j.amc.2011.09.032},
url = {https://www.sciencedirect.com/science/article/pii/S0096300311012033},
author = {Ying-Hua Chang and Tz-Ting Wu},
keywords = {Co-evolutionary model, Evolution strategies, Artificial neural network, Dynamic stock trading decision making, Optimization},
abstract = {Risk and return are interdependent in a stock portfolio. To achieve the anticipated return, comparative risk should be considered simultaneously. However, complex investment environments and dynamic change in decision making criteria complicate forecasts of risk and return for various investment objects. Additionally, investors often fail to maximize their profits because of improper capital allocation. Although stock investment involves multi-criteria decision making (MCDM), traditional MCDM theory has two shortfalls: first, it is inappropriate for decisions that evolve with a changing environment; second, weight assignments for various criteria are often oversimplified and inconsistent with actual human thinking processes. In 1965, Rechenberg proposed evolution strategies for solving optimization problems involving real number parameters and addressed several flaws in traditional algorithms, such as their use of point search only and their high probability of falling into optimal solution area. In 1992, Hillis introduced the co-evolutionary concept that the evolution of living creatures is interactive with their environments (multi-criteria) and constantly improves the survivability of their genes, which then expedites evolutionary computation. Therefore, this research aimed to solve multi-criteria decision making problems of stock trading investment by integrating evolutionary strategies into the co-evolutionary criteria evaluation model. Since co-evolution strategies are self-calibrating, criteria evaluation can be based on changes in time and environment. Such changes not only correspond with human decision making patterns (i.e., evaluation of dynamic changes in criteria), but also address the weaknesses of multi-criteria decision making (i.e., simplified assignment of weights for various criteria). Co-evolutionary evolution strategies can identify the optimal capital portfolio and can help investors maximize their returns by optimizing the preoperational allocation of limited capital. This experimental study compared general evolution strategies with artificial neural forecast model, and found that co-evolutionary evolution strategies outperform general evolution strategies and substantially outperform artificial neural forecast models. The co-evolutionary criteria evaluation model avoids the problem of oversimplified adaptive functions adopted by general algorithms and the problem of favoring weights but failing to adaptively adjust to environmental change, which is a major limitation of traditional multi-criteria decision making. Doing so allows adaptation of various criteria in response to changes in various capital allocation chromosomes. Capital allocation chromosomes in the proposed model also adapt to various criteria and evolve in ways that resemble thinking patterns.}
}
@article{CHEVRETTE20212024,
title = {The confluence of big data and evolutionary genome mining for the discovery of natural products},
journal = {Natural Product Reports},
volume = {38},
number = {11},
pages = {2024-2040},
year = {2021},
issn = {0265-0568},
doi = {https://doi.org/10.1039/d1np00013f},
url = {https://www.sciencedirect.com/science/article/pii/S0265056822008789},
author = {Marc G. Chevrette and Athina Gavrilidou and Shrikant Mantri and Nelly Selem-Mojica and Nadine Ziemert and Francisco Barona-Gómez},
abstract = {ABSTRACT
This review covers literature between 2003–2021 The development and application of genome mining tools has given rise to ever-growing genetic and chemical databases and propelled natural products research into the modern age of Big Data. Likewise, an explosion of evolutionary studies has unveiled genetic patterns of natural products biosynthesis and function that support Darwin's theory of natural selection and other theories of adaptation and diversification. In this review, we aim to highlight how Big Data and evolutionary thinking converge in the study of natural products, and how this has led to an emerging sub-discipline of evolutionary genome mining of natural products. First, we outline general principles to best utilize Big Data in natural products research, addressing key considerations needed to provide evolutionary context. We then highlight successful examples where Big Data and evolutionary analyses have been combined to provide bioinformatic resources and tools for the discovery of novel natural products and their biosynthetic enzymes. Rather than an exhaustive list of evolution-driven discoveries, we highlight examples where Big Data and evolutionary thinking have been embraced for the evolutionary genome mining of natural products. After reviewing the nascent history of this sub-discipline, we discuss the challenges and opportunities of genomic and metabolomic tools with evolutionary foundations and/or implications and provide a future outlook for this emerging and exciting field of natural product research.}
}
@article{STONE2022419,
title = {On second thoughts: changes of mind in decision-making},
journal = {Trends in Cognitive Sciences},
volume = {26},
number = {5},
pages = {419-431},
year = {2022},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2022.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S1364661322000407},
author = {Caleb Stone and Jason B. Mattingley and Dragan Rangelov},
keywords = {decision-making, change of mind, sequential sampling, metacognition},
abstract = {The ability to change initial decisions in the face of new or potentially conflicting information is fundamental to adaptive behavior. From perceptual tasks to multiple-choice tests, research has shown that changes of mind often improve task performance by correcting initial errors. Decision makers must, however, strike a balance between improvements that might arise from changes of mind and potential energetic, temporal, and psychological costs. In this review, we provide an overview of the change-of-mind literature, focusing on key behavioral findings, computational mechanisms, and neural correlates. We propose a conceptual framework that comprises two core decision dimensions – time and evidence source – which link changes of mind across decision contexts, as a first step toward an integrated psychological account of changes of mind.}
}
@article{PACINI200969,
title = {Synergy: A Framework for Leadership Development and Transformation},
journal = {Perioperative Nursing Clinics},
volume = {4},
number = {1},
pages = {69-74},
year = {2009},
note = {Leadership},
issn = {1556-7931},
doi = {https://doi.org/10.1016/j.cpen.2008.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S1556793108001022},
author = {Christine M. Pacini},
keywords = {Synergy, Leadership development, Orientation, Professional development, Staff development, Clinical education},
abstract = {Given the current demands of the health care environment, the need for nurses minimally competent in clinical judgment, caring practice, advocacy and moral agency, collaboration, responsiveness to diversity, systems thinking, inquiry, and facilitation of learning is critical in light of ever-increasing contextual complexity and variability of patient needs. The Synergy Model provides an exemplary and relevant framework for clinical practice with the ultimate aim of improving patient outcomes. Tenets of accountability and professionalism are central to the model and, in its entirety, it provides a practical and useful approach for thinking about and redesigning educational products and processes in clinical settings.}
}
@article{ZHAN2024121679,
title = {Conceptualizing future groundwater models through a ternary framework of multisource data, human expertise, and machine intelligence},
journal = {Water Research},
volume = {257},
pages = {121679},
year = {2024},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2024.121679},
url = {https://www.sciencedirect.com/science/article/pii/S0043135424005803},
author = {Chuanjun Zhan and Zhenxue Dai and Shangxian Yin and Kenneth C. Carroll and Mohamad Reza Soltanian},
keywords = {Groundwater model, Deep learning, Machine intelligence, Multisource data, Human expertise},
abstract = {Groundwater models are essential for understanding aquifer systems behavior and effective water resources spatio-temporal distributions, yet they are often hindered by challenges related to model assumptions, parametrization, uncertainty, and computational efficiency. Machine intelligence, especially deep learning, promises a paradigm shift in overcoming these challenges. A critical examination of existing machine-driven methods reveals the inherent limitations, particularly in terms of the interpretability and the ability to generalize findings. To overcome these challenges, we develop a ternary framework that synergizes the valuable insights from multisource data, human expertise, and machine intelligence. This framework capitalizes on the distinct strengths of each element: the value and relevance of multisource data, the innovative capacity of human expertise, and the analytical efficiency of machine intelligence. Our goal is to conceptualize sustainable water management practices and enhance our understanding and predictive capabilities of groundwater systems. Unlike approaches that rely solely on abundant data, our framework emphasizes the quality and strategic use of available data, combined with human intellect and advanced computing, to overcome current limitations and pave the way for more realistic groundwater simulations.}
}