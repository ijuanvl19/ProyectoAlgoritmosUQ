@article{MENG2016114,
title = {Water quality permitting: From end-of-pipe to operational strategies},
journal = {Water Research},
volume = {101},
pages = {114-126},
year = {2016},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2016.05.078},
url = {https://www.sciencedirect.com/science/article/pii/S0043135416304043},
author = {Fanlin Meng and Guangtao Fu and David Butler},
keywords = {Integrated modelling, Multi-objective optimisation, Stakeholder engagement, Urban wastewater system, Water quality permitting},
abstract = {End-of-pipe permitting is a widely practised approach to control effluent discharges from wastewater treatment plants. However, the effectiveness of the traditional regulation paradigm is being challenged by increasingly complex environmental issues, ever growing public expectations on water quality and pressures to reduce operational costs and greenhouse gas emissions. To minimise overall environmental impacts from urban wastewater treatment, an operational strategy-based permitting approach is proposed and a four-step decision framework is established: 1) define performance indicators to represent stakeholders’ interests, 2) optimise operational strategies of urban wastewater systems in accordance to the indicators, 3) screen high performance solutions, and 4) derive permits of operational strategies of the wastewater treatment plant. Results from a case study show that operational cost, variability of wastewater treatment efficiency and environmental risk can be simultaneously reduced by at least 7%, 70% and 78% respectively using an optimal integrated operational strategy compared to the baseline scenario. However, trade-offs exist between the objectives thus highlighting the need of expansion of the prevailing wastewater management paradigm beyond the narrow focus on effluent water quality of wastewater treatment plants. Rather, systems thinking should be embraced by integrated control of all forms of urban wastewater discharges and coordinated regulation of environmental risk and treatment cost effectiveness. It is also demonstrated through the case study that permitting operational strategies could yield more environmentally protective solutions without entailing more cost than the conventional end-of-pipe permitting approach. The proposed four-step permitting framework builds on the latest computational techniques (e.g. integrated modelling, multi-objective optimisation, visual analytics) to efficiently optimise and interactively identify high performance solutions. It could facilitate transparent decision making on water quality management as stakeholders are involved in the entire process and their interests are explicitly evaluated using quantitative metrics and trade-offs considered in the decision making process. We conclude that the operational strategy-based permitting shows promising for regulators and water service providers alike.}
}
@article{MOHAMED2010317,
title = {Investigating Number Sense Among Students},
journal = {Procedia - Social and Behavioral Sciences},
volume = {8},
pages = {317-324},
year = {2010},
note = {International Conference on Mathematics Education Research 2010 (ICMER 2010)},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2010.12.044},
url = {https://www.sciencedirect.com/science/article/pii/S187704281002149X},
author = {Mohini Mohamed and Jacinta Johnny},
keywords = {Number sense, Mental computation, Number sense test, Number sense framework},
abstract = {Number sense can be described as good intuition about numbers and their relationships. Individuals with good number sense tend to exhibit the following characteristics when performing mental computations; sense-making approach, planning and control, flexibility and appropriateness sense of reasonableness. This is a very important skill to be mastered by every individual to enable them to handle numerical problems in their daily life. Students rarely face problems with algorithms. Unfortunately, many studies have showed that students have poor understanding in making sense on numbers when tested on their competency in number sense component. This study aims to investigate if there is a relationship between student performance in number sense and mathematics achievement and to explore the components of number sense that students are weak in.}
}
@article{UMEREZ2001159,
title = {Howard Pattee's theoretical biology — a radical epistemological stance to approach life, evolution and complexity},
journal = {Biosystems},
volume = {60},
number = {1},
pages = {159-177},
year = {2001},
issn = {0303-2647},
doi = {https://doi.org/10.1016/S0303-2647(01)00114-9},
url = {https://www.sciencedirect.com/science/article/pii/S0303264701001149},
author = {Jon Umerez},
keywords = {Epistemological stance, Epistemic cut, Semantic closure, Code, Symbol},
abstract = {This paper offers a short review of Pattee's main contributions to science and philosophy. With no intention of being exhaustive, an account of Pattee's work is presented which discusses some of his ideas and their reception. This is done through an analysis centered in what is thought to be his main contribution: the elaboration of an internal epistemic stance to better understand life, evolution and complexity. Having introduced this core idea as a sort of a posteriori cohesive element of a complex but highly coherent and complete system of thinking, further specific elements are also reviewed.}
}
@incollection{DAVIS2024,
title = {Ideational Theories of Meaning},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00208-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041002088},
author = {Wayne A. Davis},
keywords = {Communication, Convention, Constituency, Expression, Extensions, Ideas, Intensions, Meanings, Privacy, Reference, Representation, Thoughts},
abstract = {Locke and Hobbes held that people use words as conventional signs of their thoughts and ideas. This was a predecessor of the ideational theory, which holds that for an expression to have a meaning is principally for it to express an idea (thought or thought part), and that what it means is determined by the idea it expresses. A major advantage of the theory is that it provides a solution to Frege's and Russell's problems for referentialist theories, as well as a robust account of semantic compositionality. The theory is briefly clarified, and replies to major objections are sketched.}
}
@article{WANG2024104007,
title = {An efficient certificateless blockchain-enabled authentication scheme to secure producer mobility in named data networks},
journal = {Journal of Network and Computer Applications},
volume = {232},
pages = {104007},
year = {2024},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2024.104007},
url = {https://www.sciencedirect.com/science/article/pii/S108480452400184X},
author = {Cong Wang and Tong Zhou and Maode Ma and Yuwen Xiong and Xiankun Zhang and Chao Liu},
keywords = {NDN, Producer, Mobile, Certificateless, Authentication, Blockchain},
abstract = {Named Data Networking (NDN) aims to establish an efficient content delivery architecture. In NDN, secure and effective identity authentication schemes ensure secure communication between producers and routers. Currently, there is no feasible solution to perform identity authentication of mobile producers in NDNs. Identity authentication schemes in other networks are either weak in security or performance, such as privacy leakage, difficulty to establish cross-domain trust, and long handover delays, and are not fully adaptable to the security requirements of NDNs. Additionally, the mobility of producers was not fully considered in the initial design of NDNs. This paper first revises the structure of packets and routers to support the identity authentication and mobility of producers. On this basis, this paper proposes a secure and efficient certificateless ECC-based producer identity authentication scheme (CL-BPA), which includes initial authentication and re-authentication, aimed at achieving rapid switch authentication and integrating blockchain to solve single-point failure issues. Using the Canetti and Krawczyk (CK) adversarial model and informal security analysis, the proposed CL-BPA scheme is demonstrated to be resistant to anonymity attacks, identity forgery attacks, and man-in-the-middle attacks. The performance analysis demonstrates that the proposed CL-BPA scheme exhibits excellent capabilities in terms of computation delay, communication cost, smart contract execution time, average response delay, and throughput.}
}
@incollection{HASIJA2023247,
title = {Chapter 11 - Bioinformatics workflow management systems},
editor = {Yasha Hasija},
booktitle = {All About Bioinformatics},
publisher = {Academic Press},
pages = {247-265},
year = {2023},
isbn = {978-0-443-15250-4},
doi = {https://doi.org/10.1016/B978-0-443-15250-4.00006-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044315250400006X},
author = {Yasha Hasija},
keywords = {Galaxy, GenePattern, Image analysis, KNIME, LINCS tools, NextFlow},
abstract = {In the discipline of bioinformatics, a flow of work, or a sequence of computational or analytical tasks, is managed by a bioinformatics workflow management system, which is a subtype of a workflow automation system. This type of system is used to construct and manage the flow of work. There are numerous different work process situations available at this time. Some of them have been developed with the intention that scholars in a variety of subjects, such as cosmology and geology, will be able to make use of them as frameworks for logical work processes. Workflow frameworks such as Galaxy, GenePattern, KNIME, LINCS Tools, image analysis, and NextFlow are discussed in this chapter.}
}
@article{FIROMSAWAKUMA2024,
title = {Advanced tuberculosis diagnosis system: Integrating case-based reasoning with nearest neighbor algorithm},
journal = {Indian Journal of Tuberculosis},
year = {2024},
issn = {0019-5707},
doi = {https://doi.org/10.1016/j.ijtb.2024.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0019570724002464},
author = {Amelework {Firomsa Wakuma}},
keywords = {Tuberculosis, Expert system, Case based reasoning, Nearest Neighbor Algorithm},
abstract = {Background
A serious infectious illness with a high morbidity and death rate worldwide, tuberculosis (TB) is more prevalent in low- and middle-income nations. Although there are a number of diagnostic techniques, the most only address tuberculosis in the lung and ignore drug-resistant strains (MDR-TB, XDR-TB) as well as tuberculosis lymphadenitis. A thorough diagnostic system that covers all types of tuberculosis is essential.
Objectives
To enhance TB diagnosis, particularly pulmonary TB, lymphadenitis, and drug-resistant TB, this study offers an expert system based on Case-Based Reasoning (CBR) and the Nearest Neighbor Algorithm.
Methods
Information was gathered from hospital records of prior tuberculosis cases, including 43 cases from Debre Tabor General Hospital. In addition to document analysis, information was acquired through both structured and unstructured interviews with medical specialists. The R4 model—Retrieve, Reuse, Revise, and Retain—is followed by the system architecture. Recall, expert acceptance, and precision were among the evaluation metrics.
Results
The system had an 86.5% expert acceptance rate, 84.7% precision, and 75.3% recall. Compared to previous medical diagnostic methods, it shown a notable improvement, especially in diagnosing mental health and hypertension.
Conclusion
By combining Case-Based Reasoning and the Nearest Neighbor Algorithm, it is possible to diagnose tuberculosis (TB) more effectively and with greater accuracy. This integration also makes it possible to diagnose cases that are resistant to drugs. In order to improve the system's performance even more, future research may investigate the integration of additional reasoning strategies.}
}
@article{PYKA2024107668,
title = {Unlocking the potential of higher-molecular-weight 5-HT7R ligands: Synthesis, affinity, and ADMET examination},
journal = {Bioorganic Chemistry},
volume = {151},
pages = {107668},
year = {2024},
issn = {0045-2068},
doi = {https://doi.org/10.1016/j.bioorg.2024.107668},
url = {https://www.sciencedirect.com/science/article/pii/S004520682400573X},
author = {Patryk Pyka and Sabrina Garbo and Aleksandra Murzyn and Grzegorz Satała and Artur Janusz and Michał Górka and Wojciech Pietruś and Filip Mituła and Delfina Popiel and Maciej Wieczorek and Biagio Palmisano and Alessia Raucci and Andrzej J. Bojarski and Clemens Zwergel and Ewa Szymańska and Katarzyna Kucwaj-Brysz and Cecilia Battistelli and Jadwiga Handzlik and Sabina Podlewska},
keywords = {Serotonin receptor 5-HT, G protein-coupled receptors, ADMET properties, Docking, Molecular modelling,  experiments, MTS assay, Gene expression assay},
abstract = {An increasing number of drugs introduced to the market and numerous repositories of compounds with confirmed activity have posed the need to revalidate the state-of-the-art rules that determine the ranges of properties the compounds should possess to become future drugs. In this study, we designed a series of two chemotypes of aryl-piperazine hydantoin ligands of 5-HT7R, an attractive target in search for innovative CNS drugs, with higher molecular weight (close to or over 500). Consequently, 14 new compounds were synthesised and screened for their receptor activity accompanied by extensive docking studies to evaluate the observed structure–activity/properties relationships. The ADMET characterisation in terms of the biological membrane permeability, metabolic stability, hepatotoxicity, cardiotoxicity, and protein plasma binding of the obtained compounds was carried out in vitro. The outcome of these studies constituted the basis for the comprehensive challenge of computational tools for ADMET properties prediction. All the compounds possessed high affinity to the 5-HT7R (Ki below 250 nM for all analysed structures) with good selectivity over 5-HT6R and varying affinity towards 5-HT2AR, 5-HT1AR and D2R. For the best compounds of this study, the expression profile of genes associated with neurodegeneration, anti-oxidant response and anti-inflammatory function was determined, and the survival of the cells (SH-SY5Y as an in vitro model of Alzheimer’s disease) was evaluated. One 5-HT7R agent (32) was characterised by a very promising ADMET profile, i.e. good membrane permeability, low hepatotoxicity and cardiotoxicity, and high metabolic stability with the simultaneous high rate of plasma protein binding and high selectivity over other GPCRs considered, together with satisfying gene expression profile modulations and neural cell survival. Such encouraging properties make it a good candidate for further testing and optimisation as a potential agent in the treatment of CNS-related disorders.}
}
@article{DIAZBERRIOS2022100953,
title = {High school student understanding of exponential and logarithmic functions},
journal = {The Journal of Mathematical Behavior},
volume = {66},
pages = {100953},
year = {2022},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2022.100953},
url = {https://www.sciencedirect.com/science/article/pii/S0732312322000219},
author = {Tomás Díaz-Berrios and Rafael Martínez-Planell},
keywords = {APOS theory, Exponentiation, Logarithm, Rational exponents, Exponential and logarithmic functions},
abstract = {We use Action-Process-Object-Schema theory (APOS) to study high school student understanding of exponentiation and their construction of exponential and logarithmic functions. We extend didactic materials similar to those of Ferrari-Escolá et al. (2016) to include exponentials on the rational numbers and to help students construct logarithms as numbers. Qualitative data from the problem-solving activities of two groups of eight students each during a series of teaching episodes suggests that some students can use these materials successfully. The data analysis enabled us to give specific suggestions on how to help other students do some of the constructions needed to understand these functions. Research shows these constructions are difficult for students. The findings of our study led to contributing a new and detailed genetic decomposition that can be tested and improved in future research cycles.}
}
@article{DANCKWARDTLILLIESTROM2025100906,
title = {Travelling through time in a process drama on plastic pollution – temporality in teaching about the complexity of wicked problems},
journal = {Learning, Culture and Social Interaction},
volume = {52},
pages = {100906},
year = {2025},
issn = {2210-6561},
doi = {https://doi.org/10.1016/j.lcsi.2025.100906},
url = {https://www.sciencedirect.com/science/article/pii/S221065612500025X},
author = {Kerstin Danckwardt-Lillieström and Maria Andrée and Carl-Johan Rundgren},
keywords = {Historying, Futuring, Process drama, Wicked problems, Chemistry education, Upper secondary school},
abstract = {The understanding of sustainability issues and preparedness to take action towards a sustainable future involves abilities to navigate between past, present, and future. This paper explores how the use of imaginary transitions in time – in the form of historying, and futuring in process drama – may afford student understanding of the wicked problem of plastics. The study draws on a design-based research study on process drama in upper-secondary school chemistry teaching which was conducted in collaboration with two teachers. During the process drama, the students and teachers travel in time to explore the uses of plastic; the motives and needs for using plastic as well as the consequences of plastic use in the form of plastic pollution today and in the future. The collected data consist of video- and audio recordings, which were analysed through qualitative content analysis that discerned how the students connected the temporalities, and which dimensions of the plastic problem were made visible in the temporal movements in the process drama. Our findings indicate that the temporal transitions made visible several dimensions of the plastic issue, and contributed to adding layers of complexity to the issue of plastics.}
}
@article{NYBERG2022394,
title = {Spatial goal coding in the hippocampal formation},
journal = {Neuron},
volume = {110},
number = {3},
pages = {394-422},
year = {2022},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2021.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S0896627321010291},
author = {Nils Nyberg and Éléonore Duvelle and Caswell Barry and Hugo J. Spiers},
keywords = {hippocampus, entorhinal cortex, navigation, goal, wayfinding, spatial memory, reinforcement learning, rodent, human},
abstract = {Summary
The mammalian hippocampal formation contains several distinct populations of neurons involved in representing self-position and orientation. These neurons, which include place, grid, head direction, and boundary cells, are thought to collectively instantiate cognitive maps supporting flexible navigation. However, to flexibly navigate, it is necessary to also maintain internal representations of goal locations, such that goal-directed routes can be planned and executed. Although it has remained unclear how the mammalian brain represents goal locations, multiple neural candidates have recently been uncovered during different phases of navigation. For example, during planning, sequential activation of spatial cells may enable simulation of future routes toward the goal. During travel, modulation of spatial cells by the prospective route, or by distance and direction to the goal, may allow maintenance of route and goal-location information, supporting navigation on an ongoing basis. As the goal is approached, an increased activation of spatial cells may enable the goal location to become distinctly represented within cognitive maps, aiding goal localization. Lastly, after arrival at the goal, sequential activation of spatial cells may represent the just-taken route, enabling route learning and evaluation. Here, we review and synthesize these and other evidence for goal coding in mammalian brains, relate the experimental findings to predictions from computational models, and discuss outstanding questions and future challenges.}
}
@article{CRAIG20233427,
title = {FEFOS: a method to derive oxide formation energies from oxidation states††Electronic supplementary information (ESI) available. See DOI: https://doi.org/10.1039/d3cy00107e},
journal = {Catalysis Science & Technology},
volume = {13},
number = {11},
pages = {3427-3435},
year = {2023},
issn = {2044-4753},
doi = {https://doi.org/10.1039/d3cy00107e},
url = {https://www.sciencedirect.com/science/article/pii/S2044475323006822},
author = {Michael John Craig and Felix Kleuker and Michal Bajdich and Max García-Melchor},
abstract = {ABSTRACT
Herein we report a method to extract formation energies from oxidation states, which we call FEFOS. This new scheme predicts the formation energies of binary oxides through analyzing unary oxide formation energies as a function of their oxidation states. Taking averages of fitted quadratic equations that represent how elements respond to oxidation and reduction, the weights of these averages are determined by constraining the compound to be neutral. The application of FEFOS results in mean absolute errors of ca. 0.10 eV per atom when tested against Materials Project data for oxides with general formulas A1−zBzO, A1−zBzO1.5, and A1−zBzO2 with specific coordinations. Our FEFOS method not only allows for the prediction of binary oxide formation energies with low variance and high interpretability, but also compares well with state-of-the-art deep learning methods without being biased by training data and the need for large resources to compute it. Finally, we discuss the potential applications of the FEFOS method in tackling the problem of inverse catalyst design.}
}
@incollection{GAINOTTI2025421,
title = {Chapter 27 - Emotion: An evolutionary model of lateralization in the human brain},
editor = {Costanza Papagno and Paul Corballis},
series = {Handbook of Clinical Neurology},
publisher = {Elsevier},
volume = {208},
pages = {421-432},
year = {2025},
booktitle = {Cerebral Asymmetries},
issn = {0072-9752},
doi = {https://doi.org/10.1016/B978-0-443-15646-5.00001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443156465000014},
author = {Guido Gainotti},
keywords = {Emotional lateralization, Adaptive systems, Animal asymmetries, Evolutionary perspective, Covert language, Consciousness levels, Automatic functioning},
abstract = {Since several reviews have recently discussed the lateralization of emotions, this chapter will take into account the possible evolutionary meaning of this lateralization. The organization of the chapter will be based on the following steps. I will first propose that emotions must be considered as a complex adaptive system, complementary to the more phylogenetically advanced cognitive system. Second, I will remind historical aspects and consolidated results on the lateralization of emotions. Then I will discuss the phylogenetic aspects of the problem, trying to evaluate if emotional asymmetries concern only humans and some nonhuman primates or are part of a continuum between humans and many phylogenetically distant animal species. After having reviewed various aspects of emotional lateralization across different animal species and (more specifically) in nonhuman primates, I will propose a general model of hemispheric asymmetries in the human brain, based on theoretical models and empiric data. Theoretical models stem from the influence that the presence or the absence of language can have on concomitant hemispheric functions, whereas supporting neuropsychologic data have been gathered in patients with unilateral brain damage.}
}
@incollection{KLOCKING202597,
title = {Geochemical databases},
editor = {Ariel Anbar and Dominique Weis},
booktitle = {Treatise on Geochemistry (Third edition)},
publisher = {Elsevier},
edition = {Third edition},
address = {Oxford},
pages = {97-135},
year = {2025},
isbn = {978-0-323-99763-8},
doi = {https://doi.org/10.1016/B978-0-323-99762-1.00123-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323997621001236},
author = {Marthe Klöcking and Kerstin A. Lehnert and Lesley Wyborn},
keywords = {Artificial intelligence, CARE, Community standards, Data ethics, Data management, Databases, FAIR, Geochemistry, Machine learning, Machine readable data, Repository, TRUST},
abstract = {Geochemistry is a data-driven discipline. Modern laboratories produce highly diverse data, and the recent exponential increase in data volumes is challenging established practices and capabilities for organizing, analyzing, preserving, and accessing these data. At the same time, sophisticated computational techniques, including machine learning, are increasingly applied to geochemical research questions, which require easy access to large volumes of high-quality, well-organized, and standardized data. Data management has been important since the beginning of geochemistry but has recently become a necessity for the discipline to thrive in the age of digitalization and artificial intelligence. This paper summarizes the landscape of geochemical databases, distinguishing different types of data systems based on their purpose, and their evolution in a historic context. We apply the life cycle model of geochemical data; explain the relevance of current standards, practices, and policies that determine the design of modern geochemical databases and data management; the ethics of data reuse such as data ownership, data attribution, and data citation; and finally create a vision for the future of geochemical databases: data being born digital, connected to agreed community standards, and contributing to global democratization of geochemical data.}
}
@article{RAJPUT2021104270,
title = {VLSI implementation of transcendental function hyperbolic tangent for deep neural network accelerators},
journal = {Microprocessors and Microsystems},
volume = {84},
pages = {104270},
year = {2021},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2021.104270},
url = {https://www.sciencedirect.com/science/article/pii/S014193312100435X},
author = {Gunjan Rajput and Gopal Raut and Mahesh Chandra and Santosh Kumar Vishvakarma},
keywords = {Activation function, Artificial neural network, Hyperbolic tangent (tanh), Digital implementation, Combinational logic},
abstract = {Extensive use of neural network applications prompted researchers to customize a design to speed up their computation based on ASIC implementation. The choice of activation function (AF) in a neural network is an essential requirement. Accurate design architecture of an AF in a digital network faces various challenges as these AF require more hardware resources because of its non-linear nature. This paper proposed an efficient approximation scheme for hyperbolic tangent (tanh) function which purely based on combinational design architecture. The approximation is based on mathematical analysis by considering maximum allowable error in a neural network. The results prove that the proposed combinational design of an AF is efficient in terms of area, power and delay with negligible accuracy loss on MNIST and CIFAR-10 benchmark datasets. Post synthesis results show that the proposed design area is reduced by 66% and delay is reduced by nearly 16% compared to state-of-the-art.}
}
@article{ARGYROUDIS2022100387,
title = {Digital technologies can enhance climate resilience of critical infrastructure},
journal = {Climate Risk Management},
volume = {35},
pages = {100387},
year = {2022},
issn = {2212-0963},
doi = {https://doi.org/10.1016/j.crm.2021.100387},
url = {https://www.sciencedirect.com/science/article/pii/S2212096321001169},
author = {Sotirios A. Argyroudis and Stergios Aristoteles Mitoulis and Eleni Chatzi and Jack W. Baker and Ioannis Brilakis and Konstantinos Gkoumas and Michalis Vousdoukas and William Hynes and Savina Carluccio and Oceane Keou and Dan M. Frangopol and Igor Linkov},
keywords = {Emerging digital technologies, Data-driven, Critical infrastructure, Climate change, Sustainable development goals (SDGs)},
abstract = {Delivering infrastructure, resilient to multiple natural hazards and climate change, is fundamental to continued economic prosperity and social coherence. This is a strategic priority of the United Nations Sustainable Development Goals (SDGs), the World Bank, the Organisation for Economic Co-operation and Development (OECD), public policies and global initiatives. The operability and functionality of critical infrastructure are continuously challenged by multiple stressors, increasing demands and ageing, whilst their interconnectedness and dependencies pose additional challenges. Emerging and disruptive digital technologies have the potential to enhance climate resilience of critical infrastructure, by providing rapid and accurate assessment of asset condition and support decision-making and adaptation. In this pursuit, it is imperative to adopt multidisciplinary roadmaps and deploy computational, communication and other digital technologies, tools and monitoring systems. Nevertheless, the potential of these emerging technologies remains largely unexploited, as there is a lack of consensus, integrated approaches and legislation in support of their use. In this perspective paper, we discuss the main challenges and enablers of climate-resilient infrastructure and we identify how available roadmaps, tools and emerging digital technologies, e.g. Internet of Things, digital twins, point clouds, Artificial Intelligence, Building Information Modelling, can be placed at the service of a safer world. We show how digital technologies will lead to infrastructure of enhanced resilience, by delivering efficient and reliable decision-making, in a proactive and/or reactive manner, prior, during and after hazard occurrences. In this respect, we discuss how emerging technologies significantly reduce the uncertainties in all phases of infrastructure resilience evaluations. Thus, building climate-resilient infrastructure, aided by digital technologies, will underpin critical activities globally, contribute to Net Zero target and hence safeguard our societies and economies. To achieve this we set an agenda, which is aligned with the relevant SDGs and highlights the urgent need to deliver holistic and inclusive standards and legislation, supported by coordinated alliances, to fully utilise emerging digital technologies.}
}
@article{HALL20156607,
title = {Understanding Sector Dependencies in the Stabilization and Reconstruction of Nation-states},
journal = {Procedia Manufacturing},
volume = {3},
pages = {6607-6614},
year = {2015},
note = {6th International Conference on Applied Human Factors and Ergonomics (AHFE 2015) and the Affiliated Conferences, AHFE 2015},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2015.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S2351978915009932},
author = {Steven Hall and Curtis Blais},
keywords = {Multi-agent, system dynamics, modeling and simulation, panarchy, resiliency, stabilitiy, reconstruction and humanitarian operations.},
abstract = {The United States Army is undergoing a re-definition of its Civil Affairs officer positions. A recent project to define the educational requirements for an Army Civil Affairs Officer (38G) identified an educational requirement to help officers understand the complex ways in which the operations that advance the achievement of one stabilization objective often hinder the achievement of other objectives. The system level thinking was seen to be frequently insufficiently ingrained amongst Civil Affairs Officers (and the leaders they advised), who were both often perceived to be inclined, in the face of the complexities of the situation on the ground, to become too narrowly focused on achieving their specific assigned responsibilities, limiting their ability to see how the mission effectiveness of what they were recommending would be influenced by the state and trajectory of other Sectors and how, in turn, their recommendations would influence the mission effectiveness of other Sector stewards. While system dynamics modeling has proven itself to be effective in capturing and effectively communicating feedback loops that define such non-linear (and non-intuitive) systems they do not, in themselves, provide sufficient modeling richness to comprehensively capture the critical spatial (geographical) determinants of a successful state reconstruction process. For these purposes, a multi-agent cellular automata model is recommended both as a vehicle for introducing students to the complex nature of the state reconstruction process and, eventually, for use in the field by deployed Civilian Affairs Officers at all levels. This paper describes the problem and the modeling approach to address it.}
}
@article{MILIK201022,
title = {On Efficient Implementation of Search Algorithm for Genome Patterns},
journal = {IFAC Proceedings Volumes},
volume = {43},
number = {24},
pages = {22-27},
year = {2010},
note = {10th IFAC Workshop on Programmable Devices and Embedded Systems},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20101006-2-PL-4019.00006},
url = {https://www.sciencedirect.com/science/article/pii/S1474667015309812},
author = {Adam Milik and Andrzej Pulka},
keywords = {Dynamic programming, Computational methods, Pattern identification, Pattern recognition, Parallel processing, Pipeline processing},
abstract = {The presented paper describes the implementation of the computation algorithm on modern, complex programmable hardware devices. The presented algorithm originates from computation biology and works on very long chains of symbols, which come from reference patterns of the genome. The software solutions in the field are very limited and need large time and space resources. Main research efforts have been done to investigate the properties of the searching algorithm. Especially the influence of the penalty values assigned for the mismatch, the insertion and the deletion on the algorithm has been analyzed. This allows obtaining completely new algorithm that offers extremely efficient implementation and exhibits outstanding performance. The different FPGA generations have been considered as target families for the searching algorithm based on the dynamic programming idea. The obtained results are very promising and show the dominance of the dedicated platforms over the general purpose PC-based systems.}
}
@article{KASTELLAKIS201519,
title = {Synaptic clustering within dendrites: An emerging theory of memory formation},
journal = {Progress in Neurobiology},
volume = {126},
pages = {19-35},
year = {2015},
issn = {0301-0082},
doi = {https://doi.org/10.1016/j.pneurobio.2014.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0301008214001373},
author = {George Kastellakis and Denise J. Cai and Sara C. Mednick and Alcino J. Silva and Panayiota Poirazi},
keywords = {Plasticity, Active dendrites, Associative memory, Synapse clustering, Synaptic tagging and capture},
abstract = {It is generally accepted that complex memories are stored in distributed representations throughout the brain, however the mechanisms underlying these representations are not understood. Here, we review recent findings regarding the subcellular mechanisms implicated in memory formation, which provide evidence for a dendrite-centered theory of memory. Plasticity-related phenomena which affect synaptic properties, such as synaptic tagging and capture, synaptic clustering, branch strength potentiation and spinogenesis provide the foundation for a model of memory storage that relies heavily on processes operating at the dendrite level. The emerging picture suggests that clusters of functionally related synapses may serve as key computational and memory storage units in the brain. We discuss both experimental evidence and theoretical models that support this hypothesis and explore its advantages for neuronal function.}
}
@incollection{ASHBY2016211,
title = {Chapter 14 - The Vision: A Circular Materials Economy},
editor = {Michael F. Ashby},
booktitle = {Materials and Sustainable Development},
publisher = {Butterworth-Heinemann},
address = {Boston},
pages = {211-239},
year = {2016},
isbn = {978-0-08-100176-9},
doi = {https://doi.org/10.1016/B978-0-08-100176-9.00014-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780081001769000141},
author = {Michael F. Ashby},
keywords = {Active stock, Circularity metrics, Material efficiency, Natural and industrial ecology, Product life extension, Product–service systems, Reuse, repair and recycling, Take-back schemes},
abstract = {We live at present with a largely linear materials economy. Our use of natural resources is characterized by the sequence “take – make – use – dispose” as materials progress from mine, through product, to landfill. Increasing population, rising affluence and the limited capacity for the planet to provide resources and absorb waste argue for a transition towards a more circular way of using materials. When products come to the end of their lives the materials they contain are still there. Repair, reuse and recycling (the three “Rs”) can return these to active use. Repair, reuse and recycling are not new ideas; they have been used for centuries to recirculate materials and, in less-developed economies, they still are. But in developed nations they have dwindled as the cost of materials fell and that of labor rose over time, making all three Rs uneconomic. So what is novel about the contemporary idea of a circular materials economy? Haven’t we been there before? The “circularity” concept is a way thinking that looks not just for efficiencies but also for new ways of providing the functions we need. In the last decade momentum has gathered about this transition. The idea of deploying rather than consuming materials, of using them not once but many times, and of redesign to make this a reality has economic as well as environmental appeal. Governments now sign up to programs to foster circular economic ideas and mechanisms begin to appear to advance them. This chapter examines the background, the successes and the difficulties of implementing a circular materials economy.}
}
@article{MCQUEEN2021120575,
title = {Do we really understand how drug eluted from stents modulates arterial healing?},
journal = {International Journal of Pharmaceutics},
volume = {601},
pages = {120575},
year = {2021},
issn = {0378-5173},
doi = {https://doi.org/10.1016/j.ijpharm.2021.120575},
url = {https://www.sciencedirect.com/science/article/pii/S037851732100380X},
author = {Alistair McQueen and Javier Escuer and Ankush Aggarwal and Simon Kennedy and Christopher McCormick and Keith Oldroyd and Sean McGinty},
keywords = {Pharmacodynamics, Ligand-receptor interactions, Drug-eluting stents, Smooth Muscle Cells, Cell proliferation, Mathematical Modelling},
abstract = {The advent of drug-eluting stents (DES) has revolutionised the treatment of coronary artery disease. These devices, coated with anti-proliferative drugs, are deployed into stenosed or occluded vessels, compressing the plaque to restore natural blood flow, whilst simultaneously combating the evolution of restenotic tissue. Since the development of the first stent, extensive research has investigated how further advancements in stent technology can improve patient outcome. Mathematical and computational modelling has featured heavily, with models focussing on structural mechanics, computational fluid dynamics, drug elution kinetics and subsequent binding within the arterial wall; often considered separately. Smooth Muscle Cell (SMC) proliferation and neointimal growth are key features of the healing process following stent deployment. However, models which depict the action of drug on these processes are lacking. In this article, we start by reviewing current models of cell growth, which predominantly emanate from cancer research, and available published data on SMC proliferation, before presenting a series of mathematical models of varying complexity to detail the action of drug on SMC growth in vitro. Our results highlight that, at least for Sodium Salicylate and Paclitaxel, the current state-of-the-art nonlinear saturable binding model is incapable of capturing the proliferative response of SMCs across a range of drug doses and exposure times. Our findings potentially have important implications on the interpretation of current computational models and their future use to optimise and control drug release from DES and drug-coated balloons.}
}
@article{KOH2020106,
title = {Automated detection of Alzheimer's disease using bi-directional empirical model decomposition},
journal = {Pattern Recognition Letters},
volume = {135},
pages = {106-113},
year = {2020},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2020.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167865520300921},
author = {Joel En Wei Koh and Vicnesh Jahmunah and The-Hanh Pham and Shu Lih Oh and Edward J Ciaccio and U Rajendra Acharya and Chai Hong Yeong and Mohd Kamil Mohd Fabell and Kartini Rahmat and Anushya Vijayananthan and Norlisah Ramli},
abstract = {The build-up of beta-amyloid and rapid spread of tau proteins in the brain cause the death of neurons, leading to Alzheimer's disease (AD). AD is a form of dementia, and the symptoms include memory loss and decision-making difficulties. Current advanced diagnostic modalities are costly or unable to detect the histopathological features of AD. Hence a computational intelligence tool (CIT) for AD diagnosis is proposed in this study. The magnetic resonance images (MRI) of the brain are pre-processed using an adaptive histogram, and decomposed into four IMFS using bidirectional empirical mode decomposition (BEMD). Local binary patterns (LBP) are then computed per IMF, and the histograms are concatenated. Adaptive synthetic sampling (ADASYN) is applied to balance the dataset and Student's t-test is utilized for selection of highly significant features, within each fold for ten-fold validation. Amongst other classifiers, SVM-Poly 1 and random forest(RF) were employed for classification, yielding the highest accuracy of 93.9% each. Our study concludes that the recommended CIT is useful for the automatic classification of AD versus normal MRI imagery in hospitals.}
}
@article{CASTRO2023105510,
title = {An experimental and simulation study of the impact of emotional information on analogical reasoning},
journal = {Cognition},
volume = {238},
pages = {105510},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2023.105510},
url = {https://www.sciencedirect.com/science/article/pii/S0010027723001440},
author = {Ariana A. Castro and John E. Hummel and Howard Berenbaum},
keywords = {Reasoning, Emotion, Computational models, Attention, Analogies},
abstract = {We investigated whether and how emotional information would affect analogical reasoning. We hypothesized that task-irrelevant emotional information would impair performance whereas task-relevant emotional information would enhance it. In Study 1, 233 undergraduates completed a novel version of the People Pieces Task (Emotional Faces People Task), an analogical reasoning task in which the task characters displayed emotional or neutral facial expressions (within-participants). The emotional faces were relevant or irrelevant to the task (between-participants). We simulated the behavioral results using the Learning and Inference with Schemas and Analogies (LISA) model of relational reasoning. LISA is a neurally plausible, symbolic-connectionist computational model of analogical reasoning. In comparison to neutral trials, participants were slower but more accurate on emotion-relevant trials, and were faster but less accurate on emotion-irrelevant trials. Simulations using the LISA model demonstrated that it is possible to account for the effects of emotional information on reasoning in terms of how emotional stimuli attract attention during a reasoning task. In Study 2, 255 undergraduates completed the Emotional Faces People Task at either a high- or low-working memory load. The high working memory load condition of Study 2 replicated the findings of Study 1, showing that participants were more accurate on emotion-relevant trials than on emotion-irrelevant trials; in Study 2, this increased accuracy could not be accounted for by a speed-accuracy tradeoff. The working memory manipulation influenced the manner in which the congruence (with the correct answer) of emotion-irrelevant emotion influenced performance. Simulations using the LISA model showed that manipulating the salience of emotion, the error penalty, as well as vigilance (which determines the likelihood that LISA will notice it has attended to an irrelevant relation), could reasonably reproduce the behavioral results of both low and high working memory load conditions of Study 2.}
}
@article{ARTEMOV20093884,
title = {A tribute to D.B. Spalding and his contributions in science and engineering},
journal = {International Journal of Heat and Mass Transfer},
volume = {52},
number = {17},
pages = {3884-3905},
year = {2009},
note = {Special Issue Honoring Professor D. Brian Spalding},
issn = {0017-9310},
doi = {https://doi.org/10.1016/j.ijheatmasstransfer.2009.03.038},
url = {https://www.sciencedirect.com/science/article/pii/S0017931009002026},
author = {V. Artemov and S.B. Beale and G. {de Vahl Davis} and M.P. Escudier and N. Fueyo and B.E. Launder and E. Leonardi and M.R. Malin and W.J. Minkowycz and S.V. Patankar and A. Pollard and W. Rodi and A. Runchal and S.P. Vanka},
keywords = {D.B. Spalding, Fluid dynamics, Heat transfer, Mass transfer, Combustion},
abstract = {This paper presents a summary of some of the scientific and engineering contributions of Prof. D.B. Spalding up to the present time. Starting from early work on combustion, and his unique work in mass transfer theory, Spalding’s unpublished “unified theory” is described briefly. Subsequent to this, developments in algorithms by the Imperial College group led to the birth of modern computational fluid dynamics, including the well-known SIMPLE algorithm. Developments in combustion, multi-phase flow and turbulence modelling are also described. Finally, a number of academic and industrial applications of computational fluid dynamics and heat transfer applications considered in subsequent years are mentioned.}
}
@article{BROCAS2022331,
title = {Adverse selection and contingent reasoning in preadolescents and teenagers},
journal = {Games and Economic Behavior},
volume = {133},
pages = {331-351},
year = {2022},
issn = {0899-8256},
doi = {https://doi.org/10.1016/j.geb.2022.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0899825622000616},
author = {Isabelle Brocas and Juan D. Carrillo},
keywords = {Developmental decision-making, Lab-in-the-field experiment, Contingent reasoning, Winner's curse},
abstract = {We study from a developmental viewpoint the ability to perform contingent reasoning and the cognitive abilities that facilitate optimal behavior. Individuals from 11 to 17 years old participate in a simplified version of the two-value, deterministic “acquire-a-company” adverse selection game (Charness and Levin, 2009; Martínez-Marquina et al., 2019). We find that even our youngest subjects understand well the basic principles of contingent reasoning (offer the reservation price of one of the sellers), although they do not necessarily choose the optimal price. Performance improves steadily and significantly over the developmental window but it is not facilitated by repeated exposure or feedback. High cognitive ability–measured by a high performance in a working memory task–is necessary to behave optimally in the simplest settings but it is not sufficient to solve the most complex situations.}
}
@article{BIRD2004337,
title = {Kuhn, naturalism, and the positivist legacy},
journal = {Studies in History and Philosophy of Science Part A},
volume = {35},
number = {2},
pages = {337-356},
year = {2004},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2004.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0039368104000184},
author = {Alexander Bird},
keywords = {Kuhn, Naturalism, Positivism, Preston},
abstract = {I defend against criticism the following claims concerning Thomas Kuhn: (i) there is a strong naturalist streak in The structure of scientific revolutions, whereby Kuhn used the results of a posteriori enquiry in addressing philosophical questions; (ii) as Kuhn’s career as a philosopher of science developed he tended to drop the naturalistic elements and to replace them with more traditionally philosophical, a priori approaches; (iii) at the same time there is a significant residue of positivist thought in Kuhn, which Kuhn did not recognise as such; (iv) the naturalistic elements referred to in (i) are the most original and fruitful elements of Kuhn’s thinking; (v) the positivistic elements referred to in (iii) vitiated his thought and acted as factors in preventing Kuhn from developing the naturalistic elements and from following the path taken by much subsequent philosophy of science. Preston presents an alternative reading of Kuhn which emphasizes the Wittgensteinian elements in Kuhn. I argue that this alternative view is, descriptively, poorly supported by the textual evidence and the facts of the history of philosophy of science in the twentieth century. I provide some defence of the naturalistic approach and related themes.}
}
@article{LOVE2017113,
title = {On languaging and languages},
journal = {Language Sciences},
volume = {61},
pages = {113-147},
year = {2017},
note = {Orders of Language: A festschrift for Nigel Love},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2017.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0388000117301080},
author = {Nigel Love},
keywords = {Languaging, Linguistic reflexivity, Metalanguage, Ontology of languages, Verbatim repetition, Writing},
abstract = {I consider the ontology of languages and the linguistic units said to constitute them, in the light of a speculative sketch of how languaging about language might give rise to the idea of a language. The focus is principally on the role of reflexivity and the development of writing in facilitating the decontextualisation, abstraction and reification of linguistic units and languages themselves. The main trend in modern linguistics has been to take the products of these processes as realia, and to retroject them on to languagers as the basis for their languaging activities: I touch on some of the deleterious effects of this on theorising about the acquisition, storage and production of language. Finally, I consider how in thinking about these matters the concept of different ‘orders’ of language has been and might be interpreted and deployed. Whether or not this concept has a useful role to play in formulating them, the ideas assembled here are offered in the hope that they might serve as a platform from which to debate the significance and implications of the stultifying effect our modes of metalanguaging have so far had on inquiry into our engagement with language.}
}
@article{ROLISON2022105401,
title = {Developmental differences in description-based versus experience-based decision making under risk in children},
journal = {Journal of Experimental Child Psychology},
volume = {219},
pages = {105401},
year = {2022},
issn = {0022-0965},
doi = {https://doi.org/10.1016/j.jecp.2022.105401},
url = {https://www.sciencedirect.com/science/article/pii/S0022096522000303},
author = {Jonathan J. Rolison and Thorsten Pachur and Teresa McCormack and Aidan Feeney},
keywords = {Decision making under risk, Children, Computational modeling, Description-based decision making, Experience-based decision making, Risk taking},
abstract = {The willingness to take a risk is shaped by temperaments and cognitive abilities, both of which develop rapidly during childhood. In the adult developmental literature, a distinction is drawn between description-based tasks, which provide explicit choice–reward information, and experience-based tasks, which require decisions from past experience, each emphasizing different cognitive demands. Although developmental trends have been investigated for both types of decisions, few studies have compared description-based and experience-based decision making in the same sample of children. In the current study, children (N = 112; 5–9 years of age) completed both description-based and experience-based decision tasks tailored for use with young children. Child temperament was reported by the children’s primary teacher. Behavioral measures suggested that the willingness to take a risk in a description-based task increased with age, whereas it decreased in an experience-based task. However, computational modeling alongside further inspection of the behavioral data suggested that these opposite developmental trends across the two types of tasks both were associated with related capacities: older (vs. younger) children’s higher sensitivity to experienced losses and higher outcome sensitivity to described rewards and losses. From the temperamental characteristics, higher attentional focusing was linked with a higher learning rate on the experience-based task and a bias to accept gambles in the gain domain on the description-based task. Our findings demonstrate the importance of comparing children’s behavior across qualitatively different tasks rather than studying a single behavior in isolation.}
}
@incollection{SCHAUB2022555,
title = {Chapter 22 - Conclusions},
editor = {Michael Schaub and Marc Kéry},
booktitle = {Integrated Population Models},
publisher = {Academic Press},
pages = {555-563},
year = {2022},
isbn = {978-0-323-90810-8},
doi = {https://doi.org/10.1016/B978-0-12-820564-8.24002-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128205648240025},
author = {Michael Schaub and Marc Kéry},
keywords = {Continuous time scale, Full annual cycle integrated population model, Future developments, Individual heterogeneity, Long-term ecological research, Multispecies integrated population model, Sampling design, Spatial integrated population model, Spatial scale},
abstract = {In this final chapter, we first look back and briefly summarize what we have learned in this book. We then look forward and sketch out possible avenues of future research into integrated population models (IPMs) and where it may or should go. We especially foresee likely future developments in three areas. The first is in developing alternative formulations of the population, or process, model in an IPM, which currently is mostly a classical matrix population model. In the future, we expect to see refinements along some or all of the spatial, temporal, and individual axes of fundamental demographic information—that is, a general shift away from discrete to more continuous scales along these dimensions of the description of population dynamics. In particular, we think a more widespread “spatialization” of IPMs is imminent. We also think that IPMs for two or more species with explicit links among them will increasingly be developed because they allow the study of interactions among species at a very basic mechanistic level. The second area of likely future progress in IPMs deals with the observation model, especially the Gaussian error model in the state-space model for population counts. In a sense, this model is a misspecification that cannot explicitly account for the false-positives and false-negatives that now are so commonly included in the capture-recapture class of models. The third area where we envision future progress in IPMs is with more fundamental statistical and computational work. We expect further progress in algorithm fitting, goodness-of-fit testing, models that account for dependence among the components of joint likelihood, and the study and development of more effective sampling designs. Finally, we are excited to see many more applications of existing and future IPMs to improve our scientific conclusions and conservation and wildlife management decisions.}
}
@article{ALCANTUD2022118276,
title = {Ranked hesitant fuzzy sets for multi-criteria multi-agent decisions},
journal = {Expert Systems with Applications},
volume = {209},
pages = {118276},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118276},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422014142},
author = {José Carlos R. Alcantud},
keywords = {Hesitant fuzzy set, Aggregation operator, Score, Ranking, Decision making},
abstract = {This paper introduces and investigates ranked hesitant fuzzy sets, a novel extension of hesitant fuzzy sets that is less demanding than both probabilistic and proportional hesitant fuzzy sets. This new extension incorporates hierarchical knowledge about the various evaluations submitted for each alternative. These evaluations are ranked (for example by their plausibility, acceptability, or credibility), but their position does not necessarily derive from supplementary numerical information (as in probabilistic and proportional hesitant fuzzy sets). In particular, strictly ranked hesitant fuzzy sets arise when no ties exist, i.e., when for any fixed alternative, each submitted evaluation is either strictly more plausible or strictly less plausible than any other submitted evaluation. A detailed comparison with similar models from the literature is performed. Then in order to produce a natural strategy for multi-criteria multi-agent decisions with ranked hesitant fuzzy sets, canonical representations, scores and aggregation operators are designed in the framework of ranked hesitant fuzzy sets. In order to help implementation of this model, Mathematica code is provided for the computation of both scores and aggregators. The decision-making technique that is prescribed is tested with a comparative analysis with four methodologies based on probabilistic hesitant fuzzy information. A conclusion of this numerical exercise is that this methodology is reliable, applicable and robust. All these evidences show that ranked hesitant fuzzy sets are an intuitive extension of the hesitant fuzzy set model designed by V. Torra, that can be implemented in practice with the aid of computationally assisted algorithms.}
}
@incollection{VODOVOTZ201563,
title = {Chapter 3.2 - Dynamic Knowledge Representation and the Power of Model Making},
editor = {Yoram Vodovotz and Gary An},
booktitle = {Translational Systems Biology},
publisher = {Academic Press},
address = {Boston},
pages = {63-68},
year = {2015},
isbn = {978-0-12-397884-4},
doi = {https://doi.org/10.1016/B978-0-12-397884-4.00009-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780123978844000094},
author = {Yoram Vodovotz and Gary An},
keywords = {Systems biology, mathematical modeling, computational biology, computational modeling, knowledge representation, conceptual model},
abstract = {This chapter focuses on describing the primary tool used in Translational Systems Biology: dynamic computational modeling. This chapter discusses the conceptual basis and rationale for modeling, with particular emphasis on the role of dynamic computational and mathematical models in biomedical research. We introduce the concept of using models as means of Dynamic Knowledge Representation, with the scientific target of facilitating the visualization, instantiation, evaluation, and falsification of biological hypotheses. We compare and contrast the use of modeling and simulation for this purpose versus the development and use of “engineering grade” quantitative models, noting specifically that given the state of biological knowledge, biomedical Dynamic Knowledge Representation is aimed at facilitating discovery, as opposed to the engineering goal of optimizing solutions. We discuss the fundamental step in model construction, mapping, and explain its role in the use and potential interpretation of both biological proxy models and computational models. We introduce the concept of Conceptual Model Verification, and its role as a means of accelerating the Scientific Cycle.}
}
@incollection{PIOT2006163,
title = {Gross, Maurice (1934–2001)},
editor = {Keith Brown},
booktitle = {Encyclopedia of Language & Linguistics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {163-164},
year = {2006},
isbn = {978-0-08-044854-1},
doi = {https://doi.org/10.1016/B0-08-044854-2/05135-X},
url = {https://www.sciencedirect.com/science/article/pii/B008044854205135X},
author = {M. Piot},
keywords = {comparative linguistics, computational linguistics, computer dictionaries, French language, linguistic theory, machine translation, mathematical models, syntax-based lexicon},
abstract = {Gross, Maurice (1934–2001) was a pioneer thinker in the field of modern linguistics. Long before computers could facilitate large-scale, lexically-based language study, he built an exhaustive, empirically based inventory of the ‘lexicon-grammar’ of French: the world's first lexical grammar. Since then, researchers in other countries have adopted the Gross model of description, which serves as a computational model for any language.}
}
@article{WENZLAFF200127,
title = {Mental control after dysphoria: Evidence of a suppressed, depressive bias},
journal = {Behavior Therapy},
volume = {32},
number = {1},
pages = {27-45},
year = {2001},
issn = {0005-7894},
doi = {https://doi.org/10.1016/S0005-7894(01)80042-3},
url = {https://www.sciencedirect.com/science/article/pii/S0005789401800423},
author = {Richard M. Wenzlaff and Ann R. Eisenberg},
abstract = {Previous research has generally failed to find persistent negative thinking following a depressive episode, suggesting that negative thoughts may simply be by-products of the emotional disturbance. The present research examined the idea that a persistent depressive bias does exist, but it is obscured by thought suppression. Mental control theory suggests that suppressed thoughts can be detected by assessing cognition before the effortful process of distraction is implemented. To test this prediction, formerly dysphoric, chronically dysphoric, and nondysphoric control groups interpreted audio recordings of words—some of which included homophones with emotional alternatives relevant to depression (e.g., weak/week). Participants wrote down each word either immediately or after a 10-sec delay. Although formerly dysphoric individuals did not display a depressive bias in the delayed condition, their immediate responses revealed a depressive bias. As predicted, the emergence of a negative bias was associated with high levels of chronic thought suppression.}
}
@article{AHMADI2022101232,
title = {Energy efficiency improvement and emission reduction potential of domestic gas burners through re-orientating the angle and position of burner holes: Experimental and numerical study},
journal = {Thermal Science and Engineering Progress},
volume = {32},
pages = {101232},
year = {2022},
issn = {2451-9049},
doi = {https://doi.org/10.1016/j.tsep.2022.101232},
url = {https://www.sciencedirect.com/science/article/pii/S2451904922000397},
author = {Ali Akbar Ahmadi and Alireza Rahbari and Mostafa Mohamadi},
keywords = {Experimental and numerical study, Domestic burners, Burner geometry, Turbulent combustion, Thermal efficiency},
abstract = {The trend towards enhancing the thermal performance of domestic cooking burners necessitates developing a new design for such devices. With this picture in mind, this paper numerically and experimentally investigates the effect of burner head design configurations on the energy efficiency and CO emission of domestic gas burners. The results of a three-dimensional steady-state computational fluid dynamics (CFD) model is validated with the experimental data according to Volunteers in Technical Assistance (VITA) standard in cold start, hot start, and Simmer condition for two types of burners. Having the model validated, a step-by-step approach has been undertaken to improve the design of these reference cases, resulted in a total number of nine burner configurations analysed in this research. This is followed by determining the influence of introduced geometries on the thermal efficiency of burners. Based on the insights from the numerical model, the most efficient burner exhibits 3.3–22.2% higher thermal efficiency and 20.2–32.6% lower CO emission—depending on the gas flow rate—relative to the conventional burners. The optimised design can be implemented into existing burners with relatively little need for reconstruction.}
}
@article{XIAO2023103864,
title = {APRS: Automatic pruning ratio search using Siamese network with layer-level rewardsImage 1},
journal = {Digital Signal Processing},
volume = {133},
pages = {103864},
year = {2023},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2022.103864},
url = {https://www.sciencedirect.com/science/article/pii/S105120042200481X},
author = {Huachao Xiao and Yangxin Wang and Jianyi Liu and Jiaxin Huo and Yang Hu and Yu Wang},
keywords = {Structured pruning, Deep reinforcement learning, Pruning ratio search, Siamese network},
abstract = {Structured pruning is still a mainstream model compression technique, for its merit of easy to implement and no reliance on specific hardware supporting library. In most previous works, the layer-wise channel pruning ratios were determined empirically. In this paper, we propose an Automatic Pruning Ratio Search (APRS) algorithm that can find the layer-wise optimal pruning ratio within the deep reinforcement learning framework. To solve the coarse-granularity reward problem existing in some previous works like AMC and CACP, a novel layer-level reward function is designed based on the Siamese network architecture for the fine-granularity agent-environment interaction purpose. We use a computationally efficient way to evaluate the effect of pruning action on each single layer. The incurred “backwardness disadvantage” problem has also been analyzed and addressed. The experiments are performed using the VGG-16, and MobileNet-v1 on the CIFAR10/100 and UC Merced Land-use datasets. The results verified that our method can better reveal the underlying sparse sensitivities of different layers in both high redundancy networks and compact networks, so that resulting a higher network accuracy after pruning compared to the traditional methods.}
}
@incollection{DALY20173,
title = {8.02 - Molecular Logic Gates as Fluorescent Sensors},
editor = {Jerry L. Atwood},
booktitle = {Comprehensive Supramolecular Chemistry II},
publisher = {Elsevier},
address = {Oxford},
pages = {3-19},
year = {2017},
isbn = {978-0-12-803199-5},
doi = {https://doi.org/10.1016/B978-0-12-409547-2.12626-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095472126265},
author = {B. Daly and V.A.D. Silverson and C.Y. Yao and Z.Q. Chen and A.P. {de Silva}},
keywords = {AND Logic, Fluorescent Sensors, IMPLICATION Logic, INHIBIT Logic, Intracellular AND Logic, Logic Gates, NAND Logic, XOR and XNOR Logic, YES Logic},
abstract = {Some recent developments in the use of molecular logic gates as fluorescent sensors are described. The discussion is classified in terms of the Boolean logical assignment of the sensor system. Even simple fluorescent sensors can be recognized as single-input logic gates. Several YES gates launch the analysis of examples. A consideration of various sensors driven by double inputs and higher multiple inputs then follows. Attention is particularly drawn to the appearance of double-input logical sensor molecules, which successfully operate within living cells—a milieu where conventional semiconductor-based logic devices would struggle on the grounds of compatibility and size. The value of molecular logical thinking in the understanding of fluorescent sensor behavior is emphasized throughout.}
}
@article{PANG201667,
title = {A hierarchical alternative updated adaptive Volterra filter with pipelined architecture},
journal = {Digital Signal Processing},
volume = {56},
pages = {67-78},
year = {2016},
issn = {1051-2004},
doi = {https://doi.org/10.1016/j.dsp.2016.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S1051200416000506},
author = {Yanjie Pang and Jiashu Zhang},
keywords = {Nonlinear filter, Hierarchical pipelined structure, Alternative update mechanism, Volterra filter},
abstract = {The pipelined adaptive Volterra filters (PAVFs) with a two-layer structure constitute a class of good low-complexity filters. They can efficiently reduce the computational complexity of the conventional adaptive Volterra filter. Their major drawbacks are low convergence rate and high steady-state error caused by the coupling effect between the two layers. In order to remove the coupling effect and improve the performance of PAVFs, we present a novel hierarchical pipelined adaptive Volterra filter (HPAVF)-based alternative update mechanism. The HPAVFs with hierarchical decoupled normalized least mean square (HDNLMS) algorithms are derived to adaptively update weights of its nonlinear and linear subsections. The computational complexity of HPAVF is also analyzed. Simulations of nonlinear system adaptive identification, nonlinear channel equalization, and speech prediction show that the proposed HPAVF with different independent weight vectors in nonlinear subsection has superior performance to conventional Volterra filters, diagonally truncated Volterra filters, and PAVFs in terms of initial convergence, steady-state error, and computational complexity.}
}
@article{LI2023106560,
title = {High energy capacity or high power rating: Which is the more important performance metric for battery energy storage systems at different penetrations of variable renewables?},
journal = {Journal of Energy Storage},
volume = {59},
pages = {106560},
year = {2023},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2022.106560},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X2202549X},
author = {Mingquan Li and Rui Shan and Ahmed Abdulla and Jialin Tian and Shuo Gao},
keywords = {Energy storage, Energy-to-power ratio (EPR), Decarbonization, Carbon emissions, Renewable integration, Low-carbon transition},
abstract = {Studies exploring the role and value of energy storage in deep decarbonization often overlook the balance between the energy capacity and the power rating of storage systems—a key performance parameter that can affect every part of storage operation. Here, we quantitatively evaluate the system-wide impacts of battery storage systems with various energy-to-power ratios (EPRs) and at different levels of renewable penetration. We take Jiangsu province in China as our case study, due to its high electricity consumption and aggressive renewable energy targets. Our results show the evolving role of storage: as renewable penetration increases, higher EPRs are favored, as they lead to system-wide cost reductions, lower GHG emissions, and higher power system reliability. Whereas existing studies make exogenous assumptions about the lifetime of storage, we show that lifetimes across EPRs and renewable scenarios span 10 to 20 years. Existing research can thus send false signals to investors and grid planners, delaying the deployment of storage and retarding the energy transition. By showing how different EPRs yield different benefits at different stages of the energy transition, our results help investors, policy makers, and system planners design forward-thinking and dynamic policies that encourage prudent storage uptake.}
}
@article{WANG2022101643,
title = {Model for deep learning-based skill transfer in an assembly process},
journal = {Advanced Engineering Informatics},
volume = {52},
pages = {101643},
year = {2022},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101643},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622001070},
author = {Kung-Jeng Wang and Luh {Juni Asrini} and Lucy Sanjaya and Hong-Phuc Nguyen},
keywords = {Convolutional neural network, Deep learning, Faster region-based convolutional neural network, Human machine interaction, Skill transfer},
abstract = {As the variety of products and manufacturing processes increases, the expansion of flexible training approaches is crucial to support the development of human skills. This study presents a model for skill transfer support that extracts experts’ relevant skills as actions and objects relevant to the action into a computational model for transferring skills. This model engages two modes of deep learning as the groundwork, namely, convolutional neural network (CNN) for action recognition and faster region-based convolutional neural network (R-CNN) for object detection. To evaluate the performance of the proposed model, a case study of the final assembly of a GPU card is conducted. The accuracy of CNN and faster R-CNN are 95.4% and 96.8%, respectively. The goal of this model is to guide junior operators during the assembly by providing step-by-step instructions in performing complex tasks. The present study facilitates flexible training in terms of adapting new skills from skilled operators to naïve operators by deep learning.}
}
@article{RASHEED201686,
title = {Theoretical accounts to practical models: Grounding phenomenon for abstract words in cognitive robots},
journal = {Cognitive Systems Research},
volume = {40},
pages = {86-98},
year = {2016},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2016.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041715300310},
author = {Nadia Rasheed and Shamsudin H.M. Amin and U. Sultana and Rabia Shakoor and Naila Zareen and Abdul Rauf Bhatti},
keywords = {Grounded cognition, Symbol grounding problem, Cognitive robotics, Connectionist computation},
abstract = {This review concentrates on the issue of acquisition of abstract words in a cognitive robot with the grounding principle, from relevant theories to practical models of agents and robots. Most cognitive robotics models developed for grounding of language take inspiration from the findings of neuroscience and psychology to get the theoretical skeleton of these models. To better understand these modelling approaches, it is indispensable to work from the base (theoretical accounts) to the top (computational models). Therefore in this paper, succinct definition of abstract words is presented first, and then the symbol grounding issue and accounts of grounded cognition for abstract words are given. The next section discusses the computational modelling approaches for abstract words grounding phenomenon. Finally, important cognitive robotics models are reviewed. This paper also points out the strengths and weaknesses of relevant hypotheses and models for the representation of abstract words in the grounded cognition framework and helps the understanding of issues such as where and why modelling efforts stand to address this problem in comparison with theoretical findings.}
}
@article{ZHU2024102509,
title = {Developing a fast and accurate collision detection strategy for crane-lift path planning in high-rise modular integrated construction},
journal = {Advanced Engineering Informatics},
volume = {61},
pages = {102509},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102509},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624001575},
author = {Aimin Zhu and Zhiqian Zhang and Wei Pan},
keywords = {Crane-lift, Path planning, Collision detection, Modular integrated construction},
abstract = {Crane-lift path planning (CLPP) ensures the safe and efficient installation of hefty modules in high-rise modular integrated construction (MiC). The implementation of CLPP requires effective collision detection strategies. However, existing collision detection strategies suffer from limitations in terms of computational intensity or insufficient accuracy. This paper aims to develop a fast and accurate collision detection strategy for CLPP in high-rise MiC projects using a single tower crane, thereby achieving safe and efficient module installation. It is executed with the assumptions that the geometry of the building remains unchanged, the positions and orientations of the lifted module and the tower crane are monitored, and no external loads act on the lifted module. Based on the research scope and assumptions, an octree and bounding box (Oct-Box) integrated strategy is developed. The strategy operates in two stages, the pre-execution and execution stages, supported by two critical technical components: (1) an optimized octree for lifting space division and encoding, and (2) an integrated bounding box algorithm for construction object collision detection. The strategy was evaluated using a real-life MiC project in Hong Kong. The results show that the developed strategy minimized the CLPP time by about 95 %, while ensuring continuous and accurate collision detection. In addition, the strategy was significantly affected by the depth of octree, the encoding method of octree, the bounding box algorithm and the configuration density. The developed Oct-Box strategy for CLPP is novel as it addresses temporal efficiency and spatial tightness in tandem, and marks a breakthrough for collision detection in modular construction.}
}
@article{COLOMBINI2022104631,
title = {Safety evaluations on unignited high-pressure methane jets impacting a spherical obstacle},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {74},
pages = {104631},
year = {2022},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2021.104631},
url = {https://www.sciencedirect.com/science/article/pii/S0950423021002400},
author = {Cristian Colombini and Edoardo Carminati and Andrea Parisi and Renato Rota and Valentina Busini},
keywords = {High-pressure release, Methane, Spherical obstacle influence, Risk assessment, CFD, Analytical correlation},
abstract = {Nowadays methane is a fossil fuel widely used both in industries and in civil appliances. From the safety point of view, due to its flammability, its use implies hazards for people and assets. The hazardous area related to a high-pressure jet of methane arising from an accidental loss of containment requires the estimation of the distance at which the methane concentration falls below the Lower Flammability limit. Such a topic is well covered in the literature when considering free jet conditions, i.e., jets that do not interact with any equipment or surface. The same cannot be said for high pressure jets impacting an obstacle. In this context, the present work focuses on studying high pressure methane jets impacting spherical obstacles by means of Computational Fluid Dynamics with the aim of giving some insights about such a jet-obstacle interaction, possibly providing a brief by-hand procedure that, only based on known scenario information, allows to estimate the maximum extent of the unignited high-pressure jet when interacting with a spherical obstacle.}
}
@article{BENTO2025120579,
title = {Risk analysis in ocean and maritime engineering},
journal = {Ocean Engineering},
volume = {322},
pages = {120579},
year = {2025},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2025.120579},
url = {https://www.sciencedirect.com/science/article/pii/S002980182500294X},
author = {Ana Margarida Bento and Tiago Fazeres-Ferradosa and Paulo Rosa-Santos and Francisco Taveira-Pinto}
}
@article{R2023100760,
title = {Stellar parameter estimation in O-type stars using artificial neural networks},
journal = {Astronomy and Computing},
volume = {45},
pages = {100760},
year = {2023},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2023.100760},
url = {https://www.sciencedirect.com/science/article/pii/S2213133723000756},
author = {M. Flores R. and L.J. Corral and C.R. Fierro-Santillán and S.G. Navarro},
keywords = {Methods: Data analysis, Deep learning, Stars: Fundamental parameters, Astronomical databases: Miscellaneous},
abstract = {This work presents the results of the implementation of a deep learning system capable of estimating the effective temperature and surface gravity of O-type stars. The proposed system was trained with a database of 5,557 synthetic spectra computed with the stellar atmosphere code CMFGEN that covers stars with Teff from ∼20,000 K to ∼58,000 K, log(L/L⊙) from 4.3 to 6.3 dex, logg from 2.4 to 4.2 dex, and mass from 9 to 120 M⊙. Important advantages proposed in this paper include using a set of equivalent width measurements over the optical region of the stellar spectra, which avoids processing the full spectra with the inherent computational cost and allows it to apply the same trained system over different spectra resolutions. The validation of the system was performed by processing a sample of twenty O-type stars taken from the IACOB database, and a subgroup of eleven stars of those twenty taken from The Galactic O-Star Spectroscopic Catalog (GOSC) with lower resolution. As complementary work, we show the results of a synthetic spectra fitting process with the aim of simplifying the comparison with other estimations and parameter fitting from the literature.}
}
@article{PURI2023104439,
title = {Automatic detection of Alzheimer’s disease from EEG signals using low-complexity orthogonal wavelet filter banks},
journal = {Biomedical Signal Processing and Control},
volume = {81},
pages = {104439},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.104439},
url = {https://www.sciencedirect.com/science/article/pii/S174680942200893X},
author = {Digambar V. Puri and Sanjay L. Nalbalwar and Anil B. Nandgaonkar and Jayanand P. Gawande and Abhay Wagh},
keywords = {Alzheimer’s disease, Electroencephalogram, Fractal dimension, Orthogonal filter banks, Support vector machine, Wavelets},
abstract = {Background:
Alzheimer’s disease (AD) is one of the most common neurodegenerative disorder. As the incidence of AD is rapidly increasing worldwide, detecting it at an early stage can prevent memory loss and cognitive dysfunctions in patients. Recently, Electroencephalogram (EEG) signals in AD cases show less synchronization and a slowing effect. The abrupt and transient behavior of EEG signals can be detected from specific frequency bands that are cortical rhythms of interest such as delta (0−4Hz), theta (4−8Hz), alpha (8−12Hz), beta1 (12−16Hz), beta2 (16−32Hz), and gamma (32−48Hz).
Method:
This paper proposes novel low-complexity orthogonal wavelet filter banks with vanishing moments (LCOWFBs-v) to decompose the AD and normal controlled (NC) EEG signals into subbands (SBs). A generalized design technique is suggested to reduce the computational complexity of original irrational wavelet filter banks (FBs). The two features, Higuchi’s fractal dimension (HFD) and Katz’s fractal dimension (KFD), were extracted from EEG SBs. The significance of these extracted features has been inspected using Kruskal–Wallis test.
Results:
The present study analyzed the EEG recordings of 23 subjects (AD-12 and NC-11) with the combination of LCOWFBs, HFD, and KFD. The proposed technique achieved a classification accuracy of 98.5% and 98.6% using the LCOWFBs-4 and LCOWFBs-6, respectively with a cubic-support vector machine classifier and 10-fold cross-validation technique.
Conclusion:
The proposed method with newly designed LCOWFBs is efficient compared with the well-known FBs and existing techniques for detecting AD.}
}
@article{BAMU20051794,
title = {Damage, deterioration and the long-term structural performance of cooling-tower shells: A survey of developments over the past 50 years},
journal = {Engineering Structures},
volume = {27},
number = {12},
pages = {1794-1800},
year = {2005},
note = {SEMC 2004 Structural Health Monitoring, Damage Detection and Long-Term Performance},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2005.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S0141029605002257},
author = {P.C. Bamu and A. Zingoni},
keywords = {Cooling towers, Shell structures, Long-term performance, Damage modelling, Deterioration phenomena, Concrete cracking, Shell imperfections, Durability},
abstract = {The last 50 years have seen a gradual shift in trend in research on concrete hyperbolic cooling-tower shells, from the issues of response to short-term loading and immediate causes of collapse in the early part of this period, to the issues of deterioration phenomena, durability and long-term performance in more recent times. This paper traces these developments. After a revisit of some historical collapses of cooling-tower shells, and a brief consideration of condition surveys and repair programmes instituted in the aftermath of these events, focus shifts to the important question of damage and deterioration, and progress made over the past 30 years in the understanding of these phenomena. In particular, much research has gone into the modelling of cracking and geometric imperfections, which have a considerable effect on the load-carrying capacity of the shell, and are also manifestations of long-term deterioration. While structural monitoring of the progression of deterioration in cooling-tower shells, and the accurate prediction of this through appropriate numerical models, will always be important, the thinking now seems to be shifting towards designing for durability right from the outset.}
}
@article{ROBERTS2017225,
title = {Clinical Applications of Stochastic Dynamic Models of the Brain, Part II: A Review},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {2},
number = {3},
pages = {225-234},
year = {2017},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2016.12.009},
url = {https://www.sciencedirect.com/science/article/pii/S2451902217300149},
author = {James A. Roberts and Karl J. Friston and Michael Breakspear},
keywords = {Computational neuroscience, Computational psychiatry, Epilepsy, Mathematical modeling, Melancholia, Stochastic},
abstract = {Brain activity derives from intrinsic dynamics (due to neurophysiology and anatomical connectivity) in concert with stochastic effects that arise from sensory fluctuations, brainstem discharges, and random microscopic states such as thermal noise. The dynamic evolution of systems composed of both dynamic and random fluctuations can be studied with stochastic dynamic models (SDMs). This article, Part II of a two-part series, reviews applications of SDMs to large-scale neural systems in health and disease. Stochastic models have already elucidated a number of pathophysiological phenomena, such as epilepsy and hypoxic ischemic encephalopathy, although their use in biological psychiatry remains rather nascent. Emerging research in this field includes phenomenological models of mood fluctuations in bipolar disorder and biophysical models of functional imaging data in psychotic and affective disorders. Together with deeper theoretical considerations, this work suggests that SDMs will play a unique and influential role in computational psychiatry, unifying empirical observations with models of perception and behavior.}
}
@incollection{SHERIDAN201023,
title = {Chapter 2 - The System Perspective on Human Factors in Aviation},
editor = {Eduardo Salas and Dan Maurino},
booktitle = {Human Factors in Aviation (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {23-63},
year = {2010},
isbn = {978-0-12-374518-7},
doi = {https://doi.org/10.1016/B978-0-12-374518-7.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012374518700002X},
author = {Thomas B. Sheridan},
abstract = {Publisher Summary
This chapter reviews the system perspective in terms of its origins and fundamental quantitative ideas. An appreciation of these basic concepts adds rigor to analysis and synthesis of human-machine systems, and in particular to such systems in aviation. The chapter represents an effort to remind the reader of the meaning of “system,” where it comes from, and what it implies for research, design, construction, operation, and evaluation in aviation, especially with regard to the human role in aviation. Human factors professionals, pilots, and operational personnel in air traffic management and related practitioners who know about “systems” only as a general and often vague term for something complex can benefit from knowing a bit of the history, the people, and the quantitative substance that underlies the terminology. The chapter begins by defining what is meant by a system, then discusses the history of the idea, the major contributors and what they contributed, and what made the systems idea different from previous ideas in technology. It goes on to give examples of systems thinking applied to design, development, and manufacturing of aviation systems in consideration of the people involved. Salient system models such as control, decision, information, and reliability are then explicated.}
}
@article{HUEY2022101211,
title = {Assessing the impact of standards-based grading policy changes on student performance and practice work completion in secondary mathematics},
journal = {Studies in Educational Evaluation},
volume = {75},
pages = {101211},
year = {2022},
issn = {0191-491X},
doi = {https://doi.org/10.1016/j.stueduc.2022.101211},
url = {https://www.sciencedirect.com/science/article/pii/S0191491X22000888},
author = {Maryann E. Huey and Patrick R. Silvey and Amy G. Vaughan and Asa L. Fisher},
keywords = {Grading, Standards-based grading, Mathematics, Secondary, Motivation, High-achieving students},
abstract = {We report upon an intervention study conducted over two academic calendar years involving high-achieving, grade 8 and 9 students (n = 122 and 123 respectively) enrolled in a year-long geometry course. The study assesses the impact of a change in grading policy, namely removing practice work from grade computations, on student performance levels and behaviors. After the change in grading policy was implemented, findings reveal that performance decreased on some, but not all, standards assessed. Completion rates of practice work also decreased overall. Potential causes are discussed as well as implications for implementing aspects of standards-based grading systems in secondary mathematics classrooms.}
}
@article{SZUBA2001489,
title = {A formal definition of the phenomenon of collective intelligence and its IQ measure},
journal = {Future Generation Computer Systems},
volume = {17},
number = {4},
pages = {489-500},
year = {2001},
note = {Workshop on Bio-inspired Solutions to Parallel Computing problems},
issn = {0167-739X},
doi = {https://doi.org/10.1016/S0167-739X(99)00136-3},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X99001363},
author = {Tadeusz Szuba},
keywords = {Collective intelligence, Quasi-chaotic model of computations, Synergy, IQ, PROLOG},
abstract = {This paper presents a formalization of collective intelligence (CI). A molecular, quasi-chaotic model of computations allows us to model CI in social structures, and to define its measure (IQS). This methodology works for bacterial colonies and social insects as well as for human social structures. With the CI theory some patterns of human behavior receive formal justification, others can be explained as IQS optimization. The CI formalization assumes that it is a property of a social structure, initializing when individuals interact, and as a result, acquiring the ability to solve new or more complex problems. CI amplifies if the structure improves synergy, which further increases the spectrum and complexity of the problems, which can be solved together. The formalization covers areas where CI results in physical synergy and mental/logical cooperation.}
}
@article{MOGLIA2017173,
title = {A review of Agent-Based Modelling of technology diffusion with special reference to residential energy efficiency},
journal = {Sustainable Cities and Society},
volume = {31},
pages = {173-182},
year = {2017},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2017.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S2210670716306813},
author = {Magnus Moglia and Stephen Cook and James McGregor},
keywords = {Agent-Based Modelling, Diffusion of innovation, HVAC, Lighting, Appliances},
abstract = {Residential energy efficiency is an important strategy for reducing greenhouse gas emissions. There are many technologies that help improve residential energy efficiency, and in fact, increased energy efficiency has already helped reduce global greenhouse gas emissions significantly in the past. However, with greater innovation, further improvements can be made and improving energy efficiency is an ongoing activity. Policymakers around the world are putting strategies in place to speed up the adoption of energy efficient technologies and practices, but ultimately this process is based on choice by residents themselves. Human decision making and choice however is a very complex issue, and complex computational tools are required in order to analyse and/or predict the impact of various policies. Traditionally, equation-based models such as Bass and Choice models have been used to describe the diffusion of technologies in a population, but certain limitations have been identified. This article explores what these limitations are in the context of energy efficient residential technologies and how an alternative computational and empirical paradigm, Agent-Based Modelling (ABM), can help resolve some of these limitations. As such, this is a review article into how ABM can support analysis of strategies to catalyse greater uptake of energy efficiency in the residential sector.}
}
@article{ABRUSCI2025100401,
title = {AI4Design: A generative AI-based system to improve creativity in design–A field evaluation},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100401},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100401},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000414},
author = {Luca Abrusci and Karma Dabaghi and Stefano D'Urso and Filippo Sciarrone},
keywords = {Creativity, Design, Generative artificial intelligence},
abstract = {Chatbots serve as valuable instruments for enhancing students' educational experience and aiding them in their day-to-day academic tasks. Advances in Generative AI (GAI) have ushered in increasingly sophisticated and adaptive chatbots, with ChatGPT and DALL⋅E being prime examples. ChatGPT excels at generating text-based answers across diverse areas of knowledge, while DALL⋅E is adept at converting text-based concepts into visual imagery. These technologies are increasingly used by students across various levels of education. In this study, we introduce AI4Design, a web-based system designed to assist design students with their course projects by acting as an intelligent chatbot. The field of design is propitious for such work because of the increasing use of technology and the necessity of introducing its critical use during study. Comprising two integrated modules, the system is based on a two-step workflow. The first step is anchored on ChatGPT, enabling students to prompt questions and receive answers. The second step allows for the generation of one or more images based on the system's answer to the initial question. Our research assesses whether our system can offer valuable insights and inspiration to students in their design work. We conducted an exploratory study in the Design domain involving 31 students from the Lebanese American University. Over a two- to three-day period, participants used the AI4Design system to enhance their projects. A subsequent evaluation of their work indicated improvements in conceptual clarity and visual outputs that highlighted a measurable increase in creativity, supporting the efficacy of both the system and its foundational learning model, which will be confirmed in the future through a large-scale experimental study. Meanwhile, our study suggests that in the iterative design process, GAI can assist students in making better decisions by giving them just-in-time access to a broader palette of possibilities.}
}
@article{CHEEMA2022100123,
title = {Augmented Intelligence to Identify Patients With Advanced Heart Failure in an Integrated Health System},
journal = {JACC: Advances},
volume = {1},
number = {4},
pages = {100123},
year = {2022},
issn = {2772-963X},
doi = {https://doi.org/10.1016/j.jacadv.2022.100123},
url = {https://www.sciencedirect.com/science/article/pii/S2772963X22001739},
author = {Baljash Cheema and R. Kannan Mutharasan and Aditya Sharma and Maia Jacobs and Kaleigh Powers and Susan Lehrer and Firas H. Wehbe and Jason Ronald and Lindsay Pifer and Jonathan D. Rich and Kambiz Ghafourian and Anjan Tibrewala and Patrick McCarthy and Yuan Luo and Duc T. Pham and Jane E. Wilcox and Faraz S. Ahmad},
keywords = {advanced heart failure, artificial intelligence, augmented intelligence, electronic health record, integrated healthcare system, machine learning},
abstract = {Background
Timely referral for specialist evaluation in patients with advanced heart failure (HF) is a Class 1 recommendation. However, the transition from stage C HF to advanced or stage D HF often goes undetected in routine care, resulting in delayed referral and higher mortality rates.
Objectives
The authors sought to develop an augmented intelligence-enabled workflow using machine learning to identify patients with stage D HF and streamline referral.
Methods
We extracted data on HF patients with encounters from January 1, 2007, to November 30, 2020, from a HF registry within a regional, integrated health system. We created an ensemble machine learning model to predict stage C or stage D HF and integrated the results within the electronic health record.
Results
In a retrospective data set of 14,846 patients, the model had a good positive predictive value (60%) and low sensitivity (25%) for identifying stage D HF in a 100-person, physician-reviewed, holdout test set. During prospective implementation of the workflow from April 1, 2021, to February 15, 2022, 416 patients were reviewed by a clinical coordinator, with agreement between the model and the coordinator in 50.3% of stage D predictions. Twenty-four patients have been scheduled for evaluation in a HF clinic, 4 patients started an evaluation for advanced therapies, and 1 patient received a left ventricular assist device.
Conclusions
An augmented intelligence-enabled workflow was integrated into clinical operations to identify patients with advanced HF. Endeavors such as this require a multidisciplinary team with experience in design thinking, informatics, quality improvement, operations, and health information technology, as well as dedicated resources to monitor and improve performance over time.}
}
@article{BARTH2009441,
title = {Children’s multiplicative transformations of discrete and continuous quantities},
journal = {Journal of Experimental Child Psychology},
volume = {103},
number = {4},
pages = {441-454},
year = {2009},
note = {Special Issue: Typical Development of Numerical Cognition},
issn = {0022-0965},
doi = {https://doi.org/10.1016/j.jecp.2009.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S0022096509000289},
author = {Hilary Barth and Andrew Baron and Elizabeth Spelke and Susan Carey},
keywords = {Ratio sensitivity, Ratios, Multiplicative operations, Doubling, Halving, Numerical cognition},
abstract = {Recent studies have documented an evolutionarily primitive, early emerging cognitive system for the mental representation of numerical quantity (the analog magnitude system). Studies with nonhuman primates, human infants, and preschoolers have shown this system to support computations of numerical ordering, addition, and subtraction involving whole number concepts prior to arithmetic training. Here we report evidence that this system supports children’s predictions about the outcomes of halving and perhaps also doubling transformations. A total of 138 kindergartners and first graders were asked to reason about the quantity resulting from the doubling or halving of an initial numerosity (of a set of dots) or an initial length (of a bar). Controls for dot size, total dot area, and dot density ensured that children were responding to the number of dots in the arrays. Prior to formal instruction in symbolic multiplication, division, or rational number, halving (and perhaps doubling) computations appear to be deployed over discrete and possibly continuous quantities. The ability to apply simple multiplicative transformations to analog magnitude representations of quantity may form a part of the toolkit that children use to construct later concepts of rational number.}
}
@article{HOGAN200855,
title = {Advancing the dialogue between inner and outer empiricism: A comment on O’Nualláin},
journal = {New Ideas in Psychology},
volume = {26},
number = {1},
pages = {55-68},
year = {2008},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2007.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X07000293},
author = {Michael J. Hogan},
keywords = {Consciousness, Inner empiricism, Outer empiricism, Evolution, No-mind, Mythos, Logos},
abstract = {In a recent contribution to New Ideas in Psychology, Seán O’Nualláin draws out a distinction between inner and outer empiricism, and suggests that consciousness research can benefit from analysis in both directions, that is, via the exploration of facts and relations that facilitate a third-person understanding of consciousness (by reference to an analysis of the structures, processes, and functions of the brain) and via the direct exploration of conscious experience itself, both in terms of its computational (content filled) and non-computational (content empty) aspects. In positing a substrate of subjectivity independent of the contents of consciousness (and, more specifically, a state of “nothingness”), Ò’Nualláin follows a long tradition deeply rooted in mythical, religious, and esoteric schools of belief and practice. Although there is considerable debate amongst philosophers, psychologists, and neuroscientists as to whether or not a non-computational view of consciousness is viable, O’Nualláin accepts that such a possibility does exist. Further, he suggests that a dialogue between the inner and outer empiricists will be fruitful. In this comment I, critique Ò’Nualláin's initial thoughts on the subject and draw out a series of useful distinctions that will help to advance the dialogue between inner and outer empiricism. Critical amongst these distinctions is explicit reference to (1) ontological and epistemological interdependencies in consciousness research, and (2) states of consciousness that describe the transition from “mindfulness” through “nothingness” to “no-mind”.}
}
@article{MIKITEN1995141,
title = {Intuition-based computing: A new kind of ‘virtual reality’},
journal = {Mathematics and Computers in Simulation},
volume = {40},
number = {1},
pages = {141-147},
year = {1995},
issn = {0378-4754},
doi = {https://doi.org/10.1016/0378-4754(95)00023-1},
url = {https://www.sciencedirect.com/science/article/pii/0378475495000231},
author = {Terry M. Mikiten},
keywords = {Intuition, Mind, Problem-solving, Creativity, Cognition, Computing, Grand challenge},
abstract = {It is helpful to consider the mind and the computer as two separate information domains. Each has a separate system of rules that guide behavior. The interaction between the two is characterized as an interplay between rule systems. In this view, there should be interactions which are optimal and others which are not. To understand this, the first task is to identify the rules that operate in each domain. The next is to see how they interact. It is concluded that rules of the mind which give rise to what is generally termed ‘intuition’ is altogether compatible with rules of computation. This, in turn, suggests computational systems capable of independent ‘intuitive’ processing on the one hand, and other computational systems which can serve to augment human intuition.}
}
@article{HYLAND2007437,
title = {The Category Theoretic Understanding of Universal Algebra: Lawvere Theories and Monads},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {172},
pages = {437-458},
year = {2007},
note = {Computation, Meaning, and Logic: Articles dedicated to Gordon Plotkin},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2007.02.019},
url = {https://www.sciencedirect.com/science/article/pii/S1571066107000874},
author = {Martin Hyland and John Power},
keywords = {Universal algebra, Lawvere theory, monad, computational effect},
abstract = {Lawvere theories and monads have been the two main category theoretic formulations of universal algebra, Lawvere theories arising in 1963 and the connection with monads being established a few years later. Monads, although mathematically the less direct and less malleable formulation, rapidly gained precedence. A generation later, the definition of monad began to appear extensively in theoretical computer science in order to model computational effects, without reference to universal algebra. But since then, the relevance of universal algebra to computational effects has been recognised, leading to renewed prominence of the notion of Lawvere theory, now in a computational setting. This development has formed a major part of Gordon Plotkin's mature work, and we study its history here, in particular asking why Lawvere theories were eclipsed by monads in the 1960's, and how the renewed interest in them in a computer science setting might develop in future.}
}
@article{BAUCELLS201329,
title = {Guided decisions processes},
journal = {EURO Journal on Decision Processes},
volume = {1},
number = {1},
pages = {29-44},
year = {2013},
issn = {2193-9438},
doi = {https://doi.org/10.1007/s40070-013-0003-8},
url = {https://www.sciencedirect.com/science/article/pii/S2193943821000108},
author = {Manel Baucells and Rakesh K. Sarin},
keywords = {Decision analysis, Behavioral decision making, Narrow bracket, Insurance, Multi-attribute decisions},
abstract = {The heuristics and bias research program has convincingly demonstrated that our judgments and choices are prone to systematic errors. Decision analysis requires coherent judgments about beliefs (probabilities) and tastes (utilities), and a rational procedure to combine them so that choices maximize subjective expected utility. A guided decision process is a middle-of-the-road between decision analysis and intuitive judgments in which the emphasis is on improving decisions through simple decision rules. These rules reduce cost of thinking, or decision effort, for the myriad decisions that one faces in daily life; but at the same time, they are personalized to the individual and produce near optimal choices. We discuss the principles behind the guided decision processes research program, and illustrate the approach using several examples.}
}
@article{DORR2017322,
title = {Common errors in reasoning about the future: Three informal fallacies},
journal = {Technological Forecasting and Social Change},
volume = {116},
pages = {322-330},
year = {2017},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2016.06.018},
url = {https://www.sciencedirect.com/science/article/pii/S0040162516301275},
author = {Adam Dorr},
keywords = {Technological progress, Accelerating change, Computing, Fallacy, Errors in reasoning},
abstract = {The continued exponential growth of the price-performance of computing is likely to effectuate technologies that radically transform both the global economy and the human condition over the course of this century. Conventional visions of the next 50years fail to realistically account for the full implications of accelerating technological change driven by the exponential growth of computing, and as a result are deeply flawed. These flawed visions are, in part, a consequence of three interrelated errors in reasoning: 1) the linear projection fallacy, 2) the ceteris paribus fallacy, and 3) the arrival fallacy. Each of these informal fallacies is likely a manifestation of shortcomings in our intuitions about complex dynamic systems. Recognizing these errors and identifying when and where they affect our own reasoning is an important first step toward thinking more realistically about the future.}
}
@article{TRAUTTEUR2007106,
title = {A note on discreteness and virtuality in analog computing},
journal = {Theoretical Computer Science},
volume = {371},
number = {1},
pages = {106-114},
year = {2007},
note = {Computing and the Natural Sciences},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2006.10.017},
url = {https://www.sciencedirect.com/science/article/pii/S0304397506007699},
author = {Giuseppe Trautteur and Guglielmo Tamburrini},
keywords = {Analog computing, Virtual machine, Cognitive modelling},
abstract = {The need for physically motivated discreteness and finiteness conditions emerges in models of both analog and digital computing that are genuinely concerned with physically realizable computational processes. This is brought out by a critical examination of notional analog superTuring devices which involve physically untenable idealizations about the perfect functioning of analog apparatuses and infinite precision of physical measurements. The capability for virtual behaviour, that is, the capability of interpreting, storing, transforming, creating the code, and thereby mimicking the behaviour of (Turing) machines, is used here to introduce a new dimension in the discussion of the analog–digital watershed. In the light of recent results on the analog simulation of digital computing, we examine the role of virtuality as a discriminating factor between these two species of computing, and immerse this problem in the context of natural computing. Is virtuality instantiated in parts of the natural world other than computer technology? This broad issue is examined in connection with the computational modelling of brain and mental information processing.}
}
@article{LANG2017298,
title = {Mesoscopic Simulation Models for Logistics Planning Tasks in the Automotive Industry},
journal = {Procedia Engineering},
volume = {178},
pages = {298-307},
year = {2017},
note = {RelStat-2016: Proceedings of the 16th International Scientific Conference Reliability and Statistics in Transportation and Communication October 19-22, 2016. Transport and Telecommunication Institute, Riga, Latvia},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.01.118},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817301182},
author = {Sebastian Lang and Tobias Reggelin and Toralf Wunder},
keywords = {automotive industry, logistics planning, production planning, mesoscopic simulation, discrete-rate simulation},
abstract = {The paper evaluates mesoscopic simulation models applied to logistics planning tasks in the automotive industry. In terms of level of detail, mesoscopic simulation models fall between object based discrete-event simulation models and flow based continuous simulation models. Mesoscopic models represent logistics flow processes on an aggregated level through piecewise constant flow rates instead of modeling individual flow objects. The results are not obtained by counting individual objects but by using mathematical formulas to calculate the results as continuous quantities in every modeling time step. This leads to a fast model creation and computation. The authors expect that mesoscopic simulation models can help to support decisions on the operational, tactical and strategic level of planning. The paper describes a mesoscopic simulation model of the goods receiving of an assembly plant and compares the simulation results and computation time with a discrete-event model.}
}
@article{CARVAJALRODRIGUEZ201576,
title = {Incorporación de la programación informática en el currículum de Biología},
journal = {Magister},
volume = {27},
number = {2},
pages = {76-82},
year = {2015},
issn = {0212-6796},
doi = {https://doi.org/10.1016/j.magis.2015.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0212679615000286},
author = {Antonio Carvajal-Rodríguez},
keywords = {Docencia, Bioinformática, Biología computacional, Python, Teaching, Bioinformatics, Computational biology, Python},
abstract = {Resumen
La investigación en biología ha cambiado radicalmente debido al efecto combinado de los avances en biotecnología y ciencias de la computación. En consecuencia, la biología computacional y la bioinformática son tan esenciales para la biología del siglo xxi como la biología molecular lo fue en el anterior. Sin embargo, las competencias correspondientes a razonamiento matemático y computacional en el currículo de Biología apenas han cambiado en los últimos 25 años. La formación del biólogo debería ser tan sofisticada desde el punto de vista computacional como la del físico o la del ingeniero. La incorporación de estos cambios requiere tanto de un mayor esfuerzo de integración de las asignaturas cuantitativas existentes en el ámbito de los problemas biológicos como de la contextualización de las asignaturas propias de la biología desde un punto de vista más formal y de modelización. En este trabajo se revisan algunos de los esfuerzos que en este sentido se están haciendo en el panorama internacional y se presenta también la experiencia del autor en el diseño e impartición de un curso de iniciación a la programación para biólogos usando una metodología de aprendizaje basado en problemas.
The joint effect of biotechnology and computing has changed the research in biology. Consequently, computational biology is as essential for 21st-century biologists as molecular biology was in the 20th. However, Biology curricula have little emphasis in quantitative thinking and computation. The education for biologists should become as sophisticated as the computational education of physicists and engineers. The necessary changes to reach this goal require the connection of mathematics and quantitative subjects with real biological problems and at the same time, teaching some biological subjects from a modeling and computational perspective. In the present work, some of the current international effort in this path is reviewed and additionally, the author's experience when teaching an introduction to programming for biologists is presented.}
}
@article{DEGRANDE201960,
title = {To add or to multiply? An investigation of the role of preference in children's solutions of word problems},
journal = {Learning and Instruction},
volume = {61},
pages = {60-71},
year = {2019},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2019.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0959475218304511},
author = {Tine Degrande and Lieven Verschaffel and Wim {Van Dooren}},
keywords = {Word problem solving, Additive reasoning, Multiplicative reasoning, Preference, Skill},
abstract = {Previous research has shown that upper primary school children frequently erroneously solve additive word problems multiplicatively, while younger children frequently erroneously solve multiplicative word problems additively. It has been suggested that children's preference for additive or multiplicative relations explains these errors, besides their lacking skills, but this claim has not been tested empirically yet. Therefore, we administered four test instruments (a word problem test, a preference test, and two tests measuring additive and multiplicative computation and discrimination skill) to 246 third to sixth graders. Previous research results on errors in word problems, as well as on preference were replicated and systematized. Further, they were extended by explaining this erroneous word problem solving behavior by preference, for those children who unmistakably had acquired the necessary computation and discrimination skills. This finding provides strong evidence for the unique additional role of children's preference in erroneous additive or multiplicative word problem solving behavior.}
}
@incollection{VALLERO2025503,
title = {Chapter 19 - The future},
editor = {Daniel A. Vallero},
booktitle = {Fundamentals of Water Pollution},
publisher = {Elsevier},
pages = {503-504},
year = {2025},
isbn = {978-0-443-28987-3},
doi = {https://doi.org/10.1016/B978-0-443-28987-3.00014-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044328987300014X},
author = {Daniel A. Vallero},
keywords = {Emerging water pollutants, Emerging treatment technologies, Environmental communications},
abstract = {Those chapter concludes the book with a discussion of some of the successes of water pollution controls and water supply, along with remaining challenges.}
}
@article{BENTLEY2000465,
title = {Statistics and archaeology in Israel},
journal = {Computational Statistics & Data Analysis},
volume = {32},
number = {3},
pages = {465-483},
year = {2000},
issn = {0167-9473},
doi = {https://doi.org/10.1016/S0167-9473(99)00094-8},
url = {https://www.sciencedirect.com/science/article/pii/S0167947399000948},
author = {Jim Bentley and Tammi J Schneider},
keywords = {Variability, Density estimation, Kriging, Mapping: Surface survey, Archaeology Archaeometrics},
abstract = {While the field of statistics is fairly young, the field of archaeology is quite old. Modern archaeology prides itself on its ability to glean maximum information about the past from minimal information collected in the present. This paper attempts to show how the application of statistical thinking and techniques can aid the archaeologist in retrieving as much information as possible from artifacts; thus allowing the archaeologists to leave the majority of a site for future generations. In the past few years, archaeologists working in Israel have joined forces with statisticians in an attempt to generate more accurate recordings of archaeological information than is currently the standard in the Middle East. Careful application of statistical methods has reduced collection time and improved the display of archaeological information. An understanding of statistical concepts such as variability and density estimation has already been shown to be of use to archaeologists. Conversely, the use of examples from the field have proven to be of use in motivating humanities students to learn about statistical thinking. Archaeology has also provided a field in which students of statistics may apply their new found knowledge. The combination of statistics and archaeology is clearly of benefit to both disciplines.}
}
@article{KARUNA2019161,
title = {Capital markets research in accounting: Lessons learnt and future implications},
journal = {Pacific-Basin Finance Journal},
volume = {55},
pages = {161-168},
year = {2019},
issn = {0927-538X},
doi = {https://doi.org/10.1016/j.pacfin.2019.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0927538X19301398},
author = {Christo Karuna},
abstract = {I review the capital markets literature in accounting by describing the journey taken by researchers since the inception of this stream of research in the late 1960s. Based on a discussion of topics related to the relation between earnings and stock returns, I show how thinking has evolved depending on changing paradigms, methodologies, and data availability. What is clear from a review of the literature is that the usefulness of earnings in determining firm value is both contextual and broadening over time with changes in the global environment. Thus, more research needs to be conducted on a broader notion of earnings that appeals to not just the shareholder but a wide range of firm stakeholders.}
}
@article{WANG2024130178,
title = {A measurement-device-independent quantum secure digital payment},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {655},
pages = {130178},
year = {2024},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2024.130178},
url = {https://www.sciencedirect.com/science/article/pii/S0378437124006873},
author = {Qingle Wang and Jiacheng Liu and Guodong Li and Yunguang Han and Yuqian Zhou and Long Cheng},
keywords = {Quantum digital payment, Measurement-device-independent, Quantum secure communication, Authentication},
abstract = {In contemporary society, digital payment systems are crucial, yet vulnerable to security breaches. Based on the principles of quantum physics, quantum digital payment (QDP) protocols offer a theoretically superior security paradigm compared to those reliant on computational complexity. Nevertheless, those QDP protocols in practice are frequently compromised by imperfections in measurement devices, facilitating valuable information interception by malicious entities. Addressing this vulnerability, we propose a measurement-device-independent quantum secure digital payment (MDI-QSDP) protocol, designed to enhance security in digital payment systems by eliminating side-channel attacks on measurement devices. This protocol extends the framework of a novelly developed measurement-device-independent quantum secure communication (MDI-QSC) protocol, which supports secure dialogic exchanges without prior key sharing. Utilizing the proposed MDI-QSC protocol, participants can not only engage in secure direct communication but also establish a private key for subsequent encrypted interactions. Our MDI-QSDP protocol incorporates a robust authentication mechanism, ensuring that only legitimate participants can initiate transactions, thereby bolstering security. A comprehensive security analysis of the proposed protocol demonstrates its resilience against identity theft, information leakage, and other potential security breaches. Furthermore, simulations employing practical experimental parameters validate the protocol’s applicability and effectiveness in real-world scenarios, thereby confirming its potential to significantly enhance the security of future quantum digital payments.}
}
@article{CHEAH2025100363,
title = {Integrating generative artificial intelligence in K-12 education: Examining teachers’ preparedness, practices, and barriers},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100363},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100363},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000037},
author = {Yin Hong Cheah and Jingru Lu and Juhee Kim},
keywords = {Generative artificial intelligence, In-service teachers, Preparedness, Practices, Barriers, K-12 education},
abstract = {Despite the growing body of research on developing K-12 teachers' generative AI (GenAI) knowledge and skills, its integration into daily teaching practices remains underexplored. Using a snowball sampling method, this study examined the preparedness, practices, and barriers encountered by 89 U.S. teachers in the state of Idaho. Participants were predominantly White, female teachers serving in rural schools. A mixed-methods analysis of survey responses revealed that teachers were generally underprepared for integrating GenAI, with fewer than half incorporating it into their educational practices. Unlike the widespread classroom integration patterns observed with general educational technologies, teachers in this study tended to use GenAI for out-of-classroom duties (i.e., lesson preparation, assessment, and administrative tasks) rather than for real-time teaching and learning. These preferences could be attributed to key barriers teachers faced, including doubts about GenAI's ability to manage risks (i.e., technology value beliefs), reduced human interaction in instruction (i.e., pedagogical beliefs), ethical considerations, and the absence of policies and guidance. This study highlights the need to develop support systems and targeted policies to facilitate teachers' GenAI integration, offering implications for Idaho's education system and the broader U.S. context.}
}
@incollection{SUNDARARAJAN2025326,
title = {Bioinformatics for Clinical Diagnostics},
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {326-332},
year = {2025},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00278-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027002785},
author = {Vijayaraghava Seshadri Sundararajan and Prashanth N. Suravajhala},
keywords = {Algorithms, Artificial intelligence, ChatGPT, Clinical diagnostics, Disease predictions, Microbiology, Pathology, Radiology},
abstract = {Clinical diagnostics is the process of identifying diseases or medical conditions in patients through various tests, including laboratory analyses, imaging, and genetic testing. It helps guide treatment, monitor disease progression, and evaluate overall health. Accurate diagnostics are crucial for early detection, personalized medicine, and improving patient outcomes. Bioinformatics plays a significant role in clinical diagnostics by analyzing vast amounts of biological data, especially genetic information. Bioinformatics tools can help interpret genomic sequences, identify disease-associated mutations, and uncover biomarkers for specific conditions. This accelerates diagnosis, allows for personalized treatments, and supports research into novel diagnostics, ultimately making healthcare more precise and efficient. Bioinformatics tools for image analysis have hastened the analysis of pathological images and scans, providing accurate diagnosis. This chapter provides an overview of the latest bioinformatics developments in clinical diagnostics.}
}
@article{GUPTA2005267,
title = {Power-law distribution in a learning process: competition, learning and natural selection},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {345},
number = {1},
pages = {267-274},
year = {2005},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2004.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0378437104009860},
author = {Hari M. Gupta and José R. Campanha},
keywords = {Power-law, Learning, Natural selection},
abstract = {In the present work, we propose a model for the statistical distribution of people versus number of steps acquired by them in a learning process, based on competition, learning and natural selection. We consider that learning ability is normally distributed. We found that the number of people versus step acquired by them in a learning process is given through a power law. As competition, learning and selection is also at the core of all economical and social systems, we consider that power-law scaling is a quantitative description of this process in social systems. This gives an alternative thinking in holistic properties of complex systems.}
}
@article{PELOROSSO2020101867,
title = {Modeling and urban planning: A systematic review of performance-based approaches},
journal = {Sustainable Cities and Society},
volume = {52},
pages = {101867},
year = {2020},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2019.101867},
url = {https://www.sciencedirect.com/science/article/pii/S2210670719316968},
author = {Raffaele Pelorosso},
keywords = {Systems thinking, Thermodynamics of open systems, Standards, Spatial planning, Model classification},
abstract = {New planning approaches based on performance measures of the urban system are emerging to face the current challenges to the sustainability of cities. Through modelling, planners can understand the general behavior of the system and, consequently, decide the strategic allocation of land uses and human activities with respect to performances of the considered processes and the socio-ecological and economic uncertainties. Thus, model-based planning approaches present strong similarities with the performance-based planning (PBP) approaches and modelling can represent a valuable tool for the evolution and expansion of PBP. In this paper, a systematic review has explored a) the contribution of modelling within PBP approaches in moving cities towards sustainability; b) the applicability for modeling in PBP in urban contexts. Twelve operational examples of model-based urban planning and PBP have been identified in energy, water infrastructure, land use and ecological planning areas. A scoring system for potential model applicability in urban planning was tested in the sampled case studies. Moreover, several critical elements in the relation between modeling approaches and PBP have been identified. Finally, a discussion on the system performance concept as a new urban planning paradigm has been proposed.}
}
@article{AHMAN201351,
title = {Normalization by Evaluation and Algebraic Effects},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {298},
pages = {51-69},
year = {2013},
note = {Proceedings of the Twenty-ninth Conference on the Mathematical Foundations of Programming Semantics, MFPS XXIX},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2013.09.007},
url = {https://www.sciencedirect.com/science/article/pii/S1571066113000534},
author = {Danel Ahman and Sam Staton},
keywords = {Algebraic effects, Type theory, Normalization by evaluation, Presheaves, Monads},
abstract = {We examine the interplay between computational effects and higher types. We do this by presenting a normalization by evaluation algorithm for a language with function types as well as computational effects. We use algebraic theories to treat the computational effects in the normalization algorithm in a modular way. Our algorithm is presented in terms of an interpretation in a category of presheaves equipped with partial equivalence relations. The normalization algorithm and its correctness proofs are formalized in dependent type theory (Agda).}
}
@article{UENO2011385,
title = {Lichtheim 2: Synthesizing Aphasia and the Neural Basis of Language in a Neurocomputational Model of the Dual Dorsal-Ventral Language Pathways},
journal = {Neuron},
volume = {72},
number = {2},
pages = {385-396},
year = {2011},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2011.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S0896627311008348},
author = {Taiji Ueno and Satoru Saito and Timothy T. Rogers and Matthew A. Lambon Ralph},
abstract = {Summary
Traditional neurological models of language were based on a single neural pathway (the dorsal pathway underpinned by the arcuate fasciculus). Contemporary neuroscience indicates that anterior temporal regions and the “ventral” language pathway also make a significant contribution, yet there is no computationally-implemented model of the dual pathway, nor any synthesis of normal and aphasic behavior. The “Lichtheim 2” model was implemented by developing a new variety of computational model which reproduces and explains normal and patient data but also incorporates neuroanatomical information into its architecture. By bridging the “mind-brain” gap in this way, the resultant “neurocomputational” model provides a unique opportunity to explore the relationship between lesion location and behavioral deficits, and to provide a platform for simulating functional neuroimaging data.}
}
@incollection{VALYAN202025,
title = {Chapter 4 - Decision-making deficits in substance use disorders: cognitive functions, assessment paradigms, and levels of evidence},
editor = {Antonio Verdejo-Garcia},
booktitle = {Cognition and Addiction},
publisher = {Academic Press},
pages = {25-61},
year = {2020},
isbn = {978-0-12-815298-0},
doi = {https://doi.org/10.1016/B978-0-12-815298-0.00004-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128152980000046},
author = {Alireza Valyan and Hamed Ekhtiari and Ryan Smith and Martin P. Paulus},
keywords = {Assessment, Behavioral tasks, Computational models, Decision-making dysfunction, fMRI, Intervention, Substance use disorder},
abstract = {Aberrant decision-making plays an important role in both the onset and maintenance of substance use disorders (SUDs). The current state of research within the field of SUDs can be usefully summarized within three broad dimensions: (1) the goal of characterizing the affected cognitive components that contribute to aberrant decision-making (i.e., value, probability, time, and learning functions), (2) the instruments/methods used to accomplish that goal (i.e., self-reports, behavioral tasks, computational modeling, and brain mapping), and (3) the levels of evidence afforded by those instruments/methods. In this chapter, we review and organize the most recent findings based on this three-dimensional framework. Our aim is to (1) provide a comprehensive synthesis of current research on decision-making in SUDs that can serve as a useful resource to guide future research, (2) highlight current limitations in the field and promising future research directions, and (3) illustrate ways in which the framework that we provide may inform the design and implementation of interventional strategies that can advance the field of addiction medicine.}
}
@article{WALLENTIN2017165,
title = {Dynamic hybrid modelling: Switching between AB and SD designs of a predator-prey model},
journal = {Ecological Modelling},
volume = {345},
pages = {165-175},
year = {2017},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2016.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0304380016303714},
author = {Gudrun Wallentin and Christian Neuwirth},
keywords = {Hybrid model, Multi-paradigmatic modelling, Agent-based model, System-dynamics model, Predator-prey system},
abstract = {Entities and processes in complex systems are of diverse nature and operate at various spatial and temporal scales. Hybrid agent-based (AB) and system dynamics (SD) models have been suggested to capture the essence of these systems in a natural and computationally efficient way. However, the integration of the equation-based SD and individual-based AB models is not least challenged by considerable conceptual differences between these models. Examples of tightly integrated and dynamically switching hybrid models are rare. The aim of this paper is to expand on theoretical frameworks of hybrid agent-based and system dynamics models in ecology to support the model design process of dynamically switching hybrid models. We suggested six alternative model designs that switched between the two modelling paradigms. By the example of a fish-plankton lake ecosystem we demonstrated that a well-designed switching hybrid model can be a performant modelling approach that retains relevant spatial and attributive information. Important findings with respect to optimising computational versus predictive performance were (1) the most plausible results were produced by a spatially explicit design based on spatial plankton stocks and fish switching between individual agents and aggregate school-agents, (2) higher levels of aggregation did not necessarily result in higher computational performance, and (3) adaptive, emergence-based triggers for the paradigm switches minimised information loss and could connect hierarchical and spatial scales. In conclusion, we argue to reach beyond efficiency-oriented considerations and use emergent super-individuals as structural elements of dynamically switching hybrid models.}
}
@incollection{BOGLE20031,
title = {Computer aided biochemical process engineering},
editor = {Andrzej Kraslawski and Ilkka Turunen},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {14},
pages = {1-10},
year = {2003},
booktitle = {European Symposium on Computer Aided Process Engineering-13},
issn = {1570-7946},
doi = {https://doi.org/10.1016/S1570-7946(03)80082-2},
url = {https://www.sciencedirect.com/science/article/pii/S1570794603800822},
author = {I.D.L. Bogle},
abstract = {The growth of the biochemical industries is heating up in Europe after not meeting the initial expectations. CAPE tools have made some impact and progress on computer aided synthesis and design of biochemical processes is demonstrated on a process for the production of a hormone. Systems thinking is being recognised by the life science community and to gain genuinely optimal process solutions it is necessary to design right through from product and function to metabolism and manufacturing process. The opportunities for CAPE experts to contribute in the explosion of interest in the Life Sciences is strong if we think of the ‘Process’ in CAPE as any process involving physical or (bio-)chemical change.}
}
@article{RUI2024100711,
title = {Simulation of e-learning in vocal network teaching experience system based on intelligent Internet of things technology},
journal = {Entertainment Computing},
volume = {50},
pages = {100711},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100711},
url = {https://www.sciencedirect.com/science/article/pii/S187595212400079X},
author = {Yu Rui},
keywords = {Intelligent technology, Internet of Things, E-learning, Vocal music network, Teaching experience, System simulation},
abstract = {With the rapid development of smart Internet of Things technology, education is also starting to use this technology to improve the learning experience. As an art subject, vocal music teaching has many limitations in the traditional face-to-face teaching methods, and e-learning provides new possibilities for vocal music network teaching. We analyze the problems and challenges existing in the traditional face-to-face mode of vocal music teaching, and then based on the intelligent Internet of Things technology, we design a vocal music network teaching experience system. The system combines sound acquisition equipment, intelligent audio processing algorithm, virtual classroom and other technologies to realize the simulation experience of online vocal music teaching. We have developed an intelligent audio processing algorithm for analyzing and processing students’ singing sounds. This algorithm can detect problems in tone, timbre, rhythm, and more, and provide real-time feedback and advice. In this way, students can understand the shortcomings of their own singing skills, and make timely adjustments and improvements. This study shows that e-learning based on intelligent Internet of Things technology has important application value in vocal music network teaching. By simulating the classroom environment and providing real-time feedback, students can obtain a better learning experience and improve their vocal skills.}
}
@article{HAID2024,
title = {Exploring AI: Transforming medical practice, education and research},
journal = {Journal of Pediatric Urology},
year = {2024},
issn = {1477-5131},
doi = {https://doi.org/10.1016/j.jpurol.2024.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S1477513124006740},
author = {Bernhard Haid and Caleb Nelson and M. İrfan Dönmez and Salvatore Cascio and Massimo Garriboli and Anka Nieuwhof-Leppink and Christina Ching and Luis H. Braga and Ilina Rosklija and Luke Harper},
keywords = {Artificial intelligence, Large language models, History, Education}
}
@article{SUTOYO2015435,
title = {Dynamic Difficulty Adjustment in Tower Defence},
journal = {Procedia Computer Science},
volume = {59},
pages = {435-444},
year = {2015},
note = {International Conference on Computer Science and Computational Intelligence (ICCSCI 2015)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.07.563},
url = {https://www.sciencedirect.com/science/article/pii/S187705091502092X},
author = {Rhio Sutoyo and Davies Winata and Katherine Oliviani and Dedy Martadinata Supriyadi},
keywords = {dynamic game balancing, tower defence, dynamic difficulty adjustment, computational intelligence},
abstract = {When we play tower defence game, generally we repeat the same stages several times with the same enemies. Moreover, when the players play a stage that is ridiculously hard or way too easy, they would probably quit the game because it ismoderately frustrating or boring. The purpose of this research is to createa game that can adapt to the players’ ability so the difficulty of the game becomes dynamic. In other words, the game will have different difficultiesof levels according to the players’ ability. High difficulty levels will be set if the players use good strategy and low difficulty levels will be set if the players use bad strategy. In this work, we determine the difficulties based on players’ lives, enemies’ health, and passive skills (skill points) that are chosen by the player. With three of these factors, players will have varies experience of playing tower defence because different combination will give different results to the system and difficulties of the games will be different for each gameplay. The result of this research is a dynamic difficulty tower defence game, dynamic difficulty adjustment (DDA) document, and gameplay outputs for best, average, and worst strategy cases.}
}
@article{PARK2025101119,
title = {Code suggestions and explanations in programming learning: Use of ChatGPT and performance},
journal = {The International Journal of Management Education},
volume = {23},
number = {2},
pages = {101119},
year = {2025},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2024.101119},
url = {https://www.sciencedirect.com/science/article/pii/S1472811724001903},
author = {Arum Park and Taekyung Kim},
keywords = {Future of education, Education, OpenAI, ChatGPT, Management education, Programming skills},
abstract = {This study investigates the role of generative artificial intelligence (AI) chatbots, particularly ChatGPT, in enhancing programming education for university students, specifically in big data analytics. The research addresses the growing need for innovative educational practices, especially in developed East Asian countries like South Korea, where declining university enrollment presents new challenges. Using a sample size of N = 343 students, this mixed-methods research employed controlled experiments and surveys to compare student performance in programming tasks across three groups: those using ChatGPT, those using Stack Overflow, and a control group without external assistance. Results showed that students using ChatGPT significantly outperformed those relying on Stack Overflow or no assistance, particularly in hands-on coding tasks. This research contributes to the ongoing discourse on AI in education by providing empirical evidence of generative AI's effectiveness in improving learning outcomes and engagement, while also highlighting the challenges associated with integrating AI into educational settings. The findings emphasize the potential of ChatGPT to personalize learning experiences, improve performance, and offer real-time support, underscoring the need for a balanced curriculum design that incorporates AI while maintaining academic integrity and human oversight.}
}
@article{KAPUSTINA2024100072,
title = {User-friendly and industry-integrated AI for medicinal chemists and pharmaceuticals},
journal = {Artificial Intelligence Chemistry},
volume = {2},
number = {2},
pages = {100072},
year = {2024},
issn = {2949-7477},
doi = {https://doi.org/10.1016/j.aichem.2024.100072},
url = {https://www.sciencedirect.com/science/article/pii/S2949747724000307},
author = {Olga Kapustina and Polina Burmakina and Nina Gubina and Nikita Serov and Vladimir Vinogradov},
keywords = {Machine Learning, Medicinal Chemistry, Pharmaceutics, Data-Driven Drug Discovery},
abstract = {Artificial intelligence has brought crucial changes to the whole field of natural sciences. Myriads of machine learning algorithms have been developed to facilitate the work of experimental scientists. Molecular property prediction and drug synthesis planning become routine tasks. Moreover, inverse design of compounds with tunable properties as well as on-the-fly autonomous process optimization and chemical space exploration became possible in silico. Affordable robotic platforms exist able to perform thousands of experiments every day, analyzing the results and tuning the protocols. Despite this, most of these developments get trapped at the stage of code or overlooked, limiting their use by experimental scientists. Meanwhile, visibility and the number of user-friendly tools and technologies available to date is too low to compensate for this fact, rendering the development of novel therapeutic compounds inefficient. In this Review, we set the goal to bridge the gap between modern technologies and experimental scientists to improve drug development efficacy. Here we survey advanced and easy-to-use technologies able to help medical chemists at every stage of their research, including those integrated in technological processes during COVID-19 pandemic motivated by the need for fast yet precise solutions. Moreover, we review how these technologies are integrated by industry and clinics to streamline drug development and production. These technologies already transform the current paradigm of scientific thinking and revolutionize not only medicinal chemistry, but the whole field of natural sciences.}
}
@article{ZHANG20231815,
title = {An intention inference method for the space non-cooperative target based on BiGRU-Self Attention},
journal = {Advances in Space Research},
volume = {72},
number = {5},
pages = {1815-1828},
year = {2023},
issn = {0273-1177},
doi = {https://doi.org/10.1016/j.asr.2023.04.032},
url = {https://www.sciencedirect.com/science/article/pii/S0273117723003101},
author = {Honglin Zhang and Jianjun Luo and Yuan Gao and Weihua Ma},
keywords = {Space non-cooperative target, Intention inference, Time series, BiGRU, Self-attention mechanism},
abstract = {Intention inference for space non-cooperative targets is the key to space situational awareness and assistant decision for collision avoidance. Given that the problem of target intention inference is essential to learn the dynamically changing time-series characteristics of space non-cooperative target intentions and infer their relative motion patterns for threat warning, this paper adopts a deep learning-based approach, introduces a bidirectional propagation mechanism and self-attention mechanism based on Gated Recurrent Unit (GRU) and proposes a bidirectional Gated Recurrent Unit (BiGRU)-Self Attention-based space non-cooperative target intention inference model. BiGRU is used to learn deep information in time-series characteristics of the space non-cooperative target, and self-attention mechanism is used to adaptively extract and assign weights to key characteristics to capture the internal correlations in time-series information, thus improving model performance. The line-of-sight measurements are used as the characteristics of target intention inference, and the typical target motion intentions are defined. Subsequently, the proposed model is trained and tested on the test set, with the accuracy reaching 97.1%. Besides, the effectiveness and advantages of the proposed model are verified by the simulation of a case study and comparison evaluations. The results demonstrate that our proposed model could significantly improve the accuracy, computational efficiency, and noise resistance for the space non-cooperative target intention inference compared with the existing intention inference models.}
}
@article{XIE20189,
title = {Detecting leadership in peer-moderated online collaborative learning through text mining and social network analysis},
journal = {The Internet and Higher Education},
volume = {38},
pages = {9-17},
year = {2018},
issn = {1096-7516},
doi = {https://doi.org/10.1016/j.iheduc.2018.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S1096751618300332},
author = {Kui Xie and Gennaro {Di Tosto} and Lin Lu and Young Suk Cho},
keywords = {Leadership, Computer-supported collaborative learning, Text mining, Social network analysis, Learning analytics, Online learning},
abstract = {Structured tasks and peer-moderated discussions are pedagogical models that have shown unique benefits for online collaborative learning. Students appointed with leadership roles are able to positively affect the dynamics in their groups by engaging with participants, raising questions, and advancing problem solving. To help monitoring and controlling the latent social dynamics associated with leadership behavior, we propose a methodological approach that makes use of computational techniques to mine the content of online communications and analyze group structure to identify students who behave as leaders. Through text mining and social network analysis, we systematically process the discussion posts made by students from four sections of an online course in an American university. The results allow us to quantify each individual's contribution and summarize their engagement in the form of a leadership index. The proposed methodology, when compared to judgements made by experts who manually coded samples of the data, is shown to have comparable performances, but, being fully automated, has the potential to be easily replicable. The summary offered by the leadership index is intended as actionable information that can guide just-in-time interventions together with other tools based on learning analytics.}
}
@article{BENZEKRY201553,
title = {Metronomic reloaded: Theoretical models bringing chemotherapy into the era of precision medicine},
journal = {Seminars in Cancer Biology},
volume = {35},
pages = {53-61},
year = {2015},
note = {Complexity in Cancer Biology},
issn = {1044-579X},
doi = {https://doi.org/10.1016/j.semcancer.2015.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S1044579X15000759},
author = {Sébastien Benzekry and Eddy Pasquier and Dominique Barbolosi and Bruno Lacarelle and Fabrice Barlési and Nicolas André and Joseph Ciccolini},
keywords = {Metronomic chemotherapy, Mathematical modeling, PK/PD, Precision medicine},
abstract = {Oncology has benefited from an increasingly growing number of groundbreaking innovations over the last decade. Targeted therapies, biotherapies, and the most recent immunotherapies all contribute to increase the number of therapeutic options for cancer patients. Consequently, substantial improvements in clinical outcomes for some disease with dismal prognosis such as lung carcinoma or melanoma have been achieved. Of note, the latest innovations in targeted therapies or biotherapies do not preclude the use of standard cytotoxic agents, mostly used in combination. Importantly, and despite the rise of bioguided (a.k.a. precision) medicine, the administration of chemotherapeutic agents still relies on the maximum tolerated drug (MTD) paradigm, a concept inherited from theories conceptualized nearly half a century ago. Alternative dosing schedules such as metronomic regimens, based upon the repeated and regular administration of low doses of chemotherapeutic drugs, and adaptive therapy (i.e. modulating the dose and frequency of cytotoxics administration to control disease progression rather than eradicate it at all cost) have emerged as possible strategies to improve response rates while reducing toxicities. The recent changes in paradigm in the way we theorize cancer biology and evolution, metastatic spreading and tumor ecology, alongside the recent advances in the field of immunotherapy, have considerably strengthened the interest for these alternative approaches. This paper aims at reviewing the recent evolutions in the field of theoretical biology of cancer and computational oncology, with a focus on the consequences these changes have on the way we administer chemotherapy. Here, we advocate for the development of model-guided strategies to refine doses and schedules of chemotherapy administration in order to achieve precision medicine in oncology.}
}
@article{DELEON2021281,
title = {Assessing the Efficacy of Tier 2 Mathematics Intervention for Spanish Primary School Students},
journal = {Early Childhood Research Quarterly},
volume = {56},
pages = {281-293},
year = {2021},
issn = {0885-2006},
doi = {https://doi.org/10.1016/j.ecresq.2021.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0885200621000508},
author = {Sara C. {de León} and Juan E. Jiménez and Nuria Gutiérrez and Juan Andrés Hernández-Cabrera},
keywords = {RtI model, math, early grades, Tier 2, at-risk},
abstract = {This study explored the efficacy of a Tier 2 intervention within the context of the Response to Intervention (RtI) model implemented by Spanish first- to third-grade primary school teachers to improve at-risk students’ early math skills. Teachers were instructed in the administration of a math curriculum-based measure composed of 5 isolated measures (quantity discrimination, missing number, single-digit computation, multidigit computation, and place value) to identify at-risk students and to monitor their progress; and in the implementation of a systematic and explicit instructional program to improve basic math skills in at-risk students. Implementation fidelity was analyzed using direct observations and self-reports. The intervention was conducted with adequate fidelity and had a significant positive impact on all grades. Significant differences were found between experimental and control students at risk of math failure in the improvement rate of quantity discrimination, missing number, and place value in all grades. Experimental at-risk students showed a monthly improvement, assessed using a combination of screening and progress monitoring measures. In conclusion, Spanish first to third graders at risk of math failure benefited from a Tier 2 intervention based on basic math skills, implemented by in-service teachers.}
}
@article{OSEIBRYSON20121156,
title = {A context-aware data mining process model based framework for supporting evaluation of data mining results},
journal = {Expert Systems with Applications},
volume = {39},
number = {1},
pages = {1156-1164},
year = {2012},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2011.07.117},
url = {https://www.sciencedirect.com/science/article/pii/S0957417411010797},
author = {Kweku-Muata Osei-Bryson},
keywords = {Context, Data mining process, KDDM, Evaluation, Decision analysis, Multi-criteria decision analysis, Post-processing},
abstract = {The knowledge discovery via data mining process (KDDM) is a multiple phase that aims to at a minimum semi-automatically extract new knowledge from existing datasets. For many data mining tasks, the evaluation phase is a challenging one for various reasons. Given this challenge several studies have presented techniques that could be used for the semi-automated evaluation of data mining results. When taken together, these studies suggest the possibility of a common multi-criteria evaluation framework. The use of such a multi-criteria evaluation framework, however, requires that relevant objectives, measures and preference function be identified. This implies that the context of the DM problem is particularly important for the evaluation phase of the KDDM process. Our framework utilizes and integrates a pair of established tightly coupled techniques (i.e. Value Focused Thinking (VFT) and the Goal–Question–Metric (GQM) methods) as well as established techniques from multi-criteria decision analysis in order to explicate and utilize context information in order to facilitate semi-automated evaluation.}
}
@article{LIU2024100744,
title = {Application of entertainment E-learning mode based on Apriori algorithm in intelligent English reading assistance mode},
journal = {Entertainment Computing},
volume = {51},
pages = {100744},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100744},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124001125},
author = {Shanshan Liu},
keywords = {Apriori algorithm, Entertainment E-learning model, Intelligent English teaching, Auxiliary reading mode},
abstract = {With the assistance of digital media, entertainment oriented E-learning models can effectively enhance students’ learning enthusiasm. This article analyzes the application of entertainment E-learning mode based on Apriori algorithm in intelligent English reading assistance mode. At present, English reading teaching faces some problems, such as outdated teaching methods, passive learning among students, excessive emphasis on imparting grammar knowledge while neglecting the improvement of students’ reading skills and strategies, and so on. Therefore, this article conducts research on intelligent English reading comprehension tools based on semantic analysis and Apriori algorithm. This paper proposes a recreational E-learning model based on Apriori algorithm. Based on Apriori algorithm, students’ interests and preferences on different learning resources and entertainment elements are mined and incorporated into the learning model design. Then, a set of entertaining English reading assistant model is designed, which uses a variety of entertainment elements, such as gamified learning, interactive activities and reward mechanism, to increase students’ learning participation and enthusiasm. This article adopts the idea of LSA algorithm to construct a BERT semantic analysis model. We treat nodes in the network as word items and then use singular value decomposition algorithm to decompose the word document matrix. Secondly, the original association rule Apriori algorithm was optimized, and the optimized association rule Apriori algorithm effectively solved the problem of excessive computation in traditional algorithms. Finally, based on semantic analysis and Apriori algorithm, this article designs an intelligent English reading comprehension tool, mainly analyzing the practical application of the system and greatly improving the efficiency of English reading teaching.}
}
@incollection{DETALLE2017495,
title = {2.20 - Translational Aspects in Drug Discovery},
editor = {Samuel Chackalamannil and David Rotella and Simon E. Ward},
booktitle = {Comprehensive Medicinal Chemistry III},
publisher = {Elsevier},
address = {Oxford},
pages = {495-529},
year = {2017},
isbn = {978-0-12-803201-5},
doi = {https://doi.org/10.1016/B978-0-12-409547-2.12335-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095472123352},
author = {L. Detalle and K. Vanheusden and M.L. Sargentini-Maier and T. Stöhr},
keywords = {Animal model, Biomarker, Imaging, Modeling, Simulation, Translational medicine},
abstract = {The efficiency of drug development has seen a constant decline. This observation is somewhat paradoxical since during the same time there have been huge advancements in drug discovery and development technologies that made it much cheaper, faster, and easier to identify new drug targets and new drug molecules. Translational Science or Translational Medicine (TM) has arisen as an important discipline in modern drug discovery and development. It was triggered by the fact that many promising drugs failed in clinical trials. The challenge was thus to enhance the predictivity of the preclinical models and to design exploratory clinical trial designs and methodologies to test promising molecules earlier and faster. Despite some advancements, the number of drugs that finally receive regulatory approval is still at a low level. The main reason for this drug failure rate was a lack of efficacy observed in clinical trials of drug candidates that showed great promise in drug discovery. There may be two main factors responsible for this: (1) the industrialization of drug discovery and development led to huge specialized departments that operate in isolation. (2) Tools for successful translational research have only been developed in the last one or two decades. We will describe the tools used in translational research, that is, biomarkers, animal models, imaging, in silico modeling, and simulations. Their use will be illustrated with examples and tips of how to implement those into daily project work. We believe, however, that TM is more than these tools and technologies. It is not yet another discipline or department, it is a way of thinking that should become part of every discipline involved in drug development. Thus, in addition to describing the tools and how best to use them, we will elaborate how to design a translational research strategy and exemplify with some case studies as to how this has been successfully implemented in the past.}
}
@article{BARTON201242,
title = {Looking for the future in the past: Long-term change in socioecological systems},
journal = {Ecological Modelling},
volume = {241},
pages = {42-53},
year = {2012},
note = {Modeling Across Millennia: Interdisciplinary Paths to Ancient socio-ecological Systems},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2012.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S0304380012000786},
author = {C. Michael Barton and Isaac I.T. Ullah and Sean M. Bergin and Helena Mitasova and Hessam Sarjoughian},
keywords = {Socio-ecological systems, Coupled modeling, Agent-based modeling, Surface process modeling, Simulation, Prehistoric Mediterranean, Archaeology, Agricultural land-use},
abstract = {The archaeological record has been described as a key to the long-term consequences of human action that can help guide our decisions today. Yet the sparse and incomplete nature of this record often makes it impossible to inferentially reconstruct past societies in sufficient detail for them to serve as more than very general cautionary tales of coupled socio-ecological systems. However, when formal and computational modeling is used to experimentally simulate human socioecological dynamics, the empirical archaeological record can be used to validate and improve dynamic models of long term change. In this way, knowledge generated by archaeology can play a unique and valuable role in developing the tools to make more informed decisions that will shape our future. The Mediterranean Landscape Dynamics project offers an example of using the past to develop and test computational models of interactions between land-use and landscape evolution that ultimately may help guide decision-making.}
}
@article{GUTIERREZORTIZ2022100164,
title = {Biofuel production from supercritical water gasification of sustainable biomass},
journal = {Energy Conversion and Management: X},
volume = {14},
pages = {100164},
year = {2022},
issn = {2590-1745},
doi = {https://doi.org/10.1016/j.ecmx.2021.100164},
url = {https://www.sciencedirect.com/science/article/pii/S2590174521000891},
author = {F.J. {Gutiérrez Ortiz}},
keywords = {Supercritical water, Gasification, Biofuel, Hydrogen, Sustainability, Process simulation},
abstract = {A review of biofuel production from supercritical water gasification (SCWG) of sustainable biomass has been performed, mainly organic waste, following a critical thinking in this field of knowledge. Thus, sub- and super- critical water properties and hydrothermal processing are briefly commented on. Then, the feedstocks usable in SCWG are fully reviewed and a brief description of the studies on the kinetics and mechanisms of reactions is carried out. Next, thermodynamic and process simulation are discussed, aimed at producing liquid and gas biofuels. After that, a brief comment on the viability of SCWG processes to produce biofuels is provided based on techno-economic and lifecycle assessments. Finally, some remarks on where we are and where we should go are given in order to advance this technology towards its maturity. This review explains some misleading concepts applied to SCWG processes, provides a brief but comprehensive overview of the technology focused on producing biofuels in a sustainable way, allows a better understanding of the SCWG of biomass for biofuel production, and proposes a series of improvements to be made and examined in the future research.}
}
@article{VEKSLER20169,
title = {The performance comparison problem: Universal task access for cross-framework evaluation, Turing tests, grand challenges, and cognitive decathlons},
journal = {Biologically Inspired Cognitive Architectures},
volume = {18},
pages = {9-22},
year = {2016},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2016.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X16300810},
author = {Vladislav D. Veksler and Norbou Buchler and Christian Lebiere and Don Morrison and Troy Kelley},
keywords = {Grand challenge, Cognitive decathlon, Turing test, Performance comparison, Simulation, API, Standards},
abstract = {A driver for achieving human-level AI and high-fidelity cognitive architectures is the ability to easily test and compare the performance and behavior of computational agents/models to humans and to one another. One major difficulty in setting up and getting participation in large-scale cognitive decathlon and grand challenge competitions, or even smaller scale cross-framework evaluation and Turing testing, is that there is no standard interface protocol that enables and facilitates human and computational agent “plug-and-play” participation across various tasks. We identify three major issues. First, human-readable task interfaces aren’t often translated into machine-readable form. Second, in the cases where a task interface is made available in a machine-readable protocol, the protocol is often task-specific, and differs from other task protocols. Finally, where both human and machine-readable versions of the task interface exist, the two versions often differ in content. This makes the bar of entry extremely high for comparison of humans and multiple computational frameworks across multiple tasks. This paper proposes a standard approach to task design where all task interactions adhere to a standard API. We provide examples of how this method can be employed to gather human and computational simulation data in text-and-button tasks, visual and animated tasks, and in real-time robotics tasks.}
}
@article{SUCHOW2017522,
title = {Evolution in Mind: Evolutionary Dynamics, Cognitive Processes, and Bayesian Inference},
journal = {Trends in Cognitive Sciences},
volume = {21},
number = {7},
pages = {522-530},
year = {2017},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2017.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S1364661317300773},
author = {Jordan W. Suchow and David D. Bourgin and Thomas L. Griffiths},
keywords = {Bayesian inference, cognitive processes, creativity, evolution, learning, memory},
abstract = {Evolutionary theory describes the dynamics of population change in settings affected by reproduction, selection, mutation, and drift. In the context of human cognition, evolutionary theory is most often invoked to explain the origins of capacities such as language, metacognition, and spatial reasoning, framing them as functional adaptations to an ancestral environment. However, evolutionary theory is useful for understanding the mind in a second way: as a mathematical framework for describing evolving populations of thoughts, ideas, and memories within a single mind. In fact, deep correspondences exist between the mathematics of evolution and of learning, with perhaps the deepest being an equivalence between certain evolutionary dynamics and Bayesian inference. This equivalence permits reinterpretation of evolutionary processes as algorithms for Bayesian inference and has relevance for understanding diverse cognitive capacities, including memory and creativity.}
}
@article{LIU2020102176,
title = {Using a new approach for revealing the spatiotemporal patterns of functional urban polycentricity: A case study in the Tokyo metropolitan area},
journal = {Sustainable Cities and Society},
volume = {59},
pages = {102176},
year = {2020},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2020.102176},
url = {https://www.sciencedirect.com/science/article/pii/S2210670720301633},
author = {Kai Liu and Yuji Murayama and Toshiaki Ichinose},
keywords = {Functional urban area detection, Functional urban polycentricity, Multi-step Decision-making Newman algorithm, Tokyo Master Plan, Tokyo metropolitan area},
abstract = {This research designs a new approach by modifying the Fast-Newman algorithm for better implementing the process of detecting functional urban areas (FUAs) and further revealing the spatiotemporal patterns of functional urban polycentricity through 20 view-windows of each FUA in the Tokyo metropolitan area (TMA) by using geo-tagged big data. Through the 20 view-windows of our GIS microscope, it is possible to uncover patterns of functional connections and daily urban rhythms under the same layout of the functional urban structure in the TMA. Furthermore, our findings can elucidate the double-sided thinking by combining the explanations of functional urban polycentricity with the policy effects of the Tokyo Master Plan (TMP) from the perspective of area-byarea analysis across the entire TMA. Our results imply that the functional urban structure of the TMA is a four-level, annular pattern. The TMP still has room for the improvement toward sustainable urban planning in the TMA. Based on the investigation on the patterns of functional urban polycentricity, this research has obtained many hints and clues for improving the TMP. Rethinking the effectiveness of the TMP can also provide trustworthy academic verification and provide suggestions about concrete amendments that can enlighten future urban planning.}
}
@article{JIANG2024100795,
title = {Generative urban design: A systematic review on problem formulation, design generation, and decision-making},
journal = {Progress in Planning},
volume = {180},
pages = {100795},
year = {2024},
note = {Generative urban design: A systematic review on problem formulation, design generation, and decision-making},
issn = {0305-9006},
doi = {https://doi.org/10.1016/j.progress.2023.100795},
url = {https://www.sciencedirect.com/science/article/pii/S0305900623000569},
author = {Feifeng Jiang and Jun Ma and Christopher John Webster and Alain J.F. Chiaradia and Yulun Zhou and Zhan Zhao and Xiaohu Zhang},
keywords = {Generative urban design, Urban form generation, Generative method, AI-generated content (AIGC), Generative AI, Human-machine collaboration},
abstract = {Urban design is the process of designing and shaping the physical forms of cities, towns, and suburbs. It involves the arrangement and design of street systems, groups of buildings, public spaces, and landscapes, to make the urban environment performative and sustainable. The typical design process, reliant on manual work and expert experience has unavoidable low efficiency in generating high-performing design solutions due to the involvement of complex social, institutional, and economic contexts and the trade-off between conflicting preferences of different stakeholder groups. Taking advantage of artificial intelligence (AI) and computational capacity, generative urban design (GUD) has been developed as a trending technical direction to narrow the gaps and produce design solutions with high efficiency at early design stages. It uses computer-aided generative methods, such as evolutionary optimization and deep generative models, to efficiently explore complex solution spaces and automatically generate design options that satisfy conflicting objectives and various constraints. GUD experiments have attracted much attention from academia, practitioners, and public authorities in recent years. However, a systematic review of the current stage of GUD research is lacking. This study, therefore, reports on a systematic investigation of the existing literature according to the three key stages in the GUD process: (1) design problem formulation, (2) design option generation, and (3) decision-making. For each stage, current trends, findings, and limitations from GUD studies are examined. Future directions and potential challenges are discussed and presented. The review is highly interdisciplinary and involves articles from urban study, computer science, social science, management, and other fields. It reports what scholars have found in GUD experiments and organizes a diverse and complicated technical agenda into something accessible to all stakeholders. The results and discoveries will serve as a holistic reference for GUD developers and users in both academia and industry and form a baseline for the field of GUD development in the coming years.}
}
@article{MCDANIELS200333,
title = {Decision structuring to alleviate embedding in environmental valuation},
journal = {Ecological Economics},
volume = {46},
number = {1},
pages = {33-46},
year = {2003},
issn = {0921-8009},
doi = {https://doi.org/10.1016/S0921-8009(03)00103-4},
url = {https://www.sciencedirect.com/science/article/pii/S0921800903001034},
author = {Timothy L McDaniels and Robin Gregory and Joseph Arvai and Ratana Chuenpagdee},
keywords = {Embedding, Structured decision process, Environmental valuation, Value-focused thinking, Small group process, Fisheries valuation},
abstract = {Embedding is the widely-observed phenomenon that a good is assigned a higher value if evaluated on its own rather than as part of a more inclusive set. Embedding is considered a serious problem affecting the quality of many environmental management and health risk policy judgments. This paper presents the results of an experiment involving of a structured, small-group approach for conducting environmental policy evaluations. It focuses on eliciting problem-specific values and discussion among participants about the pros and cons of multiple project alternatives, in the context of tradeoffs between fisheries production and electricity generation from dams. Study results show a significant reduction in embedding, which is viewed as an improvement in the quality of the preference judgments compared with a standard contingent valuation (CV) approach.}
}
@article{WOLFF20124051,
title = {Constraints in the generation of photonic Wannier functions},
journal = {Physica B: Condensed Matter},
volume = {407},
number = {20},
pages = {4051-4055},
year = {2012},
note = {Proceedings of the conference - Wave Propagation: From Electrons to Photonic Crystals and Metamaterials},
issn = {0921-4526},
doi = {https://doi.org/10.1016/j.physb.2012.03.022},
url = {https://www.sciencedirect.com/science/article/pii/S0921452612002554},
author = {Christian Wolff and Kurt Busch},
keywords = {Photonic Crystals, Wannier functions},
abstract = {We report on the generation of maximally localized photonic Wannier functions under constraints. This allows us to impress certain symmetry properties of the underlying Photonic Crystal onto the Wannier functions. This added flexibility enhances the utility of the Wannier function approach to Photonic Crystal circuits by providing deeper physical insight and making computations more efficient.}
}
@article{CAI2024133949,
title = {A state-of-the-art review of solar-induced ventilation technology for built environment regulation: Classification, modeling, evaluation, potential and challenges},
journal = {Energy},
volume = {313},
pages = {133949},
year = {2024},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2024.133949},
url = {https://www.sciencedirect.com/science/article/pii/S0360544224037277},
author = {Yang Cai and Zheng-Yu Shu and Jian-Wei He and Yong-Cai Li and Yuan-Da Cheng and Kai-Liang Huang and Fu-Yun Zhao},
keywords = {Solar-induced ventilation technology, Built environment regulation, Numerical model, Thermal characteristic, Performance evaluation},
abstract = {In the face of escalating environmental challenges and dwindling fossil fuel reserves, the transition to renewable and sustainable energy sources has become a paramount global objective, which has led to a surge in research and application of renewable energy sources. Among them, solar energy utilization has been placed at the forefront of energy conservation revolution owing to its significant advantages in terms of sustainability and environmentally-friendliness. Solar-induced ventilation technology (SVT) is a typical way to integrate clean energy with buildings, considerably enhancing solar energy utilization efficiency while achieving building energy conservation and indoor thermal environment regulation. However, summaries as comprehensive as possible for SVT's application in envelopes are ambiguous in the current academia. Different analytical models, parameters and evaluation indicators need to be reviewed to describe the energy flow transfer and the impact on indoor thermal environment, which makes it indispensable to carry out an comprehensive overview for the latest investigation progress. This article endeavors to carry out an elaborate review of the theoretical analysis and constructive application of SVT from an energy utilization and building thermal environment perspective. Firstly, various types of SVT envelopes are classified simultaneously according to development and innovation in solar energy utilization. Furthermore, four different analytical models, namely, heat transfer model, thermal resistance network model, pressure balance model as well as computational fluid dynamics model, have been summarized, which would be helpful to analyze the thermal performance. Through literature review, this article discusses the impact of numerous parameters on system performance, especially the ventilation effect and thermal environment in buildings, from aspects of geometry, material properties and environmental conditions. In addition, a comprehensive collection of the important evaluation indicators based on the energy, thermal comfort and economic evaluations has been introduced to evaluate the thermal performance and indoor environment regulation capability of SVT envelopes, which provided a clear reference on developing and application SVT for high energy efficiency design towards carbon-neutral building envelopes. Finally, the challenges and potential are pointed out in terms of performance enhancement and the expansion of application scenarios. The results of the survey indicated that due to the development of novel technologies and materials, SVT holds great advantages in mitigating building energy consumption and regulating thermal environment, which shows a diversified development trend and promotes the process of global sustainable development. The review of the current SVT building envelope not only clarified the high feasibility of SVT in promoting passive building ventilation, energy saving and enhancing the level of indoor thermal environments, but also provided guidance and identifies the direction of optimization for cutting-edge research.}
}
@article{WONG2021105042,
title = {Empathic accuracy of young boys and girls in ongoing parent–child interactions: Performance and (mis)perception},
journal = {Journal of Experimental Child Psychology},
volume = {203},
pages = {105042},
year = {2021},
issn = {0022-0965},
doi = {https://doi.org/10.1016/j.jecp.2020.105042},
url = {https://www.sciencedirect.com/science/article/pii/S0022096520304963},
author = {Wang Ivy Wong and Wai Bong Patrick Tsui and Tik-Sze Carrey Siu},
keywords = {Interpersonal interaction, Performance estimation, Social cognition, Gender, Empathic accuracy, Parent and child},
abstract = {Understanding others accurately is crucial in relationships and learning. Research shows that adults face challenges in empathic accuracy, that is, the ability to read the content of others’ moment-to-moment mental states during interactions. Although young children possess some empathic understanding, the extent of their empathic accuracy is uncharted. Using a new SSP, 106 Chinese children aged 60 to 80 months (M = 70 months) were assessed on their ability to infer the mental states of adults in ongoing parent–child interactions. Replicating and extending extant findings on adults and adolescents, the children’s inferences were found to be, at least computationally on a scale of .00 to 1.00, more often inaccurate than accurate regardless of the gender of the targets or participants (overall accuracy rate = .28). However, both the children and their primary caregivers overestimated the children’s performance. In addition, although the primary caregivers expected girls to outperform boys, no gender difference in empathic accuracy was found when controlling for verbal fluency. Drawing on the findings of this first-ever application of the empathic accuracy paradigm in young children, the implications of empathic accuracy performance and misperceptions about such accuracy are discussed.}
}
@article{PECIS2025100573,
title = {In blockchain we trust: Ideologies and discourses sustaining trust in bitcoin},
journal = {Information and Organization},
volume = {35},
number = {2},
pages = {100573},
year = {2025},
issn = {1471-7727},
doi = {https://doi.org/10.1016/j.infoandorg.2025.100573},
url = {https://www.sciencedirect.com/science/article/pii/S1471772725000193},
author = {Lara Pecis and Lucia Cervi and Lucas Introna},
keywords = {Bitcoin, Blockchain, Trust, Ideology, Discourse, Critical discourse analysis},
abstract = {In this paper, we examine the discourses and ideologies that underpin trust in Bitcoin (BTC) as an algorithm-driven socio-technical system, raising critical questions about how trust is established and sustained in complex socio-technical assemblages. Through a Critical Discourse Analysis (CDA) of three significant events in the cryptocurrency, we identify two interconnected, yet sometimes contradictory, ideologies enacted through four discourses that construct specific subject positions to produce and maintain trust in Bitcoin. The first, technical sovereignty, reflects adherence to notions of technical utopianism. The second, which we term peer-to-peer neoliberalism, frames BTC as a political experiment rooted in the individualization of responsibility and risk. Our paper contributes to the existing literature by arguing that algorithm-driven technologies like BTC neither establish trust solely through their apparent technical neutrality and security nor simply replace traditional institutional mechanisms of governance, control, and interaction. Instead, they are enacted through discourses and material arrangements that require continuous maintenance. This maintenance relies on power relations enabled by these ideologies yet remains contingent upon the ongoing reinforcement of the ideologies themselves—rendering trust inherently precarious and always at risk. This insight shifts the analytical focus from the dominant emphasis in the literature on technical features, social arrangements, and user perceptions to the underlying ideological frameworks that shape these elements, as such.}
}
@article{WITTKUHN2021367,
title = {Replay in minds and machines},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {129},
pages = {367-388},
year = {2021},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2021.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0149763421003444},
author = {Lennart Wittkuhn and Samson Chien and Sam Hall-McMaster and Nicolas W. Schuck},
keywords = {Replay, Reinforcement learning, Machine learning, Representation learning, Decision-making},
abstract = {Experience-related brain activity patterns reactivate during sleep, wakeful rest, and brief pauses from active behavior. In parallel, machine learning research has found that experience replay can lead to substantial performance improvements in artificial agents. Together, these lines of research suggest that replay has a variety of computational benefits for decision-making and learning. Here, we provide an overview of putative computational functions of replay as suggested by machine learning and neuroscientific research. We show that replay can lead to faster learning, less forgetting, reorganization or augmentation of experiences, and support planning and generalization. In addition, we highlight the benefits of reactivating abstracted internal representations rather than veridical memories, and discuss how replay could provide a mechanism to build internal representations that improve learning and decision-making.}
}