@article{KHOSHNAM2022116686,
title = {A dual framework for implicit and explicit emotion recognition: An ensemble of language models and computational linguistics},
journal = {Expert Systems with Applications},
volume = {198},
pages = {116686},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.116686},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422001683},
author = {Fereshteh Khoshnam and Ahmad Baraani-Dastjerdi},
keywords = {Sentiment analysis (SA), Opinion mining, Explicit emotion recognition (EER), Implicit emotion recognition (IER), Language model (LM), Feature weighing, Dual framework, Ensemble method, Computational linguistics, Machine learning},
abstract = {One of the research domains in the field of sentiment analysis is automatic emotion recognition in texts which is a worthy topic in human-computer interaction. Text processing has always faced many challenges. The main one is the structural and semantic differences of sentences which have had a significant impact on the malfunction of auto-recognition systems. This problem becomes more prominent in short texts in which words and their concurrences are limited and insufficient. As a result of this, word frequency and TF-IDF weighing cannot well represent the relationship between words and the appropriate feature vector, leading to an undesirable accuracy of emotion recognition. Thus, different strategies should be applied to improve the feature vector and to formulate the features properly. The desired strategy should be able to identify the words that can distinguish between classes well and also to find the relationships between words and meaningful phrases using natural language processing concepts. In this paper, a combination of emotional models, categorical and hierarchical, are used for an emotional text recognition which could discover simultaneously explicit and implicit emotion in a short text. Our approach called DuFER, proposed a weighed method which improves the feature vector using language models and computational linguistics through applying a modified TF-IDF weighing to words as well as Maximum Likelihood Estimation weighing to expressions. Four implicit and explicit emotion datasets are used for the experiments. The results show that the accuracy of both implicit and explicit emotion recognition has increased and DuFER is actually the first successful dual framework in recognizing implicit and explicit emotions from text.}
}
@article{KRINGELBACH2020108128,
title = {Brain States and Transitions: Insights from Computational Neuroscience},
journal = {Cell Reports},
volume = {32},
number = {10},
pages = {108128},
year = {2020},
issn = {2211-1247},
doi = {https://doi.org/10.1016/j.celrep.2020.108128},
url = {https://www.sciencedirect.com/science/article/pii/S2211124720311177},
author = {Morten L. Kringelbach and Gustavo Deco},
abstract = {Summary
Within the field of computational neuroscience there are great expectations of finding new ways to rebalance the complex dynamic system of the human brain through controlled pharmacological or electromagnetic perturbation. Yet many obstacles remain between the ability to accurately predict how and where best to perturb to force a transition from one brain state to another. The foremost challenge is a commonly agreed definition of a given brain state. Recent progress in computational neuroscience has made it possible to robustly define brain states and force transitions between them. Here, we review the state of the art and propose a framework for determining the functional hierarchical organization describing any given brain state. We describe the latest advances in creating sophisticated whole-brain computational models with interacting neuronal and neurotransmitter systems that can be studied fully in silico to predict and design novel pharmacological and electromagnetic interventions to rebalance them in disease.}
}
@article{SUI2022377,
title = {Data-driven based four examinations in TCM: a survey},
journal = {Digital Chinese Medicine},
volume = {5},
number = {4},
pages = {377-385},
year = {2022},
issn = {2589-3777},
doi = {https://doi.org/10.1016/j.dcmed.2022.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S258937772200074X},
author = {Dong SUI and Lei ZHANG and Fei YANG},
keywords = {Traditional Chinese medicine (TCM), Four examinations, Data-driven, Machine learning, Computational intelligence},
abstract = {Traditional Chinese medicine (TCM) diagnosis is a unique disease diagnosis method with thousands of years of TCM theory and effective experience. Its thinking mode in the process is different from that of modern medicine, which includes the essence of TCM theory. From the perspective of clinical application, the four diagnostic methods of TCM, including inspection, auscultation and olfaction, inquiry, and palpation, have been widely accepted by TCM practitioners worldwide. With the rise of artificial intelligence (AI) over the past decades, AI based TCM diagnosis has also grown rapidly, marked by the emerging of a large number of data-driven deep learning models. In this paper, our aim is to simply but systematically review the development of the data-driven technologies applied to the four diagnostic approaches, i.e. the four examinations, in TCM, including data sets, digital signal acquisition devices, and learning based computational algorithms, to better analyze the development of AI-based TCM diagnosis, and provide references for new research and its applications in TCM settings in the future.}
}
@article{CAMERON2017131,
title = {Lateral thinking – Interocular symmetry and asymmetry in neurovascular patterning, in health and disease},
journal = {Progress in Retinal and Eye Research},
volume = {59},
pages = {131-157},
year = {2017},
issn = {1350-9462},
doi = {https://doi.org/10.1016/j.preteyeres.2017.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S135094621630091X},
author = {James R. Cameron and Roly D. Megaw and Andrew J. Tatham and Sarah McGrory and Thomas J. MacGillivray and Fergus N. Doubal and Joanna M. Wardlaw and Emanuele Trucco and Siddharthan Chandran and Baljean Dhillon},
keywords = {Interocular symmetry, Asymmetry, Retina, Retinal imaging, Retinal vasculature, Patterning},
abstract = {No biological system or structure is likely to be perfectly symmetrical, or have identical right and left forms. This review explores the evidence for eye and visual pathway asymmetry, in health and in disease, and attempts to provide guidance for those studying the structure and function of the visual system, where recognition of symmetry or asymmetry may be essential. The principal question with regards to asymmetry is not ‘are the eyes the same?’, for some degree of asymmetry is pervasive, but ‘when are they importantly different?’. Knowing if right and left eyes are ‘importantly different’ could have significant consequences for deciding whether right or left eyes are included in an analysis or for examining the association between a phenotype and ocular parameter. The presence of significant asymmetry would also have important implications for the design of normative databases of retinal and optic nerve metrics. In this review, we highlight not only the universal presence of asymmetry, but provide evidence that some elements of the visual system are inherently more asymmetric than others, pointing to the need for improved normative data to explain sources of asymmetry and their impact on determining associations with genetic, environmental or health-related factors and ultimately in clinical practice.}
}
@article{JIN2024e32590,
title = {Research hotspots and development trends of model and modelling education research: Bibliometric analysis based on CiteSpace},
journal = {Heliyon},
volume = {10},
number = {11},
pages = {e32590},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e32590},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024086213},
author = {Dongxue Jin and Min Jian},
keywords = {Model-based education research, Model and modelling, Bibliometric analysis, CiteSpace},
abstract = {Model-based learning and teaching are vital for addressing real-world challenges and are gaining research traction. This study, employing CiteSpace, analyses 583 articles, uncovering trends in authors, regions, and highly cited documents. Noteworthy focuses include learning achievements, technical support, and teaching approaches. Keyword analysis emphasises thinking cultivation and interdisciplinary integration. The study discusses current and future developments in modelling and modelling education research, particularly in learning evaluation and teacher professional development. Offering an international perspective, this analysis provides stakeholders with valuable insights. In summary, model-based learning's growth and influence are evident in the identified trends and future directions, guiding the field toward effective teaching strategies and solving complex problems. This research contributes to the broader understanding of modelling education's dynamics, facilitating informed decision-making for educators and policymakers.}
}
@article{SCOTT2020107269,
title = {CPC’s 50th Anniversary: Celebrating 50 years of open-source software in computational physics},
journal = {Computer Physics Communications},
volume = {252},
pages = {107269},
year = {2020},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2020.107269},
url = {https://www.sciencedirect.com/science/article/pii/S0010465520300886},
author = {N.S. Scott and A. Hibbert and J. Ballantyne and S. Fritzsche and A.L. Hazel and D.P. Landau and D.W. Walker and Z. Was},
keywords = {Computer Physics Communications, CPC Program Library, Collaborative Computational Project, Mendeley Data repository, Platform for Advanced Scientific Computing, Code Ocean},
abstract = {To celebrate the leading role Computer Physics Communications (CPC) has played in publishing open-source software in computational physics for over 50 years the editors are delighted to announce this Virtual Special Issue. Since 2018, coinciding with the 50th anniversary of the start of the CPC venture, thirty-two invited articles have been published. Each has been peer reviewed and each bears the header ‘CPC 50th anniversary article’. The special issue is in keeping with CPC’s ethos: it is focused on computational physics software and is accompanied by twenty-five software systems. The introduction to the collection also includes a personal reflection on Phil Burke, CPC’s founder, by Alan Hibbert, a lifelong colleague, who joined Queen’s University with Phil in the autumn of 1967. The distinctive feature of CPC is its Program Library which houses and distributes over 3500 open-source programs in computational physics. The introduction concludes with a description of key events in the history of the Program Library, its association with Queen’s University Belfast and its transfer to Elsevier’s Mendeley Data repository.}
}
@article{TAUB201510,
title = {The effect of computer science on physics learning in a computational science environment},
journal = {Computers & Education},
volume = {87},
pages = {10-23},
year = {2015},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2015.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S0360131515000913},
author = {Rivka Taub and Michal Armoni and Esther Bagno and Mordechai (Moti) Ben-Ari},
keywords = {Interdisciplinary projects, Programming and programming languages, Secondary education, Simulations, Teaching/learning strategies},
abstract = {College and high-school students face many difficulties when dealing with physics formulas, such as a lack of understanding of their components or of the physical relationships between the two sides of a formula. To overcome these difficulties some instructors suggest combining simulations' design while learning physics, claiming that the programming process forces the students to understand the physical mechanism activating the simulation. This study took place in a computational-science course where high-school students programmed simulations of physical systems, thus combining computer science (CS) and mathematics with physics learning. The study explored the ways in which CS affected the students' conceptual understanding of the physics behind formulas. The major part of the analysis process was qualitative, although some quantitative analysis was applied as well. Findings revealed that a great amount of the time was invested by the students on representing their physics knowledge in terms of computer science. Three knowledge domains were found to be applied: structural, procedural and systemic. A fourth domain which enabled reflection on the knowledge was found as well, the domain of execution. Each of the domains was found to promote the emergence of knowledge integration processes (Linn & Eylon, 2006, 2011), thus promoting students’ physics conceptual understanding. Based on these findings, some instructional implications are discussed.}
}
@article{DIAS2024103493,
title = {Artificial intelligence in the judiciary: A critical view},
journal = {Futures},
volume = {164},
pages = {103493},
year = {2024},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2024.103493},
url = {https://www.sciencedirect.com/science/article/pii/S0016328724001770},
author = {Stephanie Almeida de Jesus Dias and Renato Máximo Sátiro},
keywords = {Artificial Intelligence, Critical Theory, Judiciary},
abstract = {The main objective of this study is to raise questions about using artificial intelligence (AI) in the judiciary based on critical thinking. The essay approach used in this work aims to foster reflection and debate on the subject, presenting a review of theoretical perspectives with particular attention to the critical theory of the first generation of the Frankfurt School and theories that seek to analyze the relationship between humans and society/technology, such as the critical theory of technology, and stays away from the dominant currents of thinking in organizational studies to contribute unexplored perspectives. In this manner, it will go beyond existing benefits and applications, focusing on a critical view that identifies the elements that guide the technological choices that have been made, thus promoting a discussion that aggregates elements for future developments and improvements. Based on this theoretical context, this essay will raise questions and present three propositions summarizing the identified difficulties and directing future studies.}
}
@article{CHANG2014335,
title = {Computational architecture: Connecting the physical and virtual worlds},
journal = {Frontiers of Architectural Research},
volume = {3},
number = {4},
pages = {335-336},
year = {2014},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2014.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S2095263514000624},
author = {Teng-Wen Chang and Weixin Huang}
}
@incollection{COMBA2021241,
title = {2.14 - Computational Coordination Chemistry},
editor = {Edwin C. Constable and Gerard Parkin and Lawrence {Que Jr}},
booktitle = {Comprehensive Coordination Chemistry III},
publisher = {Elsevier},
address = {Oxford},
pages = {241-255},
year = {2021},
isbn = {978-0-08-102689-2},
doi = {https://doi.org/10.1016/B978-0-08-102688-5.00023-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780081026885000234},
author = {Peter Comba},
keywords = {Ab-initio quantum mechanics, Catalytic cycle, Charge distribution, Complex stability, DFT, Electronic structure, Ligand field theory, Molecular magnetism, Molecular mechanics, Molecular structure, Quantum chemistry, Reaction mechanism, Redox potential, Spectroscopy, Transition state},
abstract = {The computational modeling of metal complexes has been developed to an extent where a large variety of spectroscopic properties, reactivities and stabilities of mono- and oligonuclear complexes can be efficiently and reliably computed. There is a large arsenal of computational methods for the modeling of coordination compounds, spanning a wide range of scales in terms of theoretical basis, accuracy of the data in comparison with experiment, and accessibility in terms of computer power. Relevant current approaches and their limits and possible pitfalls that are discussed include ab-initio and DFT-based quantum-chemical, molecular mechanical, ligand-field-based methods and various combinations thereof, as well as approaches related to machine-learning, artificial intelligence, molecular docking and empirical structure-property correlations.}
}
@article{CESARI2017361,
title = {Frailty and Multimorbidity: Different Ways of Thinking About Geriatrics},
journal = {Journal of the American Medical Directors Association},
volume = {18},
number = {4},
pages = {361-364},
year = {2017},
issn = {1525-8610},
doi = {https://doi.org/10.1016/j.jamda.2016.12.086},
url = {https://www.sciencedirect.com/science/article/pii/S1525861017300348},
author = {Matteo Cesari and Mario Ulises Pérez-Zepeda and Emanuele Marzetti},
keywords = {Diseases, comprehensive geriatric assessment, aging, public health},
abstract = {The terms multimorbidity and frailty are increasingly used in the medical literature to measure the risk profile of an older individual in order to support clinical decisions and design ad hoc interventions. The construct of multimorbidity was initially developed and used in nongeriatric settings. It generates a monodimensional nosological risk profile, grounding its roots in the somewhat inadequate framework of disease. On the other hand, frailty is a geriatric concept that implies a more exhaustive and comprehensive assessment of the individual and his/her environment, facilitating the implementation of multidimensional and tailored interventions. This article aims to promote among geriatricians the use of terms that may better enhance their background and provide more value to their unrivaled expertise in caring for biologically aged persons.}
}
@article{CALOFFI2023122351,
title = {Innovation intermediaries' types and functions: A computational analysis of the literature},
journal = {Technological Forecasting and Social Change},
volume = {189},
pages = {122351},
year = {2023},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2023.122351},
url = {https://www.sciencedirect.com/science/article/pii/S0040162523000367},
author = {Annalisa Caloffi and Ana Colovic and Valentina Rizzoli and Federica Rossi},
keywords = {Innovation intermediaries, Computational literature review, Open innovation intermediaries, Innovation system intermediaries, Transition intermediaries, Cluster intermediaries, KIBS, Incubators},
abstract = {Innovation intermediaries have become numerous and diverse. Faced with this growing heterogeneity, there is the need to advance understanding of the organisations that engage in innovation intermediation activities. To do so, we use a combination of text mining and bibliometric techniques, and we identify seven different streams of literature, six of which refer to distinct types of intermediaries that perform specific functions and often involve specific types of organisations. Looking at the evolution of the different streams of literature over time, we find that the early contributions focused on university incubators, science and technology parks, and the role they play within different types of innovation systems. More recently, the focus has shifted to the role of intermediaries in supporting sustainable transitions. Despite the differences between the various types of intermediaries and the literature streams that analyse them, the bibliographic coupling shows that all strands of literature have a common theoretical basis, which includes the open innovation approach and revolves around the role of science parks and incubators.}
}
@article{WALSH2021143,
title = {Computational Cognitive Modeling of Human Calibration and Validity Response Scoring for the Graduate Record Examinations (GRE)},
journal = {Journal of Applied Research in Memory and Cognition},
volume = {10},
number = {1},
pages = {143-154},
year = {2021},
note = {Culture & Memory},
issn = {2211-3681},
doi = {https://doi.org/10.1016/j.jarmac.2020.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S2211368120300747},
author = {Matthew M. Walsh and Burcu Arslan and Bridgid Finn},
keywords = {Constructed response scoring, Graduate record examinations, Predictive performance equation, Skill decay},
abstract = {Most research on skill acquisition and retention focuses on the individual being tested. Yet sometimes another person is responsible for evaluating the individual’s performance. Here, we study the acquisition and retention of rater skill using data for the Graduate Record Examinations (GRE). Our work is based on the idea that response scoring, like other cognitive skills, will gradually improve with amount of practice, and decline with elapsed time since that practice occurred. These classic findings are the focus of a computational cognitive model called the Predictive Performance Equation (PPE). However, the generalizability of these findings to response scoring and the applicability of PPE to that domain have not yet been demonstrated. To address this issue, we leveraged a naturalistic dataset containing rating performance from over 23,000 sessions. Our analyses provide empirical support for PPE and establish a basis for using a model like PPE to personalize rater training requirements.}
}
@article{SMITH1990121,
title = {Writing, thinking, computing},
journal = {Poetics},
volume = {19},
number = {1},
pages = {121-142},
year = {1990},
issn = {0304-422X},
doi = {https://doi.org/10.1016/0304-422X(90)90033-2},
url = {https://www.sciencedirect.com/science/article/pii/0304422X90900332},
author = {John B. Smith and Catherine F. Smith},
abstract = {The computer has become the preferred tool for many writers. Over the next few years, it is likely to become the predominant tool. Since writing is fundamentally a mediated activity and since the tool inevitably affects the tool user, we need to consider how a tool as powerful as the computer is affecting writers. To address this issue, we consider the following questions: &#x02022;- What are writers saying about computers?&#x02022;- How are writers using computers?&#x02022;- What does this mean for the teaching of writing?&#x02022;- What does this mean for designers of future writing systems?&#x02022;- How does the computer affect writer's thinking? We are led to the conclusion that new, comprehensive writing environments are both needed and inevitable, and they, in turn, will lead to a form of enhanced, or amplified, thinking. But using these environments and developing this kind of thinking will also require new forms of instruction. Adapting to these changes will pose practical as well as intellectual challenges for the composition community. To meet these challenges tomorrow, we must begin considering the relationships among writing, thinking, and computing today.}
}
@article{SRIHARI20141083,
title = {Role of automation in the examination of handwritten items},
journal = {Pattern Recognition},
volume = {47},
number = {3},
pages = {1083-1095},
year = {2014},
note = {Handwriting Recognition and other PR Applications},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2013.09.032},
url = {https://www.sciencedirect.com/science/article/pii/S0031320313004044},
author = {Sargur N. Srihari and Kirsten Singer},
keywords = {Handwriting examination, Forensic document examination, Writer verification, Writer identification, Computational forensics, Expert system validation},
abstract = {Several automation tools have been developed over the years for forensic document examination (FDE) of handwritten items. Integrating the developed tools into a unified framework is considered and the essential role of the human in the process is discussed. The task framework is developed by considering the approach of computational thinking whose components are abstraction, algorithms, mathematical models and ability to scale. Beginning with the human FDE procedure expressed in algorithmic form, mathematical and software implementations of individual steps of the algorithm are described. Advantages of the framework are discussed, including efficiency (ability to scale to tasks with many handwritten items), reproducibility and validation/improvement of existing manual procedures. It is indicated that as with other expert systems, such as for medical diagnosis, current automation tools are useful only as part of a larger manually intensive procedure. This viewpoint is illustrated with a well-known FDE case, concerning the Lindbergh kidnapping with a new hypothesis – in this case, there are multiple questioned documents, possibility of multiple writers of the same document, determining whether the writing is disguised, known writing is formal while questioned writing is informal, etc. Observations are made for future developments, where human examiners provide handwriting characteristics while computational methods provide the necessary statistical analysis.}
}
@article{GARG201426,
title = {Modeling, analyzing and slicing periodic distributed computations},
journal = {Information and Computation},
volume = {234},
pages = {26-43},
year = {2014},
issn = {0890-5401},
doi = {https://doi.org/10.1016/j.ic.2013.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0890540113001260},
author = {Vijay K. Garg and Anurag Agarwal and Vinit Ogale},
keywords = {Predicate detection, Liveness violation, d-Diagram, Recurrent computation},
abstract = {The earlier work on predicate detection has assumed that the given computation is finite. Detecting violation of a liveness predicate requires that the predicate be evaluated on an infinite computation. In this work, we develop the theory and associated algorithms for predicate detection in infinite runs. In practice, an infinite run can be determined in finite time if it consists of a recurrent behavior with some finite prefix. Therefore, our study is restricted to such runs. We introduce the concept of d-diagram, which is a finite representation of infinite directed graphs. Given a d-diagram that represents an infinite distributed computation, we solve the problem of determining if a global predicate ever became true in the computation. The crucial aspect of this problem is the stopping rule that tells us when to conclude that the predicate can never become true in future. We also provide an algorithm to provide vector timestamps to events in the computation for determining the dependency relationship between any two events in the infinite run. Finally, we give an algorithm to compute a slice of a d-diagram which concisely captures all the consistent global states of the computation satisfying the given predicate.}
}
@article{KAUFFMAN201525,
title = {Infinite computations and the generic finite},
journal = {Applied Mathematics and Computation},
volume = {255},
pages = {25-35},
year = {2015},
note = {Special issue devoted to the international conference ‘‘Numerical computations: Theory and Algorithms’’ June 17–23, 2013, Falerna, Italy},
issn = {0096-3003},
doi = {https://doi.org/10.1016/j.amc.2014.06.054},
url = {https://www.sciencedirect.com/science/article/pii/S009630031400890X},
author = {Louis H. Kauffman},
keywords = {Grossone, , Finite, Infinite, Generic finite, Category},
abstract = {This paper introduces the concept of a generic finite set and points out that a consistent and significant interpretation of the grossone, ① notation of Sergeyev is that ① takes the role of a generic natural number. This means that ① is not itself a natural number, yet it can be treated as one and used in the generic expression of finite sets and finite formulas, giving a new power to algebra and algorithms that embody this usage. In this view,N={1,2,3,…,①-2,①-1,①}is not an infinite set, it is a symbolic structure representing a generic finite set. We further consider the concept of infinity in categories. An object A in a given category C is infinite relative to that category if and only if there is a injection J:A⟶A in C that is not a surjection. In the category of sets this recovers the usual notion of infinity. In other categories, an object may be non-infinite (finite) while its underlying set (if it has one) is infinite. The computational methodology due to Sergeyev for executing numerical calculations with infinities and infinitesimals is considered from this categorical point of view.}
}
@incollection{LOURDUSAMY202091,
title = {7 - Computational intelligence using ontology—A case study on the knowledge representation in a clinical decision support system},
editor = {Jitendra Kumar Verma and Sudip Paul and Prashant Johri},
booktitle = {Computational Intelligence and Its Applications in Healthcare},
publisher = {Academic Press},
pages = {91-104},
year = {2020},
isbn = {978-0-12-820604-1},
doi = {https://doi.org/10.1016/B978-0-12-820604-1.00007-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128206041000078},
author = {Ravi Lourdusamy and Xavierlal J. Mattam},
keywords = {Clinical decision support systems, Knowledge representation, Computational semantics, Ontology, Ontological engineering},
abstract = {Computational intelligence has been traditionally associated with neural networks, fuzzy systems, and genetic algorithms. Over the years there have been many developments in computational intelligence. At present, many other fields are part of the study and research in computational intelligence. With advances in cognitive sciences, more techniques of information processing by machines that show characteristics closely associated with human intelligence are being found. Some of these techniques have been studied for a long time, but in recent years there has been some maturity in the understanding and use of these techniques. One such technique is the use of semantics in computational intelligence. There has been a long-drawn-out philosophical debate between lingualism, which claims that there is no human thought without language, and “language of thought” theories, which believe that natural language is inessential to private thought. In an attempt to create intelligent machines, the use of semantics for knowledge representation and knowledge-based creation in a system follows the philosophy of lingualism. Different knowledge representations are used in a knowledge-based clinical decision support system. This chapter makes a study of various knowledge representations. The different theories behind the techniques used in the knowledge representations are discussed. The philosophy of lingualism and the use of semantics in computational intelligence are explained, while a study on semantic knowledge representation in clinical decision support systems is made. The conclusion is the explanation as to how ontological engineering can be used to create computational intelligence.}
}
@incollection{BYRNE1997105,
title = {Cognitive Processes in Counterfactual Thinking about what Might Have Been},
editor = {Douglas L. Medin},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {37},
pages = {105-154},
year = {1997},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(08)60501-0},
url = {https://www.sciencedirect.com/science/article/pii/S0079742108605010},
author = {Ruth M.J. Byrne}
}
@article{YANG2024112150,
title = {Demand response strategy of user-side energy storage system and its application to reliability improvement},
journal = {Journal of Energy Storage},
volume = {92},
pages = {112150},
year = {2024},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2024.112150},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X24017365},
author = {Hejun Yang and Qiang Chen and Yue Liu and Yinghao Ma and Dabo Zhang},
keywords = {Power system reliability, Electricity pricing strategy, User-side energy storage system, Demand management},
abstract = {The time of use (TOU) strategy is being carried out in the power system for shifting load from peak to off-peak periods. For economizing the electricity bill of industry users, the trend on configuring user-side energy storage system (UES) by users will increase continuously. On the base of currently implemented TOU environment, designing an efficient and non-utility-dispatched guidance strategy for UES to realize the peak-shaving and valley-filling will have a great significance. Therefore, this paper firstly proposes a thinking based on a linear piecewise-shape pricing strategy for guiding UES to decrease the peak-valley difference although storage has not been dispatched by utility and always operates in its maximum benefit. In addition, benefit of user with UES has been guaranteed to be non-decreased compared to traditional TOU. Then, this paper establishes a planning-operation co-optimization model for UES to pursue its maximum net benefit, in which the proposed electricity pricing strategy has been incorporated. Thirdly, a linearized reliability improvement calculation method contributed by storage has been presented. Finally, the numerical results have verified the effectiveness of the proposed strategy, and in the designed case condition, there is an obvious improvement in the percentage of peak-valley difference and the reliability level.}
}
@article{LIU2023101339,
title = {Promoting primary school students’ creativity via reverse engineering pedagogy in robotics education},
journal = {Thinking Skills and Creativity},
volume = {49},
pages = {101339},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101339},
url = {https://www.sciencedirect.com/science/article/pii/S1871187123001086},
author = {Xiaohong Liu and Jianjun Gu and Li Zhao},
keywords = {Creativity, Reverse engineering pedagogy, Creative thinking, Robotics education, Creative self-efficacy},
abstract = {Creativity is an essential basic skill for students. In this study, the reverse engineering pedagogy (REP) was applied to primary school students’ robotics education course, with the aim of investigating the influence of REP and project-based pedagogy (PBP) on the cultivation of students' creativity. A quasi-experimental study that utilized a non-equivalent groups design was conducted with 91 fifth-grade students, comprising a control group (n = 46) who received the PBP intervention, and an intervention group (n = 45) who received the REP intervention. Creative self-efficacy, Torrance Tests of Creative Thinking Figural (TTCT-Figural), and assessment of the students’ robotic creative products were conducted to evaluate students’ creativity. In addition, t tests, ANCOVA, and ANOVA were used to analyze the data to determine whether REP could improve students’ creativity better than PBP. The results showed that REP could enhance students’ creative self-efficacy and their robotic creative products score more than PBP could, but not the TTCT-Figural score after the confounding effect of fluency was controlled for. In the intervention group, creative self-efficacy and creative thinking were improved after intervention. Overall, REP has more advantages than PBP for promoting primary school students' creativity. The findings of this study provide a reference and teaching strategies guidance for the cultivation of K-12 students' creativity in robotics education.}
}
@article{BICER2024101462,
title = {Mathematical creativity in upper elementary school mathematics curricula},
journal = {Thinking Skills and Creativity},
volume = {51},
pages = {101462},
year = {2024},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2024.101462},
url = {https://www.sciencedirect.com/science/article/pii/S1871187124000014},
author = {Ali Bicer and Helen Aleksani and Chuck Butler and Traci Jackson and Tricia Dawn Smith and Michael Bostick},
keywords = {Creativity in mathematics, Creativity-directed tasks, Curriculum},
abstract = {Textbooks should promote creative tasks for teachers to enhance students' math skills through creative thinking, instead of relying on memorization of step-by-step procedures. The aim of this study is to analyze elementary school mathematics curricula commonly employed in the U.S., namely GoMath, enVision Math, Math Connects, MyMath, and Investigations. The selection of these curricula is based on their widespread usage, and the analysis seeks to evaluate their effectiveness in fostering the development of students' creative thinking skills. We employed Bicer et al.'s (2021) framework for creativity-directed tasks to analyze 1,000 mathematical tasks within each curriculum. The analysis unveiled that Eureka, followed by Investigations, incorporated a higher proportion of creativity-directed tasks compared to the remaining three curricula. For some categories and subcategories of creativity-directed tasks (e.g., communication, connection), the results are less varied across five curricula. The present study enables school districts, schools, and classroom teachers to know what curricula support the development of creative thinking of students by including various options of creativity-directed tasks in their upper elementary mathematics textbooks.}
}
@article{WINITZKY19921,
title = {Structure and process in thinking about classroom management: An exploratory study of prospective teachers},
journal = {Teaching and Teacher Education},
volume = {8},
number = {1},
pages = {1-14},
year = {1992},
issn = {0742-051X},
doi = {https://doi.org/10.1016/0742-051X(92)90036-3},
url = {https://www.sciencedirect.com/science/article/pii/0742051X92900363},
author = {Nancy Winitzky},
abstract = {Current research on teaching centers on teachers' thinking. What teachers know and, especially, how they reflect on their practice are considered central to understanding teaching. Part of the research on teachers' knowledge concerns cognitive structure (schemata), how teachers organize their knowledge; it is thought that complex, highly structured schemata are related to skillful teaching performance. While it is sensible to assume that schemata and reflection, structure and process, are linked, no data exist to support that assumption. In the present study, cognitive structure and reflection data were collected from 15 prospective teachers. The strength of the correlation between those variables was .48 (p = .05). Implications for theory, research, and practice are discussed.}
}
@article{SOSA2018157,
title = {Innovation Teams and Organizational Creativity: Reasoning with Computational Simulations},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {4},
number = {2},
pages = {157-170},
year = {2018},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2018.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S240587261730076X},
author = {Ricardo Sosa and Andy Connor},
keywords = {Organizational climate, Agent-based simulation, Creative teams, Leadership},
abstract = {A computational social simulation encourages systematic reasoning about the management of innovation teams and organizational creativity. This article draws upon historical literature to identify a potential dilemma faced by business organizations: Is it better to promote creative behavior across a whole organization or focus on the development of small and highly creative teams? We formulate the dilemma from the literature on organizational creativity, and explore it using a multi-agent simulation. Our study models creative behavior abstractly, as the ability to introduce novelty. By varying the scale and scope of non-conformist behavior in the simulation, our research supports the systematic study of the breadth vs. depth dilemma. The results of this study invite an informed examination of strategies to sustain innovation based on the introduction of either a small number of significantly novel ideas, or a large number of novel but more familiar ideas. Results from this study on change agency also indicate that there is a possible trade-off between a highly creative team and its creative efficiency, drawing attention to the importance of a creative critical mass in an organization. We also discuss the implications of these results and our research approach.}
}
@article{CAIN2023197,
title = {Navigating Design, Data, and Decision in an Age of Uncertainty},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {9},
number = {2},
pages = {197-212},
year = {2023},
note = {The Future of Design Education: Rethinking Design Education for the 21st Century},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2023.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S2405872623000448},
author = {John Cain and Zach Pino},
keywords = {Data literacy, Design-driven data, Interconnected systems, Systemic implications, Uncertainty},
abstract = {The Future of Design Education working group on technical systems argues that the approach to handling data—the methods used, and the expectations for outcomes—can transform design practice. In contrast to design’s past defined by a lack of accessible data, today’s rapidly evolving age of data abundance informs the choices available—the decision space—with far-reaching consequences for organizational, social, and environmental well-being. The shifting design landscape requires new tools and techniques to navigate this data age effectively. This paper proposes a new curricular approach that intersects data, technology, and design to create an environment where students can evaluate their roles and impact, and interact effectively through data with humans and computational collaborators. This data-oriented curriculum includes foundational technical skills proficiency, data analytical skills, rhetorical skills for arguing with data, interdisciplinary design studies, and a focus on designing for society. It embraces the complexities and opportunities of the data age, and acknowledges the inherent uncertainty in this new landscape. The aim is to prepare the next generation of designers to create data-informed, human-centered, ethical, and sustainable designs, thereby fostering an inclusive, equitable, and sustainable future.}
}
@article{LEE20152858,
title = {The Benin experience: How computational modeling can assist major vaccine policy changes in low and middle income countries},
journal = {Vaccine},
volume = {33},
number = {25},
pages = {2858-2861},
year = {2015},
issn = {0264-410X},
doi = {https://doi.org/10.1016/j.vaccine.2015.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S0264410X15004752},
author = {Bruce Y. Lee and Benjamin Schreiber and Angela R. Wateska and Diana L. Connor and Hamadou M. Dicko and Philippe Jaillard and Mercy Mvundura and Carol Levin and Mélanie Avella and Leila A. Haidari and Shawn T. Brown},
keywords = {Benin, Vaccine, Supply chain, Computational modeling},
abstract = {While scientific studies can show the need for vaccine policy or operations changes, translating scientific findings to action is a complex process that needs to be executed appropriately for change to occur. Our Benin experience provided key steps and lessons learned to help computational modeling inform and lead to major policy change. The key steps are: engagement of Ministry of Health, identifying in-country “champions,” directed and efficient data collection, defining a finite set of realistic scenarios, making the study methodology transparent, presenting the results in a clear manner, and facilitating decision-making and advocacy. Generating scientific evidence is one component of policy change. Enabling change requires orchestration of a coordinated set of steps that heavily involve key stakeholders, earn their confidence, and provide them with relevant information. Our Benin EVM+CCEM+HERMES Process led to a decision to enact major changes and could serve as a template for similar approaches in other countries.}
}
@article{ANDREJCZUK2019104799,
title = {Synergistic team composition: A computational approach to foster diversity in teams},
journal = {Knowledge-Based Systems},
volume = {182},
pages = {104799},
year = {2019},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2019.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950705119302746},
author = {Ewa Andrejczuk and Filippo Bistaffa and Christian Blum and Juan A. Rodríguez-Aguilar and Carles Sierra},
keywords = {Team composition, Exact algorithms, Heuristic algorithms, Optimisation, Coalition formation},
abstract = {Co-operative learning in heterogeneous teams refers to learning methods in which teams are organised both to accomplish academic tasks and for individuals to gain knowledge. Competencies, personality and the gender of team members are key factors that influence team performance. Here, we introduce a team composition problem, the so-called synergistic team composition problem (STCP), which incorporates such key factors when arranging teams. Thus, the goal of the STCP is to partition a set of individuals into a set of synergistic teams: teams that are diverse in personality and gender and whose members cover all required competencies to complete a task. Furthermore, the STCP requires that all teams are balanced in that they are expected to exhibit similar performances when completing the task. We propose two efficient algorithms to solve the STCP. Our first algorithm is based on a linear programming formulation and is appropriate to solve small instances of the problem. Our second algorithm is an anytime heuristic that is effective for large instances of the STCP. Finally, we thoroughly study the computational properties of both algorithms in an educational context when grouping students in a classroom into teams using actual-world data.}
}
@article{NEILL2024100870,
title = {Designer delectables; exploring the design practice of haute couture and haute cuisine},
journal = {International Journal of Gastronomy and Food Science},
volume = {35},
pages = {100870},
year = {2024},
issn = {1878-450X},
doi = {https://doi.org/10.1016/j.ijgfs.2024.100870},
url = {https://www.sciencedirect.com/science/article/pii/S1878450X24000039},
author = {Lindsay Neill and Nigel Hemmington and Christine McDonald and Francesca Zampollo},
keywords = {Design practice, Haute couture, Haute cuisine, Designerly thinking},
abstract = {This study explores design practice across two domains: haute couture (fashion), and haute cuisine (food). A case study approach was taken using the voice of practitioners as the focus through in-depth qualitative interviews. The cross-domain approach revealed similarities in design practice through four design themes: visualization, ‘conversations’ with materials, co-creation and ‘pushing boundaries’. The data also revealed innovations within the four themes that could apply to other design domains, for example visualization (haute couture) and co-creation (haute cuisine). The practitioners also provided valuable and nuanced insights into their design practice – ‘You have to live something to do it’. These insights from practitioners and their practice reveal how the two domains hold similarities in design practice and provide a deeper understanding of design processes, and designerly thinking, from which creativity and innovation can emerge.}
}
@article{GHAVANLOO20231,
title = {Experimental and computational physics of fullerenes and their nanocomposites: Synthesis, thermo-mechanical characteristics and nanomedicine applications},
journal = {Physics Reports},
volume = {996},
pages = {1-116},
year = {2023},
note = {Experimental and computational physics of fullerenes and their nanocomposites: Synthesis, thermo-mechanical characteristics and nanomedicine applications},
issn = {0370-1573},
doi = {https://doi.org/10.1016/j.physrep.2022.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0370157322003775},
author = {Esmaeal Ghavanloo and Hashem Rafii-Tabar and Ayesha Kausar and Georgios I. Giannopoulos and S. Ahmad Fazelzadeh},
keywords = {Fullerene molecules, Nanocomposites, Synthesis, Computational modeling, Thin films, Mechanical properties, Thermal properties, Vibrational properties, Molecular dynamics, Molecular mechanics, Micromechanics, Nanomedicine, Nanoneuroscience application},
abstract = {It is an established paradigm in the emerging fields of nanoscience, nanotechnology and molecular engineering that a very important domain of fundamental research is associated with carbon-based materials. Ever since the discovery of the first member of the fullerene family (C60) in 1985, and the subsequent discovery of the other members, fullerenes as a nanoscopic allotrope of carbon with anticipated extensive applications in all areas of nanoscience and nanotechnology (both industrial and medical), materials science and engineering, condensed matter physics and chemistry have occupied a central position in research activities across the globe. Detailed investigations, both experimental and theoretical/computational, into their morphology, mechanical, thermal, chemical, biological, electronic, optical and structural properties have led to the emergence of a well-established and independent science of fullerenes, providing very valuable information both in basic and applied sciences. A comprehensive review of these properties of fullerenes, particularly their applications in the above fields will provide valuable up-to-date and essential background information for engaging in new research in this field and also be able to develop new concepts and applications of these exotic carbon structures. For instance, a recent development is their applications in the emerging field of nanoneuroscience, a field interfacing nanoscience and neuroscience. In this extensive, albeit selective survey, related mainly to the C60 fullerenes, the processes involving their experimental synthesis, theoretical formulation of their geometrical structures, their mechanical and thermal properties and nanomedical applications have been reviewed and summarized both within the experimental and theoretical/computational domains. Essential theoretical concepts, ranging from discrete atomistic molecular dynamics and molecular mechanics methods to continuum-based methods have been expounded in order to facilitate the pursuance of the reviewed literature and also to aid in the development of further research in this field.}
}
@incollection{KARACA202221,
title = {Chapter 3 - Multi-chaos, fractal and multi-fractional AI in different complex systems},
editor = {Yeliz Karaca and Dumitru Baleanu and Yu-Dong Zhang and Osvaldo Gervasi and Majaz Moonis},
booktitle = {Multi-Chaos, Fractal and Multi-Fractional Artificial Intelligence of Different Complex Systems},
publisher = {Academic Press},
pages = {21-54},
year = {2022},
isbn = {978-0-323-90032-4},
doi = {https://doi.org/10.1016/B978-0-323-90032-4.00016-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032390032400016X},
author = {Yeliz Karaca},
keywords = {Chaotic systems, Complex order, Complexity, Computational complexity, Data ethics, Different complex systems, Dynamics of complex systems, Evolution, Fractional thinking, Nonlinearity and irregularity},
abstract = {Modern scientific thinking adopts the systemic properties and addresses them through revealing the spontaneous processes related to self-organization in a dynamical system in a state far from the equilibrium point and close to the disequilibrium point with no existence of external force acting upon the system. The modern way of thinking poses a challenge against the dichotomy between the natural world and social world, by taking into account the concepts around complexity, evolution and order. This study provides an overview encompassing multi-chaos, fractal, fractional and Artificial Intelligence (AI) way of thinking for the solution of the complex system problems concerned with natural and social sciences. Furthermore, ethical decision-making frameworks and strategies concerning big data and AI applications to provide assistance for the identification of the related problems in different settings and help thinking in a methodical manner with a deliberative compensating process so that tensions between conflicting aspects can be managed systematically. The values related to ethical issues, which are thorny in nature, point to being practical, flexible and problem-driven rather than purely theory-driven in order that dilemmas can be addressed and critical decision-making guided in a way beyond theoretical positions with a focus on applied aspects. Through the lenses of such transformative thinking along with mathematics-informed frameworks encompassing chaos, fractal and multi-fractional ways, the incorporation of technology, with Artificial Intelligence, as the most viable and far-reaching leg, is essentially required to be able to address and tackle complexity that has chaotic, nonlinear, and dynamic characteristics. Hence, optimized solutions can be conceived and implemented efficiently and in a facilitating way with some required degree of flexibility as well. Considering the impact and ubiquity of data technologies concerning all aspects of modern life, it becomes important to establish a balance between data use and ethical matters. Computational technologies in different complex systems based on mathematical-driven informed frameworks can enable the generation of more realistic and applicable adaptive models under transient, dynamic and ever-evolving conditions of different complex systems.}
}
@article{TEZDUYAR19971349,
title = {Parallel computational methods for 3D simulation of a parafoil with prescribed shape changes},
journal = {Parallel Computing},
volume = {23},
number = {9},
pages = {1349-1363},
year = {1997},
note = {Parallel computing methods in applied fluid mechanics},
issn = {0167-8191},
doi = {https://doi.org/10.1016/S0167-8191(97)00057-4},
url = {https://www.sciencedirect.com/science/article/pii/S0167819197000574},
author = {T. Tezduyar and V. Kalro and W. Garrard},
keywords = {Parallel finite elements, Parafoil dynamics, Space-time formulation, 3D simulation},
abstract = {In this paper we describe parallel computational methods for 3D simulation of the dynamics and fluid dynamics of a parafoil with prescribed, time-dependent shape changes. The mathematical model is based on the time-dependent, 3D Navier-Stokes equations governing the incompressible flow around the parafoil and Newton's law of motion governing the dynamics of the parafoil, with the aerodynamic forces acting on the parafoil calculated from the flow field. The computational methods developed for these 3D simulations include a stabilized space-time finite element formulation to accommodate for the shape changes, special mesh generation and mesh moving strategies developed for this purpose, iterative solution techniques for the large, coupled nonlinear equation systems involved, and parallel implementation of all these methods on scalable computing systems such as the Thinking Machines CM-5. As an example, we report 3D simulation of a flare maneuver in which the parafoil velocity is reduced by pulling down the flaps. This simulation requires solution of over 3.6 million coupled, nonlinear equations at every time step of the simulation.}
}
@article{MCCLELLAND1993209,
title = {Computational approaches to cognition: top-down approaches},
journal = {Current Opinion in Neurobiology},
volume = {3},
number = {2},
pages = {209-216},
year = {1993},
issn = {0959-4388},
doi = {https://doi.org/10.1016/0959-4388(93)90212-H},
url = {https://www.sciencedirect.com/science/article/pii/095943889390212H},
author = {James L. McClelland and David C. Plaut},
abstract = {Computational models are useful tools for exploring the nature of human cognitive processes. In particular, connectionist models are providing researchers with new ways of thinking about the basic nature of cognition and its implementation in the brain. They support novel explanations of important aspects of perception, memory, language, thought and cognitive development, and allow cognitive processes to be linked with the underlying physiological mechanisms. The models also aid our understanding of how disorders of brain function lead to disorders of cognition.}
}
@article{HIBERTY1998237,
title = {Thinking and computing valence bond in organic chemistry1Dedicated to the memory of Professor Joseph Gerratt, in appreciation of his outstanding contributions to modern ab initio valence bond methodology.1},
journal = {Journal of Molecular Structure: THEOCHEM},
volume = {451},
number = {3},
pages = {237-261},
year = {1998},
issn = {0166-1280},
doi = {https://doi.org/10.1016/S0166-1280(98)00208-5},
url = {https://www.sciencedirect.com/science/article/pii/S0166128098002085},
author = {Philippe C. Hiberty},
keywords = {Valence bond, Hybridization, Symmetry breaking, Resonance energy, Breathing orbitals},
abstract = {This paper presents a short survey of some recent ab initio valence bond methods and their applications, and is aimed at justifying and encouraging a valence bond view of organic chemistry, as complementary to the molecular orbital approach. In the first section, the qualitative VB description of the elementary interactions is recalled and compared to the MO model. It is shown that the VB picture is fundamentally correct, even for the well-known cases of the low-lying states of dioxygen and the 4n/4n+2 aromaticity rule. The second section briefly discusses the classical VB method, which deals with atomic orbitals that are optimized for the free atoms and kept unchanged in molecules, then describes modern ab initio VB methods that all perform orbital optimization in molecular calculations. The generalized valence bond and spin-coupled theories both provide a one-configuration wavefunction. While the former is generally used with some time-saving restrictions such as the strong-orthogonality restriction and the perfect-pairing approximation, the latter releases any orthogonality constraint and allows all possible spin couplings. Multiconfiguration methods are also discussed, as well as methods using different orbitals for different structures. Some computational applications of these methods are presented in the last section. It is shown that if given full freedom to optimize its shape with the variational principle as a unique criterion, a one-configuration wavefunction spontaneously takes the form of a VB wavefunction displaying localized orbitals, and presents a picture in terms of hybrid orbitals and/or resonance between limiting structures, very close to the traditional qualitative picture. The concept of hybridization is firmly supported, as the unique outcome of the highest computational level still compatible with the orbital picture. The description of conjugated systems in terms of resonating Kekulé structures is also fully justified and shown to be the best framework for discussing questions such as the distortive tendencies of conjugated π-electronic systems, or violations of Hund's rules. The ab initio VB approach can be used for quantifying some traditional paradigms such as the role of the delocalization energy in the acidity of carboxylic acids and enols, or in the properties of the amide/thioamide functional group. It is also shown to be an elegant solution to some difficult computational problems like the symmetry-breaking artefact or the inclusion of dynamical correlation in the description of the chemical bond. Lastly, some of the methods presented here are shown to be appropriate for the calculation of diabatic potential surfaces, with applications to the Shaik–Pross reactivity model of the VB curve-crossing correlation diagrams.}
}
@article{QUINLAN2007413,
title = {Re-thinking stages of cognitive development: An appraisal of connectionist models of the balance scale task},
journal = {Cognition},
volume = {103},
number = {3},
pages = {413-459},
year = {2007},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2006.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0010027706000552},
author = {Philip T. Quinlan and Han L.J. {van der Maas} and Brenda R.J. Jansen and Olaf Booij and Mark Rendell},
keywords = {Connectionist models, Balance scale task, Latent class analysis},
abstract = {The present paper re-appraises connectionist attempts to explain how human cognitive development appears to progress through a series of sequential stages. Models of performance on the Piagetian balance scale task are the focus of attention. Limitations of these models are discussed and replications and extensions to the work are provided via the Cascade-Correlation algorithm. An application of multi-group latent class analysis for examining performance of the networks is described and these results reveal fundamental functional characteristics of the networks. Evidence is provided that strongly suggests that the networks are unable to acquire a mastery of torque and, although they do recover certain rules of operation that humans do, they also show a propensity to acquire rules never previously seen.}
}
@article{IDEKER2009820,
title = {The Thinking Man's Cell},
journal = {Cell},
volume = {138},
number = {5},
pages = {820-821},
year = {2009},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2009.08.024},
url = {https://www.sciencedirect.com/science/article/pii/S0092867409010411},
author = {Trey Ideker}
}
@incollection{CAPELLI2023105,
title = {4 - 3D-printed and computational models: a combined approach for patient-specific studies},
editor = {Deepak M. Kalaskar},
booktitle = {3D Printing in Medicine (Second Edition)},
publisher = {Woodhead Publishing},
edition = {Second Edition},
pages = {105-125},
year = {2023},
series = {Woodhead Publishing Series in Biomaterials},
isbn = {978-0-323-89831-7},
doi = {https://doi.org/10.1016/B978-0-323-89831-7.00011-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323898317000110},
author = {Claudio Capelli and Michele Bertolini and Silvia Schievano},
keywords = {Patient-specific models, cardiovascular models, segmentation, 3D printing, finite element analyses, computational fluid dynamics, validation},
abstract = {Three-dimensional printed models have been increasingly used in many fields of medicine. The most common benefits include a better understanding of anatomical details, an improved communication between clinicians and patients, a more accurate planning of treatments, and new opportunities for procedural training. In the cardiovascular field, this technology has contributed to improve the management of complex cases, in particular congenital heart disease, by fostering personalized preprocedural planning and increasing medical trainees’ confidence. Cardiovascular structures, however, are extremely challenging to replicate using materials compatible with current 3D printing technologies. Hence, patient-specific computational models, generated from the same set of medical images as printed ones, can be combined to 3D printing technology to simulate different conditions and identify the optimal treatment for each specific patient. A further step forward is represented by the integration of advanced visualization techniques like augmented and virtual reality, to close still existing loopholes. In this chapter, we review the current possibilities associated with the use of patient-specific models, in the context of cardiovascular applications.}
}
@article{STAHL199833,
title = {Is step-j thinking an arbitrary modelling restriction or a fact of human nature?},
journal = {Journal of Economic Behavior & Organization},
volume = {37},
number = {1},
pages = {33-51},
year = {1998},
issn = {0167-2681},
doi = {https://doi.org/10.1016/S0167-2681(98)00075-4},
url = {https://www.sciencedirect.com/science/article/pii/S0167268198000754},
author = {Dale O. Stahl},
abstract = {In `Boundedly Rational Rule Learning in a Guessing Game,' Games and Economic Behavior, 16 (1996), we combined Nagel's (1995) model of boundedly rational players with a `law of effect' learning model, and the synthesis outperformed alternative theories when confronting Nagel's data. In that model, there were four boundedly rational behavioral rules (step-j, j=0, 1, 2, 3), each corresponding to an integer level of depth of reasoning. It is legitimate to ask for a justification of the restriction to these `integer' rules. Why is it not reasonable to suppose that some player believes that 50% of the population is step-0, and 50% is step-1, and so himself adopts something like a step-1.5 rule? This paper constructs a tractable model with potentially infinitely many non-integer rules and conducts comparison tests. The main conclusion is that allowing for non-integer rules does not help to explain the data. Therefore, by Occam's Razor, the integer rule model is preferred. These results suggest that step-j thinking is a fact of human nature rather that an arbitrary modelling restriction.}
}
@incollection{ZHENG202483,
title = {Chapter Five - Middle vision: Computational Knowledge Vision for visual translation},
editor = {Wenbo Zheng and Fei-Yue Wang},
booktitle = {Computational Knowledge Vision},
publisher = {Academic Press},
pages = {83-113},
year = {2024},
isbn = {978-0-443-21619-0},
doi = {https://doi.org/10.1016/B978-0-44-321619-0.00012-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780443216190000121},
author = {Wenbo Zheng and Fei-Yue Wang},
keywords = {Computational Knowledge Vision, Visual computing, Just noticeable difference, Domain adaptation, Generative adversarial network},
abstract = {Based on the principles and paradigms of computational knowledge-based vision discussed in this chapter, we practice at the representational and algorithmic levels for the take of image-to-image translation. The image-to-image translation aims to learn the mapping between two visual domains. At the beginning of designing the existing image-to-image translation method, it was not considered whether the generated image is realistic or not. In this work, we present a novel approach to address the problem of generating fidelity in the area of image-to-image translation. In particular, humans judge whether an image is realistic or not with unique human vision's feeling rather than paying attention to the real-world semantics. Inspired by this, we propose an effective network loss to capture the pixel-level representations and human vision system information for verisimilar image-to-image translation. To enforce both structural and translation-model consistency during adaptation, we propose a novel Just-Noticeable-Difference loss based on a visual recognition task. The Just-Noticeable-Difference loss not only guides the overall representation to be discriminative but also enforces our cycle loss before and after mapping between domains. Experimental results show that our approach is able to generate realistic images using unpaired training data, on a wide range of tasks. Besides, we measure realism with Fréchet Inception Distance and diversity with the number of statistically-different bins, Jensen–Shannon divergence, and a perceptual distance metric. We also apply our approach to domain adaptation and show competitive performance when compared to others on several datasets.}
}
@incollection{ALIPPI2024251,
title = {13 - Computational intelligence in cyber-physical systems and the Internet of Things},
editor = {Robert Kozma and Cesare Alippi and Yoonsuck Choe and Francesco Carlo Morabito},
booktitle = {Artificial Intelligence in the Age of Neural Networks and Brain Computing (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {251-267},
year = {2024},
isbn = {978-0-323-96104-2},
doi = {https://doi.org/10.1016/B978-0-323-96104-2.00001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323961042000014},
author = {Cesare Alippi and Seiichi Ozawa},
keywords = {Intelligent systems, Embedded AI, Learning in nonstationary environments, Cybersecurity},
abstract = {The emergence of nontrivial embedded sensor units and cyber-physical systems and the Internet of Things has made possible the design and implementation of sophisticated applications where large amounts of real-time data are collected, possibly to constitute a big data picture as time passes. Within this framework, intelligence mechanisms based on machine learning, neural networks, and brain computing approaches play a key role to provide systems with advanced functionalities. Intelligent mechanisms are needed to guarantee appropriate performances within an evolving, time-variant environment, optimally harvest the available and manage the residual energy, reduce the energy consumption of the whole system, identify and mitigate the occurrence of faults, and provide shields against cyber-attacks. The chapter introduces the above aspects of intelligence, whose functionalities are needed to boost the next generation of cyber-physical and Internet of Things applications, a smart world generation whose footprint is already around us.}
}
@article{TIRADORAMOS2010855,
title = {Fourth Workshop on Teaching Computational Science (WTCS 2010)},
journal = {Procedia Computer Science},
volume = {1},
number = {1},
pages = {855-856},
year = {2010},
note = {ICCS 2010},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2010.04.093},
url = {https://www.sciencedirect.com/science/article/pii/S1877050910000943},
author = {A. Tirado-Ramos and A.B. Shiflet},
keywords = {Teaching, Computational science, Modeling, Simulation},
abstract = {The Workshop on Teaching Computational Science (WTCS), taking place within the International Conference on Computational Science (ICCS), is a platform for discussing innovations in teaching computational science in its various aspects, e.g. modeling and simulation, at all levels and contexts. Innovations may cover the context of formal courses or self-directed learning, involving, for example, curriculum development, introductory programming, service courses, specialist undergraduate and postgraduate topics, as well as industry-related short courses. This editorial provides an introduction to the work presented during the sessions in Amsterdam.}
}
@article{COSTA201227,
title = {Systems pathology: A critical review},
journal = {Molecular Oncology},
volume = {6},
number = {1},
pages = {27-32},
year = {2012},
issn = {1574-7891},
doi = {https://doi.org/10.1016/j.molonc.2011.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S1574789111001438},
author = {Jose Costa},
keywords = {Systems biology, Systems pathology, Translational research},
abstract = {The technological advances of the last twenty years together with the dramatic increase in computational power have injected new life into systems-level thinking in Medicine. This review emphasizes the close relationship of Systems Pathology to Systems Biology and delineates the differences between Systems Pathology and Clinical Systems Pathology. It also suggests an algorithm to support the application of systems-level thinking to clinical research, proposes applying systems-level thinking to the health care systems and forecasts an acceleration of preventive medicine as a result of the coupling of personal genomics with systems pathology.}
}
@article{PATT2024108888,
title = {The sign effect in temporal discounting does not require the hippocampus},
journal = {Neuropsychologia},
volume = {199},
pages = {108888},
year = {2024},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2024.108888},
url = {https://www.sciencedirect.com/science/article/pii/S0028393224001039},
author = {Virginie M. Patt and Caroline Strang and Mieke Verfaellie},
keywords = {Hippocampus, Amnesia, intertemporal choice, Temporal discounting, Sign effect, Money loss versus gain},
abstract = {When considering future outcomes, humans tend to discount gains more than losses. This phenomenon, referred to as the temporal discounting sign effect, is thought to result from the greater anticipated emotional impact of waiting for a negative outcome (dread) compared to waiting for a positive outcome (mixture of savoring and impatience). The impact of such anticipatory emotions has been proposed to rely on episodic future thinking. We evaluated this proposal by examining the presence and magnitude of a sign effect in the intertemporal decisions of individuals with hippocampal amnesia, who are severely impaired in their ability to engage in episodic mental simulation, and by comparing their patterns of choices to those of healthy controls. We also measured loss aversion, the tendency to assign greater value to losses compared to equivalent gains, to verify that any reduction in the sign effect in the hippocampal lesion group could not be explained by a group difference in loss aversion. Results showed that participants with hippocampal amnesia exhibited a sign effect, with less discounting of monetary losses compared to gains, that was similar in magnitude to that of controls. Loss aversion, albeit greater in the hippocampal compared to the control group, did not account for the sign effect. These results indicate that the sign effect does not depend on the integrity of hippocampally mediated episodic processes. They suggest instead that the impact of anticipatory emotions can be factored into decisions via semantic future thinking, drawing on non-contextual knowledge about oneself.}
}
@article{CHAPLESKI2020101435,
title = {A Molecular-Scale Approach to Rare-Earth Beneficiation: Thinking Small to Avoid Large Losses},
journal = {iScience},
volume = {23},
number = {9},
pages = {101435},
year = {2020},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2020.101435},
url = {https://www.sciencedirect.com/science/article/pii/S2589004220306258},
author = {Robert C. Chapleski and Azhad U. Chowdhury and Anna K. Wanhala and Vera Bocharova and Santanu Roy and Philip C. Keller and Dylan Everly and Santa Jansone-Popova and Alexander Kisliuk and Robert L. Sacci and Andrew G. Stack and Corby G. Anderson and Benjamin Doughty and Vyacheslav S. Bryantsev},
keywords = {Chemical Engineering, Spectroscopy, Physical Inorganic Chemistry, Surface Chemistry},
abstract = {Summary
Separating rare-earth-element-rich minerals from unwanted gangue in mined ores relies on selective binding of collector molecules at the interface to facilitate froth flotation. Salicylhydroxamic acid (SHA) exhibits enhanced selectivity for bastnäsite over calcite in microflotation experiments. Through a multifaceted approach, leveraging density functional theory calculations, and advanced spectroscopic methods, we provide molecular-level mechanistic insight to this selectivity. The hydroxamic acid moiety introduces strong interactions at metal-atom surface sites and hinders subsurface-cation stabilization at vacancy-defect sites, in calcite especially. Resulting from hydrogen-bond-induced interactions, SHA lies flat on the bastnäsite surface and shows a tendency for multilayer formation at high coverages. In this conformation, SHA complexation with bastnäsite metal ions is stabilized, leading to advanced flotation performance. In contrast, SHA lies perpendicular to the calcite surface due to a difference in cationic spacing. We anticipate that these insights will motivate rational design and selection of future collector molecules for enhanced ore beneficiation.}
}
@article{LERON2014126,
title = {Functions via everyday actions: Support or obstacle?},
journal = {The Journal of Mathematical Behavior},
volume = {36},
pages = {126-134},
year = {2014},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2014.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S073231231400056X},
author = {Uri Leron and Tamar Paz},
keywords = {Functions, Composition of functions, Intuitive thinking, Analytical thinking, Dual-process theory, Actions on objects, Changing-the-input misconception},
abstract = {The general context of this paper is the power of intuitive thinking, and how it can help or hinder analytical thinking. The research literature in cognitive psychology teems with tasks where intuitive thinking leads subjects to “non-normative” answers, including tasks for which they have all the knowledge necessary for the normative answer. The best explanation to date for such phenomena is dual-process theory, which stipulates the activation of a quick automatic intuitive process (System 1), together with the failure of the heavy, lazy, and computationally expensive analytical process (System 2) to intervene and correct the intuitive response. In an earlier paper, we have documented a clash between intuitive and analytical thinking concerning functions, which we have termed the changing-the-input phenomenon. The discovery of the changing-the-input phenomenon, however, left us with a puzzle: Why has this phenomenon concerning functions – a purely mathematical concept – been observed in computer science classes but not in mathematics ones? The purpose of the present paper is to address this puzzle. More generally we ask, under what conditions the changing-the-input phenomenon will or will not be manifested? Still more generally, in learning about functions, when is the intuitive scaffolding of functions via actions-on-tangible-objects helpful, and when does it get in the way of deeper understanding?}
}
@article{FORD200437,
title = {Electrophysiological evidence of corollary discharge dysfunction in schizophrenia during talking and thinking},
journal = {Journal of Psychiatric Research},
volume = {38},
number = {1},
pages = {37-46},
year = {2004},
issn = {0022-3956},
doi = {https://doi.org/10.1016/S0022-3956(03)00095-5},
url = {https://www.sciencedirect.com/science/article/pii/S0022395603000955},
author = {Judith M. Ford and Daniel H. Mathalon},
keywords = {Schizophrenia, Corollary discharge, N1, EEG coherence},
abstract = {Failure of corollary discharge, a mechanism for distinguishing self-generated from externally-generated percepts, has been posited to underlie certain positive symptoms of schizophrenia, including auditory hallucinations. Although originally described in the visual system, corollary discharge may exist in the auditory system, whereby signals from motor speech commands prepare auditory cortex for self-generated speech. While associated with sensorimotor systems, it might also apply to inner speech or thought, regarded as our most complex motor act. We had four aims in the studies summarized in this paper: (1) to demonstrate the corollary discharge phenomenon during talking and inner speech in human volunteers using event-related brain potentials (ERPs), (2) to demonstrate that the corollary discharge is abnormal in patients with schizophrenia, (3) to demonstrate the role of frontal speech areas in the corollary discharge during talking, and (4) to relate the dysfunction of the corollary discharge in schizophrenia to auditory hallucinations. Using EEG and ERP measures, we addressed each aim in patients with schizophrenia (DSM IV) and healthy control subjects. The N1 component of the ERP reflected dampening of auditory cortex responsivity during talking and inner speech in control subjects but not in patients. EEG measures of coherence indicated inter-dependence of activity in the frontal speech production and temporal speech reception areas during talking in control subjects, but not in patients, especially those who hallucinated. These data suggest that a corollary discharge from frontal areas where thoughts are generated fails to alert auditory cortex that they are self-generated, leading to the misattribution of inner speech to external sources and producing the experience of auditory hallucinations.}
}
@incollection{KAUFMANN1993123,
title = {Chapter 5 Mental Imagery: Fixed or Multiple Meanings? Nature and Function of Imagery in Creative Thinking},
editor = {Beverly Roskos-Ewoldsen and Margaret Jean Intons-Peterson and Rita E. Anderson},
series = {Advances in Psychology},
publisher = {North-Holland},
volume = {98},
pages = {123-150},
year = {1993},
booktitle = {Imagery, Creativity, and Discovery},
issn = {0166-4115},
doi = {https://doi.org/10.1016/S0166-4115(08)60141-7},
url = {https://www.sciencedirect.com/science/article/pii/S0166411508601417},
author = {Geir Kaufmann and Tore Helstrup},
abstract = {Publisher Summary
This chapter presents the composition of mental imagery and elucidates the processes involved in imaging and the ways in which the processes interact. The work of Kosslyn, aimed at clarifying the basic process components and the general mechanics of the imaging process and Finke's probing of the levels of equivalence between imagery and perception, also belong to the conceptual category of research where the focus is on the nature and properties of imagery. Experimental evidence from a task devised by Finke indicates an important role of visual imagery in the integration of initially unrelated elements of experience into new combinations. This experimental evidence lends support to theories of symbolic representations that emphasize the potential of visual imagery as a vehicle for creative thought. Even if Chambers and Reisberg have overstated their case in their distinction between perceiving and imaging, they have, nevertheless, been able to demonstrate an important dimension of difference between two types of activities.}
}
@article{FABRY2018793,
title = {Turing redux: Enculturation and computation},
journal = {Cognitive Systems Research},
volume = {52},
pages = {793-808},
year = {2018},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2018.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S1389041718301724},
author = {Regina E. Fabry},
keywords = {Enculturation, Mathematical cognition, Computation, Hybrid cognition, Neural plasticity, Embodied cognition},
abstract = {Many of our cognitive capacities are shaped by enculturation. Enculturation is the acquisition of cognitive practices such as symbol-based mathematical practices, reading, and writing during ontogeny. Enculturation is associated with significant changes to the organization and connectivity of the brain and to the functional profiles of embodied actions and motor programs. Furthermore, it relies on scaffolded cultural learning in the cognitive niche. The purpose of this paper is to explore the components of symbol-based mathematical practices. Phylogenetically, these practices are the result of concerted organism-niche interactions that have led from approximate number estimations to the emergence of discrete, symbol-based mathematical operations. Ontogenetically, symbol-based mathematical practices are associated with plastic changes to neural circuitry, action schemata, and motor programs. It will be suggested that these practices rely on previously acquired capacities such as subitizing and counting. With these considerations in place, I will argue that computations, understood in the sense of Turing (1936), are a specific kind of symbol-based mathematical practices that can be realized by human organisms, machines, or by hybrid organism-machine systems. In sum, this paper suggests a new way to think about mathematical cognition and computation.}
}
@article{HOLROYD2021316,
title = {The Best Laid Plans: Computational Principles of Anterior Cingulate Cortex},
journal = {Trends in Cognitive Sciences},
volume = {25},
number = {4},
pages = {316-329},
year = {2021},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2021.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S1364661321000103},
author = {Clay B. Holroyd and Tom Verguts},
keywords = {anterior cingulate cortex, computational models, artificial intelligence, hierarchical model-based hierarchical reinforcement learning, distributed representations, cognitive control},
abstract = {Despite continual debate for the past 30 years about the function of anterior cingulate cortex (ACC), its key contribution to neurocognition remains unknown. However, recent computational modeling work has provided insight into this question. Here we review computational models that illustrate three core principles of ACC function, related to hierarchy, world models, and cost. We also discuss four constraints on the neural implementation of these principles, related to modularity, binding, encoding, and learning and regulation. These observations suggest a role for ACC in hierarchical model-based hierarchical reinforcement learning (HMB-HRL), which instantiates a mechanism motivating the execution of high-level plans.}
}
@article{20167,
title = {What Is the Key Best Practice for Collaborating with a Computational Biologist?},
journal = {Cell Systems},
volume = {3},
number = {1},
pages = {7-11},
year = {2016},
issn = {2405-4712},
doi = {https://doi.org/10.1016/j.cels.2016.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S240547121630223X}
}
@article{BRYANT201034,
title = {Thinking inside the box: A participatory, computer-assisted approach to scenario discovery},
journal = {Technological Forecasting and Social Change},
volume = {77},
number = {1},
pages = {34-49},
year = {2010},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2009.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S004016250900105X},
author = {Benjamin P. Bryant and Robert J. Lempert},
keywords = {Scenario Discovery, Scenario planning, Robust decision making},
abstract = {Scenarios provide a commonly used and intuitively appealing means to communicate and characterize uncertainty in many decision support applications, but can fall short of their potential especially when used in broad public debates among participants with diverse interests and values. This paper describes a new approach to participatory, computer-assisted scenario development that we call scenario discovery, which aims to address these challenges. The approach defines scenarios as a set of plausible future states of the world that represent vulnerabilities of proposed policies, that is, cases where a policy fails to meet its performance goals. Scenario discovery characterizes such sets by helping users to apply statistical or data-mining algorithms to databases of simulation-model-generated results in order to identify easy-to-interpret combinations of uncertain model input parameters that are highly predictive of these policy-relevant cases. The approach has already proved successful in several high impact policy studies. This paper systematically describes the scenario discovery concept and its implementation, presents statistical tests to evaluate the resulting scenarios, and demonstrates the approach on an example policy problem involving the efficacy of a proposed U.S. renewable energy standard. The paper also describes how scenario discovery appears to address several outstanding challenges faced when applying traditional scenario approaches in contentious public debates.}
}
@article{WATFORD2019114707,
title = {Progress in data interoperability to support computational toxicology and chemical safety evaluation},
journal = {Toxicology and Applied Pharmacology},
volume = {380},
pages = {114707},
year = {2019},
issn = {0041-008X},
doi = {https://doi.org/10.1016/j.taap.2019.114707},
url = {https://www.sciencedirect.com/science/article/pii/S0041008X19303151},
author = {Sean Watford and Stephen Edwards and Michelle Angrish and Richard S. Judson and Katie {Paul Friedman}},
keywords = {Data Interoperability, Computational Toxicology, Bioinformatics, Databases, Applications},
abstract = {New approach methodologies (NAMs) in chemical safety evaluation are being explored to address the current public health implications of human environmental exposures to chemicals with limited or no data for assessment. For over a decade since a push toward “Toxicity Testing in the 21st Century,” the field has focused on massive data generation efforts to inform computational approaches for preliminary hazard identification, adverse outcome pathways that link molecular initiating events and key events to apical outcomes, and high-throughput approaches to risk-based ratios of bioactivity and exposure to inform relative priority and safety assessment. Projects like the interagency Tox21 program and the US EPA ToxCast program have generated dose-response information on thousands of chemicals, identified and aggregated information from legacy systems, and created tools for access and analysis. The resulting information has been used to develop computational models as viable options for regulatory applications. This progress has introduced challenges in data management that are new, but not unique, to toxicology. Some of the key questions require critical thinking and solutions to promote semantic interoperability, including: (1) identification of bioactivity information from NAMs that might be related to a biological process; (2) identification of legacy hazard information that might be related to a key event or apical outcomes of interest; and, (3) integration of these NAM and traditional data for computational modeling and prediction of complex apical outcomes such as carcinogenesis. This work reviews a number of toxicology-related efforts specifically related to bioactivity and toxicological data interoperability based on the goals established by Findable, Accessible, Interoperable, and Reusable (FAIR) Data Principles. These efforts are essential to enable better integration of NAM and traditional toxicology information to support data-driven toxicology applications.}
}
@article{MAITI2023e22729,
title = {Design and evaluation of a revised ARCS motivational model for online classes in higher education},
journal = {Heliyon},
volume = {9},
number = {12},
pages = {e22729},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e22729},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023099371},
author = {Monica Maiti and M. Priyaadharshini and Harini. S},
keywords = {Lifelong learning, ARCS model, ICT tools, Competency skills, Rubrics evaluation, Regression model},
abstract = {In recent years, online MOOCs (Massive Open Online Courses) have been quite popular among universities which helps learners to enhance their competency skills apart from learning the regular college/university curriculum. Although distance education and online learning have been adopted gradually recently, it has become the 'New Normal.' In this situation of uncertainty, facilitators must keep themselves updated with the various teaching/learning strategies and encourage learners to get accustomed to the online classroom environment. Furthermore, assisting the learners with active engagement in the classes is essential. Hence, to create an instigated environment for assessing the competency level and addressing the motivational behaviour of the learners in the online courses, a modified version of the "ARCS" (Attention, Relevance, Confidence, and Satisfaction) model is used in this research work. The core objective of this model is to apply a modified motivational model, namely "ARCS-PC," where PC represents Professional Competency. Professional competency includes Critical Thinking skills, Digital literacy, Creative Thinking, Problem-solving, and Time Management. The incorporation of digital quizzes, assignments, and interactive activities using Information and Communication Technology (ICT) tools was done in the ARCS-PC Model. The online classroom lectures and activities were conducted using the Microsoft Teams (MS Teams) educational platform. Linear regression is performed to analyze the modified ARCS-PC model. These technology-enabled online classes and ICT tools have helped teach lifelong learning, collaborative learning, a student-centric approach, and better competency skills to effectively engage students in online courses. In our proposed method, an improvement of 11.21 % was observed in the student's performance compared to a maximum of 8.8 % in the existing traditional models. Detailed analysis and quantification of the proposed method are given in the paper.}
}
@article{THEODOROPOULOS2021100335,
title = {Augmented Reality and programming education: A systematic review},
journal = {International Journal of Child-Computer Interaction},
volume = {30},
pages = {100335},
year = {2021},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2021.100335},
url = {https://www.sciencedirect.com/science/article/pii/S2212868921000544},
author = {Anastasios Theodoropoulos and George Lepouras},
keywords = {Augmented Reality (AR), Programming learning, Coding learning, CS education, Review study},
abstract = {In recent years, Augmented Reality (AR) usage in the learning process has been growing. AR tools and environments lead to a variety of positive outcomes and impacts for educational purposes. Similarly, AR is changing the learning process in the Computer Science (CS) Education domain. There are numerous studies that adopt the immersive AR technology in order to improve Computational Thinking (CT) or programming skills, in several contexts. However, there are not sufficient studies that analyze the meaningful characteristics or the advantages and disadvantages of AR in the field. In order to better understand the impact of AR in programming education we performed a systematic literature review. This review analyzes 31 studies in the field. It explores the evolution of this developing technology, the challenges and issues that AR offers and discusses how this work can benefit student learning and further research.}
}
@article{LIU2021107410,
title = {A new computational method for acquiring effect knowledge to support product innovation},
journal = {Knowledge-Based Systems},
volume = {231},
pages = {107410},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107410},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121006729},
author = {Hongwei Liu and Wenqiang Li and Yan Li},
keywords = {TRIZ, Product innovation design, Effect knowledge representation, Functional Basis, IPC},
abstract = {Effect provides a scientific principle-level means for product function realization. The unexpected or new application of effects can create high-level innovations enabling products long-term technical advantages and market competitiveness. Acquiring design knowledge is the vital first step of conducting product innovation activities. In order to capture the effect knowledge that can efficiently aid high-level product innovation, this article proposes a new computational method. The method stems from a novel effect knowledge representation considering both functional and technical area features, and utilizes functional-flow terms of Functional Basis and technical area categories of international patent classification (IPC) respectively to standardize the modelling of the two kinds of features. Based on such representation, the method reasonably combines syntactic analysis, WordNet and word vector technologies to extract the desired effect knowledge from IPC text. To evaluate the method, this article first compares the acquired knowledge with those in a comprehensive human-compiled effect database, and then applies the knowledge to aid the innovation design of several mechanical products with different technical backgrounds. Evaluation results and the discussion based on them suggest the feasibility and potential of the proposed method in automatically acquiring well-organized effect knowledge system, as well as in aiding high-level product innovation.}
}
@article{ARFE2020103807,
title = {The effects of coding on children's planning and inhibition skills},
journal = {Computers & Education},
volume = {148},
pages = {103807},
year = {2020},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2020.103807},
url = {https://www.sciencedirect.com/science/article/pii/S0360131520300099},
author = {Barbara Arfé and Tullio Vardanega and Lucia Ronconi},
keywords = {Coding, Computational thinking, Executive function, Primary school children, Problem-solving},
abstract = {Computational thinking (CT) and the coding element of it are progressively entering in the primary school curriculum worldwide. Yet, little is known about the effects of these skills on children's cognitive development. In a cluster-randomized controlled trial, we examined how 1st-grade children's gains in coding skills that follow instructional intervention transfer to two important executive functions (EFs): planning and response inhibition. One-hundred seventy-nine (179) first graders from 5 schools and 10 class groups, with no prior experience of coding, were randomly assigned to an experimental (coding, 5 classes) or control (standard STEM, 5 classes) instructional condition. The experimental intervention involved 8 h of coding activities (two weekly lessons for 4 weeks), through the Code.org platform. Children in the control group were exposed to standard STEM instruction. Four coding tasks drawn from Code.org, two standardized planning tasks (Elithorn maze test and Tower of London, ToL, test) and two standardized response inhibition tasks (NEPSY-II inhibition subtest and numerical Stroop), were used to assess children's skills at the pretest and posttest (after the instructional intervention). To measure retention, the same skills were also assessed for 44 children from the experimental group 5 weeks from the posttest (follow up). The results show that practice with coding through Code.org not only improved measurably children's ability to solve coding problems, but also their EFs, increasing the time children spent planning, their ability to solve standardized planning tasks, and to inhibit prepotent responses. Such findings add to the still limited literature on the cognitive effects of coding, deepening our understanding of the positive implications of introducing Computational Thinking early in the school curriculum.}
}
@article{AGUAYO2023e19205,
title = {Ethical enactivism for smart and inclusive STEAM learning design},
journal = {Heliyon},
volume = {9},
number = {9},
pages = {e19205},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e19205},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023064137},
author = {Claudio Aguayo and Ronnie Videla and Francisco López-Cortés and Sebastián Rossel and Camilo Ibacache},
keywords = {STEAM, Ethical enactivism, Immersive learning, Systems thinking, Planetary wellbeing},
abstract = {Current global challenges of the 21st century promote STEAM (science, technology, engineering, arts and mathematics) education and digitalization as a means for humans to be the central actors in the construction of a sustainable society that favors a sense of worth and global wellbeing. In this scenario, new educational technology tools and immersive learning affordances (possibilities), offer unprecedented potential for the design of smart and dynamic learning systems and contexts that can enhance learning processes across varied audiences and educational settings. However, current STEAM education practice lacks attention to equipping all citizens with the necessary skills to use digital technologies in an ethical, critical and creative way. This gap calls for attention in design processes, principles and practices that are attentive to ethical considerations and values-based approaches. On the other hand, in its formulation STEAM as an educational approach is framed in four fundamental pillars: creativity, inclusion, citizenship and emerging technologies, which also put attention on the inclusion of disadvantaged and underrepresented social groups during STEAM education design. Following an apparent need to explore ethical and inclusive design in STEAM education, and inspired in the 4E cognition framework, ethical enactivism and embodied and ecosomaesthetics experience design, here we propose a theoretical framework grounded on systems thinking for the design of smart and dynamic STEAM learning systems and settings. The framework is aimed at STEAM educational psychologists, educational technologists, learning designers and educational practitioners who wish to address the global challenges of 21st century education by means of creative, innovative and inclusive education design.}
}
@article{WINTER20181,
title = {The art of the Wunderlich cube and the development of spatial abilities},
journal = {International Journal of Child-Computer Interaction},
volume = {18},
pages = {1-7},
year = {2018},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2018.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S2212868917301010},
author = {Victor Winter and Betty Love and Cindy Corritore},
keywords = {Spatial reasoning, Mathematical analysis, Coding, 3D printing},
abstract = {This paper advocates for a future where the teaching of math and art are harmoniously intertwined as they were in the days of da Vinci. In this future, code provides the “brush” that enables the expression of artistic ideas and mathematical structures in digital and digitally-fabricated mediums. This educational idea is motivated by (1) literature supporting the position that visual thinking and spatial reasoning significantly impact STEAM disciplines, and (2) Piaget’s theory of cognitive development in which children, in the concrete operational stage, solve problems relating to physical objects (i.e., they learn-by-making). A project is then described involving the creation of a 3D artifact we call a Wunderlich cube — a mathematical artifact that embodies numerous spatial reasoning puzzles. An understanding of the properties of the Wunderlich cube is developed through manual construction using LEGO®, mathematical analysis, computational thinking, coding, and 3D printing.}
}
@article{PRADO2019727,
title = {Towards an Extensible Architecture for Ideation},
journal = {Procedia Computer Science},
volume = {159},
pages = {727-735},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.228},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919314140},
author = {Hércules A. do Prado and Elaine Coutinho Marcial and Aluizio Haendchen Filho and Edilson Ferneda and Roseane Salvio},
keywords = {design thinking, ideation, debates, sentiment analysis, foresight, Studies of Future},
abstract = {Ideation is an important activity of Design Thinking, a process that may benefit from different levels of automation in its activities. Preceded by Immersion and Analysis activities, Ideation can be enhanced by computational approaches like debate synthesis and mediation, sentiment analysis, and so on. In this paper an architecture for an extensible platform for ideation is addressed. Initially, it comprises a set of components to cope with those functionalities. The extensibility of this platform is in the sense that it shall allow the inclusion of new components like creation of domain ontologies for integration of different studies in the same domain. The general purpose of this platform is to support the creation of ideas by (i) constructing consensus among specialists and (ii) managing dissents in order to keep in track of marginal ideas that can become dominant ones as the discussion advances. It can be applied to many fields, like innovation, Studies of Future, definition of complex diagnoses, etc. Actually, this proposal came up from an experience with a study of future in which some experts had tried to envision trends for some years ahead in order to propose strategic actions for reaching a desired status for Brazil as a successful, fair, and inclusive country. The proposal includes an open and interactive computational environment to enable (i) structuring debates in threads of discussion; (ii) the gathering of ideas about topics of interest; (iii) the debate on the ideas put forward in order to identify the most relevant ones; (iv) synthesis of a debate (anytime summarization); (v) identification of the prevailing sentiments in a debate; and (vi) identification of variables relevant for the sake of the debate target.}
}
@article{BAGGIO2020100005,
title = {Computational modelling and simulations in tourism: A primer},
journal = {Annals of Tourism Research Empirical Insights},
volume = {1},
number = {1},
pages = {100005},
year = {2020},
issn = {2666-9579},
doi = {https://doi.org/10.1016/j.annale.2020.100005},
url = {https://www.sciencedirect.com/science/article/pii/S2666957920300057},
author = {Rodolfo Baggio},
keywords = {Complex systems, Modelling, Simulations, Numerical and computational methods},
abstract = {The aim of this contribution is to briefly sketch and discuss the main issues that concern the activities of modelling and simulating complex phenomena and systems. The focus is on numerical and computational techniques. We discuss the validity of these methods and examine the different steps to be taken for ensuring a correct, accurate and reliable implementation. The approach is essentially of general methodological nature, regardless of specific techniques or tools.}
}
@article{MOTOMURA20103,
title = {Multi-aspect data analysis for investigating human computation mechanism},
journal = {Cognitive Systems Research},
volume = {11},
number = {1},
pages = {3-15},
year = {2010},
note = {Brain Informatics},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2008.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S1389041708000521},
author = {Shinichi Motomura and Ning Zhong},
keywords = {Multi-aspect data analysis, Brain informatics methodology, Human computation mechanism, EEG and fMRI},
abstract = {In the paper, we present a multi-aspect data analysis approach for investigating human computation mechanism. Multi-aspect analysis in multiple human brain data sources is an important methodology in Brain Informatics, which emphasizes on a systematic way for investigating human information processing mechanisms, including measuring, collecting, modeling, transforming, managing, and mining multiple human brain data obtained from various cognitive experiments by using powerful equipments, such as fMRI and EEG. After giving an outline of Brain Informatics methodology, we describe how to design cognitive experiments of mental arithmetic task with multiple difficulty levels for obtaining multiple EEG and fMRI data sources, and how to analyze such data for investigating the spatiotemporal characteristics and flow of human computation processing. Such an investigation can be regarded as a case study using Brain Informatics methodology. Experimental results show the usefulness of our approach.}
}
@incollection{KARACA20229,
title = {Chapter 2 - Theory of complexity, origin and complex systems},
editor = {Yeliz Karaca and Dumitru Baleanu and Yu-Dong Zhang and Osvaldo Gervasi and Majaz Moonis},
booktitle = {Multi-Chaos, Fractal and Multi-Fractional Artificial Intelligence of Different Complex Systems},
publisher = {Academic Press},
pages = {9-20},
year = {2022},
isbn = {978-0-323-90032-4},
doi = {https://doi.org/10.1016/B978-0-323-90032-4.00003-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323900324000031},
author = {Yeliz Karaca},
keywords = {Chaotic behavior, Complex order processes, Complex systems, Complexity, Computational complexity, Evolution, History of complexity, Nonlinearity, Nonregularity, Self-ordering, Self-organization, Theory of complexity},
abstract = {Having existed as a term since antiquity complexity as an idea and scientific concept that requires the understanding of origin of complex components entails lengthy and meticulous computations, as well as causal processes. A complex system, in that regard, is literally one where multiple interactions occur and emerge among the components; and common to all research into complexity are the systems that have adapting, self-organizing, synchronizing, and reacting elements. As physicist Nigel Goldenfeld put very aptly, “Complexity starts when causality breaks down.” Complexity manifests as varying challenges in fields of different complex systems. The inherent complexity of the related phenomena in the fields is beyond the reductionist outlook of traditional science; thus, complexity requires an understanding that extends across a class of complex problems that have so many intricate and subtle attributes that are much more innovative and novel ways of thinking as well as applicable laws are of critical importance. Evolution, order and complexity reveal the relationship between natural and social worlds, which reflects a modern way of thinking that challenges the dichotomy of natural and social. This study provides a conceptual outline and historical account of complexity and complex systems as well as complex order processes toward modern scientific path from Darwin and onwards concerned with natural, applied and social sciences. In the parlance of complex systems, the modern way of thinking based on the transition from evolutionary dimension can be viewed as a revolutionary pedestal, which is a new paradigm for natural sciences and social sciences so that the foundation for the complex systems' interpretations can be explored by the relevant domains.}
}
@article{MULATTI2023100040,
title = {Perceived lack of control promotes creativity},
journal = {Journal of Creativity},
volume = {33},
number = {1},
pages = {100040},
year = {2023},
issn = {2713-3745},
doi = {https://doi.org/10.1016/j.yjoc.2022.100040},
url = {https://www.sciencedirect.com/science/article/pii/S2713374522000231},
author = {Claudio Mulatti and Barbara Treccani},
keywords = {Creativity, Divergent creativity, Lack of control, Compensatory control theory, Semantic control},
abstract = {The sense of lack of control has been shown to foster illusory pattern perception, superstition, conspiracy and religious beliefs. In two identical experiments we investigated whether the feeling of lacking control (vs. control) can also foster creative thinking, which we operationalized as the ability to produce associative and dissociative combinations of either related and unrelated concepts. Participants were asked to think about an incident in their life wherein they felt either to be in control or to lose control of the situation. Immediately afterwards, they had to perform a set of tasks tapping (divergent) creative thinking. In both experiments, we observed higher scores in all creativity tasks for participants who recalled loss-of-control events than for those recalling in-control events. Our findings suggest that compensatory processes, triggered by experiencing lack of control, can promote divergent thinking. We propose an account situated within current models of semantic control.}
}
@article{KHATUN2023127163,
title = {A combined experimental and computational approach on La0.6Sr0.4MnO3 perovskite},
journal = {Materials Chemistry and Physics},
volume = {295},
pages = {127163},
year = {2023},
issn = {0254-0584},
doi = {https://doi.org/10.1016/j.matchemphys.2022.127163},
url = {https://www.sciencedirect.com/science/article/pii/S0254058422014699},
author = {Mst Romana Khatun and Md Khadimul Islam and Monira Jannatul Kobra and Yuji Inagaki and Rajia Sultana and Md Abdur Razzaque Sarker and Md Saiful Islam},
keywords = {Sr-doped La manganite, XRD, Magnetization, Resistivity, Energy dispersion, Thermal properties},
abstract = {We synthesize a high quality Sr-doped lanthanum manganite using solid state reaction route to investigate the various properties for device applications. The crystal structure of the synthesized perovskite was studied by X-ray diffraction (XRD) pattern and also compared with the crystallographic data obtained from the simulation calculations. The magnetization as a function of applied magnetic field and temperature of La0.6Sr0.4MnO3 exhibits ferromagnetic metal phase with the Curie temperature of 361 K. The electrical resistivity with temperature unexpectedly shows semiconducting behavior due to the intergrain effects. On the other hand, the energy dispersion studied by first principles calculations based on density functional theory (DFT) demonstrates metallic conduction in conformity with the available experimental results. No energy gap in the absorption spectrum done by UV–Visible spectrophotometer of this manganite also confirms the nature of identical conductivity. Finally, a quasi-harmonic Debye model was employed to calculate the thermal characteristics like Debye temperature, specific heat capacities, volume expansion coefficient, etc. in this LSMO perovskite.}
}
@article{COONS2015126,
title = {Grease pencils and the persistence of individuality in computationally produced custom objects},
journal = {Design Studies},
volume = {41},
pages = {126-136},
year = {2015},
note = {Special Issue: Computational Making},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2015.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X15000599},
author = {ginger “all-lower-case” coons and Matt Ratto},
keywords = {computer aided design, design tools, participatory design, social design, computational craft},
abstract = {This article explores the relationship between an established craft production method and a computational adaptation of that method. In looking at a specific tool, the grease pencils used in the fitting and production of prosthetic limbs, we examine the ways in which complexity, tacit understandings, and human movement are translated into a collection of variables and considerations manipulable in a digital environment. We discuss, briefly, the persistent individuality of objects like prosthetic sockets, and the ways in which their materiality and necessarily custom nature push back against assumptions that computational production is generalizing, disembodied, and abstract.}
}
@article{KARSAKOV2015730,
title = {Improving Visualization Courses in Russian Higher Education in Computational Science and High Performance Computing},
journal = {Procedia Computer Science},
volume = {66},
pages = {730-739},
year = {2015},
note = {4th International Young Scientist Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.11.083},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915034328},
author = {Andrey Karsakov and Anna Bilyatdinova and Alexey Bezgodov},
keywords = {Visualization course, Computational science, Higher education},
abstract = {In order to keep up with the fast-paced and widespread technologies and applications of visualization, worldwide education community is actively implementing visualization courses in curricula of undergraduate and graduate programs. A study of the state of the art in the teaching visualization in Russian higher education shows the necessity to improve the quality and breadth of knowledge of the visualization courses. In this paper we propose our approach to overcome the national and historical challenges in teaching visualization in Russian STEM higher education on the example of Computational Science and High Performance Computing double degree Master's programs in ITMO University. We offer a smooth transition to the modern relevant syllabus content by presenting two courses’ designs with same width but with various depth in knowledge that should to be studied. At the end of the paper we give some discussions about future works in development visualization courses in Russia.}
}
@article{PETZSCHNER2017421,
title = {Computational Psychosomatics and Computational Psychiatry: Toward a Joint Framework for Differential Diagnosis},
journal = {Biological Psychiatry},
volume = {82},
number = {6},
pages = {421-430},
year = {2017},
note = {Computational Psychiatry},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2017.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0006322317315846},
author = {Frederike H. Petzschner and Lilian A.E. Weber and Tim Gard and Klaas E. Stephan},
keywords = {Allostasis, Cybernetics, Hierarchical Bayesian model, Homeostasis, Inference, Metacognition, Prediction error},
abstract = {This article outlines how a core concept from theories of homeostasis and cybernetics, the inference-control loop, may be used to guide differential diagnosis in computational psychiatry and computational psychosomatics. In particular, we discuss 1) how conceptualizing perception and action as inference-control loops yields a joint computational perspective on brain-world and brain-body interactions and 2) how the concrete formulation of this loop as a hierarchical Bayesian model points to key computational quantities that inform a taxonomy of potential disease mechanisms. We consider the utility of this perspective for differential diagnosis in concrete clinical applications.}
}
@article{ZUCKER2012297,
title = {Local field potentials and border ownership: A conjecture about computation in visual cortex},
journal = {Journal of Physiology-Paris},
volume = {106},
number = {5},
pages = {297-315},
year = {2012},
note = {New trends in neurogeometrical approaches to the brain and mind problem},
issn = {0928-4257},
doi = {https://doi.org/10.1016/j.jphysparis.2012.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0928425712000411},
author = {Steven W. Zucker},
keywords = {Computational vision, Border ownership, Local field potentials, Neural computation},
abstract = {Border ownership is an intermediate-level visual task: it must integrate (upward flowing) image information about edges with (downward flowing) shape information. This highlights the familiar local-to-global aspect of border formation (linking of edge elements to form contours) with the much less studied global-to-local aspect (which edge elements form part of the same shape). To address this task we show how to incorporate certain high-level notions of distance and geometric arrangement into a form that can influence image-based edge information. The center of the argument is a reaction—diffusion equation that reveals how (global) aspects of the distance map (that is, shape) can be “read out” locally, suggesting a solution to the border ownership problem. Since the reaction—diffusion equation defines a field, a possible information processing role for the local field potential can be defined. We argue that such fields also underlie the Gestalt notion of closure, especially when it is refined using modern experimental techniques. An important implication of this theoretical argument is that, if true, then network modeling must be extended to include the substrate surrounding spiking neurons, including glia.}
}
@article{BYSTRITSKY2012428,
title = {Computational non-linear dynamical psychiatry: A new methodological paradigm for diagnosis and course of illness},
journal = {Journal of Psychiatric Research},
volume = {46},
number = {4},
pages = {428-435},
year = {2012},
issn = {0022-3956},
doi = {https://doi.org/10.1016/j.jpsychires.2011.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S0022395611002615},
author = {A. Bystritsky and A.A. Nierenberg and J.D. Feusner and M. Rabinovich},
keywords = {Phenomenology, Mathematical models, Non-linear dynamics, Winner less competition, Psychopathology},
abstract = {The goal of this article is to highlight the significant potential benefits of applying computational mathematical models to the field of psychiatry, specifically in relation to diagnostic conceptualization. The purpose of these models is to augment the current diagnostic categories that utilize a “snapshot” approach to describing mental states. We hope to convey to researchers and clinicians that non-linear dynamics can provide an additional useful longitudinal framework to understand mental illness. Psychiatric phenomena are complex processes that evolve in time, similar to many other processes in nature that have been successfully described and understood within deterministic chaos and non-linear dynamic computational models. Dynamical models describe mental processes and phenomena that change over time, more like a movie than a photograph, with multiple variables interacting over time. The use of these models may help us understand why and how current diagnostic categories are insufficient. They may also provide a new, more descriptive and ultimately more predictive approach leading to better understanding of the interrelationship between psychological, neurobiological, and genetic underpinnings of mental illness.}
}
@article{PAPAVLASOPOULOU2020105939,
title = {Coding activities for children: Coupling eye-tracking with qualitative data to investigate gender differences},
journal = {Computers in Human Behavior},
volume = {105},
pages = {105939},
year = {2020},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2019.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0747563219300950},
author = {Sofia Papavlasopoulou and Kshitij Sharma and Michail N. Giannakos},
keywords = {Coding, Computational thinking, Eye-tracking, Gender differences, Learning strategies},
abstract = {Computational thinking and coding are becoming an integral part of K-12 education, with female students being underrepresented in such subjects. The proliferation of technological tools and programming environments offers the opportunity for creative coding activities for children and increases the need for appropriate instructional practices. In this study, we design and evaluate a coding workshop for children. Our goal is to examine differences between boys and girls using eye-tracking as an objective measure and triangulating the findings with qualitative data coming from children's interviews. The results show no statistically significant difference between female and male gaze and learning gain during the coding activity; interestingly, the qualitative data show differences in the strategies and implemented practices during coding, and in perceptions about those coding activities. Our results highlight that further studies need to utilize objective measures and unveil necessary differences in the design and implementation of coding activities. Furthermore, our results provide objective evidence that female students do not lack in competences compared to boys, but simply that they have a different approach during coding activities and different perspectives about coding, an approach that needs to be cultivated and nurtured.}
}
@article{REN2021105428,
title = {Computational fluid dynamics simulation of adsorption process in a liquid-solids fluidized bed},
journal = {Journal of Environmental Chemical Engineering},
volume = {9},
number = {4},
pages = {105428},
year = {2021},
issn = {2213-3437},
doi = {https://doi.org/10.1016/j.jece.2021.105428},
url = {https://www.sciencedirect.com/science/article/pii/S221334372100405X},
author = {Panfeng Ren and Wenbin Li and Kuotsung Yu},
keywords = {Liquid-solids fluidized bed (LSFB), Computational fluid dynamics (CFD) simulation, Turbulent mass diffusivity, Hydrodynamics, Protein adsorption},
abstract = {For simultaneously predicting the hydrodynamics and protein adsorption process in a liquid-solids fluidized bed (LSFB), a computational fluid dynamics (CFD) model is established by combining the two-fluid model (TFM) for the liquid-particles two-phase fluidization system with the c2¯−εc model for the turbulent mass transfer. In terms of hydrodynamics, the kl−εl−kp−εp−Θ equations are adopted to describe phases turbulence. The simulations of hydrodynamics using various drag models and different modelling parameters are conducted to test their sensitivity. Then for the adsorption process, the recently developed formulations of the c2¯−εc model are adopted to characterize rigorously the turbulent mass diffusion in LSFB. With the proposed model, the velocity field as well as the concentration field can be acquired. Simulated results are compared with the experimental data and a good agreement between them is found.}
}
@article{DILERNIA2023100383,
title = {Mental health meets computational neuroscience: A predictive Bayesian account of the relationship between interoception and multisensory bodily illusions in anorexia nervosa},
journal = {International Journal of Clinical and Health Psychology},
volume = {23},
number = {4},
pages = {100383},
year = {2023},
issn = {1697-2600},
doi = {https://doi.org/10.1016/j.ijchp.2023.100383},
url = {https://www.sciencedirect.com/science/article/pii/S1697260023000194},
author = {Daniele {Di Lernia} and Silvia Serino and Cosimo Tuena and Chiara Cacciatore and Nicoletta Polli and Giuseppe Riva},
keywords = {Interoception, Anorexia nervosa, Virtual reality, Bodily illusion, Bayesian, Prediction error},
abstract = {Mental health disorders pose a significant challenge to society. The Bayesian perspective on the mind offers unique insights and tools that may help address a variety of mental health conditions. Psychopathological dysfunctions are often connected to altered predictive and active inference processes, in which cognitive and physiological pathogenic beliefs shape the clinical condition and its symptoms. However, there is a lack of general empirical models that integrate cognitive beliefs, physiological experience, and symptoms in healthy and clinical populations. In this study, we examined the relationship between altered predictive mechanisms, interoception, and pathological bodily distortions in healty individuals and in individuals suffering from anorexia nervosa (AN). AN patients (N=15) completed a Virtual Reality Full-Body Illusion along with interoceptive tasks twice: at hospital admission during an acute symptomatological phase (Time 1) and after a 12-week outpatient clinical weight-restoring rehabilitative program (Time 2). Results were compared to a healthy control group. Our findings indicated that higher levels of interoceptive metacognitive awareness were associated with a greater embodiment. However, unlike in healthy participants, AN patients' interoceptive metacognition was linked to embodiment even in multisensory mismatching (asynchronous) conditions. In addition, unlike in healthy participants, higher interoceptive metacognition in AN patients was related to prior abnormal bodily distortions during the acute symptomatology phase. Prediction errors in bodily estimates predicted posterior bodily estimate distortions after the illusion, but while this relationship was only significant in the synchronous condition in healthy participants, there was no significant difference between synchronous and asynchronous conditions in AN patients. Despite the success of the rehabilitation program in restoring some dysfunctional patterns in the AN group, prediction errors and posterior estimate distortions were present at hospital discharge. Our findings suggest that individuals with AN prioritize interoceptive metacognitive processes (i.e., confidence in their own perceived sensations rather than their actual perceptions), disregarding bottom-up bodily inputs in favour of their prior altered top-down beliefs. Moreover, even if the rehabilitative program partially mitigated these alterations, the pathological condition impaired the patients' ability to coherently update their prior erroneous expectations with real-time multisensory bottom-up bodily information, possibly locking the patients in the experience of a distorted prior top-down belief. These results suggest new therapeutic perspectives and introduce the framework of regenerative virtual therapy (RVT), which aims at utilizing technology-based somatic modification techniques to restructure the maladaptive priors underlying a pathological condition.}
}
@article{FLETCHER1998747,
title = {Computational fluid dynamics modelling of an entrained flow biomass gasifier},
journal = {Applied Mathematical Modelling},
volume = {22},
number = {10},
pages = {747-757},
year = {1998},
issn = {0307-904X},
doi = {https://doi.org/10.1016/S0307-904X(98)10025-2},
url = {https://www.sciencedirect.com/science/article/pii/S0307904X98100252},
author = {D.F. Fletcher and B.S. Haynes and J. Chen and S.D. Joseph},
abstract = {A mathematical model, based on the Computational Fluid Dynamics package CFX4, has been developed to study the flow within an entrained flow biomass gasifier. The gasifier is designed to convert sawdust and chopped cotton gin trash into a low calorific value gas which can be burned in a modified engine to run a generator. Calculations of the flowfield are performed using the standard k–ϵ model and a Differential Reynolds Stress Model (DSM). In line with current thinking, it is shown that the k–ϵ model gives unphysical results for complex swirling flows, whereas the DSM model performs well. Particle tracking was performed to determine typical trajectories for the biomass and char and the results used to determine means of avoiding slagging in the gasifier base. The simulations have proved to be very useful to the designers who are now using the model to optimise the design.}
}
@article{KUMAR20223122,
title = {Experimental Spectroscopic, Quantum Computational, Hirshfeld Surface, Molecular Docking, and Electronic Excitation Studies on an Antibiotic Agent: SDZ},
journal = {Polycyclic Aromatic Compounds},
volume = {43},
number = {4},
pages = {3122-3146},
year = {2022},
issn = {1040-6638},
doi = {https://doi.org/10.1080/10406638.2022.2063909},
url = {https://www.sciencedirect.com/science/article/pii/S1040663822010284},
author = {Mukesh Kumar and Aysha Fatima and Meenakshi Singh and Indresh Verma and Ghazala Khanum and S. Muthu and Khaled Althubeiti and khamael M. Abualnaja and Musheer Ahmad and Nazia Siddiqui and Saleem Javed},
keywords = {DFT, NBO, EDD and HDD, molecular docking},
abstract = {In this report sulfadiazine (SDZ) has been experimentally and quantum chemically investigated. Computational analysis was carried out theoretically using the density functional theory (DFT) approach/B3LYP and 6-311++G(d,p) level to obtain optimized geometrical structure and vibrational modes analysis and other various calculations. A detailed description of the intermolecular interactions of the crystal surface were carried out by means of Hirshfeld surface analysis and fingerprint plots. Exploration of electron excitation from occupied to unoccupied orbitals in a single electron pair occurs, with dimethyl sulfoxide (DMSO) and MeOH as solvents and electron density distribution (EDD) and hole density distribution (HDD) maps were drawn in an excited state. The molecule reactivity region MEP, molecular stability, natural bond orbital (NBO), HOMO–LUMO, dipole moment (μ), polarizability (α), and hyperpolarizability (β) nonlinear optical (NLO), have all been taken into account. NBO analysis was carried out and the hybridization of atoms that form bonds was evaluated. The charge transfer of the title molecule has been examined by TD-DFT method The UV–Vis spectrum was obtained by employing the TDDFT/PCM method and compared with experimental spectra. Calculated HOMO→LUMO energy gap and charge transfer in the molecule was investigated. Chemical descriptors indicate the reactivity of the molecule as a whole, and Fukui function calculations were used to examine the reactive locations of the compound. The electrophilicity index was calculated and the bio-activity of the molecule was studied. However, biological research like drug-likeness and molecular docking are also done on the molecule.}
}
@article{FINDLAY1988165,
title = {Thinking creatively about creative thinking},
journal = {Journal of Social and Biological Structures},
volume = {11},
number = {1},
pages = {165-175},
year = {1988},
issn = {0140-1750},
doi = {https://doi.org/10.1016/0140-1750(88)90059-0},
url = {https://www.sciencedirect.com/science/article/pii/0140175088900590},
author = {C.Scott Findlay and Charles J. Lumsden}
}
@article{VARGASROJAS2022110093,
title = {Prescriptive comprehensive approach for the engineering of products made with composites centered on the manufacturing process and structured design methods: Review study performed on filament winding},
journal = {Composites Part B: Engineering},
volume = {243},
pages = {110093},
year = {2022},
issn = {1359-8368},
doi = {https://doi.org/10.1016/j.compositesb.2022.110093},
url = {https://www.sciencedirect.com/science/article/pii/S1359836822004693},
author = {Erik Vargas-Rojas},
keywords = {Composites thinking, Design method, Filament winding, Product engineering, TRIZ},
abstract = {At first, this research seeks to develop the technology required to fabricate two surfaces of revolution via filament winding: a concavity and a convexity. Concerning mandrels technology, the detachable mandrel concept is chosen among others. Their engineering is conducted conventionally based on free-thinking design approaches, as well as expertise and overconfidence. Consequently, the demoulding process of the concave surface is inefficient due to the lack of adequate dismantling functions of the respective mandrel, leading to damage of the composite material during demoulding, mandrel rework and delays. These inconveniences motivated a reexamination of the mandrels design process. Thus, three structured design methods were incorporated: Design for Manufacturing and Assembly (DFMA), Functional Analysis (FA) and Theory of Inventive Problem Solving (TIPS, a.k.a. TRIZ). Their synergistic implementation allowed the correct demoulding of the concave surface of revolution. In a second stage, this experience serves as reference for proposing a comprehensive, iterative, prescriptive and unified approach aimed at filament-wound products. It focuses on the base material (composites) and the manufacturing process (filament winding), being applicable to other fabrication processes of composite products. It is based on three models reported in the literature: one for metals, one for composites and one for filament-wound composites. Each of its steps is carried out with well-known structured design methods employed in products and systems engineering, including but not limited to DFMA, FA and TRIZ. As regards results, at the level of the design problem of the mandrels, the importance of the correct establishment of mechanical functions – dismantling in this case – is observed. In particular, how the lack of demoulding functions impacts unfavorably the quality of the final product. As respects the comprehensive approach, a significant outcome is the “filament winding thinking,” as an evolution of other schemes such as “metals” or “composites thinking” followed to generically develop products according to the nature of their base material.}
}
@incollection{SEJNOWSKI2015480,
title = {Computational Neuroscience},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {480-484},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.55011-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868550119},
author = {Terrence J. Sejnowski},
keywords = {Algorithms, Computational models, Neural systems},
abstract = {The goal of computational neuroscience is to understand how brains generate behaviors using computational approaches. Computational models of the brain explore how populations of highly interconnected neurons are formed during development and how they represent, process, store, act upon, and become altered by information present in the body and the environment. Techniques from physics, computer science, and mathematics are used to simulate and analyze these computational models and provide links between the wide range of levels that brains are investigated, from molecular interactions to large-scale systems. Models are also used for interpreting experimental data and providing a conceptual framework for the dynamical properties of neural systems, which should lead to more comprehensive theories of brain function.}
}
@article{BROO2022100290,
title = {Transdisciplinarity and three mindsets for sustainability in the age of cyber-physical systems},
journal = {Journal of Industrial Information Integration},
volume = {27},
pages = {100290},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100290},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X2100087X},
author = {Didem Gürdür Broo},
keywords = {Cyber-physical system, Design thinking, Future studies, Sustainability, Systems thinking},
abstract = {Cyber-physical systems (CPS), such as collaborative robots, smart cities, and autonomous vehicles, are seen as decisive contributions to addressing many societal challenges. These systems have the power to provide solutions to cope with an aging population, address climate change, and improve issues of health, public safety and mobility. As a product of the fourth industrial revolution, these systems are currently inviting much interest. However, there are barriers that need to be considered and understood to be able to optimise the potential of these systems to support a more sustainable future. To this end, transdisciplinary skills and a combination of different mindsets are needed to be able to ask the right questions at the right time. There are several approaches that can help us to initiate constructing innovative, transformative, future-oriented and systematic ideas and questions. The three important approaches that are suggested in this article are systems mindset, design mindset and futuristic mindset. These mindsets combined with the transdisciplinary perspective – where different disciplines work jointly to create sustainable solutions not only for today but also for tomorrow – have the power to change the world. This article underlines the importance of transdisciplinarity, presents the three mindsets and illustrates a hypothetical use case on how to blend these three mindsets to enable creative work in the future's transdisciplinary world for human-centred and sustainable future.}
}
@article{ESCOLAGASCON2022e11303,
title = {'Feeling' or 'sensing' the future? Testing for anomalous cognitions in clinical versus healthy populations},
journal = {Heliyon},
volume = {8},
number = {11},
pages = {e11303},
year = {2022},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2022.e11303},
url = {https://www.sciencedirect.com/science/article/pii/S2405844022025919},
author = {Álex Escolà-Gascón and Abigail C. Wright and James Houran},
keywords = {Boundary functioning, Emotional intelligence, Parapsychology, Premonitions, Schizophrenia, Thinking styles},
abstract = {In the study and treatment of psychosis, emotional intelligence (EI) and thinking styles are important patient characteristics for successful outcomes in clinical intervention. Anticipation of unpredictable stimuli (AUS) may be understood as an anomalous perception and anomalous cognition in which an individual supposedly senses and recognizes future stimuli in an unexpected way, also referred to as “hunches or premonitions.” This examined the roles of EI and thinking styles in AUSs in convenience samples of healthy participants (n = 237) versus patients diagnosed with psychosis (n = 118). We adjusted several quadratic and exponential regression models according to the obtained functions. Group means were also compared to examine differences in EI scores for participants with psychosis compared to healthy participants. In the healthy group, EI predicted AUSs with a weight between 42% and 58%. Thinking styles were not correlated with AUSs. However, EI was not correlated with AUSs in the clinical group. Patients with psychosis tended to score higher on AUSs and lower on EI and thinking styles compared to participants in the healthy group. We discuss EI as a variable that can contextualize some anomalous perceptions which are otherwise difficult to classify or measure within the classic psychosis continuum model.}
}
@incollection{WARE2004351,
title = {Chapter 11 - Thinking with visualizations},
editor = {Colin Ware},
booktitle = {Information Visualization (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {351-386},
year = {2004},
series = {Interactive Technologies},
isbn = {978-1-55860-819-1},
doi = {https://doi.org/10.1016/B978-155860819-1/50014-5},
url = {https://www.sciencedirect.com/science/article/pii/B9781558608191500145},
author = {Colin Ware},
abstract = {Publisher Summary
The best visualizations are not static images to be printed in books, but fluid, dynamic artifacts that respond to the need for a different view or for more detailed information. Visualization can be an interface to a simulation of a complex system; the visualization, combined with the simulation, can create a powerful cognitive augmentation. The visualization is a two-way interface, although highly asymmetric, with far higher bandwidth communication from the machine to the human than in the other direction. The high-bandwidth visualization channel is then used to deliver the results of modeling exercises and database searches. One way to approach the design of an information system is to consider the cost of knowledge. The result of this approach is a kind of cognitive information economics. Activities are analyzed according to the value of what is gained and the cost incurred. There are two kinds of costs: resource costs and opportunity costs. The chapter explores both of these and the economics of cognition and the cognitive cost of knowledge.}
}
@article{ANDREWS2019102188,
title = {Black hole as a model of computation},
journal = {Results in Physics},
volume = {13},
pages = {102188},
year = {2019},
issn = {2211-3797},
doi = {https://doi.org/10.1016/j.rinp.2019.102188},
url = {https://www.sciencedirect.com/science/article/pii/S2211379719304036},
author = {G.R. Andrews},
keywords = {Black hole computation, Kerr/CFT correspondence, Holographic principle, Information theory, Gamma-ray spectroscopy, Shannon entropy},
abstract = {This paper focuses on an alternative, more physically realistic model of computation than Etesi and Németi’s relativistic computer in a Malament-Hogarth spacetime (2002) that uses the black hole itself combined with an external observer equipped with a source and some method of measurement of gamma-rays, as opposed to sending a classical computer into a black hole and exploiting the properties of the spacetime to achieve hypercomputation. The source of output, Hawking radiation, is considered along with the constraints imposed by the holographic principle which limit the number of degrees of freedom in the system and consequently the maximum usable information. The Bekenstein-Hawking entropy is converted from the traditional form in terms of the horizon area to that of the Shannon entropy, establishing an analogy between the physical and computational perspectives of the system. Next examples are considered to establish the approximate order of the necessary excitation energy and the resulting gamma-ray interactions which form the input from the observer. Finally, the Turing completeness of the language for this model is considered through a simulation of the Turing machine. The goal is to introduce a model of computation that can later be used to study the relationship between computability and physical systems.}
}
@article{JAY201976,
title = {Intensional computation with higher-order functions},
journal = {Theoretical Computer Science},
volume = {768},
pages = {76-90},
year = {2019},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2019.02.016},
url = {https://www.sciencedirect.com/science/article/pii/S0304397519301227},
author = {Barry Jay},
keywords = {Intensional computation, Higher-order functions, SF-calculus, Foundations of computation},
abstract = {Intensional computations are those that query the internal structure of their arguments. In a higher-order setting, such queries perform program analysis. This is beyond the expressive power of traditional term rewriting systems, such as lambda-calculus or combinatory logic, as they are extensional. In such settings it has been necessary to encode or quote the program before analysis. However, there are intensional calculi, specifically confluent term rewriting systems, that can analyse higher-order programs within the calculus proper, without quotation; there are even programs that produce the Goedel numbers of their program argument. This paper summarizes the current situation. Highlights include the following observations. We have known since 2011 that the simplest intensional calculus, SF-calculus, supports arbitrary queries of closed normal forms, including equality, pattern-matching, searching and self-interpretation. Recent work, verified using the Coq proof assistant, has shown that all recursive programs can be represented as closed normal forms in SF-calculus, and even in combinatory logic. Thus, we can here deduce that SF-calculus (but not combinatory logic) can define queries of programs. These results are compatible with direct support for lambda-abstraction. Although these results conflict with the traditional understanding of expressive power of combinatory logic and λ-calculus, as developed by Church and Kleene, our recent publication has shown that their approach is compromised by its reliance on encodings. To drive the point home, this paper uses a non-standard encoding to lambda-define a trivial solution of the Halting Problem.}
}
@article{DEV2015232,
title = {Unsolved problems in biology—The state of current thinking},
journal = {Progress in Biophysics and Molecular Biology},
volume = {117},
number = {2},
pages = {232-239},
year = {2015},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2015.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0079610715000115},
author = {Sukhendu B. Dev},
keywords = {Unsolved biological problems, Millennium Prize, Origin of life},
abstract = {Many outstanding problems have been solved in biology and medicine for which scientists have been awarded prestigious prizes including the Nobel Prize, Lasker Award and Breakthrough Prizes in life sciences. These have been the fruits of years of basic research. From time to time, publications have appeared listing “unsolved” problems in biology. In this article, I ask the question whether it is possible to have such a list, if not a unique one, at least one that is analogous to the Millennium Prize in mathematics. My approach to finding an answer to this question was to gather views of leading biologists. I have also included my own views. Analysis of all the responses received over several years has convinced me that it is difficult, but not impossible, to have such a prize. Biology is complex and very interdisciplinary these days at times involving large numbers of teams, unlike mathematics, where Andrew Wiles spent seven years in complete isolation and secrecy solving Fermat's last theorem. Such an approach is simply not possible in biology. Still I would like to suggest that a similar prize can be established by a panel of distinguished scientists. It would be awarded to those who solved one of the listed problems in biology that warrant a verifiable solution. Despite many different opinions, I found that there is some commonality in the responses I received – I go on to discuss what these are and how they may impact future thinking.}
}
@article{GURSOY201529,
title = {Visualizing making: Shapes, materials, and actions},
journal = {Design Studies},
volume = {41},
pages = {29-50},
year = {2015},
note = {Special Issue: Computational Making},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2015.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X15000617},
author = {Benay Gürsoy and Mine Özkar},
keywords = {material computing, computational models, design activity, parametric design, reasoning},
abstract = {The increasing interest in materiality currently challenges the long existing traditions that consider visual thinking as the primary actor in design creativity. Shape grammars offer a formalism to represent visual reasoning in design, which is never purely limited to the visual aspects of design processes. Aiming to develop ways to explicitly include material manipulation in a computational formalism, we report on an ongoing exploration of how shape computation extends beyond abstract visual shapes to incorporate material shapes that have a physical existence. We present a materially informed process with shape rules and show that we can apply these rules creatively to explore the physical character of the material.}
}
@article{MAYER2017107,
title = {Understanding scientists’ computational modeling decisions about climate risk management strategies using values-informed mental models},
journal = {Global Environmental Change},
volume = {42},
pages = {107-116},
year = {2017},
issn = {0959-3780},
doi = {https://doi.org/10.1016/j.gloenvcha.2016.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S0959378016306197},
author = {Lauren A. Mayer and Kathleen Loa and Bryan Cwik and Nancy Tuana and Klaus Keller and Chad Gonnerman and Andrew M. Parker and Robert J. Lempert},
keywords = {Values-informed mental models, Climate change, Risk management, Decision making under uncertainty},
abstract = {When developing computational models to analyze the tradeoffs between climate risk management strategies (i.e., mitigation, adaptation, or geoengineering), scientists make explicit and implicit decisions that are influenced by their beliefs, values and preferences. Model descriptions typically include only the explicit decisions and are silent on value judgments that may explain these decisions. Eliciting scientists’ mental models, a systematic approach to determining how they think about climate risk management, can help to gain a clearer understanding of their modeling decisions. In order to identify and represent the role of values, beliefs and preferences on decisions, we used an augmented mental models research approach, namely values-informed mental models (ViMM). We conducted and qualitatively analyzed interviews with eleven climate risk management scientists. Our results suggest that these scientists use a similar decision framework to each other to think about modeling climate risk management tradeoffs, including eight specific decisions ranging from defining the model objectives to evaluating the model’s results. The influence of values on these decisions varied between our scientists and between the specific decisions. For instance, scientists invoked ethical values (e.g., concerns about human welfare) when defining objectives, but epistemic values (e.g., concerns about model consistency) were more influential when evaluating model results. ViMM can (i) enable insights that can inform the design of new computational models and (ii) make value judgments explicit and more inclusive of relevant values. This transparency can help model users to better discern the relevance of model results to their own decision framing and concerns.}
}
@article{HUIJSER2018170,
title = {The wandering self: Tracking distracting self-generated thought in a cognitively demanding context},
journal = {Consciousness and Cognition},
volume = {58},
pages = {170-185},
year = {2018},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2017.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S1053810017301927},
author = {Stefan Huijser and Marieke K. {van Vugt} and Niels A. Taatgen},
keywords = {Self-generated thought, Mind wandering, Self-referential processing, Task demand, Computational cognitive modeling, Eye-tracking},
abstract = {We investigated how self-referential processing (SRP) affected self-generated thought in a complex working memory task (CWM) to test the predictions of a computational cognitive model. This model described self-generated thought as resulting from competition between task- and distracting processes, and predicted that self-generated thought interferes with rehearsal, reducing memory performance. SRP was hypothesized to influence this goal competition process by encouraging distracting self-generated thinking. We used a spatial CWM task to examine if SRP instigated such thoughts, and employed eye-tracking to examine rehearsal interference in eye-movement and self-generated thinking in pupil size. The results showed that SRP was associated with lower performance and higher rates of self-generated thought. Self-generated thought was associated with less rehearsal and we observed a smaller pupil size for mind wandering. We conclude that SRP can instigate self-generated thought and that goal competition provides a likely explanation for how self-generated thoughts arises in a demanding task.}
}
@article{ALEXANDRU20221,
title = {Theories of life and computation: Special issue on the occasion of the 65th birthday of Professor Gabriel Ciobanu},
journal = {Theoretical Computer Science},
volume = {926},
pages = {1-2},
year = {2022},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2022.06.041},
url = {https://www.sciencedirect.com/science/article/pii/S0304397522004157},
author = {Andrei Alexandru and Bogdan Aman and Ross Horne}
}
@article{OERS199051,
title = {The development of mathematical thinking in school: a comparison of the action- psychological and information-processing approaches},
journal = {International Journal of Educational Research},
volume = {14},
number = {1},
pages = {51-66},
year = {1990},
issn = {0883-0355},
doi = {https://doi.org/10.1016/0883-0355(90)90016-2},
url = {https://www.sciencedirect.com/science/article/pii/0883035590900162},
author = {Bert Van Oers},
abstract = {The learning and teaching of mathematics can be analyzed from different psychological points of view. Information-processing theories focus on the various information-processing mechanisms underlying mathematical competence and try to foster the development of these mechanisms in order to stimulate the growth of mathematical competence. In the action-psychological approach of the cultural-historical school, mathematics is viewed as a kind of culturally developed human activity governed by the rules that mathematicians themselves follow while doing their job. Consequently, the development of mathematical competence is regarded as the formation of a system of meaningful mathematical actions that constitute that activity. At a general theoretical level these approaches can be shown to be basically different and even incompatible. With regard to mathematics education the differences are illustrated with respect to several themes such as task-analysis, automatization, and the learning of elementary arithmetic. Considering insight and meaningful and sophisticated problem-solving as the core of mathematical thinking, the action-psychological approach appears to be the more promising candidate as an aid in the design of future mathematics education.}
}
@article{ADAMATZKY2017469,
title = {East-West paths to unconventional computing},
journal = {Progress in Biophysics and Molecular Biology},
volume = {131},
pages = {469-493},
year = {2017},
note = {Integral Biomathics 2017: The Necessary Conjunction of Western and Eastern Thought Traditions for Exploring the Nature of Mind and Life},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2017.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0079610717301177},
author = {Andrew Adamatzky and Selim Akl and Mark Burgin and Cristian S. Calude and José Félix Costa and Mohammad Mahdi Dehshibi and Yukio-Pegio Gunji and Zoran Konkoli and Bruce MacLennan and Bruno Marchal and Maurice Margenstern and Genaro J. Martínez and Richard Mayne and Kenichi Morita and Andrew Schumann and Yaroslav D. Sergeyev and Georgios Ch. Sirakoulis and Susan Stepney and Karl Svozil and Hector Zenil},
keywords = {Unconventional computing, East, West, Spirituality},
abstract = {Unconventional computing is about breaking boundaries in thinking, acting and computing. Typical topics of this non-typical field include, but are not limited to physics of computation, non-classical logics, new complexity measures, novel hardware, mechanical, chemical and quantum computing. Unconventional computing encourages a new style of thinking while practical applications are obtained from uncovering and exploiting principles and mechanisms of information processing in and functional properties of, physical, chemical and living systems; in particular, efficient algorithms are developed, (almost) optimal architectures are designed and working prototypes of future computing devices are manufactured. This article includes idiosyncratic accounts of ‘unconventional computing’ scientists reflecting on their personal experiences, what attracted them to the field, their inspirations and discoveries.}
}
@article{KELLER2018424,
title = {Predictive Processing: A Canonical Cortical Computation},
journal = {Neuron},
volume = {100},
number = {2},
pages = {424-435},
year = {2018},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2018.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0896627318308572},
author = {Georg B. Keller and Thomas D. Mrsic-Flogel},
keywords = {predictive processing, predictive coding, sensory processing, cortex, canonical microcircuit},
abstract = {This perspective describes predictive processing as a computational framework for understanding cortical function in the context of emerging evidence, with a focus on sensory processing. We discuss how the predictive processing framework may be implemented at the level of cortical circuits and how its implementation could be falsified experimentally. Lastly, we summarize the general implications of predictive processing on cortical function in healthy and diseased states.}
}
@article{NIKOLIC2023107820,
title = {Where is the mind within the brain? Transient selection of subnetworks by metabotropic receptors and G protein-gated ion channels},
journal = {Computational Biology and Chemistry},
volume = {103},
pages = {107820},
year = {2023},
issn = {1476-9271},
doi = {https://doi.org/10.1016/j.compbiolchem.2023.107820},
url = {https://www.sciencedirect.com/science/article/pii/S1476927123000117},
author = {Danko Nikolić},
keywords = {Scaling problem, Explanatory gap, Connectionism, Metabotropic receptors, G protein-gated ion channels, Practopoiesis},
abstract = {Perhaps the most important question posed by brain research is: How the brain gives rise to the mind. To answer this question, we have primarily relied on the connectionist paradigm: The brain’s entire knowledge and thinking skills are thought to be stored in the connections; and the mental operations are executed by network computations. I propose here an alternative paradigm: Our knowledge and skills are stored in metabotropic receptors (MRs) and the G protein-gated ion channels (GPGICs). Here, mental operations are assumed to be executed by the functions of MRs and GPGICs. As GPGICs have the capacity to close or open branches of dendritic trees and axon terminals, their states transiently re-route neural activity throughout the nervous system. First, MRs detect ligands that signal the need to activate GPGICs. Next, GPGICs transiently select a subnetwork within the brain. The process of selecting this new subnetwork is what constitutes a mental operation – be it in a form of directed attention, perception or making a decision. Synaptic connections and network computations play only a secondary role, supporting MRs and GPGICs. According to this new paradigm, the mind emerges within the brain as the function of MRs and GPGICs whose primary function is to continually select the pathways over which neural activity will be allowed to pass. It is argued that MRs and GPGICs solve the scaling problem of intelligence from which the connectionism paradigm suffers.}
}
@article{PINE2017385,
title = {Clinical Advances From a Computational Approach to Anxiety},
journal = {Biological Psychiatry},
volume = {82},
number = {6},
pages = {385-387},
year = {2017},
note = {Computational Psychiatry},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2016.09.020},
url = {https://www.sciencedirect.com/science/article/pii/S0006322316328669},
author = {Daniel S. Pine}
}
@article{1999209,
title = {99/02041 Strategic thinking about nuclear energy: implications of the emerging market structure in electric generation: Bodde, D. L. Energy Policy, 1998, 26, (12), 957–962},
journal = {Fuel and Energy Abstracts},
volume = {40},
number = {3},
pages = {209},
year = {1999},
issn = {0140-6701},
doi = {https://doi.org/10.1016/S0140-6701(99)97811-6},
url = {https://www.sciencedirect.com/science/article/pii/S0140670199978116}
}
@article{BERNARD20153982,
title = {Developing a Capability to Elicit and Structure Psychosocial Decision Information within Computational Models},
journal = {Procedia Manufacturing},
volume = {3},
pages = {3982-3989},
year = {2015},
note = {6th International Conference on Applied Human Factors and Ergonomics (AHFE 2015) and the Affiliated Conferences, AHFE 2015},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2015.07.945},
url = {https://www.sciencedirect.com/science/article/pii/S2351978915009464},
author = {Michael L. Bernard},
keywords = {Knowledge elicitation, Knowledge structure, Cognitive modeling, Social modeling, Systems modeling, Country assessments},
abstract = {There is a recognized need to develop computational models that can represent and simulate the decision making process of various groups across socio-cultural domains [5]. Yet, developing such models can be greatly hampered by the need to acquire and represent information pertaining to the psychological and social aspects of decision-making within these groups. Currently, there are numerous techniques and tools to help facilitate the elicitation and structuring of knowledge within expert-type systems—particularly those that focus on technical processes such as mechanical troubleshooting [3]. However, few techniques and tools have been developed for models that are intended to represent and assess the decision making of groups within different societies—particularly including cultural elements within these societies. This paper seeks to help address this challenge by discussing an approach to eliciting and structuring cross-cultural psychosocial and behavioral-economic elements within a theory-based assessment model. This work was developed to address the needs of Sandia National Laboratories’ Behavioral Influence Assessment modeling capability, which assesses decision-making within societies. The main component of the knowledge engineering effort is what we call the “knowledge structure.” The knowledge structure acts as scaffolding for the organization of psychosocial processes underlying decision-making, as well as the actual content of that knowledge with respect to a modeled society.}
}
@article{20161,
title = {Credit for Computation},
journal = {Cell Systems},
volume = {3},
number = {1},
pages = {1-2},
year = {2016},
issn = {2405-4712},
doi = {https://doi.org/10.1016/j.cels.2016.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S2405471216302289}
}
@article{LEIRMO2024761,
title = {Digital Twins for Industry 5.0: Unlocking the Human Potential},
journal = {Procedia CIRP},
volume = {130},
pages = {761-766},
year = {2024},
note = {57th CIRP Conference on Manufacturing Systems 2024 (CMS 2024)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.10.161},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124013180},
author = {Torbjørn L. Leirmo},
keywords = {Digital twin, Human aspect, Industry 5.0},
abstract = {The holy grail of Industry 5.0 resides in the intersection of the three dimensions; sustainable, resilient, and human-centric manufacturing. While the Industry 4.0 paradigm addresses resiliency and sustainability through increased flexibility and efficiency, the human component of manufacturing systems has been largely neglected. Despite rapid developments in artificial intelligence, human intelligence remains superior in terms of creative and critical thinking. Digital twins have emerged as a concept that effectively merges the physical and the digital worlds in cyber-physical production systems. The computational power of digital systems is leveraged to collect and aggregate data that are analyzed and presented to a human decision-maker. Taking a human-centric perspective, the digital twin should be designed to enhance human capabilities, accommodate the needs of people, and mitigate shortcomings of the human mind. This paper addresses these issues by discussing how humans may utilize and better interact with digital twins. A conceptual framework for a human-centric digital twin is proposed with use cases for various interfaces for operators, engineers, and managers.}
}
@article{DECAROLIS2011145,
title = {Using modeling to generate alternatives (MGA) to expand our thinking on energy futures},
journal = {Energy Economics},
volume = {33},
number = {2},
pages = {145-152},
year = {2011},
issn = {0140-9883},
doi = {https://doi.org/10.1016/j.eneco.2010.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0140988310000721},
author = {Joseph F. DeCarolis},
keywords = {Mathematical methods (JEL: C02), Optimization, Uncertainty, Modeling},
abstract = {Energy-economy optimization models – encoded with a set of structured, self-consistent assumptions and decision rules – have emerged as a key tool for the analysis of energy and climate policy at the national and international scale. Given the expansive system boundaries and multi-decadal timescales involved, addressing future uncertainty in these models is a critical challenge. The approach taken by many modelers is to build larger models with greater complexity to deal with structural uncertainty, and run a few highly detailed scenarios under different input assumptions to address parametric uncertainty. The result is often large and inflexible models used to conduct analysis that offers little insight. This paper introduces a technique borrowed from the operations research literature called modeling to generate alternatives (MGA) as a way to flex energy models and systematically explore the feasible, near-optimal solution space in order to develop alternatives that are maximally different in decision space but perform well with regard to the modeled objectives. The resultant MGA alternatives serve a useful role by challenging preconceptions and highlighting plausible alternative futures. A simple, conceptual model of the U.S. electric sector is presented to demonstrate the utility of MGA as an energy modeling technique.}
}
@incollection{PALANIAPPAN2019789,
title = {Computational Systems Biology},
editor = {Shoba Ranganathan and Michael Gribskov and Kenta Nakai and Christian Schönbach},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology},
publisher = {Academic Press},
address = {Oxford},
pages = {789-795},
year = {2019},
isbn = {978-0-12-811432-2},
doi = {https://doi.org/10.1016/B978-0-12-809633-8.20287-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128096338202872},
author = {Sucheendra K. Palaniappan and Ayako Yachie-Kinoshita and Samik Ghosh},
keywords = {Analytics, Automated pathway curation, Computational systems biology, Data analytics, Mathematical modeling, Network analysis, NLP, Pathway curation, Simulation, Systems biology, Text mining},
abstract = {The unprecedented development in novel and high throughput techniques to understand biology at multiple dimensions has opened unique challenges and opportunities for computational methodologies to harness “big data in biology” and extract actionable insights. New models and methodologies are need for systems biology-based approaches to reconcile data from different spatio-temporal scales, connecting diverse set of computational techniques towards a systems-level understand of living organisms. Current tools and techniques in computational systems biology have demonstrated their usage in various application areas. At the same time, paradigm shifts in experimental techniques, powerful data analytics, modeling and visualization methodologies, have resulted in empowering computational systems biology models and methodologies. These developments will leverage on the advancements in machine learning models, big data management and analysis as well as large scale modeling and simulations. This topic article endeavors to provide key areas of modeling and methodologies– highlighting new directions and developments, to enable computational systems biology to address the new challenges in biology and medicine.}
}
@article{NAGURNEY19953,
title = {Massively parallel computation of spatial price equilibrium problems as dynamical systems},
journal = {Journal of Economic Dynamics and Control},
volume = {19},
number = {1},
pages = {3-37},
year = {1995},
issn = {0165-1889},
doi = {https://doi.org/10.1016/0165-1889(93)00772-V},
url = {https://www.sciencedirect.com/science/article/pii/016518899300772V},
author = {Anna Nagurney and Takashi Takayama and Ding Zhang},
keywords = {Spatial price equilibrium, Dynamical systems, Variational inequalities, Massively parallel computation},
abstract = {In this paper we introduce a dynamical system for the formulation and computation of spatial price equilibrium problems in quantity variables. The set of stationary points of the system corresponds to the set of solutions of the variational inequality problem governing the problem. We propose the Euler-type method for the computation of the equilibrium pattern and provide convergence results. We then demonstrate that the algorithm can be implemented on a massively parallel architecture and illustrate its performance on the Thinking Machine's CM-2 architecture. This research represents the first implementation of a massively parallel approach for the computation of either dynamical systems or variational inequality problems arising in economics.}
}
@incollection{HORWITZ2023265,
title = {8 - Improved force models for Euler–Lagrange computations},
editor = {Shankar Subramaniam and S. Balachandar},
booktitle = {Modeling Approaches and Computational Methods for Particle-Laden Turbulent Flows},
publisher = {Academic Press},
pages = {265-298},
year = {2023},
series = {Computation and Analysis of Turbulent Flows},
isbn = {978-0-323-90133-8},
doi = {https://doi.org/10.1016/B978-0-32-390133-8.00015-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323901338000153},
author = {Jeremy A.K. Horwitz},
keywords = {undisturbed fluid velocity, two-way coupling, Euler–Lagrange, spherical particle equation of motion, drag, lift, unsteady effects, wall-bounded flows, multi-particle effects},
abstract = {This chapter focuses on force models governing spherical particle motion which are used in Euler–Lagrange methods. These forces also represent a momentum exchange and are important for modeling how fluid dynamics change in the presence of particles. A survey of widely used drag and lift correlations for single- and multiple-particle systems will be presented along with some physical discussion as to their origins. Our focus is on incompressible applications though a few compressible force formulations will be mentioned. A central quantity that arises in these force correlations is the notion of the undisturbed fluid velocity. Seldom taught in fluid mechanics curricula and often confused with “free-stream” velocity, the undisturbed fluid velocity is discussed in detail to develop the reader's intuition. Modeling of the undisturbed fluid velocity is addressed in the context of two-way coupled correction schemes.}
}
@article{NENSA2025100001,
title = {Embracing generative AI: A necessary evolution in professional writing},
journal = {European Journal of Radiology Artificial Intelligence},
volume = {1},
pages = {100001},
year = {2025},
issn = {3050-5771},
doi = {https://doi.org/10.1016/j.ejrai.2024.100001},
url = {https://www.sciencedirect.com/science/article/pii/S305057712400001X},
author = {Felix Nensa},
keywords = {GenAI, LLM, ChatGPT, AI, Writing},
abstract = {Generative artificial intelligence (AI), particularly large language models (LLMs), has become an integral part of our professional lives. Despite their transformative potential, many professionals remain cautious about using these tools for drafting and editing manuscripts. While it is reasonable for academic journals to request transparency regarding AI usage, fundamental reservations against employing generative AI (GenAI) are outdated. A useful analogy can be drawn from the film Hidden Figures, which depicts the arrival of IBM computers at NASA, eventually replacing human “computers” for manual calculations. Dorothy Vaughan, the supervisor of these human experts, anticipated the change and adapted proactively by teaching her team programming skills. Today, it is unthinkable for scientific calculations to be done without software, just as it will soon be unthinkable to draft professional texts without AI assistance. GenAI should be seen as a tool that enhances human creativity rather than replacing it. By handling mundane aspects of writing, it allows authors to focus on critical thinking and idea generation. Transparency in AI use fosters trust and maintains ethical standards. Authors are encouraged to use GenAI under supervision and disclose its use openly. This will not only improve manuscript quality but also help authors allocate more time to innovation and creative thinking. Embracing GenAI is not merely an option; it represents an essential evolution in the way we approach writing.}
}