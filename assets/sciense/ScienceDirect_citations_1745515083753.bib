@article{ZHUAN2012248,
title = {The Assimilation Rule on the Parameters of Feedback System},
journal = {AASRI Procedia},
volume = {1},
pages = {248-260},
year = {2012},
note = {AASRI Conference on Computational Intelligence and Bioinformatics},
issn = {2212-6716},
doi = {https://doi.org/10.1016/j.aasri.2012.06.039},
url = {https://www.sciencedirect.com/science/article/pii/S2212671612000406},
author = {Ping Zhuan and Su Yun Gan and Shi Ming Zhou},
keywords = {Feedback, Assimilation factor, Supporting structure, Assimilation of parameters, Evolutionary mechanism},
abstract = {In this paper, the reaction mechanism of system feedback on the changes of external environmental parameters has been discussed. And the conclusions have been attributed to the assimilation rule. According to the research results of the circuit system, assimilated factors should be defined at first--- the parts which have been isolated from the system equivalent parameters and the external parameters have been also contained. Afterwards, the analog inductive method has been adopted to conduct the overall feasibility study for the establishment of the rules. Then, several new ideas have been also provided in accordance with the applications of assimilation rules in the fields of biology, cognitive science and social organizations, etc. Finally, the block diagram has been provided so as to give a comprehensive overview for the thinking ideas of system.}
}
@article{HONG2023116066,
title = {Portfolio allocation strategy for active learning Kriging-based structural reliability analysis},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {412},
pages = {116066},
year = {2023},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2023.116066},
url = {https://www.sciencedirect.com/science/article/pii/S0045782523001901},
author = {Linxiong Hong and Bin Shang and Shizheng Li and Huacong Li and Jiaming Cheng},
keywords = {Structural reliability analysis, Portfolio allocation, Kriging, Active learning, Failure probability},
abstract = {Recently, numerous studies have focused on structural reliability analysis, with the Kriging-based active learning method being particularly popular. A variety of Kriging-based learning functions have been proposed, and shown to perform well in various tasks. However, no single learning function has been demonstrated to consistently outperformed the others in all tasks, and selecting the most appropriate learning function for a given task remains a challenge in engineering applications. In this paper, inspired by the multi-armed bandit strategy, a portfolio allocation of different learning functions is proposed to resolve the issue of selecting a single one, where the better learning functions are selected online according to their past performance. Finally, three classical numerical examples and two engineering applications are adopted to validate the effectiveness of the proposed method.}
}
@article{KABOSOVA2022109668,
title = {Shape optimization during design for improving outdoor wind comfort and solar radiation in cities},
journal = {Building and Environment},
volume = {226},
pages = {109668},
year = {2022},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2022.109668},
url = {https://www.sciencedirect.com/science/article/pii/S0360132322008988},
author = {Lenka Kabošová and Angelos Chronis and Theodoros Galanos and Stanislav Kmeť and Dušan Katunský},
keywords = {Parametric architecture, Real-time wind analysis, Real-time sun analysis, Performative design, InFraRed, CFD, Computational fluid dynamics},
abstract = {This paper delivers an idea of weather-based optimization as a sustainable design strategy addressing the changing climate. An environment-driven design technique is introduced and tested at the urban and architectural scale. Utilizing the interplay between the architectural intention and weather influences (specifically solar radiation and wind effects), the optimal design solution for the urban configuration and architectural shape emerges. An exploratory case study near the amphitheater in Kosice, Slovakia, demonstrates the proposed approach. Through the real-time iterative analysis of the environmental performance of multiple design variants, an urban concept of offices/apartment blocks, reacting to the local wind and sun situation, is formed. The wind flow situation and comfort are investigated in a design loop, blending the newly developed AI-driven simulation prediction models of InFraRed11InFraRed: AI-based real-time analysis of wind, sun, and thermal comfort in the streets developed by the CIL of the AIT in Vienna, Austria. and the sun hours analysis in Ladybug with the Galapagos optimization within Grasshopper. The final step of this method is the design and subsequent wind and sun analysis of fluid-shaped lamellae in three variants acting as wind catchers/shading systems. Improved pedestrian wind comfort for outdoor sitting (more than 30% increase in areas suitable for short and prolonged sitting) and optimum sunlight hours (25% gain in sunlight during winter solstice) is achieved using the proposed technique.}
}
@article{ABRAHAM200738,
title = {Creative cognition: The diverse operations and the prospect of applying a cognitive neuroscience perspective},
journal = {Methods},
volume = {42},
number = {1},
pages = {38-48},
year = {2007},
note = {Neurocognitive Mechanisms of Creativity: A Toolkit},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2006.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S1046202306002994},
author = {Anna Abraham and Sabine Windmann},
keywords = {Creative cognition, Conceptual expansion, Creative imagery, Constraints of examples, Insight, Alternate uses task, Cognitive neuroscience, Neuropsychology, Top-down and bottom-up processes},
abstract = {Creativity is defined quite simply as “the ability to create” in most lexicons, but, in reality, this is a complex and heterogeneous construct about which there is much to be discovered. The cognitive approach to investigating creativity recognizes and seeks to understand this complexity by investigating the component processes involved in creative thinking. The cognitive neuroscience approach, which has only limitedly been applied in the study of creativity, should ideally build on these ideas in uncovering the neural substrates of these processes. Following an introduction into the early experimental ideas and the cognitive approach to creativity, we discuss the theoretical background and behavioral methods for testing various processes of creative cognition, including conceptual expansion, the constraining influence of examples, creative imagery and insight. The complex relations between the underlying component processes of originality and relevance across these tasks are presented thereafter. We then outline how some of these conceptual distinctions can be evaluated by neuroscientific evidence and elaborate on the neuropsychological approach in the study of creativity. Given the current state of affairs, our recommendation is that despite methodological difficulties that are associated with investigating creativity, adopting the cognitive neuroscience perspective is a highly promising framework for validating and expanding on the critical issues that have been raised in this paper.}
}
@article{SACOUTO202297,
title = {Using brain inspired principles to unsupervisedly learn good representations for visual pattern recognition},
journal = {Neurocomputing},
volume = {495},
pages = {97-104},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.04.130},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222005306},
author = {Luis Sa-Couto and Andreas Wichert},
keywords = {Hubel Wiesel’s Hypothesis, Brain inspired architectures, Invariant pattern recognition, Deep learning},
abstract = {Although deep learning has solved difficult problems in visual pattern recognition, it is mostly successful in tasks where there are lots of labeled training data available. Furthermore, the global back-propagation based training rule and the amount of employed layers represents a departure from biological inspiration. The brain is able to perform most of these tasks in a very general way from limited to no labeled data. For these reasons it is still a key research question to look into computational principles in the brain that can help guide models to unsupervisedly learn good representations which can then be used to perform tasks like classification. To that end, we start by recalling four key brain-inspired principles that relate to simple vision: modeling ”whats” and ”wheres” separately; including a time component; context dependency; and layer-wise learning. Then, we take these principles and use them to convey an a priori structure to our model that makes the learning problem easier. With that, our model is able to generate such high quality representations for the MNIST data set. We compare the obtained results with similar recent works and verify extremely competitive results.}
}
@article{STEGER2021127,
title = {Mental models of a social-ecological system facilitate social learning among a diverse management team},
journal = {Environmental Science & Policy},
volume = {122},
pages = {127-138},
year = {2021},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2021.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S1462901121001039},
author = {Cara Steger and Kflay Gebrehiwot and Shambel Alemu Chengere and Jake Marinkovich and Bikila Warkineh Dullo and Sisay Wube Zewde and Julia A. Klein},
keywords = {Participatory modeling, Social learning, Collaborative environmental management, Community-based conservation, Social-ecological systems, Ethiopia},
abstract = {Managing social-ecological systems increasingly requires collaboration among diverse teams with a wide range of worldviews and perspectives. Increased attention to the social and cultural factors that shape environmental outcomes is needed for these collaborative teams to function effectively. Mental models are cognitive representations of the external world which guide an individual’s thinking, decision-making, and behavior. They are critical elements of collaborative environmental management because they shape our understanding of social-ecological systems, our perceptions of environmental problems, and our preferences for certain management actions. In this paper, we describe an iterative process of constructing and revising mental models at both individual and small group levels over the course of a year in a community-based conservation area in the Ethiopian highlands. We compared mental models of the conservation area from four groups involved in management to identify commonalities and differences in the way people conceptualize the area. While we found high variability in mental models both within and across groups, most participants perceived social, economic, and political variables to be the key drivers of change in this system. Economic variables were also identified as key sensitivities, along with biotic and livelihood variables. However, groups differed considerably in how they thought about relationships between these variables, particularly political and economic variables. We used interviews with participants to assess how they learned throughout the mental modeling process, finding evidence of changes to stakeholder relationships, system understanding, and the time horizons used in planning. Women farmers differed from other groups at multiple stages in our process, both in the structure of the models they produced and in the social learning they experienced. Our study was strengthened by the iterative process that allowed individuals and small groups to reflect on their own understanding and share it with others, resulting in increased communication, mutual respect, and understanding among members of the management team. These findings point to the complementarity of both individual and group-level mental modeling for nuanced system understanding, and emphasize the need for diverse perspectives in collaborative environmental management in order for holistic understanding of both problems and solutions to emerge.}
}
@article{JANG2024132519,
title = {Comparative study on gradient-free optimization methods for inverse source-term estimation of radioactive dispersion from nuclear accidents},
journal = {Journal of Hazardous Materials},
volume = {461},
pages = {132519},
year = {2024},
issn = {0304-3894},
doi = {https://doi.org/10.1016/j.jhazmat.2023.132519},
url = {https://www.sciencedirect.com/science/article/pii/S0304389423018022},
author = {Siho Jang and Juryong Park and Hyun-Ha Lee and Chun-Sil Jin and Eung Soo Kim},
keywords = {Gradient-free optimization, Multi-units & multiple radionuclides release scenario, Improving source-term estimation, Environmental radioactivity monitoring, GPU parallelization},
abstract = {In this study, we rigorously assess the performance of three gradient-free optimization algorithms—Ensemble Kalman Inversion (EKI), Particle Swarm Optimization (PSO), and Genetic Algorithm (GA)—for estimating source terms in diverse radionuclide release scenarios. Our analysis encompasses both single and multiple sources with varying radionuclide compositions, delving into the influence of decay constants and radioactivity on source estimation accuracy. Although estimating a single radionuclide from a single source exhibits outstanding results, estimating multiple radionuclides from a single source proves more arduous due to the limited information available for discerning gamma dose rates. Contrary to expectations, increasing the number of observation stations does not consistently improve the likelihood of finding accurate solutions in ill-posed inverse problems. Impressively, under our simulation settings, EKI demonstrates competitive performance in terms of convergence, accuracy, and runtime compared to PSO and GA, with GPU parallelization further bolstering computational efficiency. We explore strategies for enhancing source term estimation, including incorporating prior information, applying uncertainty removal techniques, and optimizing observation placement. Additionally, this study underscores the intricate role of relative error in determining multi-radionuclide estimation accuracy from gamma dose measurements. By employing the Gaussian plume model under steady-state conditions, our research lays the groundwork for future applications of Lagrangian dispersion models with real-time data integration. The insights gleaned from our study promise to advance environmental radioactivity monitoring and catalyze the development of cutting-edge, real-time source estimation technologies in full-scale systems.}
}
@article{WALTER2016597,
title = {The financial Logos: The framing of financial decision-making by mathematical modelling},
journal = {Research in International Business and Finance},
volume = {37},
pages = {597-604},
year = {2016},
issn = {0275-5319},
doi = {https://doi.org/10.1016/j.ribaf.2016.01.022},
url = {https://www.sciencedirect.com/science/article/pii/S0275531916300228},
author = {Christian Walter},
keywords = {Performativity, Mathematisation, Mathematical modelling, Financialisation, Ethics, Finance},
abstract = {This paper introduces the notion of “financial Logos”, defined as a structuring discourse embedded in management tools and beliefs of financial practices. I hypothesize that this discourse contains a specific representation of risk mathematically modelled by probability measures. Next I use a performativity based approach to describe the concrete action of the financial Logos on financial practices: the framing of financial decision-making by mathematical modelling. I argue that it is not possible to think of a given financial practice without epistemologically and sociologically thinking of the contribution of the mathematical modelling to this practice. I conclude with consequences for ethics of finance: extending ethics of action to epistemic ethics, I suggest that, in finance, any preference in mathematical modelling is also a preference in ethics.}
}
@article{AMOORE2024102547,
title = {The deep border},
journal = {Political Geography},
volume = {109},
pages = {102547},
year = {2024},
issn = {0962-6298},
doi = {https://doi.org/10.1016/j.polgeo.2021.102547},
url = {https://www.sciencedirect.com/science/article/pii/S0962629821002079},
author = {Louise Amoore},
keywords = {Borders, Machine learning, Immigration, Computation, Algorithms, Biometric},
abstract = {Deep neural network algorithms are becoming intimately involved in the politics of the border, and are themselves bordering devices in that they classify, divide and demarcate boundaries in data. Deep learning involves much more than the deployment of technologies at the border, and is reordering what the border means, how the boundaries of political community can be imagined. Where the biometric border rendered the border mobile through its inscription in the body, the deep border generates the racialized body in novel forms that extend the reach of state violence. The deep border is written through the machine learning models that make the world in their own image – as clusters of attributes and feature spaces from which data examples can be drawn. The ‘depth’ that becomes imaginable in computer science models of the indefinite multiplication of layers in a neural network begins to resonate with state desires for a reach into the attributes of population. The border is spatially reimagined as a set of always possible functions, features, and clusters – as a ‘line of best fit’ where the fraught politics of the border can be condensed and resolved.}
}
@article{CHEN2021105850,
title = {Coupled crash mechanics and biomechanics of aircraft structures and passengers},
journal = {Communications in Nonlinear Science and Numerical Simulation},
volume = {101},
pages = {105850},
year = {2021},
issn = {1007-5704},
doi = {https://doi.org/10.1016/j.cnsns.2021.105850},
url = {https://www.sciencedirect.com/science/article/pii/S1007570421001623},
author = {Goong Chen and Jing Yang and Alexey Sergeev and Mingwei Wang and Chunqiu Wei and Jean Yeh and Philip J. Morris and Noah J. Fournier and Yining Chen and Xingong Cheng and Donghui Yang and Shuhuang Xiang and Marlan O. Scully},
keywords = {Aircraft crash mechanics, Passenger biomechanics, Aircraft structural components andfixtures, Injury analysis, LS-DYNA modeling, Supercomputer simulations, Vibration},
abstract = {The DYCAST (Dynamic Crash Analysis of Structures) experiments that started at NASA Langley Research Center during the late 1970s have greatly influenced the methodology and thinking of aircraft crashworthiness and survivability studies, and was continued and refined at other aerospace establishments. Nevertheless, so far most of the existing work has emphasized the impact damage to the aircraft section. Issues related to potential passenger injuries have not been properly addressed in the literature, to the best of our knowledge. Here, we study the DYCAST problem integrally by treating and combining impact damage and passenger injuries altogether. We develop the biomechanics by way of modal analysis of passenger dummy motions coupled with the vibration of aircraft structures in order to understand their basic interactions. Two types of mechanical dummies are used in this study. Such a modal analysis can help identify basic injury types, but is valid only in the constructed models, linear regime. However, we are able to extend the linear elastic model to a nonlinear elastoplastic computational model by using the versatile software LS-DYNA as the platform. Computer simulations are carried out on the supercomputer clusters and the numerical results are rendered into video animations for visualization and analysis. One can see, for example, how the passenger-dummy interactive motions with the fuselage and fixtures and the potential injuries caused in the event of general aircraft crashes on a fractal domain.}
}
@article{WANG2023100113,
title = {Scheduling power-to-ammonia plants considering uncertainty and periodicity of electricity prices},
journal = {Smart Energy},
volume = {11},
pages = {100113},
year = {2023},
issn = {2666-9552},
doi = {https://doi.org/10.1016/j.segy.2023.100113},
url = {https://www.sciencedirect.com/science/article/pii/S2666955223000205},
author = {Shunchao Wang and Pengfei Zhang and Tuo Zhuo and Hua Ye},
keywords = {Power-to-ammonia, Markov decision process, Hydrogen, Haber-Bosch reactor},
abstract = {Developing affordable and scalable energy storage solutions are essential to decarbonizing power systems. The conversion of renewable electricity into chemical energy carriers such as ammonia has attracted extensive attention from academia and industry. Many Power-to-Ammonia (PtA) plants have been conceptualized and developed worldwide in recent years. The PtA plant is an integration of multiple electrochemical processes, each with a distinct set of operational constraints and cost structure. One of the problems in the operation of PtA plants is the optimal scheduling of the hydrogen buffer in PtA plants considering the operational characteristics of electrochemical processes and the volatility and uncertainty of electricity prices. In this paper, a two-stage Markov-Decision-Process (MDP) approach is proposed. The computational challenges brought by the infinite optimization horizon and non-concavity of cost functions are resolved. The first stage solution is based on the periodic MDP approach, which captures the periodic structure of electricity prices. The second stage solution gives optimal real-time decisions based on a rolling-horizon MDP approach. Numerical results show that the accurate representations of the cost functions and the optimization horizon using the proposed method are necessary, while the linearization of cost functions and the truncation of the optimization horizon lead to notable deviations from the optimality.}
}
@article{RUIZ201575,
title = {A transformational creativity tool to support chocolate designers},
journal = {Pattern Recognition Letters},
volume = {67},
pages = {75-80},
year = {2015},
note = {Cognitive Systems for Knowledge Discovery},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2015.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0167865515001579},
author = {Francisco J. Ruiz and Cristóbal Raya and Albert Samà and Núria Agell},
keywords = {Cognigtive system, Creativity, Creativity support system},
abstract = {A new formulation of the central ideas of Boden's well-established theory on combinational, exploratory and transformational creativity is presented. This new formulation, based on the idea of conceptual space, redefines some terms and includes several types of concept properties (appropriateness and relevance), whose relationship facilitates the computational implementation of the transformational creativity mechanism. The presented formulation is applied to a real case of chocolate designing in which a novel and flavorful combination of chocolate and fruit is generated. The experimentation was conducted jointly with a Spanish chocolate chef. Experimental results prove the relationship between appropriateness and relevance in different frameworks and show that the formulation presented is not only useful for understanding how the creative mechanisms of design works but also facilitates its implementation in real cases to support creativity processes.}
}
@article{DAVEY2012e139,
title = {Results from a study with Threshold Concepts in two chemical engineering undergraduate courses},
journal = {Education for Chemical Engineers},
volume = {7},
number = {3},
pages = {e139-e152},
year = {2012},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2012.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S1749772812000127},
author = {K.R. Davey},
keywords = {Threshold Concepts, Undergraduate peer presentations, Enhancement of teaching and learning in core chemical engineering, Learning and teaching in higher education},
abstract = {A new study in peer presentation of Threshold Concepts as the focus of learning in two core chemical engineering undergraduate courses has shown that students benefit from an explanatory and illustrative presentation they give to their class peers in place of the traditional lecturer. The methodology was that the lecturer identified a (progressively linked) inventory of Threshold Concepts and had students critically prepare and then explain these in brief (3–5min) presentation-and-question sessions to their cohort. The inventory was informed by Rowbottom's (2007) notion of looking for abilities for which a concept is necessary. The two courses were a level III core course on separations processing with 74 students and a level IV elective in specialist heat transfer with 15 students. Students welcomed and highly valued this type of learning with more than 90% agreeing that it improved understanding of the course material both because it revealed things better than their experiences in lectures and because it promoted a mental organisation of necessary course ideas. It is concluded that peer presentations of Threshold Concepts is a useful and economic instrument to overcoming traditional barriers to student learning. The findings could be readily applied to other courses in distinctive chemical engineering thinking and practise.}
}
@article{CHEN2022100602,
title = {Modern views of machine learning for precision psychiatry},
journal = {Patterns},
volume = {3},
number = {11},
pages = {100602},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100602},
url = {https://www.sciencedirect.com/science/article/pii/S2666389922002276},
author = {Zhe Sage Chen and Prathamesh (Param) Kulkarni and Isaac R. Galatzer-Levy and Benedetta Bigio and Carla Nasca and Yu Zhang},
keywords = {machine learning, ML, artificial intelligence, AI, deep learning, precision psychiatry, digital psychiatry, computational psychiatry, neuroimaging, neurobiomarker, molecular biomarker, digital phenotyping, multi-modal data fusion, neuromodulation, causality, explainable AI, XAI, teletherapy},
abstract = {Summary
In light of the National Institute of Mental Health (NIMH)’s Research Domain Criteria (RDoC), the advent of functional neuroimaging, novel technologies and methods provide new opportunities to develop precise and personalized prognosis and diagnosis of mental disorders. Machine learning (ML) and artificial intelligence (AI) technologies are playing an increasingly critical role in the new era of precision psychiatry. Combining ML/AI with neuromodulation technologies can potentially provide explainable solutions in clinical practice and effective therapeutic treatment. Advanced wearable and mobile technologies also call for the new role of ML/AI for digital phenotyping in mobile mental health. In this review, we provide a comprehensive review of ML methodologies and applications by combining neuroimaging, neuromodulation, and advanced mobile technologies in psychiatry practice. We further review the role of ML in molecular phenotyping and cross-species biomarker identification in precision psychiatry. We also discuss explainable AI (XAI) and neuromodulation in a closed human-in-the-loop manner and highlight the ML potential in multi-media information extraction and multi-modal data fusion. Finally, we discuss conceptual and practical challenges in precision psychiatry and highlight ML opportunities in future research.}
}
@incollection{COPLIEN201425,
title = {Chapter 2 - The DCI Paradigm: Taking Object Orientation into the Architecture World},
editor = {Muhammad {Ali Babar} and Alan W. Brown and Ivan Mistrik},
booktitle = {Agile Software Architecture},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {25-62},
year = {2014},
isbn = {978-0-12-407772-0},
doi = {https://doi.org/10.1016/B978-0-12-407772-0.00002-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780124077720000022},
author = {James O. Coplien and Trygve Reenskaug},
keywords = {use case, mental models, postmodernism, restricted object orientation, full object orientation.},
abstract = {Abstract
We find surprisingly strong parallels in a playful comparison of the progression of thought in the architecture of the built world and its namesake in software. While some architectural progression in both fields owes to fashion, much more of it owes to learning—in both the field of design and collective human endeavor. We have been working on a paradigm called DCI (Data, Context, and Interaction) that places the human experiences of design and use of programs equally at center stage. It brings software design out of the technology-laced modern school of the 1980s into a postmodern era that places human experience at the center. DCI offers a vision of computers and people being mutually alive in the sense of Christopher Alexander’s great design. DCI opens a dialog contrasting metaphors of collective human reasoning and Kay’s vision of object computation, as well as a dialog between the schools of design in the built world and in software.}
}
@article{UCAR2017249,
title = {Managing disruptions in the multi-depot vehicle scheduling problem},
journal = {Transportation Research Part B: Methodological},
volume = {105},
pages = {249-269},
year = {2017},
issn = {0191-2615},
doi = {https://doi.org/10.1016/j.trb.2017.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0191261516305495},
author = {Ezgi Uçar and Ş. {İlker Birbil} and İbrahim Muter},
keywords = {Multi-depot vehicle scheduling, Robust planning, Column-and-row generation},
abstract = {We consider two types of disruptions arising in the multi-depot vehicle scheduling; the delays and the extra trips. These disruptions may or may not occur during operations, and hence they need to be indirectly incorporated into the planned schedule by anticipating their likely occurrence times. We present a unique recovery method to handle these potential disruptions. Our method is based on partially swapping two planned routes in such a way that the effect on the planned schedule is minimal, if these disruptions are actually realized. The mathematical programming model for the multi-depot vehicle scheduling problem, which incorporates these robustness considerations, possesses a special structure. This special structure causes the conventional column generation method fall short as the resulting problem grows also row-wise when columns are generated. We design an exact simultaneous column-and-row generation algorithm to find a valid lower-bound. The novel aspect of this algorithm is the pricing subproblem, which generates pairs of routes that form recovery solutions. Compromising on exactness, we modify this algorithm in order to enable it to solve practical-sized instances efficiently. This heuristic algorithm is shown to provide very tight bounds on the randomly generated instances in a short computation time.}
}
@article{FORRY2013634,
title = {Ready or not: Associations between participation in subsidized child care arrangements, pre-kindergarten, and Head Start and children’s school readiness},
journal = {Early Childhood Research Quarterly},
volume = {28},
number = {3},
pages = {634-644},
year = {2013},
issn = {0885-2006},
doi = {https://doi.org/10.1016/j.ecresq.2013.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S0885200613000367},
author = {Nicole D. Forry and Elizabeth E. Davis and Kate Welti},
keywords = {Low-income, School readiness, Pre-kindergarten, Head Start, Child care subsidies},
abstract = {Research has found disparities in young children’s development across income groups. A positive association between high-quality early care and education and the school readiness of children in low-income families has also been demonstrated. This study uses linked administrative data from Maryland to examine the variations in school readiness associated with different types of subsidized child care, and with dual enrollment in subsidized child care and state pre-kindergarten or Head Start. Using multivariate methods, we analyze linked subsidy administrative data and portfolio-based kindergarten school readiness assessment data to estimate the probability of children’s school readiness in three domains: personal and social development, language and literacy, and mathematical thinking. Compared to children in subsidized family child care or informal care, those in subsidized center care are more likely to be rated as fully ready to learn on the two pre-academic domains. Regardless of type of subsidized care used, enrollment in pre-kindergarten, but not Head Start, during the year prior to kindergarten is strongly associated with being academically ready for kindergarten. No statistically significant associations are found between type of subsidized care, pre-kindergarten enrollment, or Head Start and assessments of children’s personal/social development.}
}
@article{LACHANCE2001503,
title = {Helping students build a path of understanding from ratio and proportion to decimal notation},
journal = {The Journal of Mathematical Behavior},
volume = {20},
number = {4},
pages = {503-526},
year = {2001},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(02)00087-1},
url = {https://www.sciencedirect.com/science/article/pii/S0732312302000871},
author = {Andrea Lachance and Jere Confrey},
keywords = {Decimal notation, Ratio and proportion, Multiplicative thinking},
abstract = {Various studies have shown that students of all levels struggle to understand decimal numbers. This paper discusses a novel approach to increasing students’ conceptual understanding of decimal numbers. Rather than approach decimal notation as a discrete and separate mathematical topic, this approach enables students to work with contextual problems to gain a solid understanding of ratio and proportion. Using their understanding of ratio and proportion as a foundation, students can then build connected and related understandings of fractions, decimals and percents. The study discussed in this paper illustrates that grounding decimal instruction in the broader context of ratio can help students gain deeper conceptual understandings of decimal notation as well as fractions and percents.}
}
@article{DERBEL2014731,
title = {Distributed localized bi-objective search},
journal = {European Journal of Operational Research},
volume = {239},
number = {3},
pages = {731-743},
year = {2014},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2014.05.040},
url = {https://www.sciencedirect.com/science/article/pii/S0377221714004639},
author = {Bilel Derbel and Jérémie Humeau and Arnaud Liefooghe and Sébastien Verel},
keywords = {Multiple objective programming, Combinatorial optimization, Parallel and distributed computing, Evolutionary computation},
abstract = {We propose a new distributed heuristic for approximating the Pareto set of bi-objective optimization problems. Our approach is at the crossroads of parallel cooperative computation, objective space decomposition, and adaptive search. Given a number of computing nodes, we self-coordinate them locally, in order to cooperatively search different regions of the Pareto front. This offers a trade-off between a fully independent approach, where each node would operate independently of the others, and a fully centralized approach, where a global knowledge of the entire population is required at every step. More specifically, the population of solutions is structured and mapped into computing nodes. As local information, every node uses only the positions of its neighbors in the objective space and evolves its local solution based on what we term a ‘localized fitness function’. This has the effect of making the distributed search evolve, over all nodes, to a high quality approximation set, with minimum communications. We deploy our distributed algorithm using a computer cluster of hundreds of cores and study its properties and performance on ρMNK-landscapes. Through extensive large-scale experiments, our approach is shown to be very effective in terms of approximation quality, computational time and scalability.}
}
@article{COHEN2013620,
title = {Autoantibody repertoires, natural biomarkers, and system controllers},
journal = {Trends in Immunology},
volume = {34},
number = {12},
pages = {620-625},
year = {2013},
issn = {1471-4906},
doi = {https://doi.org/10.1016/j.it.2013.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S147149061300080X},
author = {Irun R. Cohen},
abstract = {The immune system is composed of networks of interacting cells and molecules; therefore, to understand and control immune behavior we need to adopt the thinking and tools of systems immunology. This review describes the use of an antigen microarray device and informatics to profile the repertoires of autoantibodies in health and disease. Autoantibody profiling provides an insight into the biomarkers used by the immune system in its dialog with the body. Heat shock protein 60 (HSP60) and HSP70 are cited as examples of key hubs in physiological regulatory networks; HSP molecules and peptides can be viewed as natural regulators because the immune system itself deploys them to modulate inflammatory reactions. The discovery of such natural biomarkers paves the way towards natural control.}
}
@incollection{HERNANDEZGARCIA2021307,
title = {Chapter 16 - Smart and informal? Self-organization and everyday},
editor = {Alessandro Aurigi and Nancy Odendaal},
booktitle = {Shaping Smart for Better Cities},
publisher = {Academic Press},
pages = {307-319},
year = {2021},
isbn = {978-0-12-818636-7},
doi = {https://doi.org/10.1016/B978-0-12-818636-7.00012-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186367000123},
author = {Jaime Hernández-García and Iliana Hernández-García},
keywords = {Smart cities, Smart technologies, Informal settlements, Self-organization, Everyday, Complexity},
abstract = {With the aim to offer an alternative understanding of smart cities, this chapter explores the relationship between smart and informal characteristics, presenting a discussion of two concepts arguably found in both smart and informal types of urban development: self-organization and the everyday. For this purpose, this chapter discusses the social and spatial production of informal settlements—how these areas show high degrees of self-organization based on everyday actions and interactions. In line with Rauws (2016), observers can see smart cities as networks of knowledge, actions, and selection of choices; yet this view also aligns with the actions informal settlers in Latin America take to produce their own living environments via self-organization and everyday practices. The chapter suggests how smart technologies can utilize computational logics to help measure and interpret these self-organized systems, as well as help decipher everyday creativity, based on uncertainty, autonomy, and freedom. An urban area may possess no formal planning processes, yet residents’ bottom-up social and spatial initiatives give shape to their settlements and to the city. In this sense the use of smart technologies can bring heightened understandings to informality; therefore not only the smart but also the informal can undergo reconceptualizing. We suggest viewing the smart and the informal as collective and adaptive self-organized systems fuelled by everyday practices where the social emerges as everyday creativity.}
}
@article{SHAWKY2023103476,
title = {Blockchain-based secret key extraction for efficient and secure authentication in VANETs},
journal = {Journal of Information Security and Applications},
volume = {74},
pages = {103476},
year = {2023},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.jisa.2023.103476},
url = {https://www.sciencedirect.com/science/article/pii/S2214212623000601},
author = {Mahmoud A. Shawky and Muhammad Usman and David Flynn and Muhammad Ali Imran and Qammer H. Abbasi and Shuja Ansari and Ahmad Taha},
keywords = {AVISPA simulation, BAN-logic, Key reconciliation, Public key infrastructure, Secret key extraction, Smart contracts-based blockchain},
abstract = {Intelligent transportation systems are an emerging technology that facilitates real-time vehicle-to-everything communication. Hence, securing and authenticating data packets for intra- and inter-vehicle communication are fundamental security services in vehicular ad-hoc networks (VANETs). However, public-key cryptography (PKC) is commonly used in signature-based authentication, which consumes significant computation resources and communication bandwidth for signatures generation and verification, and key distribution. Therefore, physical layer-based secret key extraction has emerged as an effective candidate for key agreement, exploiting the randomness and reciprocity features of wireless channels. However, the imperfect channel reciprocity generates discrepancies in the extracted key, and existing reconciliation algorithms suffer from significant communication costs and security issues. In this paper, PKC-based authentication is used for initial legitimacy detection and exchanging authenticated probing packets. Accordingly, we propose a blockchain-based reconciliation technique that allows the trusted third party (TTP) to publish the correction sequence of the mismatched bits through a transaction using a smart contract. The smart contract functions enable the TTP to map the transaction address to vehicle-related information and allow vehicles to obtain the transaction contents securely. The obtained shared key is then used for symmetric key cryptography (SKC)-based authentication for subsequent transmissions, saving significant computation and communication costs. The correctness and security robustness of the scheme are proved using Burrows–Abadi–Needham (BAN)-logic and Automated Validation of Internet Security Protocols and Applications (AVISPA) simulator. We also discussed the scheme’s resistance to typical attacks. The scheme’s performance in terms of packet delay and loss ratio is evaluated using the network simulator (OMNeT++). Finally, the computation analysis shows that the scheme saves ∼99% of the time required to verify 1000 messages compared to existing PKC-based schemes.}
}
@article{AGRAWAL20241,
title = {A systematic review on metaheuristic approaches for autonomous path planning of unmanned aerial vehicles},
journal = {Drone Systems and Applications},
volume = {12},
pages = {1-28},
year = {2024},
issn = {2564-4939},
doi = {https://doi.org/10.1139/dsa-2023-0093},
url = {https://www.sciencedirect.com/science/article/pii/S2564493924000158},
author = {Sameer Agrawal and Bhumeshwar K. Patle and Sudarshan Sanap},
keywords = {artificial intelligence, path planning, metaheuristic algorithms, UAV, mobile robot navigation},
abstract = {In the path planning of UAVs, autonomous decision-making and control are challenging tasks in the uncertain 3D environment consisting of static and dynamic obstacles. Hence, the selection of appropriate path-planning approaches is essential. In the proposed work, we have considered the meta-heuristic approaches only for an in-depth review. Metaheuristic approaches have been remarkably known for solving complex problems, optimal solutions, and lesser computational complexity compared to deterministic approaches that produce an inefficient solution. An in-depth review has been made by considering the approaches used for path planning, their advantages, disadvantages, applications, the type of time domain (offline or online), type of environment (simulation or real time), hybridization with other approaches, single or multiple UAV system, and obstacle handled (static or dynamic). It is observed that current meta-heuristic methods face constraints like inadequate convergence rates, entrapment in local optima, and complex operations, necessitating continuous development of novel approaches. Implementation of path-planning approaches are very much limited to simulation study over experimental analysis. Hybrid algorithms emerge as a potential solution for tackling these hurdles and optimizing UAV navigation, particularly in dynamic environments involving multiple UAVs. The paper highlights key research gaps, trends, along with prospects in the field of research.}
}
@article{BERTO2024101278,
title = {A motivational-based learning model for mobile robots},
journal = {Cognitive Systems Research},
volume = {88},
pages = {101278},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101278},
url = {https://www.sciencedirect.com/science/article/pii/S138904172400072X},
author = {Letícia Berto and Paula Costa and Alexandre Simões and Ricardo Gudwin and Esther Colombini},
keywords = {Motivation, Action selection and planning, Models of internal states, Internal reinforces},
abstract = {Humans have needs motivating their behavior according to intensity and context. However, we also create preferences associated with each action’s perceived pleasure, which is susceptible to changes over time. This makes decision-making more complex, requiring learning to balance needs and preferences according to the context. To understand how this process works and enable the development of robots with a motivational-based learning model, we computationally model a motivation theory proposed by Hull. In this model, the agent (an abstraction of a mobile robot) is motivated to keep itself in a state of homeostasis. We introduced hedonic dimensions to explore the impact of preferences on decision-making and employed reinforcement learning to train our motivated-based agents. In our experiments, we deploy three agents with distinct energy decay rates, simulating different metabolic rates, within two diverse environments. We investigate the influence of these conditions on their strategies, movement patterns, and overall behavior. The findings reveal that agents excel at learning more effective strategies when the environment allows for choices that align with their metabolic requirements. Furthermore, we observe that incorporating pleasure as a component of the motivational mechanism affects behavior learning, particularly for agents with regular metabolisms depending on the environment. Our study also unveils that, when confronted with survival challenges, agents prioritize immediate needs over pleasure and equilibrium. These insights shed light on how robotic agents can adapt and make informed decisions in demanding scenarios, demonstrating the intricate interplay between motivation, pleasure, and environmental context in autonomous systems.}
}
@article{TYFLOPOULOS2019979,
title = {Messing with boundaries - quantifying the potential loss by pre-set parameters in topology optimization},
journal = {Procedia CIRP},
volume = {84},
pages = {979-985},
year = {2019},
note = {29th CIRP Design Conference 2019, 08-10 May 2019, Póvoa de Varzim, Portgal},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.04.307},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119309552},
author = {Evangelos Tyflopoulos and Martin Steinert},
keywords = {topology optimization, SIMP, additive manufacturing, product development, design, finite element analysis},
abstract = {Additive manufacturing can increase the flexibility in the design phase of product development and that, in its turn, has changed the designer’s way of thinking. The design problem has reformulated; from designs that were not possible to be constructed, due to lack of equipment and technology, to constructions that the designer could not think to design. Topology optimization and generative design are useful tools in the hands of designer that can help him/her in the pursuit of the global optimum of a construction and in the choice of an alternative design solution respectively. However, topology optimization results are always depended on the given boundary conditions and restrictions. In other words, the designer’s decisions can affect the results of topology optimization and can easily lead to a local and not a global solution. In this paper, an identification and categorization of the most important parameters, that can affect the topology optimization results, were conducted. The main focus of the implemented research was on the pre-processing of topology optimization and especially on the designer’s decisions. The applied topology optimization approach here was a simple compliance optimization based on the SIMP interpolation methodology (Solid Isotropic Material with Penalization) and it was executed with the use of the commercial software Tosca (Abaqus). Different alternative designs of a wall bracket were used as a case study to test the sensitivity of the optimization algorithm and quantify the potential loss.}
}
@article{MARIA2007695,
title = {Emotional agents: A modeling and an application},
journal = {Information and Software Technology},
volume = {49},
number = {7},
pages = {695-716},
year = {2007},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2006.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0950584906001030},
author = {Khulood Abu Maria and Raed Abu Zitar},
keywords = {Agent, Emotions, Behavior, Personality},
abstract = {This paper proposes modeling of artificial emotions through agents based on symbolic approach. The symbolic approach utilizes symbolic emotional rule-based systems (rule base that generated emotions) with continuous interactions with environment and an internal “thinking” machinery that comes as a result of series of inferences, evaluation, evolution processes, adaptation, learning, and emotions. We build two models for agent based systems; one is supported with artificial emotions and the other one without emotions. We use both in solving a bench mark problem; “The Orphanage Care Problem”. The two systems are simulated and results are compared. Our study shows that systems with proper model of emotions can perform in many cases better than systems without emotions. We try to shed the light here on how artificial emotions can be modeled in a simple rule-based agent systems and if emotions as they exist in “real intelligence” can be helpful for “artificial intelligence”. Agent architectures are presented as a generic blueprint on which the design of agents can be based. Our focus is on the functional design, including flow of information and control. With this information provided, the generic blueprints of architectures should not be difficult to implement agents, thus putting these theoretical models into practice. We build the agents using this architecture, and many experiments and analysis are shown.}
}
@article{RANTANEN20153612,
title = {The Future of Pharmaceutical Manufacturing Sciences},
journal = {Journal of Pharmaceutical Sciences},
volume = {104},
number = {11},
pages = {3612-3638},
year = {2015},
issn = {0022-3549},
doi = {https://doi.org/10.1002/jps.24594},
url = {https://www.sciencedirect.com/science/article/pii/S0022354916301514},
author = {Jukka Rantanen and Johannes Khinast},
keywords = {quality by design (QBD), process analytical technology (PAT), mathematical model, materials science,  modeling},
abstract = {ABSTRACT
The entire pharmaceutical sector is in an urgent need of both innovative technological solutions and fundamental scientific work, enabling the production of highly engineered drug products. Commercial-scale manufacturing of complex drug delivery systems (DDSs) using the existing technologies is challenging. This review covers important elements of manufacturing sciences, beginning with risk management strategies and design of experiments (DoE) techniques. Experimental techniques should, where possible, be supported by computational approaches. With that regard, state-of-art mechanistic process modeling techniques are described in detail. Implementation of materials science tools paves the way to molecular-based processing of future DDSs. A snapshot of some of the existing tools is presented. Additionally, general engineering principles are discussed covering process measurement and process control solutions. Last part of the review addresses future manufacturing solutions, covering continuous processing and, specifically, hot-melt processing and printing-based technologies. Finally, challenges related to implementing these technologies as a part of future health care systems are discussed.}
}
@article{GALLAGHER2023100127,
title = {Navigating the uncertainty of precision cancer screening: The role of shared decision-making},
journal = {PEC Innovation},
volume = {2},
pages = {100127},
year = {2023},
issn = {2772-6282},
doi = {https://doi.org/10.1016/j.pecinn.2023.100127},
url = {https://www.sciencedirect.com/science/article/pii/S2772628223000079},
author = {Joseph H. Gallagher and Jason L. Vassy and Marla L. Clayman},
keywords = {Shared decision making, Polygenic risk scores, Cancer screening, Genetic counseling, Genomic testing, Patient-provider communication},
abstract = {Objective
Describe how applying a shared decision making (SDM) lens to the implementation of new technologies can improve patient-centeredness.
Methods
This paper argues that the emergence of polygenic risk scores (PRS) for cancer screening presents an illustrative opportunity to include SDM when novel technologies enter clinical care.
Results
PRS are novel tools that indicate an individual’s genetic risk of a given disease relative to the population. PRS are anticipated to help identify individuals most and least likely to benefit from screening. However, PRS have several types of uncertainty, including validity across populations, disparate computational methods, and inclusion of different genomic data across laboratories.
Conclusion
Implementing SDM alongside new technologies could prove useful for their ethical and patient-centered utilization. SDM’s importance as an approach to decision-making will not diminish, as evidence, uncertainty, and patient values will remain intrinsic to the art and science of clinical care.
Innovation
SDM can help providers and patients navigate the considerable uncertainty inherent in implementing new technologies, enabling decision-making based on existing evidence and patient values.}
}
@article{KHADE20214955,
title = {hdANM: a new comprehensive dynamics model for protein hinges},
journal = {Biophysical Journal},
volume = {120},
number = {22},
pages = {4955-4965},
year = {2021},
issn = {0006-3495},
doi = {https://doi.org/10.1016/j.bpj.2021.10.017},
url = {https://www.sciencedirect.com/science/article/pii/S0006349521008766},
author = {Pranav M. Khade and Domenico Scaramozzino and Ambuj Kumar and Giuseppe Lacidogna and Alberto Carpinteri and Robert L. Jernigan},
abstract = {Hinge motions are essential for many protein functions, and their dynamics are important to understand underlying biological mechanisms. The ways that these motions are represented by various computational methods differ significantly. By focusing on a specific class of motion, we have developed a new hinge-domain anisotropic network model (hdANM) that is based on the prior identification of flexible hinges and rigid domains in the protein structure and the subsequent generation of global hinge motions. This yields a set of motions in which the relative translations and rotations of the rigid domains are modulated and controlled by the deformation of the flexible hinges, leading to a more restricted, specific view of these motions. hdANM is the first model, to our knowledge, that combines information about protein hinges and domains to model the characteristic hinge motions of a protein. The motions predicted with this new elastic network model provide important conceptual advantages for understanding the underlying biological mechanisms. As a matter of fact, the generated hinge movements are found to resemble the expected mechanisms required for the biological functions of diverse proteins. Another advantage of this model is that the domain-level coarse graining makes it significantly more computationally efficient, enabling the generation of hinge motions within even the largest molecular assemblies, such as those from cryo-electron microscopy. hdANM is also comprehensive as it can perform in the same way as the well-known protein dynamics models (anisotropic network model, rotations-translations of blocks, and nonlinear rigid block normal mode analysis), depending on the definition of flexible and rigid parts in the protein structure and on whether the motions are extrapolated in a linear or nonlinear fashion. Furthermore, our results indicate that hdANM produces more realistic motions as compared to the anisotropic network model. hdANM is an open-source software, freely available, and hosted on a user-friendly website.}
}
@article{HUSSAIN2023135948,
title = {Efficient synthesis of nicotinaldehyde-based crystalline organic derivatives: Comparative analysis between experimental and DFT study},
journal = {Journal of Molecular Structure},
volume = {1290},
pages = {135948},
year = {2023},
issn = {0022-2860},
doi = {https://doi.org/10.1016/j.molstruc.2023.135948},
url = {https://www.sciencedirect.com/science/article/pii/S0022286023010426},
author = {Shahid Hussain and Muhammad Adeel and Muhammad Khalid and Ume Aiman and Alexander Villinger and Ataualpa A.C. Braga and Saad M. Alshehri and Muhammad {Adnan Asghar}},
keywords = {Pyridine, Nicotinaldehyde, NBO analysis, Density functional theory, MEP},
abstract = {The analogues of pyridine ring structures demonstrate various physiological as well as biological activities. The current research is based on experimental along with computational investigations of two new phenyl substituted nicotinaldehyde derivatives; 2-(2,4-difluorophenyl)pyridine-3-carbaldehyde (DFPPC) and 2-(2,5-dichlorophenyl)pyridine-3-carbaldehyde (DCPPC) . For structural optimization of DFPPC as well as DCPPC and to explore nonlinear optical properties, computational quantum chemical analysis was executed via density functional theory (DFT) calculations by employing M06 level with 6–311G(d,p) basis set. A consensus among theoretical (DFT) and experimental (SC-XRD) results was observed by the calculation of geometric parameters. Molecular electrostatic potential (MEP), natural bond orbital (NBO) analysis, natural population analysis (NPA), nonlinear optical (NLO), global reactivity parameters (GRPs), and frontier molecular orbital (FMO) exploration were carried out at M06/6–311G(d,p), to comprehend hyper-conjugative interactions, electron density, electronic communications and oscillation strength. The HOMO/LUMO energy gap of DCPPC (5.108 eV) was observed to be lower than DFPPC i.e., 5.170 eV, which resulted in its higher value of global softness (0.196 Eh) along with lower global hardness (2.554 Eh) value than DFPPC. The NLO attributes of DFPPC as well as DCPPC was calculated by evaluating the total dipole moment (μtot), average linear polarizability ⟨α⟩ and second hyperpolarizability (γtot) at aforementioned level. From the NLO results, it was observed that DCPPC exhibits a higher average linear polarizability value such as 3.0772 × 10−23 esu than DFPPC i.e., 2.6116 × 10−23 esu . Whereas, higher results of γtot were observed for DFPPC i.e., 3.2455 × 10−35 than DCPPC (3.0708 × 10−35 esu). The distinguished NLO characteristics revealed that, both the chromophores (DFPPC and DCPPC) can be recognized as highly efficient NLO materials for future applications.}
}
@article{MENEGHETTI2021101614,
title = {Learning from navigation, and tasks assessing its accuracy: The role of visuospatial abilities and wayfinding inclinations},
journal = {Journal of Environmental Psychology},
volume = {75},
pages = {101614},
year = {2021},
issn = {0272-4944},
doi = {https://doi.org/10.1016/j.jenvp.2021.101614},
url = {https://www.sciencedirect.com/science/article/pii/S0272494421000670},
author = {Chiara Meneghetti and Laura Miola and Enrico Toffalini and Massimiliano Pastore and Francesca Pazzaglia},
keywords = {Navigation, Route retracing, Shortcut, Landmark locating, Visuospatial abilities, Wayfinding inclinations, Informed priors},
abstract = {How individual differences in visuospatial thinking relate to environment learning from navigation is of growing interest and needs to be approached systematically. Here, a sample of 292 undergraduates learnt a virtual path (desktop-based), and their learning accuracy was assessed with recall tasks, i.e. route retracing, shortcut finding and landmark locating tasks. Several individual visuospatial measures, tasks and questionnaires, were administered. Relations between individual measures and recall tasks were estimated with regression models taking quantitative evidence available in the literature into account, and treated as Bayesian informed priors established by a meta-analysis. The results provide robust evidence of visuospatial abilities and wayfinding inclinations (composing two distinct factors) both affecting recall task performance, particularly the former. A different contribution of individual measures as a function of recall task is envisaged. This study offers new insight on the role of individual visuospatial measures in environment learning (navigation-like) and how they are related.}
}
@article{HERTZ2025105993,
title = {Beyond the matrix: Experimental approaches to studying cognitive agents in social-ecological systems},
journal = {Cognition},
volume = {254},
pages = {105993},
year = {2025},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2024.105993},
url = {https://www.sciencedirect.com/science/article/pii/S0010027724002798},
author = {Uri Hertz and Raphael Köster and Marco A. Janssen and Joel Z. Leibo},
abstract = {Studying social-ecological systems, in which agents interact with each other and their environment are important both for sustainability applications and for understanding how human cognition functions in context. In such systems, the environment shapes the agents' experience and actions, and in turn collective action of agents changes social and physical aspects of the environment. Here we review current investigation approaches, which rely on a lean design, with discrete actions and outcomes and little scope for varying environmental parameters and cognitive demands. We then introduce a multiagent reinforcement learning (MARL) approach, which builds on modern artificial intelligence techniques, and provides new avenues to model complex social worlds, while preserving more of their characteristics, and allowing them to capture a variety of social phenomena. These techniques can be fed back to the laboratory where they make it easier to design experiments in complex social situations without compromising their tractability for computational modeling. We showcase the potential MARL by discussing several recent studies that have used it, detailing the way environmental settings and cognitive constraints can lead to the emergence of complex cooperation strategies. This novel approach can help researchers bring together insights from human cognition, sustainability, and AI, to tackle real world problems of social-ecological systems.}
}
@article{PLANT20173335,
title = {Can a systems approach produce a better understanding of mood disorders?},
journal = {Biochimica et Biophysica Acta (BBA) - General Subjects},
volume = {1861},
number = {1, Part A},
pages = {3335-3344},
year = {2017},
issn = {0304-4165},
doi = {https://doi.org/10.1016/j.bbagen.2016.08.016},
url = {https://www.sciencedirect.com/science/article/pii/S0304416516303051},
author = {Nick Plant},
keywords = {Affective disorder, Bipolar disorder, Computational biology, Drug development, Systems biology},
abstract = {Background
One in twenty-five people suffer from a mood disorder. Current treatments are sub-optimal with poor patient response and uncertain modes-of-action. There is thus a need to better understand underlying mechanisms that determine mood, and how these go wrong in affective disorders. Systems biology approaches have yielded important biological discoveries for other complex diseases such as cancer, and their potential in affective disorders will be reviewed.
Scope of review
This review will provide a general background to affective disorders, plus an outline of experimental and computational systems biology. The current application of these approaches in understanding affective disorders will be considered, and future recommendations made.
Major conclusions
Experimental systems biology has been applied to the study of affective disorders, especially at the genome and transcriptomic levels. However, data generation has been slowed by a lack of human tissue or suitable animal models. At present, computational systems biology has only be applied to understanding affective disorders on a few occasions. These studies provide sufficient novel biological insight to motivate further use of computational biology in this field.
General significance
In common with many complex diseases much time and money has been spent on the generation of large-scale experimental datasets. The next step is to use the emerging computational approaches, predominantly developed in the field of oncology, to leverage the most biological insight from these datasets. This will lead to the critical breakthroughs required for more effective diagnosis, stratification and treatment of affective disorders.}
}
@article{CHEN2020103670,
title = {A visual learning analytics (VLA) approach to video-based teacher professional development: Impact on teachers’ beliefs, self-efficacy, and classroom talk practice},
journal = {Computers & Education},
volume = {144},
pages = {103670},
year = {2020},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2019.103670},
url = {https://www.sciencedirect.com/science/article/pii/S0360131519302234},
author = {Gaowei Chen},
keywords = {Teacher professional development, Data science applications in education, Improving classroom teaching, Pedagogical issues},
abstract = {To address the challenge of overwhelming data inherent in classroom lesson videos, this study proposed a visual learning analytics (VLA) approach to video-based teacher professional development (TPD). Using a two-year experimental design, 46 secondary mathematics teachers were divided randomly into a treatment group (N = 24) and a control group (N = 22) to learn about and integrate academically productive talk into their teaching. The treatment teachers participated in a VLA-supported TPD program, while the control teachers participated in conventional knowledge-based workshops. Results show that teachers in the treatment group had more positive beliefs and higher self-efficacy in the post-test and delayed-post-test, while the control group improved, but not significantly, in their beliefs about the usefulness of classroom talk. In addition, although the control group made a significant improvement in their self-efficacy in guiding classroom talk in the post-test, this improvement was not sustained to the delayed post-test. Moreover, the coding of classroom teaching behaviour revealed that teachers in the treatment group relative to the control group significantly increased their use of academically productive talk in the post-test lessons to encourage the students' elaboration, reasoning, and thinking with others in the classroom. The results suggest that, while attending knowledge-based workshops had, to some degree, positive effects on the control teachers' beliefs and self-efficacy, these effects were not sustainable over time. In contrast, the use of visual learning analytics to support the treatment group's reflection on the classroom data not only had significant and sustained effects on the teachers' beliefs and self-efficacy but also significantly influenced their actual classroom teaching behaviour. Implications for designing VLA to support teacher learning and professional development are discussed.}
}
@article{TRANVAN2025101353,
title = {What is the biggest motivator for 10 th-graders when learning programming? Case study in Vietnam},
journal = {Social Sciences & Humanities Open},
volume = {11},
pages = {101353},
year = {2025},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2025.101353},
url = {https://www.sciencedirect.com/science/article/pii/S2590291125000804},
author = {Hung {Tran Van} and Hoa Nguyen Thi and Kiet Tran {Ly Anh} and Kiet Nguyen {Ba Tuan}},
keywords = {Learning motivation, Impact, Learning programming, 10th grade students, Learning environment},
abstract = {Motivation is one of the important elements in the learning process, particularly for complex subjects like programming, which demand consistent engagement and strong problem-solving abilities. Grasping the nuances of student motivation is essential for educational researchers and practitioners dedicated to crafting more effective and engaging learning environments. This study examines four pivotal factors influencing 10th-grade students' motivation to learn programming in Da Nang, Vietnam: family influence, teacher influence, learning environment, and self-influence. By exploring these dimensions, we aim to propose actionable strategies that educators and parents can implement to foster a supportive and motivating educational atmosphere. Our analysis, conducted using SPSS24 and Structural Equation Modeling (SEM), highlighted that self-influence, family influence, learning environment, and teaching influence significantly drive student motivation. Key metrics include a Kaiser-Meyer-Olkin (KMO) value of .803, Cronbach's Alpha values exceeding 0.6, a Comparative Fit Index (CFI) of 0.946, Tucker-Lewis Index (TLI) of 0.932, and a Root Mean Square Error of Approximation (RMSEA) of 0.056. Additionally, ANOVA results with a significant value of .000 < .005 confirms that the independent variables positively impact students' motivation. These findings are valuable for educators, especially those involved in programming instruction at both high school and university levels.}
}
@article{KHOURY2024102470,
title = {Compassion questionnaire for animals: Scale development and validation},
journal = {Journal of Environmental Psychology},
volume = {100},
pages = {102470},
year = {2024},
issn = {0272-4944},
doi = {https://doi.org/10.1016/j.jenvp.2024.102470},
url = {https://www.sciencedirect.com/science/article/pii/S0272494424002433},
author = {Bassam Khoury and Rodrigo C. Vergara},
keywords = {Compassion, Animal, Nature, Scale, Questionnaire},
abstract = {Objectives
No measure of compassion for animals exists. Previous scales measured empathy or attitudes towards animals. In line with previous compassion questionnaires for self (CQS) and others (CQO), the proposed Compassion Questionnaire for Animals (CQA) aims to operationalize compassion for animals by grounding it in affective, cognitive, behavioral, and interrelatedness dimensions, each representing a set of skills that can be cultivated through training and practice.
Methods
Based on the proposed theoretical approach, the CQA items were developed through consultations with a panel of eight graduate students. A large study was conducted to validate the CQA, investigate the relationship between empathy/compassion for other human beings and compassion for animals, and test the role of gender and age in compassion for animals.
Results
Results suggested the presence of three dimensions along with a global latent variable. Psychometric characteristics of the CQA and its subscales were robust. These findings were additionally supported by convergent and discriminate evidence; as such, the CQA presented strong associations with measures of empathy for animals and nature relatedness. In addition, empathy and compassion for other human beings and for animals were found to be moderately associated. Gender and age were found to be related to compassion for animals, with women and older individuals displaying higher levels of compassion.
Conclusions
The CQA is the first scale that operationalizes compassion for animals as a set of affective, cognitive, behavioral, and interrelatedness skills/abilities with important theoretical and practical implications. Limitations as well as theoretical and practical implications of the CQA are thoroughly discussed.}
}
@article{VAZIRIZADE2017230,
title = {Seismic reliability assessment of structures using artificial neural network},
journal = {Journal of Building Engineering},
volume = {11},
pages = {230-235},
year = {2017},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2017.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S2352710216303163},
author = {Sayyed Mohsen Vazirizade and Saeed Nozhati and Mostafa Allameh Zadeh},
keywords = {Seismic reliability, Artificial neural network, Monte Carlo Simulation, Failure probability},
abstract = {Localization and quantification of structural damage and estimating the failure probability are key outputs in the reliability assessment of structures. In this study, an Artificial Neural Network (ANN) is used to reduce the computational effort required for reliability analysis and damage detection. Toward this end, one demonstrative structure is modeled and then several damage scenarios are defined. These scenarios are considered as training data sets for establishing an ANN model. In this regard, the relationship between structural response (input) and structural stiffness (output) is established using ANN models. The established ANN is more economical and achieves reasonable accuracy in detection of structural damage under a set of ground motions. Furthermore, in order to assess the reliability of a structure, five random variables are considered. These are columns’ area of the first, second, and third floor, elasticity modulus, and gravity loads. The ANN is trained by suing the Monte Carlo Simulation (MCS) technique. Finally, the trained neural network specifies the failure probability of the proposed structure. Although MCS can predict the failure probability for a given structure, the ANN model helps simulation techniques to receive an acceptable accuracy and reduce computational effort.}
}
@incollection{ZANNI202057,
title = {Chapter 4 - Life cycle sustainability assessment: An ongoing journey},
editor = {Jingzheng Ren and Sara Toniolo},
booktitle = {Life Cycle Sustainability Assessment for Decision-Making},
publisher = {Elsevier},
pages = {57-93},
year = {2020},
isbn = {978-0-12-818355-7},
doi = {https://doi.org/10.1016/B978-0-12-818355-7.00004-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012818355700004X},
author = {Sara Zanni and Eric Awere and Alessandra Bonoli},
keywords = {Life cycle sustainability assessment, Integrated sustainability assessment, Sustainable development, Review, Standardization, Case studies},
abstract = {As the call for sustainable solutions at operational, industrial, and policy level increases, the need for a comprehensive assessment tool has been addressed by literature and practitioners. In particular, for the definition of a complete framework, the application of a life cycle thinking lens is required to explore the longitudinal dimension of the impacts and possible indirect effects triggered on environmental, social, and economic levels. The definition of an integrated life cycle sustainability assessment framework is currently an ongoing journey, which is summarized in the present chapters. The narrative follows a set of milestones, namely the definition of the concept and the preliminary scheme in the early years, the pathway towards the implementation of a standardized set of tools, an anthology of significant case studies in different sectors, and an overview of the challenges identified by literature and yet remaining open for future researches.}
}
@article{BANERJEE2025100511,
title = {Challenges and opportunities in the industrial usage controller synthesis tools: A review of LTL-based opensource tools for automated control design},
journal = {Results in Control and Optimization},
volume = {18},
pages = {100511},
year = {2025},
issn = {2666-7207},
doi = {https://doi.org/10.1016/j.rico.2024.100511},
url = {https://www.sciencedirect.com/science/article/pii/S2666720724001401},
author = {Amar Banerjee and Venkatesh Choppella},
keywords = {Controller synthesis, Industrial control, LTL, Tool study},
abstract = {Controller synthesis is pivotal in automating control system design from formal specifications and enhancing industrial system verification and optimization processes. This paper critically evaluates LTL-based controller synthesis, highlighting significant gaps in tool support that hinder its widespread adoption in the industry. Despite substantial theoretical progress, an apparent disparity persists between academic research outcomes and the robust, practical tools demanded by industry. Through a comprehensive evaluation, this study reveals mismatches between industrial requirements and the capabilities of current open-source tools. The findings emphasize underexplored challenges and propose future research directions and strategies for practical integration. This work aims to bridge the gap by advocating for enhanced tool support, enabling solutions that align with industrial standards and fostering the broader application of controller synthesis across various sectors.}
}
@article{ADLER20258,
title = {Cancer cell populations},
journal = {Seminars in Cancer Biology},
volume = {109},
pages = {8-9},
year = {2025},
issn = {1044-579X},
doi = {https://doi.org/10.1016/j.semcancer.2024.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S1044579X24000981},
author = {Frederick R. Adler and Herbert Levine and Amy Brock}
}
@article{KE2024111909,
title = {Improving the transferability of adversarial examples through neighborhood attribution},
journal = {Knowledge-Based Systems},
volume = {296},
pages = {111909},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111909},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124005434},
author = {Wuping Ke and Desheng Zheng and Xiaoyu Li and Yuanhang He and Tianyu Li and Fan Min},
keywords = {Adversarial examples, Computer vision, Neural networks, AI security},
abstract = {Adversarial examples, which add carefully planned perturbations to images, pose a serious threat to neural network applications. Transferable adversarial attacks, in which adversarial examples generated on the source model can successfully attack the target model, provide a realistic and undetectable method. Existing transfer-based attacks tend to improve the transferability of adversarial examples by destroying their intrinsic features. They destabilized features differentially by assessing their importance, thus rendering the model incapable of inference. However, the existing methods generate feature-importance assessments that are overly dependent on the source model, leading to inaccurate importance guidance and insufficient feature destruction. In this paper, we propose neighborhood expectancy attribution attacks (NEAA) that accurately guide the destruction of deep features, leading to highly transferable adversarial examples. First, we design a highly versatile attribution tool called neighborhood attribution to represent the importance of features that attribute highly similar results to various source models. Specifically, we discard the imputation of a single baseline and adopt the imputed expectation of a baseline within the neighborhood of the image. Subsequently, we generalize the neighborhood attribution to the middle layer of the model and simplify the computation by assuming linear independence. Finally, the attribution result guides the attack to destroy the intrinsic features of the image and obtain highly transferable adversarial examples. Numerous experiments demonstrate the effectiveness of the proposed method. Code is available at Github: https://github.com/KWPCCC/NEAA.}
}
@article{MAXVILLE20111953,
title = {eScience: Building our Body of Knowledge},
journal = {Procedia Computer Science},
volume = {4},
pages = {1953-1963},
year = {2011},
note = {Proceedings of the International Conference on Computational Science, ICCS 2011},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2011.04.213},
url = {https://www.sciencedirect.com/science/article/pii/S1877050911002717},
author = {Valerie Maxville},
abstract = {This paper describes the need for an eScience BoK, particularly as a resource for educators. eScience is a term representing the computational technology and techniques utilised when undertaking research. As eScience matures, stakeholders, and particularly educators, can benefit from the clarity that a defined Body of Knowledge (BOK) can provide. The BOK would require domain-specific and technological aspects to be addressed. This paper describes a framework for a prototype BOK for eScience and discusses how the BOK can be used as a tool to drive education, outreach and infrastructure planning.}
}
@article{ROMANATO2000277,
title = {Computation of the strain field generated by dislocations with a position-dependent Burgers' vector distribution},
journal = {Micron},
volume = {31},
number = {3},
pages = {277-283},
year = {2000},
issn = {0968-4328},
doi = {https://doi.org/10.1016/S0968-4328(99)00094-3},
url = {https://www.sciencedirect.com/science/article/pii/S0968432899000943},
author = {F Romanato and M Natali and E Napolitani and A Drigo},
keywords = {Burgers' vector, Misfit dislocation, Reciprocal space maps},
abstract = {A new phenomenon of strain relaxation will be presented. In a series of InxGa1−xAs graded composition buffer layers grown on well cut (001) GaAs substrates, a curvature of the epilayer lattice has been found, i.e. a tilt of the epilayer lattice orientation with respect to the substrate which varies coherently along the sample surface on the scale of several mm. The most recent data analysis performed on a buffer layer compositionally graded with a six-step profile shows also a thickness functional dependence of the curvature. The epilayer lattice curvature has been attributed to a coherent lateral distribution of the Burgers’ vectors. An analytical model has been developed in the framework of the continuum elasticity theory to compute the related strain field. The results show small but unexpected contributions to the parallel strain.}
}
@article{SRIDHAR2022113207,
title = {Extraction techniques in food industry: Insights into process parameters and their optimization},
journal = {Food and Chemical Toxicology},
volume = {166},
pages = {113207},
year = {2022},
issn = {0278-6915},
doi = {https://doi.org/10.1016/j.fct.2022.113207},
url = {https://www.sciencedirect.com/science/article/pii/S0278691522004057},
author = {Adithya Sridhar and Vijay Vaishampayan and P. {Senthil Kumar} and Muthamilselvi Ponnuchamy and Ashish Kapoor},
keywords = {Extraction, Food, Modelling, Optimization, Sustainability},
abstract = {This review presents critical evaluation of the key parameters that affect the extraction of targeted components, giving due consideration to safety and environmental aspects. The crucial aspects of the extraction technologies along with protocols and process parameters for designing unit operations have been emphasized. The parameters like solvent usage, substrate type, concentration, particle size, temperature, quality and storage of extract as well as stability of extraction have been elaborately discussed. The process optimization using mathematical and computational modeling highlighting information and communication technologies have been given importance aiming for a green and sustainable industry level scaleup. The findings indicate that the extraction processes vary significantly depending on the category of food and its structure. There is no single extraction method or universal set of process conditions identified for extracting all value-added products from respective sources. A comprehensive understanding of process parameters and their optimization as well as synergistic combination of multiple extraction processes can aid in enhancement of the overall extraction efficiency. Future efforts must be directed toward the design of integrated unit operations that cause minimal harm to the environment along with investigations on economic feasibility to ensure sustainable extraction systems.}
}
@article{SHAHAB2025121507,
title = {Droplet sorting computer: Design, optimization and device dynamics},
journal = {Chemical Engineering Science},
volume = {311},
pages = {121507},
year = {2025},
issn = {0009-2509},
doi = {https://doi.org/10.1016/j.ces.2025.121507},
url = {https://www.sciencedirect.com/science/article/pii/S0009250925003306},
author = {Mohammad Shahab and Raghunathan Rengaswamy},
keywords = {Droplet sorting, Microfluidics, Reinforcement learning, Multi-agent systems, Optimization, Scalability},
abstract = {Droplet sorting is a crucial component for integrated lab-on-a-chip applications, but conventional methods require active control and a sorter component. This study presents a state-of-the-art droplet sorting computer that incorporates a quality droplet sorter into a microfluidic device without the need for an additional mechanism for active detection and control. The device is computationally optimized using a novel combined design framework based on multi-agent reinforcement learning. The beauty of this sorting computer is that a device optimized for fewer drops can be used for thousands of drops, based on optimal hydrodynamic interactions between drops. The framework for developing a sorting computer is described, and the dynamics behind passive routing of droplets inside the device is explained. The proposed device can achieve droplet movement similar to that of an automated microfluidic device for droplet sorting or any given objective, making it a pioneer in passive droplet routing when implemented with appropriate process conditions.}
}
@article{DIGLIO2023233,
title = {Approximation schemes for districting problems with probabilistic constraints},
journal = {European Journal of Operational Research},
volume = {307},
number = {1},
pages = {233-248},
year = {2023},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2022.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0377221722007172},
author = {Antonio Diglio and Juanjo Peiró and Carmela Piccolo and Francisco Saldanha-da-Gama},
keywords = {Location, Districting, Stochastic demand, Chance-constraint balancing, Heuristics},
abstract = {In this work a districting problem with stochastic demand is investigated. Chance-constraints are used to model the balancing requirements. Explicit contiguity constraints are also considered. After motivating the problem and discussing several modeling aspects, an approximate deterministic counterpart is proposed which is the core of new solution algorithms devised. The latter are based upon a location-allocation scheme, whose first step consists of considering either a problem with a sample of scenarios or a sample of single-scenario problems. This leads to two variants of a new heuristic. The second version calls for the use of a so-called attractiveness function as a means to find a good trade-off between the (approximate) solutions obtained for the single-scenario problems. Different definitions of such functions are discussed. Extensive computational tests were performed whose results are reported.}
}
@article{IM2009193,
title = {Diagnosing skills of statistical hypothesis testing using the Rule Space Method},
journal = {Studies in Educational Evaluation},
volume = {35},
number = {4},
pages = {193-199},
year = {2009},
issn = {0191-491X},
doi = {https://doi.org/10.1016/j.stueduc.2009.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0191491X0900042X},
author = {Seongah Im and Yue Yin},
keywords = {Cognitive diagnostic assessment, Rule Space Method, Statistical hypothesis testing, Educational evaluation},
abstract = {This study illustrated the use of the Rule Space Method to diagnose students’ proficiencies in, skills and knowledge of statistical hypothesis testing. Participants included 96 undergraduate and, graduate students, of whom 94 were classified into one or more of the knowledge states identified by, the rule space analysis. Analysis at the level of proficiency groups showed that the critical difference, between low and medium proficiency groups was the understanding of statistical concepts and, knowledge while the critical skill discriminating the medium proficiency group from the high, proficiency group was to mange complex computational procedures. In addition, attribute profiles of, two students showed how students with the same total score can possess different strengths and, weaknesses.}
}
@article{CHEN2023103837,
title = {Recursive reasoning-based training-time adversarial machine learning},
journal = {Artificial Intelligence},
volume = {315},
pages = {103837},
year = {2023},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103837},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222001771},
author = {Yizhou Chen and Zhongxiang Dai and Haibin Yu and Bryan Kian Hsiang Low and Teck-Hua Ho},
keywords = {Recursive reasoning, Adversarial machine learning, Game theory},
abstract = {The training process of a machine learning (ML) model may be subject to adversarial attacks from an attacker who attempts to undermine the test performance of the ML model by perturbing the training minibatches, and thus needs to be protected by a defender. Such a problem setting is referred to as training-time adversarial ML. We formulate it as a two-player game and propose a principled Recursive Reasoning-based Training-Time adversarial ML (R2T2) framework to model this game. R2T2 models the reasoning process between the attacker and the defender and captures their bounded reasoning capabilities (due to bounded computational resources) through the recursive reasoning formalism. In particular, we associate a deeper level of recursive reasoning with the use of a higher-order gradient to derive the attack (defense) strategy, which naturally improves its performance while requiring greater computational resources. Interestingly, our R2T2 framework encompasses a variety of existing adversarial ML methods which correspond to attackers (defenders) with different recursive reasoning capabilities. We show how an R2T2 attacker (defender) can utilize our proposed nested projected gradient descent-based method to approximate the optimal attack (defense) strategy at an arbitrary level of reasoning. R2T2 can empirically achieve state-of-the-art attack and defense performances on benchmark image datasets.}
}
@article{EISENBERGER2004294,
title = {Why rejection hurts: a common neural alarm system for physical and social pain},
journal = {Trends in Cognitive Sciences},
volume = {8},
number = {7},
pages = {294-300},
year = {2004},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2004.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S1364661304001433},
author = {Naomi I. Eisenberger and Matthew D. Lieberman},
abstract = {Numerous languages characterize ‘social pain’, the feelings resulting from social estrangement, with words typically reserved for describing physical pain (‘broken heart’, ‘broken bones’) and perhaps for good reason. It has been suggested that, in mammalian species, the social-attachment system borrowed the computations of the pain system to prevent the potentially harmful consequences of social separation. Mounting evidence from the animal lesion and human neuroimaging literatures suggests that physical and social pain overlap in their underlying neural circuitry and computational processes. We review evidence suggesting that the anterior cingulate cortex plays a key role in the physical–social pain overlap. We also suggest that the physical–social pain circuitry might share components of a broader neural alarm system.}
}
@article{KRAVCHENKO2020310,
title = {Multi-faceted approach to solving issue of ensuring “zero mortality” on Russian roads},
journal = {Transportation Research Procedia},
volume = {50},
pages = {310-320},
year = {2020},
note = {XIV International Conference on Organization and Traffic Safety Management in Large Cities (OTS-2020)},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2020.10.037},
url = {https://www.sciencedirect.com/science/article/pii/S2352146520307833},
author = {Pavel Kravchenko and Sultan Zhankaziev and Elena Oleshchenko},
keywords = {system analysis, efficiency, automated monitoring system, vehicle, stochastic approach, traffic safety},
abstract = {“The fundamental principle of systems thinking is the ability to look at events, objects and phenomena from various perspectives considered as an aggregate” (O’Connor and McDermott 2006). The article continues studying the issue of preventing the occurrence of causes of death on Russian roads, i.e. “zero” mortality. The study results are presented in a series of articles published in the “Transport of the Russian Federation” journal. It provides a rationale for the feasibility and relevance of applying a methodological approach, which is new for Russian science and practice, to solving the issue of ensuring road traffic safety (RTS) in the interpretation of its meaning, comparable with the meaning of the term “zero mortality”, namely, a multi-faceted approach that considers a multilateral assessment of the quality of management decisions adopted using the knowledge of a full set of different opinions on the objects of any complexity being studied. They include RTS assurance systems, for which a set of different opinions (aspects) regarding all their possible facets, sides, properties, features, etc. — on behalf of the state, authorities, operating structures, and society — can be transformed into a set of factors affecting the level of provided RTS, having a subset of “dangerous” factors that can become causes of death in road traffic accidents (RTAs) in the road traffic environment. The article contains: a digest of the above subset of factors, which are essential for the issue being studied and can serve as the basis for expanding the possibilities of forming a set of death causes, adjusting the functions and corresponding types of required activities functionally bound by a common goal; substantiation of a functional and structural mathematical model for the state RTS system, an algorithmic model for the mechanisms forming its main functional properties and subsets of non-accidental causes of death, which can be understood and addressed preventing the moment of a possible serious RTA, and accidental ones, which cannot be understood and addressed; a “starting” sample of literary sources, capable of expanding (when referring to the publications) and ensuring an increase in the progress of solving the issue of “zero mortality” on Russian roads by 2030 as established by the state strategy for RTS.}
}
@article{MIRZAEI2021102839,
title = {CFD modeling of micro and urban climates: Problems to be solved in the new decade},
journal = {Sustainable Cities and Society},
volume = {69},
pages = {102839},
year = {2021},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.102839},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721001293},
author = {Parham A. Mirzaei},
keywords = {Computational fluid dynamics, Urban climate, Microclimate, Data acquisition, Building energy simulation, Sustainable, Resilient, Smart cities},
abstract = {Despite the popularity of the micro/urban climate CFD modeling as a powerful approach to simulate convective exchanges in urban areas, yet its application faces three profound limitations, including (1) computational barriers, (2) data acquisition, and (3) over-simplifications of underlying physics. Computational resources are not qualitatively studied to be allocated to their best of performance in urban climate models. Moreover, bigdata of city components and inhabitants are sometimes inaccessible or difficult to be effectively interpreted to be fed into CFD models. Furthermore, commonly adopted oversimplifications, and misinterpretation of underlying physics of urban climate can substantially render falsified results, no matter if they look otherwise followed by extravagant visual reports. This paper, hence, aims to explore the capabilities and limitations of urban climate CFD modeling. It further scrutinizes the common oversimplifications in the modeling techniques, potentially resulting in CFD capacities to be lost in the translation. The paper describes the extend to which CFD tools can be the favourable options and otherwise, while it underpins the areas in which further research is needed to conform urban climate CFD models as practical design and decision-making tools. It also offers a brief overview in the recent advancements in response to the mentioned challenges.}
}
@article{KARTHIK2022243,
title = {Prognostic Kalman Filter Based Bayesian Learning Model for Data Accuracy Prediction},
journal = {Computers, Materials and Continua},
volume = {72},
number = {1},
pages = {243-259},
year = {2022},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2022.023864},
url = {https://www.sciencedirect.com/science/article/pii/S1546221822010840},
author = {S. Karthik and Robin Singh Bhadoria and Jeong Gon Lee and Arun Kumar Sivaraman and Sovan Samanta and A. Balasundaram and Brijesh Kumar Chaurasia and S. Ashokkumar},
keywords = {Bayesian learning model, kalman filter, machine learning, data accuracy prediction},
abstract = {Data is always a crucial issue of concern especially during its prediction and computation in digital revolution. This paper exactly helps in providing efficient learning mechanism for accurate predictability and reducing redundant data communication. It also discusses the Bayesian analysis that finds the conditional probability of at least two parametric based predictions for the data. The paper presents a method for improving the performance of Bayesian classification using the combination of Kalman Filter and K-means. The method is applied on a small dataset just for establishing the fact that the proposed algorithm can reduce the time for computing the clusters from data. The proposed Bayesian learning probabilistic model is used to check the statistical noise and other inaccuracies using unknown variables. This scenario is being implemented using efficient machine learning algorithm to perpetuate the Bayesian probabilistic approach. It also demonstrates the generative function for Kalman-filer based prediction model and its observations. This paper implements the algorithm using open source platform of Python and efficiently integrates all different modules to piece of code via Common Platform Enumeration (CPE) for Python.}
}
@incollection{YASH2025495,
title = {Chapter 17 - In silico techniques, artificial intelligence, and machine learning for enhanced efficacy of extracellular vesicle–based diagnosis and therapeutics},
editor = {Krishnan Anand and Chithravel Vadivalagan and Prakash Gangadaran and Sathish Muthu and Ben Peacock},
booktitle = {Extracellular Vesicles for Therapeutic and Diagnostic Applications},
publisher = {Elsevier},
pages = {495-521},
year = {2025},
series = {Nanotechnology in Biomedicine},
isbn = {978-0-443-23891-8},
doi = {https://doi.org/10.1016/B978-0-443-23891-8.00013-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780443238918000135},
author = {Jain Yash and Jayawant Maharsh and Shashank Shivaji Kamble and Abhishek Guldhe and Virupaksha Bastikar},
keywords = {Extracellular vesicles, omics data analysis, machine learning algorithms, artificial intelligence, diagnostics, and potential therapeutics},
abstract = {The nanosized extracellular vesicles are the secretions from cells, either prokaryotic or eukaryotic; these are the cluster of various biomolecules, including but not limited to RNA, proteins, and polysaccharides, which function in cell signaling and act as carriers. The recent advancement in their isolation and long storage has enabled them to be used for various diagnostic and therapeutic applications. Using these vesicles, novel drug delivery systems are designed; moreover, they can help diagnose many lethal diseases in their very early stages (e.g., cancer), which gives us an edge to act against them. Bioinformatics tools like molecular docking and dynamic simulation studies can help design a better binding site for the targeted extracellular vesicles for diagnostic purposes. By contrast, simulation studies can help us design novel extracellular vesicles for therapeutic purposes by studying the behavior and interactions of the vesicles with their surroundings inside the body. Additionally, the omics data analysis of these extracellular vesicles helps characterize genomic, metabolomic, and proteomic levels, which tells us about their functions and potential application in various fields. Furthermore, these studies can be validated using advanced computational techniques such as machine learning and artificial intelligence, as the machine learning approach creates algorithms based on the data to build models that can perform complex analyses. By contrast, an artificial intelligence would predict how a human would perform complex tasks such as learning, analyzing, and reasoning. The parametric data received through extensive bioinformatical analysis can be analyzed by many patterns’ recognition, reinforcement, or generative machine learning models. These models help devise physical diagnostic tests for various diseases, predict newer and better versions of existing extracellular vesicles, and strengthen our knowledge of the drug delivery mechanisms affiliated with these extracellular vesicles. More extensive research in integrating artificial intelligence into these machine learning models can enhance the model’s prediction rate and output quality.}
}
@article{LUND2017556,
title = {Smart energy and smart energy systems},
journal = {Energy},
volume = {137},
pages = {556-565},
year = {2017},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2017.05.123},
url = {https://www.sciencedirect.com/science/article/pii/S0360544217308812},
author = {Henrik Lund and Poul Alberg Østergaard and David Connolly and Brian Vad Mathiesen},
keywords = {Renewable energy systems, Smart grid, Energy system modelling, Electro fuels, Power-to-Gas, Power-to-heat},
abstract = {In recent years, the terms “Smart Energy” and “Smart Energy Systems” have been used to express an approach that reaches broader than the term “Smart grid”. Where Smart Grids focus primarily on the electricity sector, Smart Energy Systems take an integrated holistic focus on the inclusion of more sectors (electricity, heating, cooling, industry, buildings and transportation) and allows for the identification of more achievable and affordable solutions to the transformation into future renewable and sustainable energy solutions. This paper first makes a review of the scientific literature within the field. Thereafter it discusses the term Smart Energy Systems with regard to the issues of definition, identification of solutions, modelling, and integration of storage. The conclusion is that the Smart Energy System concept represents a scientific shift in paradigms away from single-sector thinking to a coherent energy systems understanding on how to benefit from the integration of all sectors and infrastructures.}
}
@article{RAMFUL2014119,
title = {Reversible reasoning in fractional situations: Theorems-in-action and constraints},
journal = {The Journal of Mathematical Behavior},
volume = {33},
pages = {119-130},
year = {2014},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2013.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0732312313000990},
author = {Ajay Ramful},
keywords = {Division, Fraction, Multiplicative reasoning, Reversibility, Units, Vergnaud's theory},
abstract = {The aim of this study was to investigate, at a fine-grained level of detail, the theorems-in-action deployed and the constraints encountered by middle-school students in reasoning reversibly in the multiplicative domain of fraction. A theorem-in-action (Vergnaud, 1988) is a conceptual construct to trace students’ reasoning in a problem solving situation. Two seventh grade students were interviewed in a rural middle-school in the southern part of the United States. The students’ strategies were examined with respect to the numerical features of the problem situations and the ways they viewed and operated on fractional units. The results show that reversible reasoning is sensitive to the numeric feature of problem parameters. Relatively prime numbers and fractional quantities acted as inhibitors preventing the cueing of the multiplication–division invariant, thereby constraining students from reasoning reversibly. Among others, two key resources were identified as being essential for reasoning reversibly in fractional contexts: firstly, interpreting fractions in terms of units, which enabled the students to access their whole number knowledge and secondly, the unit-rate theorem-in-action. Failure to conceptualize multiplicative relations in reverse constrained the students to use more primitive strategies, leading them to solve problems non-deterministically and at higher computational costs.}
}
@article{1987755,
title = {Computation of signatures of linear airgun arrays: Vaage, S. and B. Ursin, 1987. Geophys. Prospect., 35(3):281–287. SERES A/S, P.O. Box 1965, Moholtan, 7001 Trondheim, Norway},
journal = {Deep Sea Research Part B. Oceanographic Literature Review},
volume = {34},
number = {9},
pages = {755},
year = {1987},
issn = {0198-0254},
doi = {https://doi.org/10.1016/0198-0254(87)90164-6},
url = {https://www.sciencedirect.com/science/article/pii/0198025487901646}
}
@article{TIAN2015338,
title = {Safety assessment method of performance-based navigation airspace planning},
journal = {Journal of Traffic and Transportation Engineering (English Edition)},
volume = {2},
number = {5},
pages = {338-345},
year = {2015},
issn = {2095-7564},
doi = {https://doi.org/10.1016/j.jtte.2015.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S2095756415000690},
author = {Yong Tian and Lili Wan and Chun-hung Chen and Ye Yang},
keywords = {Air traffic management, Safety assessment, Operational planning, Conflict risk, Adverse weather},
abstract = {The paper introduces a computational model of airspace conflict risk in the hierarchy of performance-based navigation (PBN) airspace operation and combines it with air traffic controller (ATC) workload to propose a method for safety assessment of PBN airspace operational planning. Capacity probability distribution is employed to depict airspace capacity in uncertain weather, errors of deviating from nominal PBN track are taken into consideration, and the stochastic process based on Gaussian distribution is used to depict random aircraft motion according to airspace PBN specification, so as to build an airspace conflict risk computational model in corresponding capacity scenario. Guangzhou No. 15 sector is chosen for simulation validation. The analysis results suggest that 60% of ATC workload is corresponding to sector traffic flow of 31 aircraft/h and airspace risk of 0.018 conflict/h, while 70% of ATC workload is corresponding to sector traffic flow of 35 aircraft/h and airspace risk of 0.03 conflict/h. As air traffic flow increases, both airspace conflict risk value and ATC workload will increase, resulting in reduction of airspace safety, though their increasing magnitudes differ with different capacity scenarios. The safety assessment method enables effective quantization of safety with regard to airspace operational planning strategy, and benefits the development of optimal operational scheme that balances risk with capacity demand.}
}
@article{BRODIN2009345,
title = {Univariate and bivariate GPD methods for predicting extreme wind storm losses},
journal = {Insurance: Mathematics and Economics},
volume = {44},
number = {3},
pages = {345-356},
year = {2009},
issn = {0167-6687},
doi = {https://doi.org/10.1016/j.insmatheco.2008.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167668708001455},
author = {Erik Brodin and Holger Rootzén},
keywords = {Extreme value statistics, Generalized Pareto distribution, Likelihood prediction intervals, Peaks over threshold, Trend analysis, Wind storm losses},
abstract = {Wind storm and hurricane risks are attracting increased attention as a result of recent catastrophic events. The aim of this paper is to select, tailor, and develop extreme value methods for use in wind storm insurance. The methods are applied to the 1982–2005 losses for the largest Swedish insurance company, the Länsförsäkringar group. Both a univariate and a new bivariate Generalized Pareto Distribution (GPD) gave models which fitted the data well. The bivariate model led to lower estimates of risk, except for extreme cases, but taking statistical uncertainty into account the two models lead to qualitatively similar results. We believe that the bivariate model provided the most realistic picture of the real uncertainties. It additionally made it possible to explore the effects of changes in the insurance portfolio, and showed that loss distributions are rather insensitive to portfolio changes. We found a small trend in the sizes of small individual claims, but no other trends. Finally, we believe that companies should develop systematic ways of thinking about “not yet seen” disasters.}
}
@article{BUSCEMA2022112439,
title = {A nonlinear, data-driven, ANNs-based approach to culture-led development policies in rural areas: The case of Gjakove and Peć districts, Western Kosovo},
journal = {Chaos, Solitons & Fractals},
volume = {162},
pages = {112439},
year = {2022},
issn = {0960-0779},
doi = {https://doi.org/10.1016/j.chaos.2022.112439},
url = {https://www.sciencedirect.com/science/article/pii/S096007792200649X},
author = {Massimo Buscema and Guido Ferilli and Christer Gustafsson and Giulia Massini and Pier Luigi Sacco},
keywords = {Theory of impossible worlds, Culture, Cultural policy, Topologically weighted centroid, AutoCM, Kosovo},
abstract = {We develop a computational approach to the analysis of cultural vibrancy and to the role of the cultural and creative sectors in the socio-economic organization of two districts of Western Kosovo, Gjakove and Peć. Our analysis is built on a geolocalized mapping of the cultural activities and facilities, and on the main socio-economic variables for the two districts, and makes use of innovative data analysis techniques: Theory of Impossible Words (TIW), the Topological Weighted Centroid (TWC), and the AutoCM ANN. We find that the dynamics of cultural vibrancy of the territory is mainly driven by the competing attraction pulls of the nearby countries of Serbia and Albania, that also form the region's main and often conflicting ethnicities, and that such dynamics are likely to further polarize in the future. We also find that the cultural system plays a marginal role in the territory's socio-economic organization. This situation makes a case for a more active role of cultural policy in shaping future local developmental models in rural areas and in acting as an agent of social cohesion.}
}
@article{LI2025112800,
title = {Optimizing mass transfer performance in triply periodic minimal surface porous scaffolds through isosurface offset},
journal = {Thin-Walled Structures},
volume = {208},
pages = {112800},
year = {2025},
issn = {0263-8231},
doi = {https://doi.org/10.1016/j.tws.2024.112800},
url = {https://www.sciencedirect.com/science/article/pii/S0263823124012394},
author = {Kun Li and Chunlin Zuo and Ruobing Liao and Haisong Liang and Xuan Liang and David Z. Zhang and Lawrence E. Murr and Huajun Cao},
keywords = {Triply periodic minimal surface, Bone scaffold, Permeability, Wall shear stress, Mass transfer performance},
abstract = {Efficient bone tissue regeneration remains a critical challenge in orthopedic medicine, with scaffold mass transfer capabilities playing a pivotal role. Triply periodic minimal surface (TPMS) scaffolds have emerged as promising candidates due to their unique structure characterized by smooth, continuous surfaces with zero mean curvature and high specific surface area. However, optimizing their mass transfer performance to meet the diverse needs of bone tissues at different anatomical sites has been a persistent challenge. This study addresses this gap by investigating the effects of isosurface offset on mass transfer performance in three TPMS scaffolds (Fisher-Koch S, Gyroid, and Split-P) using computational fluid dynamics (CFD). The results showed that isosurface offset significantly increased the effective scaffold permeability range (by 116.8 %, 5.3 %, and 64.3 % for F, G, and S scaffolds, respectively) and improved the wall shear stress (WSS) distribution, enhancing the area that effectively stimulates cell proliferation (by 25.2 %, 8.7 %, and 14.3 % increase, respectively). Additionally, it was found that porosity, specific surface area, the ratio of maximum pore size to tortuosity, and curvature significantly influenced the permeability and WSS distribution of the scaffolds. Finally, permeation experiments using porous scaffolds fabricated by laser powder bed fusion (LPBF) technology were performed to validate the simulation results. This study provides new insights into the design of TPMS porous scaffolds and customized bone implants, enhancing their application prospects in bone tissue engineering.}
}
@article{RINO2024102366,
title = {Timed alignments with mixed moves},
journal = {Data & Knowledge Engineering},
volume = {154},
pages = {102366},
year = {2024},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2024.102366},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X24000909},
author = {Neha Rino and Thomas Chatain},
keywords = {Conformance checking, Alignments, Timestamps, Time Petri nets},
abstract = {We study conformance checking for timed models, that is, process models that consider both the sequence of events that occur, as well as the timestamps at which each event is recorded. Time-aware process mining is a growing subfield of research, and as tools that seek to discover timing-related properties in processes develop, so does the need for conformance-checking techniques that can tackle time constraints and provide insightful quality measures for time-aware process models. One of the most useful conformance artefacts is the alignment, that is, finding the minimal changes necessary to correct a new observation to conform to a process model. In this paper, we extend the notion of timed distance from a previous work where an edit on an event’s timestamp came in two types, depending on whether or not it would propagate to its successors. Here, these different types of edits have a weighted cost each, and the ratio of their costs is denoted by α. We then solve the purely timed alignment problem in this setting for a large class of these weighted distances (corresponding to α∈{1}∪[2,∞)). For these distances, we provide linear time algorithms for both distance computation and alignment on models with sequential causal processes.}
}
@article{DAI2025200249,
title = {Practical Exploration of English Translation Activity Courses in Universities under the Background of Artificial Intelligence},
journal = {Systems and Soft Computing},
pages = {200249},
year = {2025},
issn = {2772-9419},
doi = {https://doi.org/10.1016/j.sasc.2025.200249},
url = {https://www.sciencedirect.com/science/article/pii/S2772941925000675},
author = {Jiexia Dai},
keywords = {Artificial Intelligence, English Translation Activity Class, Recurrent Neural Network, Translation Level},
abstract = {In recent years, along with the increasing international exchange, cultivating English translation talents has become particularly important. This study aims to explore the application effect of artificial intelligence technology in college English translation activity courses and compare the differences between traditional teaching methods and AI-based methods. Through comparative analysis of the improvement of the translation ability of the two groups of students, the practical value and feasibility of AI technology in translation teaching is evaluated to provide reference and guidance for future translation education. This paper used the neural network machine translation model in artificial intelligence to solve the practical problems in implementing of college English translation activities. Intelligent translation through Recurrent Neural Network (RNN) can better leverage the role of artificial intelligence, which provides new methods for the sustainable development of translation activity classes in universities. Group A adopts the traditional English translation teaching method, mainly relying on teachers' explanations and students' practice. Course content includes interpretation of translation theory, text translation practice, and discussion and analysis based on translation cases. It mainly uses paper textbooks and reference books, as well as some online translation resources, but does not involve the application of artificial intelligence technology. Group B integrates artificial intelligence technology into English translation teaching method, using neural network translation model (such as RNN) to assist teaching. In addition to traditional translation theory and practice, the course also includes training and practice on the use of AI translation tools. Using AI translation software, an online translation platform and a specially developed AI-based translation practice platform, the platform is an artificial intelligence translation model integrated with RNN, designed to assist teaching and improve students' translation skills. The experimental results of this article indicate that the average translation scores of Group A and Group B students in the first 6 months of the experiment were 65.77 and 65.71, respectively. The scores of Group A and Group B students after the experiment for 6 months were 68.57 and 82.69, respectively. The results show that translation teaching in the context of artificial intelligence significantly improves the efficiency and accuracy of students' translation, and enhances the interaction and interest of learning. This finding shows that the AI-based technology in translation teaching is not only feasible and effective, but also can provide new ideas for the modernization and intelligent development of translation education, and improve students' practical translation ability and professional competitiveness.}
}
@article{DUFVA201697,
title = {Metaphors of code—Structuring and broadening the discussion on teaching children to code},
journal = {Thinking Skills and Creativity},
volume = {22},
pages = {97-110},
year = {2016},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2016.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S1871187116301055},
author = {Tomi Dufva and Mikko Dufva},
keywords = {Code, Code literacy, Metaphors, Education, Programming, Teaching programming, Pedagogy, Media literacy},
abstract = {Digital technology has become embedded into our daily lives. Code is at the heart of this technology. The way code is perceived influences the way our everyday interaction with digital technologies is perceived: is it an objective exchange of ones and zeros, or a value- laden power struggle between white male programmers and those who think they are users, when they are, in fact, the product being sold. Understanding the nature of code thus enables the imagination and exploration of the present state and alternative future developments of digital technologies. A wider imagination is especially important for developing basic education so that it provides the capabilities for coping with these developments. Currently, the discussion has been mainly on the technical details of code. We study how to broaden this narrow view in order to support the design of more comprehensive and future-proof education around code and coding. We approach the concept of code through nine different metaphors from the existing literature on systems thinking and organisational studies. The metaphors we use are machine, organism, brain, flux and transformation, culture, political system, psychic prison, instrument of domination and carnival. We describe their epistemological backgrounds and give examples of how code is perceived through each of them. We then use the metaphors in order to suggest different complementary ways that ICT could be taught in schools. The metaphors illustrate different contexts and help to interpret the discussions related to developments in digital technologies such as free software movement, democratization of information and internet of things. They also help to identify the dominant views and the tensions between the views. We propose that the systematic use of metaphors described in this paper would be a useful tool for broadening and structuring the dialogue about teaching children to code.}
}
@article{YUROVSKY201873,
title = {A communicative approach to early word learning},
journal = {New Ideas in Psychology},
volume = {50},
pages = {73-79},
year = {2018},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2017.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X17300077},
author = {Daniel Yurovsky},
keywords = {Language acquisition, Learning, Cognitive development},
abstract = {Young children learn the meanings of thousands of words by the time they can run down the street. Many efforts to explain this rapid development begin by assuming that the computational-level problem being solved is acquisition. Consequently, work in this line has sought to understand how children infer the meanings of words from cues in the communicative signals of the speakers around them. I will argue, however, that this formulation of the problem is backwards: the computational problem is communication, and language acquisition provides cues about how to communicate successfully. Under this framing, the natural unit of analysis is not the child, but the parent-child dyad. A necessary consequence of this shift is the realization that the statistical structure of the input to the child is itself dependent on the child. This dependency radically simplifies the computational problem of learning and using language.}
}
@article{JIANG2023102217,
title = {Cooperative localization for master–salve multi-AUVs based on range measurements},
journal = {Physical Communication},
volume = {61},
pages = {102217},
year = {2023},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2023.102217},
url = {https://www.sciencedirect.com/science/article/pii/S1874490723002203},
author = {Ling Jiang and Wengen Gao and Yunfei Li and Mengxing Pan and Shaopeng Mu},
keywords = {AUV, Cooperative localization, Distance-dependent noise, Gaussian belief propagation},
abstract = {Underwater localization has consistently remained a prominent technical challenge for autonomous underwater vehicles (AUVs). The advent of cooperative localization techniques has emerged as a novel avenue for enhancing localization accuracy. The master–slave cooperative localization mode has gained widespread adoption due to its cost-effectiveness in implementation. In view of the complexity of underwater noise characteristics, in the multi-AUVs cooperative localization system, this paper addresses scenarios involving distance-dependent noise in a master–slave-based multi-AUVs cooperative localization system. To tackle the negative impact of distance-dependent noise and the non-linearity of the distance function, a two-step algorithm is proposed that combines maximum likelihood estimation and the Gaussian belief propagation algorithm (ML-GBP) to estimate the positions of AUVs. The maximum likelihood estimation is employed to cope with the interference caused by distance-dependent noise, and subsequently, the Gaussian belief propagation algorithm, based on range observations and reference information, is used to achieve accurate estimation of AUV positions and implement position correction. Simulation results demonstrate that the proposed ML-GBP algorithm outperforms traditional extended Kalman filter (EKF) and nonparametric belief propagation (NBP) methods by enhancing the localization accuracy of the system while exhibiting superior performance in terms of computational complexity and system communication overhead.}
}
@article{DOBREW2025104941,
title = {Make-up strategies with incomplete markets and bounded rationality},
journal = {European Economic Review},
volume = {173},
pages = {104941},
year = {2025},
issn = {0014-2921},
doi = {https://doi.org/10.1016/j.euroecorev.2024.104941},
url = {https://www.sciencedirect.com/science/article/pii/S0014292124002708},
author = {Michael Dobrew and Rafael Gerke and Sebastian Giesen and Joost Röttger},
keywords = {Make-up strategies, Incomplete markets, Bounded rationality, Effective lower bound, Average inflation targeting},
abstract = {We study the impact of market incompleteness and bounded rationality on the effectiveness of make-up strategies. Using a heterogeneous-agent New Keynesian model with reflective expectations, we show that make-up strategies can mitigate the negative consequences of an occasionally-binding effective lower bound. However, the benefits are small when cognitive ability is in line with micro-evidence. These findings are independent of market (in)completeness, emphasising the importance of rational expectations. While market incompleteness and bounded rationality complement each other in attenuating the effects of forward guidance, we do not observe such a complementarity for the effectiveness of make-up strategies.}
}
@article{AHMAD20225041,
title = {Decision Level Fusion Using Hybrid Classifier for Mental Disease Classification},
journal = {Computers, Materials and Continua},
volume = {72},
number = {3},
pages = {5041-5058},
year = {2022},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2022.026077},
url = {https://www.sciencedirect.com/science/article/pii/S1546221822009067},
author = {Maqsood Ahmad and Noorhaniza Wahid and Rahayu A Hamid and Saima Sadiq and Arif Mehmood and Gyu Sang Choi},
keywords = {Mental health diagnosis, machine learning, depression, shrewd probing, diagnostic approach},
abstract = {Mental health signifies the emotional, social, and psychological well-being of a person. It also affects the way of thinking, feeling, and situation handling of a person. Stable mental health helps in working with full potential in all stages of life from childhood to adulthood therefore it is of significant importance to find out the onset of the mental disease in order to maintain balance in life. Mental health problems are rising globally and constituting a burden on healthcare systems. Early diagnosis can help the professionals in the treatment that may lead to complications if they remain untreated. The machine learning models are highly prevalent for medical data analysis, disease diagnosis, and psychiatric nosology. This research addresses the challenge of detecting six major psychological disorders, namely, Anxiety, Bipolar Disorder, Conversion Disorder, Depression, Mental Retardation and Schizophrenia. These challenges are mined by applying decision level fusion of supervised machine learning algorithms. A dataset was collected from a clinical psychologist consisting of 1771 observations that we used for training and testing the models. Furthermore, to reduce the impact of a conflicting decision, a voting scheme Shrewd Probing Prediction Model (SPPM) is introduced to get output from ensemble model of Random Forest and Gradient Boosting Machine (RF + GBM). This research provides an intuitive solution for mental disorder analysis among different target class labels or groups. A framework is proposed for determining the mental health problem of patients using observations of medical experts. The framework consists of an ensemble model based on RF and GBM with a novel SPPM technique. This proposed decision level fusion approach by combining RF + GBM with SPPM-MIN significantly improves the performance in terms of Accuracy, Precision, Recall, and F1-score with 71\%, 73\%, 71\% and 71\% respectively. This framework seems suitable in the case of huge and more diverse multi-class datasets. Furthermore, three vector spaces based on TF-IDF (unigram, bi-gram, and tri-gram) are also tested on the machine learning models and the proposed model.}
}
@article{JABEEN2023108475,
title = {Deep learning-based prediction of inhibitors interaction with Butyrylcholinesterase for the treatment of Alzheimer's disease},
journal = {Computers and Electrical Engineering},
volume = {105},
pages = {108475},
year = {2023},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.108475},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622006905},
author = {Farah Jabeen and Zia Ur Rehman and Sajid Shah and Rima D. Alharthy and Saquib Jalil and Imtiaz Ali Khan and Jamshed Iqbal and Ahmed A. Abd El-Latif},
keywords = {Deep learning, Machine learning, Personalized and precision medicine, Alzheimer's disease, Computer-aided diagnosis and detection},
abstract = {Butyrylcholinesterase (BChE) is a significant pharmaceutical drug for treating Alzheimer's disease (AD) . Thanks to the computational methods as which decreases significantly the overhead for screening BChE inhibitors. However, some of them have used one-hot encoding which ignores the sequential information. In this study, Term Frequency-Inverse Document Frequency (TF-IDF) is used for encoding SMILES expressions and Long Short-Term Memory (LSTM) for classification to preserve sequential information. Apart from LSTM, different models were used to evaluate the discriminative power of TF-IDF and to show the significance of sequential information. The dataset used in this study con-sists of 4,515 records of BChE inhibitors and non-inhibitors in the form of SMILES. The results obtained by the machine learning models were tested through invitro activity assays as well. The molecular docking study further confirmed the binding modes inside the BChE. The LSTM model showed 98.20% testing ac-curacy for the prediction of BChE inhibitors.}
}
@article{ZHANG2024110034,
title = {Comprehensive reliability assessment method for distribution networks considering IIDG low voltage ride-through control},
journal = {International Journal of Electrical Power & Energy Systems},
volume = {159},
pages = {110034},
year = {2024},
issn = {0142-0615},
doi = {https://doi.org/10.1016/j.ijepes.2024.110034},
url = {https://www.sciencedirect.com/science/article/pii/S0142061524002552},
author = {Shuai Zhang and Wenxia Liu and Haiyang Wan and Tianlong Wang and Rui Cheng and Hanshen Li},
keywords = {Distribution networks, Reliability assessment, Low voltage ride through, Temporary fault, Permanent fault},
abstract = {Upon the large-scale integration of inverter-interfaced distributed generator (IIDG) into the grid, random faults in the distribution network can lead to momentary and sustained interruptions, significantly impacting system reliability. Although reliability methods have been widely used in distribution network adequacy assessment, using probabilistic method for reliability assessment including system dynamic security need to be investigated. To address this issue, a new method is designed to assess the reliability of distribution network using sequential Monte Carlo simulation. Firstly, the IIDG low-voltage ride-through (LVRT) control strategy is formulated, and the off-grid probability model for IIDG is developed based on the Gaussian distribution. Secondly, the component model is defined to consider the impact of random faults on power quality in security assessment. Following the fault tree analysis method, a protection action probability model was formulated to assess the failure probability of line current differential protection, IIDG anti-islanding protection, and reclosing protection. Finally, the method for momentary fault consequence analysis, based on depth-first search (DFS), and the method for sustained fault consequence analysis, based on the mixed-integer linear programming model, are developed. The study establishes a comprehensive probability reliability assessment framework. The validity of the method is demonstrated on the IEEE RBTS BUS6 F4 system, indicating good scalability.}
}
@article{AUCONI2020856,
title = {Computer-aided heuristics in orthodontics},
journal = {American Journal of Orthodontics and Dentofacial Orthopedics},
volume = {158},
number = {6},
pages = {856-867},
year = {2020},
issn = {0889-5406},
doi = {https://doi.org/10.1016/j.ajodo.2019.10.018},
url = {https://www.sciencedirect.com/science/article/pii/S0889540620304248},
author = {Pietro Auconi and James A. McNamara and Lorenzo Franchi},
abstract = {Introduction
During the decision-making process, physicians rely on heuristics that consist of simple, useful procedures for solving problems, intuitive shortcuts that produce reliable decisions based on limited information. In clinical situations characterized by a high degree of uncertainty such as those encountered in orthodontics, cognitive biases and judgment errors related to heuristics are not uncommon. This study aimed at promoting trust in the effective interface between the intuitive reasoning of the orthodontic practitioner and the computational heuristics emerging from simple statistical models.
Methods
We propose an integrative model based on the interaction between clinical reasoning and 2 computational tools, cluster analysis and fast-and-frugal trees, to extract a structured craniofacial representation of untreated subjects with Class III malocclusion and to forecast the worsening of the malocclusion over time.
Results
Cluster analysis of cephalometric values from 144 growing subjects with Class III malocclusion followed longitudinally (T1: mean age, 10.2 ± 1.9 years; T2: mean age, 13.8 ± 2.7 years) produced 3 morphologic subgroups with predominant sagittal, vertical, and slight maxillomandibular imbalances. Fast-and-frugal trees applied to different subgroups extracted heuristics that improved the prediction of key features associated with adverse craniofacial growth.
Conclusions
Provided that cephalometric values are placed in the appropriate framework, the matching between simple and fast computational approaches and clinical reasoning could help the intuitive logic, perception, and cognitive inferences of orthodontic practitioners on the outcome of patients affected by Class III disharmony, decreasing errors associated with flawed judgments and improving the accuracy of decision making.}
}
@article{SU2022100072,
title = {Artificial Intelligence (AI) in early childhood education: Curriculum design and future directions},
journal = {Computers and Education: Artificial Intelligence},
volume = {3},
pages = {100072},
year = {2022},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2022.100072},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X22000273},
author = {Jiahong Su and Yuchun Zhong},
keywords = {Artificial intelligence (AI), AI curriculum, Kindergarten, Early childhood education, AI literacy},
abstract = {With the rapid technological development of society brought on by Artificial Intelligence (AI), the demand for AI-literate workers will increase in the future. It is critical to develop the next generation's AI competencies and educate them about how to work with and use AI. Previous studies on AI were predominantly focused on secondary and university education; however, research on the Artificial Intelligence curriculum in early childhood education is scarce. Due to the lack of conformity on the standardisation of AI curriculum for early childhood education, this study examines the AI curriculum for kindergarten children using the framework which consists of four key components, including (1) aims, goals, objectives, or declarations of outcome, (2) subject matter, domains, or content, (3) methods or procedure, (4) evaluation and assessment. We recommend that AI literacy be achieved by three competencies: AI Knowledge, AI Skill, and AI Attitude. The employment of a social robot as a learning companion and programmable artifact was proven to be helpful in assisting young children in grasping AI principles. We also discovered which teaching methods had the most greatest influence on students' learning. We recommend problem-based learning for future AI education based on the findings.}
}
@article{ZHANG20242311026,
title = {Cutting-Edge Applications of Multi-Angle Numerical Simulations for Capacitive Deionization},
journal = {Acta Physico-Chimica Sinica},
volume = {40},
number = {11},
pages = {2311026},
year = {2024},
issn = {1000-6818},
doi = {https://doi.org/10.3866/PKU.WHXB202311026},
url = {https://www.sciencedirect.com/science/article/pii/S1000681824001590},
author = {Xiaochen Zhang and Fei Yu and Jie Ma},
keywords = {Capacitive deionization, Molecular dynamics, Density functional theory, Finite element analysis},
abstract = {Capacitive deionization (CDI) technology is considered to be an emerging water treatment technology in the 21st century, owing to its low energy consumption, absence of secondary pollution, and straightforward operation. The advancement of basic theory and computer science has facilitated the use of multi-angle numerical simulations for CDI. However, due to errors in experimental methods, a direct understanding of mechanisms such as the kinetic characteristics of ion diffusion inside electrode materials, structural evolution during charging and discharging, and the intrinsic connection between potentials and structures is lacking. Existing experimental methods fall short of providing clear theoretical explanations for these phenomena. In contrast, numerical simulations offer a better comprehension of the chemical and electrochemical evolution in CDI. Beyond electrode materials, the device configuration of CDI significantly impacts its performance. Utilizing numerical simulations to study the optimal device configuration is expected to enhance economic efficiency and promote the practical application of CDI. While current reviews of CDI focus primarily on electrode materials and device configurations, there is a dearth of comprehensive reviews on cutting-edge numerical simulation research in the CDI field. This review commences with the earliest continuous-scale model used to describe the dynamic process of CDI. It systematically categorizes multi-angle numerical simulations in CDI, summarizes the strengths and weaknesses of different numerical simulation methods, and anticipates future development directions. Continuous-scale models accurately characterize the ion dynamics of CDI, determining rate and process constraints. Pore-scale models analyze the microstructure of porous media, obviating the need for empirical formulas to preset transport parameters for continuous-scale models. Researchers have introduced molecular dynamics simulation and density functional theory into CDI research, effectively analyzing the influence of structural features at the molecular/atomic level of electrode materials on the CDI system. This aids researchers in enhancing the efficacy and ionic selectivity of CDI electrode materials through pore engineering, defect engineering, and electrochemical microcosmic modulation engineering. Finite element analysis guides improvements in ion diffusion and stability of electrode materials, while computational fluid dynamics provides references for designing high-performance CDI devices. Data-driven machine learning excels in handling nonlinear data and uncovering complex mechanisms of CDI water treatment processes, while digital twin technology can reduce operation and maintenance costs of CDI. Considering costs in practical applications, techno-economic analysis plays a pivotal role in promoting the practical application of CDI technology. This review, the first of its kind, provides an essential theoretical foundation and research ideas for the new paradigm of CDI research by summarizing the advantages and disadvantages of different numerical simulation methods and offering insights into cutting-edge perspectives in the field of CDI. }
}
@article{CURTO201911,
title = {Relating network connectivity to dynamics: opportunities and challenges for theoretical neuroscience},
journal = {Current Opinion in Neurobiology},
volume = {58},
pages = {11-20},
year = {2019},
note = {Computational Neuroscience},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2019.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0959438819300443},
author = {Carina Curto and Katherine Morrison},
abstract = {We review recent work relating network connectivity to the dynamics of neural activity. While concepts stemming from network science provide a valuable starting point, the interpretation of graph-theoretic structures and measures can be highly dependent on the dynamics associated to the network. Properties that are quite meaningful for linear dynamics, such as random walk and network flow models, may be of limited relevance in the neuroscience setting. Theoretical and computational neuroscience are playing a vital role in understanding the relationship between network connectivity and the nonlinear dynamics associated to neural networks.}
}
@article{REN2024127126,
title = {FedBoosting: Federated learning with gradient protected boosting for text recognition},
journal = {Neurocomputing},
volume = {569},
pages = {127126},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.127126},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223012493},
author = {Hanchi Ren and Jingjing Deng and Xianghua Xie and Xiaoke Ma and Yichuan Wang},
keywords = {Deep learning, Federated learning, Privacy preserving},
abstract = {Conventional machine learning methodologies require the centralization of data for model training, which may be infeasible in situations where data sharing limitations are imposed due to concerns such as privacy and gradient protection. The Federated Learning (FL) framework enables the collaborative learning of a shared model without necessitating the centralization or sharing of data among the data proprietors. Nonetheless, in this paper, we demonstrate that the generalization capability of the joint model is suboptimal for Non-Independent and Non-Identically Distributed (Non-IID) data, particularly when employing the Federated Averaging (FedAvg) strategy as a result of the weight divergence phenomenon. Consequently, we present a novel boosting algorithm for FL to address both the generalization and gradient leakage challenges, as well as to facilitate accelerated convergence in gradient-based optimization. Furthermore, we introduce a secure gradient sharing protocol that incorporates Homomorphic Encryption (HE) and Differential Privacy (DP) to safeguard against gradient leakage attacks. Our empirical evaluation demonstrates that the proposed Federated Boosting (FedBoosting) technique yields significant enhancements in both prediction accuracy and computational efficiency in the visual text recognition task on publicly available benchmarks.}
}
@article{GRAMELSBERGER2011296,
title = {What do numerical (climate) models really represent?},
journal = {Studies in History and Philosophy of Science Part A},
volume = {42},
number = {2},
pages = {296-302},
year = {2011},
note = {Model-Based Representation in Scientific Practice},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2010.11.037},
url = {https://www.sciencedirect.com/science/article/pii/S0039368110001160},
author = {Gabriele Gramelsberger},
keywords = {Climate models, Computing, Symbolic forms, Scientific modelling},
abstract = {The translation of a mathematical model into a numerical one employs various modifications in order to make the model accessible for computation. Such modifications include discretizations, approximations, heuristic assumptions, and other methods. The paper investigates the divergent styles of mathematical and numerical models in the case of a specific piece of code in a current atmospheric model. Cognizance of these modifications means that the question of the role and function of scientific models has to be reworked. Neither are numerical models pure intermediaries between theory and data, nor are they autonomous tools of inquiry. Instead, theory and data are transformed into a new symbolic form of research due to the fact that computation has become an essential requirement for every scientific practice. Therefore the question is posed: What do numerical (climate) models really represent?}
}
@article{WANG2024119513,
title = {Real-sea validation of a model predictive controller's inherent robustness for medium-scale unmanned trimaran heading},
journal = {Ocean Engineering},
volume = {313},
pages = {119513},
year = {2024},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2024.119513},
url = {https://www.sciencedirect.com/science/article/pii/S0029801824028518},
author = {Jun Wang and Jian Wang and Xiaofeng Liang and Junjie Liu and Shunzhao Cheng and Hong Yi},
keywords = {Unmanned surface vehicle, Heading control, Path following, Trimaran, Model predictive control, Robustness},
abstract = {The development of medium-to large-scale and high-performance unmanned surface vehicles (USVs) is a burgeoning trend in intelligent marine systems. The heading controller is crucial for USVs to execute diverse missions, especially given their inherent underactuation characteristics and constraints. Recent efforts have focused on enhancing the robustness of controllers against external disturbances and employed two main strategies: one converges the control error to a bounded residual set through robust modifications, while the other eliminates the error by modelling disturbances. Notably, these enhancements have primarily catered to small USVs, where disturbances significantly impact their manoeuvrability, necessitating such robust control strategies. This focus has somewhat overshadowed the inherent robustness of closed-loop control systems. Compared with small USVs, medium-to large-scale USVs are less affected by external disturbances, despite undertaking more complex missions. Leveraging the intrinsic robustness of controllers presents an opportunity to simplify controller design, thereby reallocating computational resources towards enhancing mission capabilities. Model Predictive Control (MPC) has attracted significant attention recently, and its receding horizon and optimality theoretically provides a new level of inherent robustness, which remains under-explored in real sea. This paper focuses on the inherent robustness of MPC in managing the heading of a medium-scale unmanned trimaran subjected to the thrust angle and angular velocity constraints. A model predictive controller considering the constraints is designed based on the identified Nomoto model and the asymptotic stability is ensured with a terminal cost. Conducted real-sea experiments and comparative analyses with a Proportional-Integral-Derivative (PID) controller, the most widespread and dominant control algorithm in practical USV engineering, underscore the superiority of MPC in maintaining satisfactory closed-loop performance. Furthermore, the MPC controller is also successfully applied to real-sea path-following missions and demonstrated good tracking performance in various sea conditions ranging from level 3 to 5 and wind speeds spanning from level 6 to 8. This validation opens up new avenues for motion control strategies in the evolving landscape of larger-scale USVs.}
}
@article{MAMOLO2022101014,
title = {Coding and climate change: Investigating prospective teachers’ pathways of attention},
journal = {The Journal of Mathematical Behavior},
volume = {68},
pages = {101014},
year = {2022},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2022.101014},
url = {https://www.sciencedirect.com/science/article/pii/S0732312322000827},
author = {Ami Mamolo and Sheree Rodney and Diane Tepylo},
keywords = {Awareness, Curiosity, Climate change, Coding, Pathways of attention, Mathematics for social justice, Teacher education},
abstract = {This research is part of a broader research program that explores teacher educators’ mathematical knowledge. We examine the experiences, perceptions, and needs of prospective teachers as they navigate a complex set of new and interweaving ideas for how to teach mathematics with socially relevant and responsible connections. In doing so, we draw on Mason’s (1998) perspectives about the structure of attention and awareness for mathematics teaching, to investigate the pathways of attention of middle school prospective teachers in a technology-intensive undergraduate coding course. The research findings show that teachers face challenges when they try to navigate the interdisciplinary space of mathematics, technology and societal issues (climate change) and that curiosity acts as a potential stimulus for determining how each pathway is developed and sustained.}
}
@article{20244431,
title = {The wide-reaching power of technology},
journal = {Cell},
volume = {187},
number = {17},
pages = {4431-4432},
year = {2024},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2024.07.046},
url = {https://www.sciencedirect.com/science/article/pii/S0092867424008407}
}
@article{TANG2025741,
title = {Remote sensing scene graph generation for improved retrieval based on spatial relationships},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {220},
pages = {741-752},
year = {2025},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2025.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S0924271625000127},
author = {Jiayi Tang and Xiaochong Tong and Chunping Qiu and Yuekun Sun and Haoshuai Song and Yaxian Lei and Yi Lei and Congzhou Guo},
keywords = {Remote sensing (RS) scene graph generation, Spatial relationship computation, Spatial grid, Graph neural network, RS scene retrieval},
abstract = {RS scene graphs represent RS scenes as graphs with objects as nodes and their spatial relationships as edges, playing a crucial role in understanding and interpreting RS scenes at a higher level. However, existing RS scene graph generation methods, relying on deep learning models, face limitations due to their dependence on extensive relationship labels, restricted generation accuracy, and limited generalizability. To address these challenges, we proposed a spatial relationship computing model based on prior geographic information knowledge for RS scene graph generation. We refer to the RS scene graph generated using our method as SG-SSR for short. Furthermore, we investigated the application of SG-SSR in RS scene retrieval, demonstrating improved retrieval accuracy for spatial relationships between entities. The experiments show that our scene graph generation method does not rely on relationship labels, and has higher generation accuracy and greater universality. Moreover, the retrieval method based on SG-SSR outperformed other retrieval methods based on image feature vectors, with a retrieval accuracy index 0.098 higher than the alternatives(RemoteCLIP(mask)). The dataset and code are available at https://gitee.com/tangjiayitangjiayi/sg-ssr.}
}
@article{SISTLA20212464,
title = {Evaluating the performance of nature inspired algorithms using 52-bar steel truss subjected to dynamic load},
journal = {Materials Today: Proceedings},
volume = {38},
pages = {2464-2470},
year = {2021},
note = {International Conference & Exposition on Mechanical, Material and Manufacturing Technology (ICE3MT)},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2020.07.390},
url = {https://www.sciencedirect.com/science/article/pii/S2214785320355000},
author = {Saiteja Sistla and J.S. {Kalyana Rama}},
keywords = {Moth flame optimizer, Salp Swarm Algorithm, Whale optimization algorithm, MATLAB, Steel truss},
abstract = {Linear programing problem revolutionized during the world war since it has helped the army to minimize the costs and increase the efficiency in battlefield. Since then optimization techniques have gained popularity in various fields like science & technology, biology, mathematics etc. Nature inspired algorithms mimic the nature’s behavior in order to achieve certain optimization objectives which can achieve productive results to complex problems. Classical algorithms like Genetic algorithm, Particle swarm optimization etc. are widely known but the accuracy of the solution for complex engineering problems is less. Performance evaluation of latest nature inspired algorithms i.e. Moth flame optimizer, Salp Swarm optimizer and Whale optimizer is carried out in the present study. These algorithms are proposed recently and they possess salient features like better convergence rate, avoiding the local optimum and robustness, which is the motivation behind choosing these algorithms. A 52-bar steel truss has been chosen for the present study to assess the performance of the chosen optimization techniques. The behavior of steel truss subjected to two different ground motions is also assessed using the three optimization techniques. A comparative study is done to assess the performance of the chosen techniques. MATLAB is adopted for the simulation of chosen problem statement. Based on the results it is observed that Mouth Flame Optimizer has better performance in terms of accuracy, convergence rate and computational time and is suggested for various types of mechanical and structural problems involving 52-bar trusses.}
}
@article{MYERS2021100349,
title = {Mechanistic and data-driven models of cell signaling: Tools for fundamental discovery and rational design of therapy},
journal = {Current Opinion in Systems Biology},
volume = {28},
pages = {100349},
year = {2021},
issn = {2452-3100},
doi = {https://doi.org/10.1016/j.coisb.2021.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S2452310021000342},
author = {Paul J. Myers and Sung Hyun Lee and Matthew J. Lazzara},
keywords = {Uncertainty, Sensitivity, Parameter sampling, Parameter estimation, Regression, Clustering, Classification, Cancer, Immunology},
abstract = {A full understanding of cell signaling processes requires knowledge of protein structure–function relationships, protein–protein interactions, and the abilities of pathways to control phenotypes. Computational models offer a valuable framework for integrating that knowledge to predict the effects of system perturbations and interventions in health and disease. Whereas mechanistic models are well suited for understanding the biophysical basis for signal transduction and principles of therapeutic design, data-driven models are particularly suited to distill complex signaling relationships among samples and between multivariate signaling changes and phenotypes. Both approaches have limitations and provide incomplete representations of signaling biology, but their careful implementation and integration can provide new understanding for how manipulating system variables impacts cellular decisions.}
}
@article{GEORGARA2025100388,
title = {Optimising team dynamics: The role of AI in enhancing challenge-based learning participation experience and outcomes},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100388},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100388},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000281},
author = {Athina Georgara and Marc Santolini and Olga Kokshagina and Camila Justine {Jacinta Haux} and Desmé Jacobs and Gloria Biwott and Marcela Correa and Carles Sierra and Jose Luis Fernandez-Marquez and Juan A. Rodriguez-Aguilar},
keywords = {Artificial intelligence, Challenge-based learning, Participation experience, Teamwork, Relational well-being},
abstract = {The approach of engaging students with real-world challenges to enhance collaboration and problem-solving has attracted significant interest from scholars and practitioners across diverse disciplines. Often called Challenge-Based Learning (CBL), this educational approach emphasises developing collaborative and problem-solving skills, with significant learning occurring within team settings. Prior studies highlight the influence of team composition on the efficacy of learning outcomes, pointing out that factors such as gender diversity, personality trait diversity, and a wide range of skills affect team dynamics and performance. Despite these insights, the practical organisation of these teams remains a challenge, often reliant on ad-hoc methods driven primarily by the nature of the setting at hand. Importantly, CBL is typically assessed through the final product, neglecting the impact of CBL on how the participants experience the overall process. That is, CBL is usually considered effective if the outcome is of high quality, ignoring participants' experience and participation quality. This study investigates the potential of an Artificial Intelligence team composition algorithm to improve participation quality and outcomes in collaborative CBL environments.}
}
@article{KERNER202154,
title = {Machine learning and big data provide crucial insight for future biomaterials discovery and research},
journal = {Acta Biomaterialia},
volume = {130},
pages = {54-65},
year = {2021},
issn = {1742-7061},
doi = {https://doi.org/10.1016/j.actbio.2021.05.053},
url = {https://www.sciencedirect.com/science/article/pii/S1742706121003639},
author = {Jacob Kerner and Alan Dogan and Horst {von Recum}},
keywords = {Machine learning, QSAR, QSPR, Material informatics},
abstract = {Machine learning have been widely adopted in a variety of fields including engineering, science, and medicine revolutionizing how data is collected, used, and stored. Their implementation has led to a drastic increase in the number of computational models for the prediction of various numerical, categorical, or association events given input variables. We aim to examine recent advances in the use of machine learning when applied to the biomaterial field. Specifically, quantitative structure properties relationships offer the unique ability to correlate microscale molecular descriptors to larger macroscale material properties. These new models can be broken down further into four categories: regression, classification, association, and clustering. We examine recent approaches and new uses of machine learning in the three major categories of biomaterials: metals, polymers, and ceramics for rapid property prediction and trend identification. While current research is promising, limitations in the form of lack of standardized reporting and available databases complicates the implementation of described models. Herein, we hope to provide a snapshot of the current state of the field and a beginner's guide to navigating the intersection of biomaterials research and machine learning.
Statement of significance
Machine learning and its methods have found a variety of uses beyond the field of computer science but have largely been neglected by those in realm of biomaterials. Through the use of more computational methods, biomaterials development can be expediated while reducing the need for standard trial and error methods. Within, we introduce four basic models that readers can potentially apply to their current research as well as current applications within the field. Furthermore, we hope that this article may act as a “call to action” for readers to realize and address the current lack of implementation within the biomaterials field.}
}
@article{MORA2020140,
title = {A collaborative working model for enhancing the learning process of science & engineering students},
journal = {Computers in Human Behavior},
volume = {103},
pages = {140-150},
year = {2020},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2019.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S074756321930336X},
author = {Higinio Mora and María Teresa Signes-Pont and Andrés Fuster-Guilló and María L. Pertegal-Felices},
keywords = {Higher education, ICT learning technologies, Quality assessments, Computers in human behavior, Collaborative learning, Student experiences, Collective intelligence},
abstract = {Science and engineering education are mostly based on content assimilation and development of skills. However, to adequately prepare students for today's world, it is also necessary to stimulate critical thinking and make them reflect on how to improve current practices using new tools and technologies. In this line, the main motivation of this research consists in exploring ways supported by technology to enhance the learning process of students and to better prepare them to face the challenges of today's world. To this end, the purpose of this work is to design an innovative learning project based on collaborative work among students, and research its impact in achieving better learning outcomes, generating of collective intelligence and further motivation. The proposed collaborative working model is based on peer review assessment methodology implemented through a learning web-platform. Thus, students were encouraged to peer review their classmates' works. They had to make comments, suggest improvements, and assess final assignments. Teaching staff managed and supervised the whole process. Students were selected from computer science engineering at the University of Alicante (Spain). Results suggested greater content assimilation and enhanced learning in several scientific skills. The students' final grade exceeded what any student could produce individually, but we cannot conclude that real collective intelligence was generated. Learning methodologies based on the possibilities of Information and Communication Technologies (ICT) provide new ways to transmit and manage knowledge in higher education. Collaborating in peer assessment enhances the students' motivation and promotes the active learning. In addition, this method can be very helpful and time saving for instructors in the management of large groups.}
}
@article{HAHN2020363,
title = {Argument Quality in Real World Argumentation},
journal = {Trends in Cognitive Sciences},
volume = {24},
number = {5},
pages = {363-374},
year = {2020},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2020.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S1364661320300206},
author = {Ulrike Hahn},
keywords = {argument quality, logic, probability, rationality},
abstract = {The idea of resolving dispute through the exchange of arguments and reasons has been central to society for millennia. We exchange arguments as a way of getting at the truth in contexts as diverse as science, the court room, and our everyday lives. In democracies, political decisions should be negotiated through argument, not deception, or even worse, brute force. If argument is to lead to the truth or to good decisions, then some arguments must be better than others and ‘argument strength’ must have some meaningful connection with truth. Can argument strength be measured in a way that tracks an objective relationship with truth and not just mere persuasiveness? This article describes recent developments in providing such measures.}
}
@incollection{GIOVANNONE202441,
title = {Chapter Two - Further steps towards a mechanistic functionalist framework for understanding individual differences in language and cognition},
editor = {Kara D. Federmeier},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {81},
pages = {41-73},
year = {2024},
issn = {0079-7421},
doi = {https://doi.org/10.1016/bs.plm.2024.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0079742124000380},
author = {Nikole Giovannone and Joseph C. Toscano},
keywords = {Individual differences, Cognition, Language processing, Computational modeling, Mechanistic functionalism},
abstract = {Despite a growing focus on individual differences in cognitive psychology, research in this area is complicated by several issues related to how such differences are defined and measured. These challenges create a significant roadblock for the field. To combat this issue, we argue that the next critical step for language and cognitive science is careful and thorough investigation of the specific mechanisms that drive individual differences. In this chapter, our goal is to extend the process-based mechanistic functional normativist framework and to provide a test case for how researchers can leverage computational modeling to investigate individual differences in cognitive mechanisms (using pattern learning in the serial reaction time task as an example). By shifting our focus to characterizing the mechanisms that drive individual differences in language and cognition, the field stands to advance both theoretical frameworks and methodological approaches for studying these processes.}
}
@article{LI2023106688,
title = {Machine learning assisted advanced battery thermal management system: A state-of-the-art review},
journal = {Journal of Energy Storage},
volume = {60},
pages = {106688},
year = {2023},
issn = {2352-152X},
doi = {https://doi.org/10.1016/j.est.2023.106688},
url = {https://www.sciencedirect.com/science/article/pii/S2352152X23000853},
author = {Ao Li and Jingwen Weng and Anthony Chun Yin Yuen and Wei Wang and Hengrui Liu and Eric Wai Ming Lee and Jian Wang and Sanghoon Kook and Guan Heng Yeoh},
keywords = {Battery thermal management, Thermal runaway, Mitigation, Artificial neural networks, Machine learning},
abstract = {With an increasingly wider application of the lithium-ion battery (LIB), specifically the drastic increase of electric vehicles in cosmopolitan cities, improving the thermal and fire resilience of LIB systems is inevitable. Thus, in-depth analysis and performance-based study on battery thermal management system (BTMs) design have arisen as a popular research topic in energy storage systems. Among the LIB system parameters, such as battery temperature distribution, battery heat generation rate, cooling medium properties, electrical properties, physical dimension design, etc., multi-factor design optimisation is one of the most difficult experimental tasks. Computational simulations deliver a holistic solution to the BTMs design, yet it demands an immense amount of computational power and time, which is often not practical for the design optimisation process. Therefore, machine learning (ML) models play a non-substitute role in the safety management of battery systems. ML models aid in temperature prediction and safety diagnosis, thereby assisting in the early warning of battery fire and its mitigation. In this review article, we summarise extensive lists of literature on BTMs employing ML models and identify the current state-of-the-art research, which is expected to serve as a much-needed guideline and reference for future design optimisation. Following that, the application of various ML models in battery fire diagnosis and early warning is illustrated. Finally, the authors propose improved approaches to advanced battery safety management with ML. This review paper aims to bring new insights into the application of ML in the LIB thermal safety issue and BTMs design and anticipate boosting further advanced battery system design not limited to the thermal management system, as well as proposing potential digital twin modelling for BTMs.}
}
@incollection{KADAR201425,
title = {2 - Developing a personalized and adapted curriculum for engineering education through an ambient intelligence environment},
editor = {J. {Paulo Davim}},
booktitle = {Engineering Education},
publisher = {Chandos Publishing},
address = {Oxford},
pages = {25-65},
year = {2014},
isbn = {978-1-84334-687-6},
doi = {https://doi.org/10.1533/9781780633589.25},
url = {https://www.sciencedirect.com/science/article/pii/B978184334687650002X},
author = {M. Kadar and M. Muntean and L. Marina},
keywords = {engineering education, ambient intelligence environment, brain lateralization system, adapted and personalized curriculum},
abstract = {Abstract:
This chapter describes a research model that enables students to become all that they are capable of becoming, and educators and decision makers to maximize their efforts in the field of engineering education through an ambient intelligence environment. This research proposes to translate conceptual ideas for the functionality of the environment and appliances into concrete designs. The core of the intelligent educational environment is an information system called the brain lateralization information system (BLIS). The BLIS can provide valuable information on users’ brain lateralization and students’ thinking style. Such information can be used by educators in designing new teaching methodologies that will finally lead to adapted, personalized study programs within the university curricula. The chapter shows how this approach, which has hitherto been applied to students, teaching staff and management staff from the departments of Computer Science, Applied Electronics, and Environmental Engineering of the University of Alba Iulia, was validated to allow future development of methodologies, strategies, and operational programs in the field of engineering education. In order to achieve this vision the chapter introduces a number of novel concepts and a model, in particular a new brain lateralization information system embedded into an ambient intelligent environment. Finally, the chapter reports on conclusions, recommendations and examples of adapted and personalized courseware designed for blended learning, and a user evaluation of this model, which demonstrates that users find the ambient intelligence environment useful for their career choice and easy and enjoyable to use for teachers and decision makers.}
}
@article{BOUVRY2025105081,
title = {The European master for HPC curriculum},
journal = {Journal of Parallel and Distributed Computing},
volume = {201},
pages = {105081},
year = {2025},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2025.105081},
url = {https://www.sciencedirect.com/science/article/pii/S0743731525000486},
author = {Pascal Bouvry and Mats Brorsson and Ramon Canal and Aryan Eftekhari and Siegfried Höfinger and Didier Smets and Harald Köstler and Tomáš Kozubek and Ezhilmathi Krishnasamy and Josep Llosa and Alexandra Lukas-Rother and Xavier Martorell and Dirk Pleiter and Ana Proykova and Maria-Ribera Sancho and Olaf Schenk and Cristina Silvano},
keywords = {Computing education, High-performance computing, Master in HPC, Model curricula},
abstract = {The use of High-Performance Computing (HPC) is crucial for addressing various grand challenges. While significant investments are made in digital infrastructures that comprise HPC resources, its realisation, operation, and, in particular, its use critically depends on suitably trained experts. In this paper, we present the results of an effort to design and implement a pan-European reference curriculum for a master's degree in HPC.}
}
@article{ROUSE201472,
title = {Human interaction with policy flight simulators},
journal = {Applied Ergonomics},
volume = {45},
number = {1},
pages = {72-77},
year = {2014},
note = {Systems Ergonomics/Human Factors},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2013.03.019},
url = {https://www.sciencedirect.com/science/article/pii/S0003687013000604},
author = {William B. Rouse},
keywords = {Computational modeling, Interactive visualization, Policy flight simulators},
abstract = {Policy flight simulators are designed for the purpose of exploring alternative management policies at levels ranging from individual organizations to national strategy. This article focuses on how such simulators are developed and on the nature of how people interact with these simulators. These interactions almost always involve groups of people rather than individuals, often with different stakeholders in conflict about priorities and courses of action. The ways in which these interactions are framed and conducted are discussed, as well as the nature of typical results.}
}
@article{PHAN2024,
title = {Precision synbiotics increase gut microbiome diversity and improve gastrointestinal symptoms in a pilot open-label study for autism spectrum disorder},
journal = {mSystems},
volume = {9},
number = {5},
year = {2024},
issn = {2379-5077},
doi = {https://doi.org/10.1128/msystems.00503-24},
url = {https://www.sciencedirect.com/science/article/pii/S2379507724001077},
author = {Joann Phan and Diana C. Calvo and Divya Nair and Suneer Jain and Thibaut Montagne and Summer Dietsche and Kelsey Blanchard and Shirin Treadwell and James Adams and Rosa Krajmalnik-Brown and Nicholas Chia},
keywords = {ASD, synbiotics, probiotics, prebiotics, gut microbiome, open-label, precision, supplements, metagenomics},
abstract = {ABSTRACT

The efficacy of prebiotics and probiotics (synbiotics when combined) to improve symptoms associated with autism spectrum disorder (ASD) has shown considerable inter-study variation, likely due to the complex, heterogeneous nature of the disorder and its associated behavioral, developmental, and gastrointestinal symptoms. Here, we present a precision synbiotic supplementation study in 296 children and adults diagnosed with ASD versus 123 age-matched neurotypical controls. One hundred seventy ASD participants completed the study. Baseline and post-synbiotic assessment of ASD and gastrointestinal (GI) symptoms and deep metagenomic sequencing were performed. Within the ASD cohort, there were significant differences in microbes between subpopulations based on the social responsiveness scale (SRS2) survey (Prevotella spp., Bacteroides, Fusicatenibacter, and others) and gluten and dairy-free diets (Bifidobacterium spp., Lactococcus, Streptococcus spp., and others). At the baseline, the ASD cohort maintained a lower taxonomic alpha diversity and significant differences in taxonomic composition, metabolic pathways, and gene families, with a greater proportion of potential pathogens, including Shigella, Klebsiella, and Clostridium, and lower proportions of beneficial microbes, including Faecalibacterium compared to controls. Following the 3-month synbiotic supplementation, the ASD cohort showed increased taxonomic alpha diversity, shifts in taxonomy and metabolic pathway potential, and improvements in some ASD-related symptoms, including a significant reduction in GI discomfort and overall improved language, comprehension, cognition, thinking, and speech. However, the open-label study design may include some placebo effects. In summary, we found that precision synbiotics modulated the gut microbiome and could be used as supplementation to improve gastrointestinal and ASD-related symptoms.
IMPORTANCE
Autism spectrum disorder (ASD) is prevalent in 1 out of 36 children in the United States and contributes to health, financial, and psychological burdens. Attempts to identify a gut microbiome signature of ASD have produced varied results. The limited pre-clinical and clinical population sizes have hampered the success of these trials. To understand the microbiome associated with ASD, we employed whole metagenomic shotgun sequencing to classify microbial composition and genetic functional potential. Despite being one of the most extensive ASD post-synbiotic assessment studies, the results highlight the complexity of performing such a case–control supplementation study in this population and the potential for a future therapeutic approach in ASD.
Autism spectrum disorder (ASD) is prevalent in 1 out of 36 children in the United States and contributes to health, financial, and psychological burdens. Attempts to identify a gut microbiome signature of ASD have produced varied results. The limited pre-clinical and clinical population sizes have hampered the success of these trials. To understand the microbiome associated with ASD, we employed whole metagenomic shotgun sequencing to classify microbial composition and genetic functional potential. Despite being one of the most extensive ASD post-synbiotic assessment studies, the results highlight the complexity of performing such a case–control supplementation study in this population and the potential for a future therapeutic approach in ASD.}
}
@article{VELICHKOVSKY2018227,
title = {Consciousness in a multilevel architecture: Evidence from the right side of the brain},
journal = {Consciousness and Cognition},
volume = {64},
pages = {227-239},
year = {2018},
note = {Visual Experience and Guidance of Action: A Tribute to Bruce Bridgeman},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2018.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1053810018300412},
author = {Boris M. Velichkovsky and Olga A. Krotkova and Artemy A. Kotov and Vyacheslav A. Orlov and Vitaly M. Verkhlyutov and Vadim L. Ushakov and Maxim G. Sharaev},
keywords = {Consciousness, Dynamic Causal Modeling (DCM), Resting state, Lateralization, Frontopolar cortex, Hippocampal formation, Ventrolateral prefrontal-amygdala emotional pathway, Egocentric spatial representation, Self-referential cognition, Levels of cognitive organization},
abstract = {By taking into account Bruce Bridgeman's interest in an evolutionary framing of human cognition, we examine effective (cause-and-effect) connectivity among cortical structures related to different parts of the triune phylogenetic stratification: archicortex, paleocortex and neocortex. Using resting-state functional magnetic resonance imaging data from 25 healthy subjects and spectral Dynamic Causal Modeling, we report interactions among 10 symmetrical left and right brain areas. Our results testify to general rightward and top-down biases in excitatory interactions of these structures during resting state, when self-related contemplation prevails over more objectified conceptual thinking. The right hippocampus is the only structure that shows bottom-up excitatory influences extending to the frontopolar cortex. The right ventrolateral cortex also plays a prominent role as it interacts with the majority of nodes within and between evolutionary distinct brain subdivisions. These results suggest the existence of several levels of cognitive-affective organization in the human brain and their profound lateralization.}
}
@incollection{MEY200651,
title = {Pragmatics: Overview},
editor = {Keith Brown},
booktitle = {Encyclopedia of Language & Linguistics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {51-62},
year = {2006},
isbn = {978-0-08-044854-1},
doi = {https://doi.org/10.1016/B0-08-044854-2/00306-0},
url = {https://www.sciencedirect.com/science/article/pii/B0080448542003060},
author = {J.L. Mey},
keywords = {affordance, assimilation, bestrangement, common scene, communication, community of, context, conversational maxim, cooperative principle, cotext, deixis, felicity condition, flouting, ghettoization, illocutionary force, implicature, indexical, indexing, indirect speech act, interact, institutional, placement, mandate, misunderstanding, pragmatic acts, presuppositions, relevance, semantics, situated speech, situation, placement, situational condition, social, social environment, social practice, societal stance, speech act, syntax},
abstract = {Pragmatics is the study of human language use as it is exercised in a community of social practice. The exercise is, however, not limited to verbal signs: other communicative means are also included under the definition (e.g., as becomes clear when one studies pragmatic acts in addition to the traditional speech acts). In addition, emphasis is placed on the way communication is organized, first of all in the classical models put forward by Austin, Searle, and Grice, but extending also this approach to comprise more recent advances in pragmatic thinking, especially in relation to what used to be called the hyphenated disciplines: sociolinguistics, psycholinguistics, the study of humans in interaction with computers, the teaching of first and second languages, and a host of other practically and theoretically oriented fields of study.}
}
@article{LIU2024102118,
title = {Personalized fuzzy semantic model of PHFLTS: Application to linguistic group decision making},
journal = {Information Fusion},
volume = {103},
pages = {102118},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.102118},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523004347},
author = {Yaya Liu and Lina Zhu and Rosa M. Rodríguez and Luis Martínez},
keywords = {Computing with words, Linguistic group decision making, Proportional hesitant fuzzy linguistic term set, Personalized individual semantic},
abstract = {The proportional hesitant fuzzy linguistic term set (PHFLTS) has been effectively employed in analyzing the group’s hesitancy in linguistic group decision making (LGDM). The application of PHFLTS assists in capturing the individual’s hesitancy across diverse time periods. It is acknowledged that a single word could potentially convey various meanings to different decision makers, such differences can be proficiently managed by utilizing personalized individual semantic (PIS) models. Previous approaches for calculating PIS failed to incorporate the individual’s updating preference information over time, which increases the risk that the computation of PIS is affected by random factors in a specific moment. In our current research, individual linguistic preference gathered over a time period are leveraged to form the PHFLTS. Additionally, a consistency driven optimization model based on PHFLTS is formulated to obtain PIS of linguistic terms. Subsequently, a fuzzy representation model termed as the fuzzy envelope of PHFLTS is introduced to facilitate the computation with words processes, integrating PHFLTS in LGDM. The practicality and legitimacy of these proposed models are evaluated through a comparative analysis. Lastly, these proposed models are tested and applied in a dedicated case study to further prove their usefulness and efficacy.}
}
@article{ALMOJEL2000297,
title = {The implementation and performance evaluation of N-body gravitational simulation algorithm on high-performance computers},
journal = {Computers & Electrical Engineering},
volume = {26},
number = {3},
pages = {297-316},
year = {2000},
issn = {0045-7906},
doi = {https://doi.org/10.1016/S0045-7906(99)00048-8},
url = {https://www.sciencedirect.com/science/article/pii/S0045790699000488},
author = {Abdullah I. Almojel},
keywords = {Astrophysics simulations, Dynamic load balancing, Performance analysis, MIMD machines, Barnes–Hut algorithm},
abstract = {Gravity determines the dynamical evaluation of many astrophysical systems. Each of these systems can be described as N gravitationally interacting particles. Solutions of the resulting N-body problem by direct simulation entails the calculation of O(N2) forces at each time step. To increase the number of particles that can be simulated, approximate algorithms have been developed and implemented in so-called tree codes. In these algorithms the particles are stored into a spatial hierarchy that forms a tree data structure. For a fixed level of accuracy the complexity of this algorithm is O(Nlog(N)). In this paper, a “manager–worker” model for a parallel implementation of hierarchical N-body algorithm is introduced. We describe a load-balanced, efficient algorithm for solving the Astrophysics simulation of N-body problem using tree-based data structures and massively parallel computing architectures. This algorithm, based on the Barnes–Hut method, first assembles a tree data structure that represents the distribution of bodies, or particles, at all length-scales. An adaptive load balancing technique is used to assign bodies to processors as well as to insure that processors are assigned equal amounts of work. A number of performance measurements were carried out in order to reveal the behavior of the N-body application with respect to a partitioning technique and load imbalance overhead. We also show that, with using the introduced manager–worker model and the cost zones domain decomposition technique, the algorithm is load balanced and that the majority of the time of the algorithm is spent in performing on-processor functions and not in inter-processor communications. We have conducted our study on several high-performance MIMD supercomputer machines such as the 256-processor Cray T3D and the 64-processor Intel Paragon at NASA/JPL and on the 32-processor Thinking Machines CM-5 at UMC.}
}
@article{ADAMS2010324,
title = {Why we still need a mark of the cognitive},
journal = {Cognitive Systems Research},
volume = {11},
number = {4},
pages = {324-331},
year = {2010},
note = {Special Issue on Extended Mind},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2010.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041710000331},
author = {Frederick Adams},
keywords = {Causal spread, Cognitive, Cognitive process, Computation, Constitutes, Content, Coupling, Extended mind, Information processing, Parity principle, Representation, Semantics, Think},
abstract = {What makes a process a cognitive process? I’m not just asking for a list of cognitive processes, but for what makes an item on that list a cognitive process. Why should it be on the list? This is a question that has been ignored far too long in the domain of research calling itself cognitive science. It is time to give an answer and that is what I propose in this paper. I contrast my answer with others that have been given and defend the need against some claims in the literature that a mark of the cognitive is not needed.}
}
@article{GOMOLLONBEL20245056,
title = {Connecting chemical worlds for a sustainable future},
journal = {Chemical Science},
volume = {15},
number = {14},
pages = {5056-5060},
year = {2024},
issn = {2041-6520},
doi = {https://doi.org/10.1039/d3sc06815c},
url = {https://www.sciencedirect.com/science/article/pii/S2041652024004267},
author = {Fernando Gomollón-Bel and Javier García-Martínez},
abstract = {Chemistry plays a central role in science and is the basis of one of the major, more impactful, and diverse industries. However, to address the most pressing global challenges, we must learn to create connections in an effective and meaningful way, with other disciplines, industries, and society at large. Here, we present the IUPAC Top Ten Emerging Technologies in Chemistry as an example of an initiative that highlights the value of the most promising advances in chemistry and contributes to creating connections to accelerate sustainable solutions for our society and our planet.}
}
@article{BARON2019319,
title = {Machine Learning and Other Emerging Decision Support Tools},
journal = {Clinics in Laboratory Medicine},
volume = {39},
number = {2},
pages = {319-331},
year = {2019},
note = {Clinical Decision Support: Tools, Strategies, and Emerging Technologies},
issn = {0272-2712},
doi = {https://doi.org/10.1016/j.cll.2019.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S0272271219300101},
author = {Jason M. Baron and Danielle E. Kurant and Anand S. Dighe},
keywords = {Machine learning, Clinical decision support, Artificial intelligence, Knowledge discovery, Computational pathology}
}
@article{DEMPSTER1990261,
title = {Causality and statistics},
journal = {Journal of Statistical Planning and Inference},
volume = {25},
number = {3},
pages = {261-278},
year = {1990},
issn = {0378-3758},
doi = {https://doi.org/10.1016/0378-3758(90)90076-7},
url = {https://www.sciencedirect.com/science/article/pii/0378375890900767},
author = {A.P. Dempster},
keywords = {Causal inference, causal effects, subjectivity, objectivity, randomization, experimentation.},
abstract = {Many aspects of statistical design, modelling, and inference have close and important connections with causal thinking. These are analyzed in the paper against a philosophical background that regards formal mathematical models as having dual interpretations, reflecting both objectivist reality and subjectivist rationality. The latter aspect weakens the need for an objective theory of probabilistic causation, and suggests that a traditional image of causes as deterministic mechanisms should remain primary. It is argued that such causes should guide much preformal thinking about what to include in formal statistical models, especially of dynamic phenomena. The statistical measurement of causal effects is facilitated by good statistical design, including randomization where feasible, and requires other methodologies for controlling and assessing uncertainties, for example in model construction and inference. Illustrative examples include case studies where the problem is to assess retrospectively the causes of observed events and where the task is to assess future risks from controllable factors.}
}
@article{LI20241035,
title = {Current status and construction scheme of smart geothermal field technology},
journal = {Petroleum Exploration and Development},
volume = {51},
number = {4},
pages = {1035-1048},
year = {2024},
issn = {1876-3804},
doi = {https://doi.org/10.1016/S1876-3804(24)60523-9},
url = {https://www.sciencedirect.com/science/article/pii/S1876380424605239},
author = {Gensheng LI and Xianzhi SONG and Yu SHI and Gaosheng WANG and Zhongwei HUANG},
keywords = {smart geothermal field, intelligent development of geothermal reservoirs, application scenario, intelligent characterization, intelligent simulation, intelligent optimization control, smart management},
abstract = {To address the key problems in the application of intelligent technology in geothermal development, smart application scenarios for geothermal development are constructed. The research status and existing challenges of intelligent technology in each scenario are analyzed, and the construction scheme of smart geothermal field system is proposed. The smart geothermal field is an organic integration of geothermal development engineering and advanced technologies such as the artificial intelligence. At present, the technology of smart geothermal field is still in the exploratory stage. It has been tested for application in scenarios such as intelligent characterization of geothermal reservoirs, dynamic intelligent simulation of geothermal reservoirs, intelligent optimization of development schemes and smart management of geothermal development. However, it still faces many problems, including the high computational cost, difficult real-time response, multiple solutions and strong model dependence, difficult real-time optimization of dynamic multi-constraints, and deep integration of multi-source data. The construction scheme of smart geothermal field system is proposed, which consists of modules including the full database, intelligent characterization, intelligent simulation and intelligent optimization control. The connection between modules is established through the data transmission and the model interaction. In the next stage, it is necessary to focus on the basic theories and key technologies in each module of the smart geothermal field system, to accelerate the lifecycle intelligent transformation of the geothermal development and utilization, and to promote the intelligent, stable, long-term, optimal and safe production of geothermal resources.}
}