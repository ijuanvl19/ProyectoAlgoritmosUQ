@article{SIGAYRET2022104505,
title = {Unplugged or plugged-in programming learning: A comparative experimental study},
journal = {Computers & Education},
volume = {184},
pages = {104505},
year = {2022},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2022.104505},
url = {https://www.sciencedirect.com/science/article/pii/S0360131522000768},
author = {Kevin Sigayret and André Tricot and Nathalie Blanc},
keywords = {Elementary education, Improving classroom teaching, Programming and programming languages, Teaching/learning strategies},
abstract = {In recent years, computer programming has reappeared in school curricula with the aim of transmitting knowledge and skills beyond the simple ability to code. However, there are different ways of teaching this subject and very few experimental studies compare plugged-in and unplugged programming learning. The purpose of this study is to highlight the impact of plugged-in or unplugged learning on students' performance and subjective experience. To this end, we designed an experimental study with 217 primary school students divided into two groups and we measured their knowledge of computational concepts, ability to solve algorithmic problem, motivation toward the instruction, self-belief and attitude toward science. The programming sessions were designed to be similar between the two conditions, only the tools were different. Computers and Scratch software were used in the plugged-in group while the unplugged group used paper instructions, pictures, figurines and body movements instead. The results show better learning performance in the plugged-in group. Furthermore, although motivation dropped slightly in both groups, this drop was only significant in the unplugged condition. Gender also seems to be an important factor, as girls exhibit a lower post-test motivation and a lower willingness to pursue their practice in programming outside the school context. However, this effect on motivation was only observable in the plugged-in group which suggests that educational programming software may have a positive but gendered motivational impact.}
}
@article{RODRIGUEZJORDA2025101702,
title = {Linguistic relativity from an enactive perspective: the entanglement of language and cognition},
journal = {Language Sciences},
volume = {108},
pages = {101702},
year = {2025},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2024.101702},
url = {https://www.sciencedirect.com/science/article/pii/S0388000124000913},
author = {Ulises {Rodríguez Jordá} and Ezequiel A. {Di Paolo}},
keywords = {Linguistic relativity, Enactive approach, Modularity, Post-cognitivism, Languaging},
abstract = {We seek to relate the fields of linguistic relativity (LR) and the enactive approach in cognitive science. We distinguish contemporary research on LR, starting after the mid-1990s, from earlier approaches to the field. Current studies are characterised by a nuanced methodology rooted in the psycholinguistics tradition. While improving on earlier research, they also move away from philosophically oriented discussions about the relation between language and cognition and focus instead on experimentally testing relativistic effects for specific cognitive domains. We claim that this procedure retains some fundamental assumptions from classical cognitive science, precisely those that are challenged by an enactive perspective. These include a commitment to the modularity of mind and a computational understanding of the interactions between cognitive domains. We contend that contemporary LR research is, in fact, compatible with these classical cognitivist ideas, despite superficial points of tension. We then survey recent post-cognitivist approaches to language in cognitive science and explore ways in which LR and the enactive framework could be mutually enriched. Whereas the structural or categorial aspects of language are central for LR research, these are usually downplayed in post-cognitivist approaches, often influenced by the integrationist distinction between first-order linguistic practices and second-order constructs. We advance a specifically enactive perspective that seeks to preserve the systematic features of language while also integrating them within a dynamical understanding of the relation between language and cognition at multiple timescales.}
}
@article{EDELMANN20181,
title = {Formal studies of culture: Issues, challenges, and current trends},
journal = {Poetics},
volume = {68},
pages = {1-9},
year = {2018},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2018.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X18301323},
author = {Achim Edelmann and John W. Mohr},
keywords = {Formal study of culture, Cultural matrix approach, Measuring duality, Formalist theorization of culture, Computational hermeneutics},
abstract = {Over the last two decades, the formal study of culture has grown into one of the most exciting, systematic, and dynamic sub-fields in sociology. In this essay, we take stock of recent developments in this field. We highlight four emerging themes: (1) the maturation of the field that has occurred over the last two decades, (2) the rise and formalization of the “cultural matrix” approach to studying culture, (3) the development of various efforts to advance a more formal theory of culture, and (4) the proliferation of Big Data and the development of new kinds of quantitative and computational approaches to the study of culture, including the emergence of a new area focused on “computational hermeneutics.” We conclude by discussing future opportunities, challenges, and questions in formalizing culture.}
}
@article{DEY2016177,
title = {A probabilistic approach to diagnose faults of air handling units in buildings},
journal = {Energy and Buildings},
volume = {130},
pages = {177-187},
year = {2016},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2016.08.017},
url = {https://www.sciencedirect.com/science/article/pii/S0378778816306958},
author = {Debashis Dey and Bing Dong},
keywords = {Air Handling Unit, Bayesian belief network, APAR rules, Fault detection and diagnosis},
abstract = {Air handling unit (AHU) is one of the most extensively used equipment in large commercial buildings. This device is typically customized and lacks quality system integration which can result in hardwire failures and control errors. Air handling unit Performance Assessment Rules (APAR) is a fault detection tool that uses a set of expert rules derived from mass and energy balances to detect faults in air handling units. APAR is computationally simple enough that it can be embedded in commercial building automation and control systems and relies only upon sensor data and control signals that are commonly available in these systems. Although APAR has advantages over other methods, for example no training data required and easy to implement commercially, most of the time it is unable to provide the root diagnosis of the faults. For instance, a fault on temperature sensor could be bias, drifting bias, inappropriate location, or complete failure. In addition a fault in mixing box can be return and/or outdoor damper leak or stuck. In addition, when multiple rules are satisfied, the list of faults increases. There is no proper way to have the correct diagnosis for rule based fault detection system. To overcome this limitation, we proposed Bayesian Belief Network (BBN) as a diagnostic tool. BBN can be used to simulate diagnostic thinking of FDD experts through a probabilistic way. In this study we developed a new way to detect and diagnose faults in AHU through combining APAR rules and Bayesian Belief network. Bayesian Belief Network is used as a decision support tool for rule based expert system. BBN is highly capable to prioritize faults when multiple rules are satisfied simultaneously. Also it can get information from previous AHU operating conditions and maintenance records to provide proper diagnosis. The proposed model is validated with real time measured data of a campus building. The results show that BBN correctly prioritize faults that are verified by manual investigation.}
}
@article{MATINFAR2025100304,
title = {Unmasking the brain in cocaine use disorder: A deep learning approach with graph convolutional networks and principal component analysis},
journal = {Next Research},
volume = {2},
number = {2},
pages = {100304},
year = {2025},
issn = {3050-4759},
doi = {https://doi.org/10.1016/j.nexres.2025.100304},
url = {https://www.sciencedirect.com/science/article/pii/S3050475925001757},
author = {Mohammad-Mehdi Matinfar and Mozafar Bag-Mohammdi and Mojtaba Karami},
keywords = {Deep learning, Cocaine use disorder, Graph conventional network, Principal component analysis, fMRI analysis},
abstract = {The SUDMEX CONN dataset, comprising extensive brain imaging data and demographic information, serves as a valuable resource for exploring the neurobiological mechanisms underlying Cocaine Use Disorder (CUD). In this study, we employed Graph Convolutional Networks (GCNs) to diagnose CUD using resting-state functional magnetic resonance imaging (rs-fMRI) data. Kernel Principal Component Analysis (PCA) was utilized for dimensionality reduction and enhancing computational efficiency. We construct brain graphs from the augmented rs-fMRI images and use GCN for the classification task. The simulation results demonstrate that the GCN model, trained on the SUDMEX CONN dataset, can accurately distinguish between CUD patients and healthy controls (HC) based on brain connectivity patterns. Our approach provides valuable insights into the underlying neurobiological mechanisms of CUD and highlights the potential of innovative tools for understanding and treating this chronic disorder. Ablation studies confirmed kernel PCA is highly efficient in reducing the dataset dimensionality and accelerating the simulation time. The simulation results revealed that the combined approach of GCN and kernel PCA could achieve an impressive accuracy of 98.25 % in diagnosing CUD cases. Our implementation codes are available at https://github.com/MehdiMatinfar/GPCA.}
}
@article{ZHOU2022100001,
title = {Science in One Health: A new journal with a new approach},
journal = {Science in One Health},
volume = {1},
pages = {100001},
year = {2022},
issn = {2949-7043},
doi = {https://doi.org/10.1016/j.soh.2022.100001},
url = {https://www.sciencedirect.com/science/article/pii/S2949704322000014},
author = {Xiao-Nong Zhou and Marcel Tanner},
keywords = {One Health, Human health, Animal health, Ecosystem health, Research and implementation science},
abstract = {One Health recognizes the close links and interdependence among human health, animal health and environmental health. With the pandemic of COVID-19 and the risk of many emerging or reemerging infectious diseases of zoonotic nature as well as the spreading antimicrobial resistance, One Health has become one of top concerns globally, as it entails the essential global public health challenges from antimicrobial resistance over zoonoses, to climate change, food security and societal well-being. Research priorities in One Health include the study on interactions of human-animal-plants-nature ecology interface, systems thinking, integrated surveillance and response systems, and the overall One Health governance as part of the global health and sustainability governance. The now launched journal, Science in One Health, aims to be a resource platform that disseminates scientific evidence, knowledge, and tools on the One Health approaches and respective possible socio-ecological interventions. Thus, aims at providing fruitful exchanges of information and experience among researchers, and decision makers as well as public health actors.}
}
@article{EDELSON2021100986,
title = {How fuzzy-trace theory predicts development of risky decision making, with novel extensions to culture and reward sensitivity},
journal = {Developmental Review},
volume = {62},
pages = {100986},
year = {2021},
issn = {0273-2297},
doi = {https://doi.org/10.1016/j.dr.2021.100986},
url = {https://www.sciencedirect.com/science/article/pii/S0273229721000411},
author = {Sarah M. Edelson and Valerie F. Reyna},
keywords = {Risk-taking, Risky decision making, Reward sensitivity, COVID-19, Fuzzy-trace theory, Adolescence},
abstract = {Comprehensive meta-analyses of risky decision making in children, adolescents, and adults have revealed that age trends in disambiguated laboratory tasks confirmed fuzzy-trace theory’s prediction that preference for risk decreases monotonically from childhood to adulthood. These findings are contrary to predictions of dual systems or neurobiological imbalance models. Assumptions about increasing developmental reliance on mental representations of the gist of risky options are essential to account for this developmental trend. However, dual systems theory appropriately emphasizes how cultural context changes behavioral manifestation of risk preferences across age and neurobiological imbalance models appropriately emphasize developmental changes in reward sensitivity. All of the major theories include the assumption of increasing behavioral inhibition. Here, we integrate these theoretical constructs—representation, cultural context, reward sensitivity, and behavioral inhibition—to provide a novel framework for understanding and improving risky decision making in youth. We also discuss the roles of critical tests, scientific falsification, disambiguating assessments of psychological and neurological processes, and the misuse of such concepts as ecological validity and reverse inference. We illustrate these concepts by extending fuzzy-trace theory to explain why youth are a major conduit of viral infections, including the virus that causes COVID-19. We conclude by encouraging behavioral scientists to embrace new ways of thinking about risky decision making that go beyond traditional stereotypes about adolescents and that go beyond conceptualizing ideal decision making as trading off degrees of risk and reward.}
}
@article{CHASTAIN200083,
title = {Cultivating design competence: online support for beginning design studio},
journal = {Automation in Construction},
volume = {9},
number = {1},
pages = {83-91},
year = {2000},
issn = {0926-5805},
doi = {https://doi.org/10.1016/S0926-5805(99)00053-9},
url = {https://www.sciencedirect.com/science/article/pii/S0926580599000539},
author = {Thomas Chastain and Ame Elliott},
abstract = {A primary lesson of a beginning design studio is the development of a fundamental design competence. This entails acquiring skills of integration, projection, exploration, as well as critical thinking—forming the basis of thinking “like a designer”. Plaguing the beginning architectural design student as she develops this competence are three typical problems: a lagging visual intelligence, a linking of originality with creativity, and the belief that design is an act of an individual author instead of a collaborative activity. We believe that computation support for design learning has particular attributes for helping students overcome these problems. These attributes include its inherent qualities for visualization, for explicitness, and for sharing. This paper describes five interactive multi-media exercises exploiting these attributes which were developed to support a beginning design studio. The paper also reports how they have been integrated into the course curriculum. Le développement des compétences en design: support on-line pour le studio de design élémentaire Une des premières leçons lors du studio de design est le développement d’une compétence fondamentale en conception. Ceci implique l’acquisition des habiletés d’intégration, de projection, d’exploration ainsi que la pensée critique—antérieurement les bases de la façon de penser nommée “comme un concepteur”. Il y a trois problèmes fondamentaux qui pèsent sur l’étudiant débutant en architecture lors du développement cette compétence: une intelligence visuelle insuffisante, le fait de lier l’originalité à la créativité, et la croyance que le processus de conception est une activité individuelle, plutôt que collaborative. Nous sommes de l’avis que le soutien en informatique lors de l’apprentissage de la conception architecturale posséde des attributs bien particuliers pour aider les étudiants à surmonter ces difficultés. Ces attributs comprennent des qualités inhérentes pour la visualisation, pour être explicite, et pour le partage. Ce papier décrit cinq exercices de médias interactifs qui exploitent ces attributs, et qui ont été développés pour supporter un studio de design élémentaire. Il présente aussi un reportage sur la façon dont ces exercices ont été intégrés dans le curriculum du cours.}
}
@incollection{CARPENDALE2013125,
title = {Chapter Six - A Relational Developmental Systems Approach to Moral Development},
editor = {Richard M. Lerner and Janette B. Benson},
series = {Advances in Child Development and Behavior},
publisher = {JAI},
volume = {45},
pages = {125-153},
year = {2013},
booktitle = {Embodiment and Epigenesis: Theoretical and Methodological Issues in Understanding the Role of Biology within the Relational Developmental System},
issn = {0065-2407},
doi = {https://doi.org/10.1016/B978-0-12-397946-9.00006-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780123979469000063},
author = {Jeremy I.M. Carpendale and Stuart I. Hammond and Sherrie Atwood},
keywords = {Developmental systems theory, Moral development, Moral norms, Nativism, Social interaction},
abstract = {Morality and cooperation are central to human life. Psychological explanations for moral development and cooperative behavior will have biological and evolutionary dimensions, but they can differ radically in their approach to biology. In particular, many recent proposals have pursued the view that aspects of morality are innate. We briefly review and critique two of these claims. In contrast to these nativist assumptions about the role of biology in morality, we present an alternative approach based on a relational developmental systems view of moral development. The role for biology in this approach is in setting up the conditions—the developmental system—in which forms of interaction and later forms of thinking emerge.}
}
@article{CHANG2023101823,
title = {Stakeholder requirement evaluation of smart industrial service ecosystem under Pythagorean fuzzy environment for complex industrial contexts: A case study of renewable energy park},
journal = {Advanced Engineering Informatics},
volume = {55},
pages = {101823},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101823},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622002816},
author = {Yuan Chang and Xinguo Ming and Zhihua Chen and Tongtong Zhou and Xiaoqiang Liao and Wenyan Song},
keywords = {Smart industrial product-service system (IPS), Requirement evaluation, Service ecosystem, Pythagorean fuzzy sets, Multi-criteria decision making, Viable systems model},
abstract = {This study focuses on ways to systematically evaluate stakeholder requirements when developing a smart industrial service ecosystem (SISE) in a complex industrial context. The SISE development requires considering the service requirement from both the complex industrial context and service ecosystem manners. This study proposes a systematic framework for stakeholder requirement evaluation in SISE. The first part of the framework is the industrial context-viable system model with ecological thinking (IC-VESM) to elicit the service requirements for the SISE, which facilitates a systematic analysis of the service value proposition and service requirement elicitation in the operational lifecycle of an entire industrial context. This second part of the framework proposes a method for evaluating service requirements that is both feasible and systematic. This is achieved by combining the Fuzzy Kano and AHP methods in a Pythagorean fuzzy (PF) environment. The PF Kano computes the categories and determines the weights of service requirements from a consumer perspective, while the PF AHP hierarchically analyzes the service requirements and provides pairwise comparison paths for design experts. Finally, an illustrative case study in a renewable energy context was used to demonstrate the feasibility and effectiveness of the methodology. The proposed theoretical model provides more reliable and systematic outcomes than traditional methods when eliciting service requirements and evaluating complex smart industrial service solutions. The study has practical implications by providing useful insights for companies to recognize key smart service requirements in complex industrial contexts and to improve sustainable development.}
}
@article{WISE2024144,
title = {Naturalistic reinforcement learning},
journal = {Trends in Cognitive Sciences},
volume = {28},
number = {2},
pages = {144-158},
year = {2024},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2023.08.016},
url = {https://www.sciencedirect.com/science/article/pii/S1364661323002127},
author = {Toby Wise and Kara Emery and Angela Radulescu},
keywords = {reinforcement learning, decision-making, naturalistic, computational modeling},
abstract = {Humans possess a remarkable ability to make decisions within real-world environments that are expansive, complex, and multidimensional. Human cognitive computational neuroscience has sought to exploit reinforcement learning (RL) as a framework within which to explain human decision-making, often focusing on constrained, artificial experimental tasks. In this article, we review recent efforts that use naturalistic approaches to determine how humans make decisions in complex environments that better approximate the real world, providing a clearer picture of how humans navigate the challenges posed by real-world decisions. These studies purposely embed elements of naturalistic complexity within experimental paradigms, rather than focusing on simplification, generating insights into the processes that likely underpin humans’ ability to navigate complex, multidimensional real-world environments so successfully.}
}
@article{CALUDE2023844,
title = {What perceptron neural networks are (not) good for?},
journal = {Information Sciences},
volume = {621},
pages = {844-857},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.11.083},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522013743},
author = {Cristian S. Calude and Shahrokh Heidari and Joseph Sifakis},
keywords = {Perceptrons, Sensitive and robust functions, Quantum computing},
abstract = {Perceptron Neural Networks (PNNs) are essential components of intelligent systems because they produce efficient solutions to problems of overwhelming complexity for conventional computing methods. Many papers show that PNNs can approximate a wide variety of functions, but comparatively, very few discuss their limitations and the scope of this paper. To this aim, we define two classes of Boolean functions – sensitive and robust –, and prove that an exponentially large set of sensitive functions are exponentially difficult to compute by multi-layer PNNs (hence incomputable by single-layer PNNs). A comparatively large set of functions in the second one, but not all, are computable by single-layer PNNs. Finally, we used polynomial threshold PNNs to compute all Boolean functions with quantum annealing and present in detail a QUBO computation on the D-Wave Advantage. These results confirm that the successes of PNNs, or lack of them, are in part determined by properties of the learned data sets and suggest that sensitive functions may not be (efficiently) computed by PNNs.}
}
@article{DANAHY2001127,
title = {Technology for dynamic viewing and peripheral vision in landscape visualization},
journal = {Landscape and Urban Planning},
volume = {54},
number = {1},
pages = {127-138},
year = {2001},
note = {Our Visual Landscape: analysis, modeling, visualization and protection},
issn = {0169-2046},
doi = {https://doi.org/10.1016/S0169-2046(01)00131-1},
url = {https://www.sciencedirect.com/science/article/pii/S0169204601001311},
author = {John W. Danahy},
keywords = {Visualization, Real-time immersive virtual reality, Panorama, Peripheral vision, Foveal vision, Dynamic viewing},
abstract = {The dynamic qualities of looking around and moving about, directly sensing spatial queues, using one’s peripheral vision, and focusing with foveal vision on objects of attention are fundamental to a person’s visual experience in landscape. Unfortunately, the visual media commonly used to structure scientific analysis, professional design, decision-making and artistic interpretation of visual landscapes are quite weak at portraying the dynamic and peripheral dimensions of human vision. Also, visual media whether it be manual drawing, photomontage or state-of-the-art computer animation tend to be time consuming and difficult to apply to these dimensions of seeing. The absence of a convenient, cost-effective means for showing all the fundamental visual aspects of landscape in a balanced way is a serious limitation. This deficiency begs the following questions. Is the current state of knowledge in visual landscape management biased by the relative ease with which established media, such as illustration, photography, and photo-realistic rendering can be used? Do the characteristics of these media bias our perception and thinking about landscape toward static foveal aspects of visual experience? Are our ideas about dynamic viewing and computer animation limited by the didactic frame-by-frame approach characteristic of cinematography and video? Can the introduction of equally robust tools and methods for dynamic and peripheral viewing balance any bias caused by current visualization technology? If McLuhan’s insights about media are correct, then we need to do more research on this question. This paper suggests that the field of landscape visualization needs to develop instruments for research that more fully capture the fundamental components of human vision before we can properly study the question or advance practice. It outlines some ways the next generation of visualization technology can be used to balance the disproportionate emphasis on foveal ways of visual thinking commonly used in the past for the study of visual landscapes. The paper explains this deficiency and proposes some area for research and development of visualization instruments more capable of redressing this imbalance. The paper outlines this issue and proposes that as electronic media and computational media become more developed and are applied to the realm of visual concerns, it will become more practical to include peripheral vision and dynamic viewing in deliberations about visual landscapes. This paper reflects on the potential of visualization automation techniques to overcome these shortcomings through illustrations of project work using innovative software tools developed to explore this question at the Centre for Landscape Research (CLR) at the University of Toronto.}
}
@article{GREENE201766,
title = {The rat-a-gorical imperative: Moral intuition and the limits of affective learning},
journal = {Cognition},
volume = {167},
pages = {66-77},
year = {2017},
note = {Moral Learning},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2017.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0010027717300690},
author = {Joshua D. Greene},
keywords = {Deontology, Utilitarianism, Consequentialism, Reinforcement learning, Model-free learning, Machine learning, Ethics, Normative ethics, Moral judgment},
abstract = {Decades of psychological research have demonstrated that intuitive judgments are often unreliable, thanks to their inflexible reliance on limited information (Kahneman, 2003, 2011). Research on the computational underpinnings of learning, however, indicates that intuitions may be acquired by sophisticated learning mechanisms that are highly sensitive and integrative. With this in mind, Railton (2014) urges a more optimistic view of moral intuition. Is such optimism warranted? Elsewhere (Greene, 2013) I’ve argued that moral intuitions offer reasonably good advice concerning the give-and-take of everyday social life, addressing the basic problem of cooperation within a “tribe” (“Me vs. Us”), but that moral intuitions offer unreliable advice concerning disagreements between tribes with competing interests and values (“Us vs. Them”). Here I argue that a computational perspective on moral learning underscores these conclusions. The acquisition of good moral intuitions requires both good (representative) data and good (value-aligned) training. In the case of inter-tribal disagreement (public moral controversy), the problem of bad training looms large, as training processes may simply reinforce tribal differences. With respect to moral philosophy and the paradoxical problems it addresses, the problem of bad data looms large, as theorists seek principles that minimize counter-intuitive implications, not only in typical real-world cases, but in unusual, often hypothetical, cases such as some trolley dilemmas. In such cases the prevailing real-world relationships between actions and consequences are severed or reversed, yielding intuitions that give the right answers to the wrong questions. Such intuitions—which we may experience as the voice of duty or virtue—may simply reflect the computational limitations inherent in affective learning. I conclude, in optimistic agreement with Railton, that progress in moral philosophy depends on our having a better understanding of the mechanisms behind our moral intuitions.}
}
@article{ARMSTRONG20161,
title = {A NIT-picking analysis: Abstractness dependence of subtests correlated to their Flynn effect magnitudes},
journal = {Intelligence},
volume = {57},
pages = {1-6},
year = {2016},
issn = {0160-2896},
doi = {https://doi.org/10.1016/j.intell.2016.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S0160289616300812},
author = {Elijah L. Armstrong and Jan {te Nijenhuis} and Michael A. {Woodley of Menie} and Heitor B.F. Fernandes and Olev Must and Aasa Must},
keywords = {Abstract thinking, Flynn effect, Intelligence, National Intelligence Test, Estonia, g loading},
abstract = {We examine the association between the strength of the Flynn effect in Estonia and highly convergent panel-ratings of the ‘abstractness’ of nine subtests on the National Intelligence Test, in order to test the theory that the Flynn effect results in part from an increase in the use of abstract reference frames in solving cognitive problems. The vectors of abstractness ratings and Flynn effect gains, controlled for guessing) exhibit a near-zero correlation (r=−.02); however, abstractness correlates positively with (and is therefore confounded by) g-loadings (r=.61). A General Linear Model is used to determine the degree to which the abstractness vector predicts the Flynn effect vector, independently of subtest g-loadings and the portion of the secular IQ gain due to guessing (the Brand effect). Consistent with the abstract reasoning model of the Flynn effect, abstractness positively predicts Flynn effect magnitudes, once controlled for confounds (sr=.44), which indicates an increasing tendency to utilize factors external to the items in order to abstract their solutions.}
}
@article{TALANOV201641,
title = {Emotional simulations and depression diagnostics},
journal = {Biologically Inspired Cognitive Architectures},
volume = {18},
pages = {41-50},
year = {2016},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2016.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X16300676},
author = {Max Talanov and Jordi Vallverdú and Bin Hu and Philip Moore and Alexander Toschev and Diana Shatunova and Anzhela Maganova and Denis Sedlenko and Alexey Leukhin},
keywords = {Dopamine, Serotonin, Fear, Artificial intelligence, Simulation, Rat brain, Affective computing, Emotion modelling, Neuromodulation},
abstract = {In this work we propose the following hypothesis: the neuromodulatory mechanisms that control the emotional states of mammals can be translated and re-implemented in a computer by controlling the computational performance of a hosted computational system. In our specific implementation, we represent the simulation of the ‘fear-like’ state based on the three dimensional neuromodulatory model of affects, in this paper ‘affects’ refer to the basic emotional inborn states, inherited from works of Hugo Lövheim. Whilst dopamine controls attention, serotonin is the key for inhibition, and fear is a elicitator for inhibitory and protective processes. This inhibition can promote [in a cognitive system] to blocking behaviour which can be labelled as ’depression’. Therefore, our interest is how to reimplement biomimetically both action-regulators without the computational system to resulting in a ‘failed’ scenario. We have simulated 1000ms of the dopamine system using NEST Neural Simulation Tool with the rat brain as the model. The results of the simulation experiments are reported with an evaluation to demonstrate the correctness of our hypothesis.}
}
@incollection{VOINOV201733,
title = {Participatory Modeling for Sustainability},
editor = {Martin A. Abraham},
booktitle = {Encyclopedia of Sustainable Technologies},
publisher = {Elsevier},
address = {Oxford},
pages = {33-39},
year = {2017},
isbn = {978-0-12-804792-7},
doi = {https://doi.org/10.1016/B978-0-12-409548-9.10532-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780124095489105329},
author = {Alexey Voinov},
keywords = {Biases, Modeling process, Social media, Stakeholders, Wicked problem},
abstract = {Sustainability is a wicked problem, which is hard to define in a unique way. It cannot be solved and should be treated in a participatory approach involving as many stakeholders in the process as possible. Participatory modeling is an efficient method for dealing with wicked problems. It involves stakeholders in an open-ended process of shared learning and can be essential for developing sustainable technologies. While there may be various levels of participation, the process evolves around a model of the system at stake. The model is built in interaction with the stakeholders; it provides formalism to synchronize stakeholder thinking and knowledge about the system and to move toward consensus about the possible decision making.}
}
@article{MIHAI20221082,
title = {Multimodal emotion detection from multiple data streams for improved decision making},
journal = {Procedia Computer Science},
volume = {214},
pages = {1082-1089},
year = {2022},
note = {9th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.281},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922019937},
author = {Neghina Mihai and Matei Alexandru and Zamfirescu Bala-Constantin},
keywords = {emotion detection, sensor fusion, multimodal, affect},
abstract = {Recent neurological studies shows that emotions are tightly connected to the thinking and cognitive actions, being part of the decision-making process. Considering this, having a way to help decision making processes based on current emotion of the user or to consider the potential emotional impact if a decision is made, would be beneficial. This paper introduces a novel method for fusing multiple emotional signals, using a weighted average, where each weight value adapts to real time conditions, based on signal type, presence, and quality. In the context of a training station for manual operation, we implemented and tested separately several emotion detection methods, each based on a different signal acquired from audio, video, and galvanic skin response data streams. The final goal is to include the proposed method together with state of the art emotion detection machine learning algorithms as part of the digital twin training station for manual operation.}
}
@article{SPARAPANI2023102186,
title = {Factors associated with classroom participation in preschool through third grade learners on the autism spectrum},
journal = {Research in Autism Spectrum Disorders},
volume = {105},
pages = {102186},
year = {2023},
issn = {1750-9467},
doi = {https://doi.org/10.1016/j.rasd.2023.102186},
url = {https://www.sciencedirect.com/science/article/pii/S1750946723000867},
author = {Nicole Sparapani and Nancy Tseng and Laurel Towers and Sandy Birkeneder and Sana Karimi and Cameron J. Alexander and Johanna Vega Garcia and Taffeta Wood and Amanda Dimachkie Nunnally},
keywords = {Autism, Instructional opportunities, Mathematical tasks, Teacher language, Active engagement, Spontaneous communication},
abstract = {Background
Access to mathematics instruction that involves opportunities for critical thinking and procedural fluency promotes mathematics learning. Studies have outlined effective strategies for teaching mathematics to children on the autism spectrum, however, the focus of these interventions often represent a narrow set of mathematical skills and concepts centered on procedural learning without linking ideas to underlying concepts.
Methods
This study utilized classroom video observations to evaluate the variability in and nature of mathematical learning opportunities presented to 76 autistic students within 49 preschool–3rd grade general and special education learning contexts. We examined teacher instructional practices and student participation across 109 mathematical tasks within larger mathematics lessons.
Results
Students were most often presented with mathematical tasks that required low-level cognitive demand, such as tasks focusing on rote memorization and practicing predetermined steps to solve basic algorithms. Furthermore, the nature of the mathematical task was linked with the language that teachers used, and this in turn, was associated with students’ participation within the learning opportunity.
Conclusions
Our findings indicate that features of talk within specific types of mathematical tasks, including math-related talk and responsive language, were associated with increased student active engagement and spontaneous communication. The knowledge gained from this study contributes to the development of optimized instructional practices for school-aged children on the autism spectrum—information that could be used to prepare both preservice and in-service teachers.}
}
@article{XU2024102430,
title = {A temporal approach to online discussion during disasters: Applying SIR infectious disease model to predict topic growth and examining effects of temporal distance},
journal = {Public Relations Review},
volume = {50},
number = {2},
pages = {102430},
year = {2024},
issn = {0363-8111},
doi = {https://doi.org/10.1016/j.pubrev.2024.102430},
url = {https://www.sciencedirect.com/science/article/pii/S0363811124000092},
author = {Sifan Xu and Xinyan Zhao and Jie Chen},
keywords = {Disaster, SIR, Computational modeling, SIR model, Twitter big data, Climate change, Topic growth, Construal level},
abstract = {Discussions on social media during major disasters are robust and often have multiple frames of reference. Temporal perspectives, however, are still lacking in current understandings of social-mediated discussions during disasters and crises, but incorporating temporal perspectives can significantly enhance environmental scanning efforts as prescribed in the issues management framework. The purpose of the current research is twofold: to apply and validate the SIR (Susceptible-Infectious-Recovered) model to examine topics’ growth over time on social media and to understand how future orientation of social media users (an indicator of temporal distance) affects their construal of a disaster through supervised machine learning. We based our analysis on Twitter discussions during the Texas winter storm in 2021. Results of the study show great fit of the SIR model for topic growth, and that temporal distance affects users’ construal of the event in line with core predictions of construal level theory. Theoretical, methodological, and practical implications on social-mediated discussions related to climate change-induced and -intensified disasters and issues management are discussed.}
}
@article{WAHLHEIM2025380,
title = {Memory updating and the structure of event representations},
journal = {Trends in Cognitive Sciences},
volume = {29},
number = {4},
pages = {380-392},
year = {2025},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2024.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S1364661324003152},
author = {Christopher N. Wahlheim and Jeffrey M. Zacks},
keywords = {memory updating, prediction error, interference, event cognition, pattern separation, hippocampus},
abstract = {People form memories of specific events and use those memories to make predictions about similar new experiences. Living in a dynamic environment presents a challenge: How does one represent valid prior events in memory while encoding new experiences when things change? There is evidence for two seemingly contradictory classes of mechanism: One differentiates outdated event features by making them less similar or less accessible than updated event features. The other integrates updated features of new events with outdated memories, and the relationship between them, into a structured representation. Integrative encoding may occur when changed events trigger inaccurate predictions based on remembered prior events. We propose that this promotes subsequent recollection of events and their order, enabling adaptation to environmental changes.}
}
@article{MEINTZ1994273,
title = {Future directions in computational nursing sciences},
journal = {Mathematical and Computer Modelling},
volume = {19},
number = {6},
pages = {273-288},
year = {1994},
issn = {0895-7177},
doi = {https://doi.org/10.1016/0895-7177(94)90199-6},
url = {https://www.sciencedirect.com/science/article/pii/0895717794901996},
author = {S.L. Meintz and E.A. Yfantis and W.P. Graebel},
keywords = {Computational nursing, Health care data sets, Interdisciplinary, Nurmetrics, Nursing informatics, Nursing science, Supercomputers},
abstract = {The advent of the supercomputer and its capabilities for dealing with terabyte-sized data bases has provided a unique opportunity for nursing sciences to enhance and add to its theories. An interdisciplinary team has formed at the University of Nevada, Las Vegas (UNLV), to provide new tools and methodologies for analyzing large-scale data bases. Their first project is a study of infant mortality. The strategy and goals for this project are presented, along with an assessment of the present state of health care data base analysis.}
}
@article{CORFIELD2011571,
title = {Understanding the infinite II: Coalgebra},
journal = {Studies in History and Philosophy of Science Part A},
volume = {42},
number = {4},
pages = {571-579},
year = {2011},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2011.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S003936811100077X},
author = {David Corfield},
keywords = {Philosophy, Mathematics, Category theory, Coalgebra, Infinite},
abstract = {In this paper we give an account of the rise and development of coalgebraic thinking in mathematics and computer science as an illustration of the way mathematical frameworks may be transformed. Originating in a foundational dispute as to the correct way to characterise sets, logicians and computer scientists came to see maximizing and minimizing extremal axiomatisations as a dual pair, each necessary to represent entities of interest. In particular, many important infinitely large entities can be characterised in terms of such axiomatisations. We consider reasons for the delay in arriving at the coalgebraic framework, despite many unrecognised manifestations occurring years earlier, and discuss an apparent asymmetry in the relationship between algebra and coalgebra.}
}
@article{ROMEROCRISTOBAL2025502215,
title = {Why your doctor is not an algorithm: Exploring logical principles of different clinical inference methods using liver transplantation as a model},
journal = {Gastroenterología y Hepatología (English Edition)},
volume = {48},
number = {3},
pages = {502215},
year = {2025},
issn = {2444-3824},
doi = {https://doi.org/10.1016/j.gastre.2025.502215},
url = {https://www.sciencedirect.com/science/article/pii/S244438242500015X},
author = {Mario Romero-Cristóbal and Magdalena {Salcedo Plaza} and Rafael Bañares},
keywords = {Epistemology, Logic, Clinical reasoning, Machine learning, Liver transplantation, Epistemología, Lógica, Razonamiento clínico, Algoritmos automáticos, Trasplante hepático},
abstract = {The development of machine learning (ML) tools in many different medical settings is largely increasing. However, the use of the resulting algorithms in daily medical practice is still an unsolved challenge. We propose an epistemological approach (i.e., based on logical principles) to the application of computational tools in clinical practice. We rely on the classification of scientific inference into deductive, inductive, and abductive comparing the characteristics of ML tools with those derived from evidence-based medicine [EBM] and experience-based medicine, as paradigms of well-known methods for generation of knowledge. While we illustrate our arguments using liver transplantation as an example, this approach can be applied to other aspects of the specialty. Regarding EBM, it generates general knowledge that clinicians apply deductively, but the certainty of its conclusions is not guaranteed. In contrast, automatic algorithms primarily rely on inductive reasoning. Their design enables the integration of vast datasets and mitigates the emotional biases inherent in human induction. However, its poor capacity for abductive inference (a logical mechanism inherent to human clinical experience) constrains its performance in clinical settings characterized by uncertainty, where data are heterogeneous, results are highly influenced by context, or where prognostic factors can change rapidly.
Resumen
Asistimos en la actualidad a un desarrollo asombroso de las herramientas de aprendizaje automático, lo que se traduce en un número creciente de estudios que ensayan su desempeño ante diferentes problemas médicos. Sin embargo, la implementación de estos algoritmos en la práctica diaria permanece como un reto no completamente abordado. Proponemos una aproximación epistemológica (según los fundamentos lógicos) de la aplicabilidad de las herramientas computacionales en la clínica. Nos basamos en la clasificación de los tipos de inferencia científica en deductiva, inductiva y abductiva, y comparamos las características fundamentales de estas herramientas con las de otros métodos de trasferencia del conocimiento a la práctica diaria (medicina basada en la evidencia [MBE] y medicina basada en la experiencia). Ejemplificamos nuestros razonamientos con el caso del trasplante hepático, si bien pueden ser aplicables a otras áreas de la especialidad. La MBE genera conocimientos generales que el clínico aplica de manera deductiva, a pesar de lo cual la certeza de sus conclusiones no es segura. La inducción predomina en el caso de los algoritmos automáticos. Su diseño permite interrelacionar gran cantidad de datos de manera menos sensible a los sesgos emocionales propios de la inducción humana. Sin embargo, su menor capacidad para la inferencia abductiva (mecanismo lógico propio de la experiencia clínica humana) limita su desempeño en aquellos contextos clínicos que están sujetos a gran incertidumbre, en donde los datos son heterogéneos, los resultados están muy influenciados por el contexto o en los que los factores pronósticos pueden cambiar rápidamente.}
}
@article{MORAWSKI200231,
title = {Are measurement-oriented courses getting too difficult for Polish students?},
journal = {Measurement},
volume = {32},
number = {1},
pages = {31-38},
year = {2002},
issn = {0263-2241},
doi = {https://doi.org/10.1016/S0263-2241(01)00053-7},
url = {https://www.sciencedirect.com/science/article/pii/S0263224101000537},
author = {Roman Z Morawski},
keywords = {Measurement, Abstract thinking, Experimentation skills, Teaching methodology},
abstract = {The measurement is assumed to be the most reliable means of acquiring information on physical reality; consequently, measurement science and technology is of fundamental importance for all the branches of engineering which have to deal with real-world objects and phenomena. Unfortunately, the ability of today’s students of engineering to grasp basic ideas of measurement science and to master basic skills related to measurement technology seems to be seriously endangered, inter alia, by the omnipresence of computer-related topics in engineering curricula. Paradoxically, it is also endangered by some cultural changes that undermine the historically established role of abstract thinking in the development of Latin civilisation. Educators cannot avoid the question: what kind of remedial measures should be undertaken? This paper aims to contribute to better understanding of various difficulties the teachers of measurement-related courses are facing today.}
}
@article{WANG20071997,
title = {DIANA: A computer-supported heterogeneous grouping system for teachers to conduct successful small learning groups},
journal = {Computers in Human Behavior},
volume = {23},
number = {4},
pages = {1997-2010},
year = {2007},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2006.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0747563206000094},
author = {Dai-Yi Wang and Sunny S.J. Lin and Chuen-Tsai Sun},
keywords = {Cooperative learning, Small-group learning, Computer assisted grouping system, Group composition, Thinking styles, University students},
abstract = {Teachers interested in small-group learning can benefit from using psychological factors to create heterogeneous groups. In this paper we describe a computer-supported grouping system named DIANA that uses genetic algorithms to achieve fairness, equity, flexibility, and easy implementation. Grouping was performed so as to avoid the creation of exceptionally weak groups. We tested DIANA with 66 undergraduate computer science students assigned to groups of three either randomly (10 groups) or using an algorithm reflecting [Sternberg, R. J. (1994). Thinking styles: theory and assessment at the interface between intelligence and personality. In R. J. Sterberg, & P. Ruzgis (Eds.), Personality and Intelligence (pp. 169–187). New York: Cambridge University Press.] three thinking styles (12 groups). The results indicate that: (a) the algorithm-determined groups were more capable of completing whatever they were “required to do” at a statistically significant level, (b) both groups were equally capable of solving approximately 80% of what they “chose to do,” and (c) the algorithm-determined groups had smaller inter-group variation in performance. Levels of satisfaction with fellow group member attitudes, the cooperative process, and group outcomes were also higher among members of the algorithm-determined groups. Suggestions for applying computer-supported group composition systems are offered.}
}
@incollection{KERN202469,
title = {Chapter 5 - The turbinates—an overview},
editor = {Eugene Barton Kern and Oren Friedman},
booktitle = {Empty Nose Syndrome},
publisher = {Elsevier},
pages = {69-96},
year = {2024},
isbn = {978-0-443-10715-3},
doi = {https://doi.org/10.1016/B978-0-443-10715-3.00005-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443107153000056},
author = {Eugene Barton Kern and Oren Friedman},
keywords = {Acetylcholine, secondary atrophic rhinitis, autonomic nervous system, turbinate anatomy, middle turbinate anatomy, inferior turbinate anatomy, capacitance vessels (sinusoids), “diffusor function,” functional residual capacity of the nose (FRCn), “,” hypertrophy (increase in cell ), hyperplasia (increase in cell ), nasal cycle, nasal obstruction, on-urgical urbinate eduction djunctive rocedure (n-sTRAP), out-fracture (lateralization), squamous metaplasia, submucous resection, ozaena, “resistor function,” total inferior turbinectomy, turbinates, turbinectomy, turbinoplasty, acoustic rhinometry, and rhinomanometry},
abstract = {This chapter presents an overview of the turbinates. To the best of our knowledge, it was a New Yorker, William M. Jarvis, MD, who in 1882 described three cases of utilizing a snare to affect a partial turbinectomy. At the dawn of the 20th century, most surgeons were promoting total inferior turbinectomy not only for nasal airway obstruction but astoundingly also for hearing impairment and tinnitus. The turbinate enlargement or “hypertrophy” is neither the cause nor a complication of hearing loss. Fortunately, and for the most part, dazed blunders and egregious errors in thinking by esteemed experts, for the most part, have remedied itself through scientific studies, since the late 19th century. This chapter traces the more than a century long history of turbinate thinking and surgery offering both sides of the turbinate debate in their “own words.” All the various procedures used to reduce the inferior turbinate are presented. To be as fair minded as possible, numerous authors are quoted, spanning more than one hundred years; some observed and reported the serious adverse effects of aggressive turbinate surgery, pleading for a conservative approach to inferior turbinate surgical intervention, while others claimed that turbinectomy was without any serious sequelae which is challenged by the facts.}
}
@article{ROMAN1992161,
title = {Pavane: a system for declarative visualization of concurrent computations},
journal = {Journal of Visual Languages & Computing},
volume = {3},
number = {2},
pages = {161-193},
year = {1992},
issn = {1045-926X},
doi = {https://doi.org/10.1016/1045-926X(92)90014-D},
url = {https://www.sciencedirect.com/science/article/pii/1045926X9290014D},
author = {Gruia-Catalin Roman and Kenneth C Cox and C.Donald Wilcox and Jerome Y Plun},
abstract = {This paper describes the conceptual model, specification method and visualization methodology for Pavane—a visualization environment concerned with exploring, monitoring and presenting concurrent computations. The underlying visualization model is declarative in the sense that visualization is treated as a mapping from program states to a three-dimensional world of geometric objects. The latter is rendered in full color and may be examined freely by a viewer who is allowed to navigate through the geometric world. The state-to-geometry mapping is defined as a composition of several simpler mappings. The choice is determined by methodological and architectural considerations. This paper shows how this decomposition was molded by two methodological objectives: (1) the desire visually to capture abstract formal properties of programs (e.g. safety and progress) rather than operational details; and (2) the need to support complex animations of atomic computational events. All mappings are specified using a rule-based notation; rules may be added, deleted and modified at any time during the visualization. An algorithm for termination detection in diffusing computations is used to illustrate the specification method and to demonstrate its conceptual elegance and flexibility. A concurrent version of a popular artificial intelligence program provides a vehicle for demonstrating how we derive graphical representations and animation scenarios from key formal properties of the program, i.e. from those safety and progress assertions about the program which turn out to be important in verifying its correctness.}
}
@incollection{KRINGELBACH2019139,
title = {24 - Whole-brain modeling of neuroimaging data: Moving beyond correlation to causation},
editor = {Amir Raz and Robert T. Thibault},
booktitle = {Casting Light on the Dark Side of Brain Imaging},
publisher = {Academic Press},
pages = {139-143},
year = {2019},
isbn = {978-0-12-816179-1},
doi = {https://doi.org/10.1016/B978-0-12-816179-1.00024-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128161791000244},
author = {Morten L. Kringelbach and Gustavo Deco},
keywords = {Whole-brain modeling, neuroimaging, causative mechanisms},
abstract = {Neuroimaging has offered an unprecedented window on human brain activity. While this advance has led to great expectations, many neuroscientists have grown increasingly frustrated with the lack of causal insights that this technique has provided into human brain function, in turn, leading to heated discussions on the potential rise of neophrenology. Elsewhere in this book, you can read about the apparent failure of brain imaging to tell us much new or meaningful about thinking and cognition in general. Such views are true to a certain extent; brain imaging often takes indirect measures of neural activity such as blood flow and, just because such brain measures correlate with behavioral output, does not mean that they cause the output. But, these new tools do measure important information about brain activity that could potentially tell us a great deal about brain and mind.}
}
@article{BAMBINI2022106196,
title = {It is time to address language disorders in schizophrenia: A RCT on the efficacy of a novel training targeting the pragmatics of communication (PragmaCom)},
journal = {Journal of Communication Disorders},
volume = {97},
pages = {106196},
year = {2022},
issn = {0021-9924},
doi = {https://doi.org/10.1016/j.jcomdis.2022.106196},
url = {https://www.sciencedirect.com/science/article/pii/S0021992422000156},
author = {Valentina Bambini and Giulia Agostoni and Mariachiara Buonocore and Elisabetta Tonini and Margherita Bechi and Ilaria Ferri and Jacopo Sapienza and Francesca Martini and Federica Cuoco and Federica Cocchi and Luca Bischetti and Roberto Cavallaro and Marta Bosia},
keywords = {Pragmatics, Schizophrenia, Rehabilitation, Concretism, Metaphor, Functioning},
abstract = {Introduction: Language and communication disruptions in schizophrenia are at the center of a large body of investigation. Yet, the remediation of such disruptions is still in its infancy. Here we targeted what is known to be one of the most damaged language domains in schizophrenia, namely pragmatics, by conducting a pragmatics-centered intervention with a randomized controlled trial design and assessing also durability and generalization. To the best of our knowledge, this is the first study with these characteristics. Methods: Inspired by the Gricean account of natural language use, we tailored a novel treatment addressing the pragmatics of communication (PragmaCom) and we tested its efficacy in a sample of individuals with schizophrenia randomized to the experimental group or to an active control group. The primary outcome with respect to the efficacy of the PragmaCom was measured by changes in pragmatic abilities (as evaluated with the global score of the Assessment of Pragmatic Abilities and Cognitive Substrates test) from baseline to 12 weeks and at 3-month follow-up. The secondary outcome was measured by changes in metaphor comprehension, abstract thinking, and global functioning from baseline to 12 weeks and at 3-month follow-up. Results: Relative to the control group, at post-test the PragmaCom group showed greater and enduring improvement in global pragmatic skills and in metaphor comprehension. At follow-up, these improvements persisted and the PragmaCom exerted beneficial effects also on functioning. Conclusions: Despite the limited sample size, we believe that these findings offer initial yet encouraging evidence of the possibility to improve pragmatic skills with a theoretically grounded approach and to obtain durable and clinically relevant benefits. We argue that it is time that therapeutic efforts embrace communicative dysfunctions in order to improve illness outcome.}
}
@article{LI2024e40037,
title = {The application and impact of artificial intelligence technology in graphic design: A critical interpretive synthesis},
journal = {Heliyon},
volume = {10},
number = {21},
pages = {e40037},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e40037},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024160689},
author = {Hong Li and Tao Xue and Aijia Zhang and Xuexing Luo and Lingqi Kong and Guanghui Huang},
keywords = {Atificial intelligence, Graphic design, Machine learning, Computer vision, Visual communication design, Systematic review},
abstract = {In the field of graphic design, the application of Artificial Intelligence (AI) is reshaping the design process. This study employs the Critical Interpretive Synthesis (CIS) approach to explore the impacts and challenges of AI on graphic design. Through a comprehensive review of 33 papers, this research reveals four research paradigms of AI in graphic design: Artificial Intelligence Driven Design Automation and Generation (AIDAG), Artificial Intelligence Assisted Graphic Design and Image Processing (AGDIP), Artificial Intelligence in Art and Creative Design Processes (AACDP), and Artificial Intelligence Enhanced Visual Attention and Emotional Response Modeling (AVERM). These paradigms demonstrate the multidimensional role of AI in design, ranging from automation to emotional interaction. The findings suggest that AI serves a dual role as both a design tool and a medium for innovation. AI not only enhances the automation and efficiency of the design process but also fosters designers' creative thinking and understanding of users' emotional needs. This study also proposes a path for the application of the four paradigms in the graphic design process, providing effective design ideas for future design workflows.}
}
@article{FAHIMI2024,
title = {Improving the Efficiency of Inferences From Hybrid Samples for Effective Health Surveillance Surveys: Comprehensive Review of Quantitative Methods},
journal = {JMIR Public Health and Surveillance},
volume = {10},
year = {2024},
issn = {2369-2960},
doi = {https://doi.org/10.2196/48186},
url = {https://www.sciencedirect.com/science/article/pii/S2369296024000188},
author = {Mansour Fahimi and Elizabeth C Hair and Elizabeth K Do and Jennifer M Kreslake and Xiaolu Yan and Elisa Chan and Frances M Barlas and Abigail Giles and Larry Osborn},
keywords = {hybrid samples, composite estimation, optimal composition factor, unequal weighting effect, composite weighting, weighting, surveillance, sample survey, data collection, risk factor},
abstract = {Background
Increasingly, survey researchers rely on hybrid samples to improve coverage and increase the number of respondents by combining independent samples. For instance, it is possible to combine 2 probability samples with one relying on telephone and another on mail. More commonly, however, researchers are now supplementing probability samples with those from online panels that are less costly. Setting aside ad hoc approaches that are void of rigor, traditionally, the method of composite estimation has been used to blend results from different sample surveys. This means individual point estimates from different surveys are pooled together, 1 estimate at a time. Given that for a typical study many estimates must be produced, this piecemeal approach is computationally burdensome and subject to the inferential limitations of the individual surveys that are used in this process.
Objective
In this paper, we will provide a comprehensive review of the traditional method of composite estimation. Subsequently, the method of composite weighting is introduced, which is significantly more efficient, both computationally and inferentially when pooling data from multiple surveys. With the growing interest in hybrid sampling alternatives, we hope to offer an accessible methodology for improving the efficiency of inferences from such sample surveys without sacrificing rigor.
Methods
Specifically, we will illustrate why the many ad hoc procedures for blending survey data from multiple surveys are void of scientific integrity and subject to misleading inferences. Moreover, we will demonstrate how the traditional approach of composite estimation fails to offer a pragmatic and scalable solution in practice. By relying on theoretical and empirical justifications, in contrast, we will show how our proposed methodology of composite weighting is both scientifically sound and inferentially and computationally superior to the old method of composite estimation.
Results
Using data from 3 large surveys that have relied on hybrid samples composed of probability-based and supplemental sample components from online panels, we illustrate that our proposed method of composite weighting is superior to the traditional method of composite estimation in 2 distinct ways. Computationally, it is vastly less demanding and hence more accessible for practitioners. Inferentially, it produces more efficient estimates with higher levels of external validity when pooling data from multiple surveys.
Conclusions
The new realities of the digital age have brought about a number of resilient challenges for survey researchers, which in turn have exposed some of the inefficiencies associated with the traditional methods this community has relied upon for decades. The resilience of such challenges suggests that piecemeal approaches that may have limited applicability or restricted accessibility will prove to be inadequate and transient. It is from this perspective that our proposed method of composite weighting has aimed to introduce a durable and accessible solution for hybrid sample surveys.}
}
@article{CHAUDHARI2024100953,
title = {PSOGSA: A parallel implementation model for data clustering using new hybrid swarm intelligence and improved machine learning technique},
journal = {Sustainable Computing: Informatics and Systems},
volume = {41},
pages = {100953},
year = {2024},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2023.100953},
url = {https://www.sciencedirect.com/science/article/pii/S2210537923001087},
author = {Shruti Chaudhari and Anuradha Thakare and Ahmed M. Anter},
keywords = {Clustering, Swarm intelligence, PSO, Gravitational search algorithm, Neural network, GPU},
abstract = {With the digitization of the entire world and huge requirements of understanding unknown patterns from the data, clustering becomes an important research area. The quick and accurate division of large datasets with a range of properties or features becomes challenging. The parallel implementation of clustering algorithms must satisfy stringent computational requirements to handle large amounts of data. This can be achieved by designing a GPU based optimal computational model with a heuristic approach. Swarm Intelligence (SI), a family of bio-inspired algorithms, that has been effectively applied to a number of real-world clustering problems. The Gravitational Search Algorithm (GSA) is a heuristic search optimization approach based on Newton's Law of Gravitation and mass interactions. Although it has a slow searching rate in the last iterations, this strategy has been proved to be capable of discovering the global optimum. This paper presents GPU based hybrid parallel algorithms for data clustering. A newly developed, hybrid Particle Swarm Optimization (PSO) and Gravitational Search Algorithm (GSA) i.e., PSOGSA achieves the global optima. PSOGSA utilizes novel training methods for enhanced Neural Networks (NN) in order to examine the efficiency of algorithms and resolves the challenges of trapping in local minima. This also shows the sluggish convergence rate of standard evolutionary learning algorithms. The Nearest Neighbour Partition (Partitioning of the Neighbourhood) algorithm can be used to improve the performance of NN. A parallel version of Hybrid PSOGSA with NN is implemented to achieve optimal results with better computational time. Compared to the CPU-based regular PSO, the suggested Hybrid PSOGSA with NN achieved optimal clustering with 71% improved computational time.}
}
@article{YAO2022107747,
title = {Regional attention reinforcement learning for rapid object detection},
journal = {Computers & Electrical Engineering},
volume = {98},
pages = {107747},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.107747},
url = {https://www.sciencedirect.com/science/article/pii/S004579062200057X},
author = {Hongge Yao and Peng Dong and Siyi Cheng and Jun Yu},
keywords = {Regional attention, Reinforcement learning, Object detection, Information fusion, Location and recognition},
abstract = {When people observe a picture, they first pay attention to local areas of the picture, rather than the whole areas, then combine them with previous experience in the brain, and finally make judgments through thinking. This is human visual logic. In this paper, we propose a regional attention reinforcement learning model for object detection. The proposed model uses human visual logical to solve the detection problem of small and complex targets in the picture. The model uses a recurrent network structure as the main framework to extract historical information, and fuse the historical information with the current concerned information. At each recurrent time step, it can pay attention to the fused information, especially pay more attention to the information that may have objects. Experimental results show that the proposed method has more than 5% improved in recognition accuracy to the conventional methods. In terms of FLOPs, the conventional methods normally require 170 M, while the proposed method only needs 25.4M This means that the proposed method has higher detection efficiency.}
}
@incollection{FINI2019161,
title = {Chapter 7 - Sustainable Procurement and Transport of Construction Materials},
editor = {Vivian W.Y. Tam and Khoa N. Le},
booktitle = {Sustainable Construction Technologies},
publisher = {Butterworth-Heinemann},
pages = {161-209},
year = {2019},
isbn = {978-0-12-811749-1},
doi = {https://doi.org/10.1016/B978-0-12-811749-1.00005-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128117491000055},
author = {Alireza Ahmadian Fard Fini and Ali Akbarnezhad},
keywords = {Sustainable construction materials, life cycle thinking, procurement and transport, prefabrication technologies},
abstract = {Construction industry is the largest global consumer of materials. This huge share comes with the huge responsibility to account for economic, environmental, and social impacts associated with the materials through adoption of sustainable procurement strategies. Sustainable material procurement requires reconciliation among economic, environmental and social impacts of procurement decisions throughout the life cycle of materials. However, this is challenging mainly due to the broad range of economic, environmental and social impacts associated with different stages of material’s life cycle as well as the overlapping impacts that various supply decisions may have on multiple performance areas. Current practices of material procurement are, on the other hand, predominantly influenced by economy of construction stage and little attention is paid to environmental and social considerations over a long-term horizon. Moreover, material supply decisions made currently in practice are commonly traditional and tend to largely overlook the opportunities made available by advances in material science, computing, and decision-making areas. This chapter starts by presenting an overview of sustainability challenges associated with current material procurement practices to highlight the need for adoption of new sustainable approaches and technologies. It then continues by highlighting the challenges associated with adoption of new approaches and the important sustainability criteria to be considered in selection of new sustainable materials, technologies, and procurement strategies. A comprehensive decision-making framework for identifying the most sustainable procurement options in a construction project among various procurement options available is then presented. The framework is founded on the concepts of life cycle thinking and supply chain structure which are incorporated in to a computational module to compare the life cycle impacts of various supply decision based on the selection criteria determined collaboratively by different project stakeholders. The results of such comparative analysis leads to a ranking of various procurement decision alternatives comprised of different combinations of supply decision including material type, material supply structure, location of supplier, and mode of transport.}
}
@article{WATHEN2022176,
title = {Some observations on preconditioning for non-self-adjoint and time-dependent problems},
journal = {Computers & Mathematics with Applications},
volume = {116},
pages = {176-180},
year = {2022},
note = {New trends in Computational Methods for PDEs},
issn = {0898-1221},
doi = {https://doi.org/10.1016/j.camwa.2021.05.037},
url = {https://www.sciencedirect.com/science/article/pii/S0898122121002388},
author = {Andy Wathen},
keywords = {Iterative methods, Preconditioning, Non-self-adjoint problems, Parallel-in-time computation},
abstract = {Numerical Linear Algebra—specifically the computational solution of equations—forms a significant part of Computational Methods for Partial Differential Equations. Here we discuss the contrast between the solution of symmetric systems of equations that arise from self-adjoint problems and non-symmetric systems that arise from non-self-adjoint problems when iterative methods are employed; such methods are the only feasible methods for very large scale computation with PDEs. We then go on to consider non-symmetric all-at-once systems that arise in approximation of time-dependent problems, discuss causality and the parallel-in-time paradigm, suggesting an approach that involves preconditioning initial value problems with time-periodic problems.}
}
@article{NADOLSKI2019210,
title = {Complex systems analysis of hybrid warfare},
journal = {Procedia Computer Science},
volume = {153},
pages = {210-217},
year = {2019},
note = {17th Annual Conference on Systems Engineering Research (CSER)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.05.072},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919307318},
author = {Molly Nadolski and James Fairbanks},
keywords = {Multi-level modelling, Sociotechnical systems, Complex Systems, Toolsets, Unstructured Spaces, Conceptual Modeling, Quantitative Modeling},
abstract = {Being empowered with the appropriate toolset will enable decision-makers to analyze how best to intervene in ever-changing complex systems. This research project explored deconstructing qualitative methods to identify and document requirements to connect the models to computational social science approaches. Previous efforts from our research provided a customizable toolset that assesses the current and future impact that decisions, policies, or strategies can deliver in a system to tackle particularly complex problems. This paper presents a portion of the research effort that developed a threat analysis framework by establishing formally documented research methods to effectively combine conceptual and computational tools. This enables more accurate, efficient, and foresightful knowledge capture and depictions of a particular problem space. The case that the tools and approach are tested against is Russia’s application of grey zone warfare tools in Moldova.}
}
@article{HOSSEINI2019186,
title = {A morphological approach for kinetic façade design process to improve visual and thermal comfort: Review},
journal = {Building and Environment},
volume = {153},
pages = {186-204},
year = {2019},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2019.02.040},
url = {https://www.sciencedirect.com/science/article/pii/S0360132319301416},
author = {Seyed Morteza Hosseini and Masi Mohammadi and Alexander Rosemann and Torsten Schröder and Jos Lichtenberg},
keywords = {Kinetic façade, Biomimicry, Morphological approach, Comfort condition, Parametric design thinking},
abstract = {Visual and thermal comfort for occupants significantly depend on exterior environmental climatic conditions, which are continuously changing. In particular, optimizing visual and thermal comfort simultaneously is a difficult topic due to mutual conflicts between them. This literature review article studies the façade, as a complex interface between inside of buildings and the outside that has a capability to function as a protective or regulatory element against severe fluctuations of external climate. Six interrelated subjects are studied including kinetic façade, biomimicry, building form as a microclimate modifier, energy efficiency, comfort condition, parametric design thinking. The literature review process answers following research questions: (1) what are the interdisciplinary subjects corresponding to kinetic façade design process for creating an innovative architectural process? (2) What is the most important factor in kinetic façade design with the aim to improve occupants’ visual and thermal comfort simultaneously based on multidisciplinary investigation? Many research has been carried out about kinetic façade concepts strategies, principles, and criteria. However, interdisciplinary studies for proposing kinetic façade form is relatively rare. Also, adaptive daylight façade with daily solar geometry variation has been highly required. Therefore, generative-parametric and quick form finding method for responding to different climates would be a solution for providing more adaptability to dynamic daylight. This study aims to propose a kinetic façade design process which have capability to improve occupant visual & thermal comfort simultaneously by controlling on-site renewable energy resources consist of solar radiation and wind. Façade as an only interface between inside and outside of building, far from the literal and historical perceptions, is recognized by intrinsic functional attributes including complexity, heterogeneity and multidisciplinary. Moreover, the interrelated subjects impact façade form individually and aggregately regard to functional scenario that is changed the perception of kinetic façade from elegant and fashionable state to a functional and practical element.}
}
@article{MONRO199293,
title = {Parallel computation of ECG fields},
journal = {Journal of Electrocardiology},
volume = {25},
pages = {93-100},
year = {1992},
note = {Research and Applications in Computerized Electrocardiology},
issn = {0022-0736},
doi = {https://doi.org/10.1016/0022-0736(92)90068-B},
url = {https://www.sciencedirect.com/science/article/pii/002207369290068B},
author = {D.M. Monro and D.M. Budgett},
keywords = {electrocardiogram, modeling, forward solution, magnetic resonance imaging, inverse solution, parallel computers},
abstract = {A parallel implementation of a finite difference model for computing the electric field of cardiac sources is presented. On a relatively inexpensive SIMD parallel computer, a full-forward solution is obtained in minutes, using accurate thoracic detail including anisotropy if required. Because the computation is based on a volume grid with constant size voxels, it readily accepts anatomical data from classified magnetic resonance imaging scans. By using a variation of the colored successive over-relaxation iteration, our finite difference model takes full advantage of the performance of massively parallel computers. Evaluations of the accuracy and performance of the model show the practicality of using specific anatomical models to recover the electrocardiographic field distributions for individual subjects. A relatively modest parallel machine is capable of assembling and computing a specific direct inverse solution from body surface potentials within an hour of measurement, assuming the magnetic resonance imaging classification has been previously completed.}
}
@article{NIKIFORIDOU2010795,
title = {Statistical literacy at university level: the current trends},
journal = {Procedia - Social and Behavioral Sciences},
volume = {9},
pages = {795-799},
year = {2010},
note = {World Conference on Learning, Teaching and Administration Papers},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2010.12.236},
url = {https://www.sciencedirect.com/science/article/pii/S1877042810023414},
author = {Zoi Nikiforidou and Aspasia Lekka and Jenny Pange},
keywords = {statistical literacy, Statistics Education},
abstract = {Active and critical citizens, in contemporary information-driven societies, are considered to possess capacities and skills of statistical literacy. There are numerous definitions and descriptions concerning statistical literacy, statistical reasoning and statistical thinking. Thus, all these terms converge to the principle that statistical citizenship develops from school settings and relates mainly to the processes of evaluating, interpreting and communicating data. If these are not acquired on time, then students build up errors and misunderstandings. In the current paper general issues concerning Statistics Education at the University level are addressed and aspects for future research are stressed in terms of technology use, content and pedagogic approaches.}
}
@article{SZYJEWSKI20203476,
title = {Future management},
journal = {Procedia Computer Science},
volume = {176},
pages = {3476-3485},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.049},
url = {https://www.sciencedirect.com/science/article/pii/S187705092031944X},
author = {Zdzisław Szyjewski},
keywords = {Future management, forecasting the future, new technologies},
abstract = {Accurate forecasting, good identification of trends is the basis of business success. Strategic management methods and techniques that use experience and historical economic data are not adequate to the rapidly changing business environment. In particular, technological changes, and in particular the widespread use of ICT, forces a new approach to management style and changes in the way data is acquired on the basis of which future decisions are made. Innovation thinking, a flexible and dynamic approach to making future-oriented decisions using new technologies are the foundations of future management. Therefore, the aim of the paper is to show the role and position of technology in creating the future.}
}
@article{LIU2024101910,
title = {Understanding ceiling temperature as a predictive design parameter for circular polymers},
journal = {Cell Reports Physical Science},
volume = {5},
number = {4},
pages = {101910},
year = {2024},
issn = {2666-3864},
doi = {https://doi.org/10.1016/j.xcrp.2024.101910},
url = {https://www.sciencedirect.com/science/article/pii/S2666386424001462},
author = {Xiaoyang Liu and Shivani Kozarekar and Alexander Shaw and Tie-Qi Xu and Eugene Y.-X. Chen and Linda J. Broadbelt},
keywords = {circular polymers, ceiling temperature, thermodynamic parameters, uncertainty propagation, density functional theory},
abstract = {Summary
The rise of polymeric materials marks a notable achievement of the past century, yet challenges in recycling have led to their accumulation in various environments. Efforts to address this include advancements in mechanical recycling, degradation processes, and chemical recycling techniques, particularly chemical recycling to monomer, which offers a path toward a circular economy for plastics. In this perspective, we discuss how ceiling temperature (Tc) can be used as a design parameter for circular (closed-loop recyclable) polymers and provide an overview of typical experimental approaches for deriving Tc, focusing on ΔHp and ΔSp as the key parameters for prediction. The concept of Tc is heavily embedded in the polymer literature and provides a simple but still useful way of quickly ranking different polymers in terms of their relative thermodynamic stability of polymer versus monomer states. While Tc in the bulk state as an intrinsic value is a desirable quantity, it is infeasible in many cases to measure equilibrium states in the bulk; thus, many researchers have focused on investigating Tc in solution, where there may be dependencies of Tc on the solvent, concentration, or other factors, resulting in a family of apparent Tc values at each set of conditions. We thus explore computational studies as a complement to experimental measurements of Tc. To this end, we focus here on the advantages, obstacles, and outlook of the establishment of predictive computational approaches to calculate key thermodynamic parameters related to polymer circularity, namely ΔHp, ΔSp, ΔGp, and Tc values.}
}
@incollection{RIVELA202293,
title = {Chapter 6 - Life Cycle Sustainability Assessment-based tools},
editor = {Carmen Teodosiu and Silvia Fiore and Almudena Hospido},
booktitle = {Assessing Progress Towards Sustainability},
publisher = {Elsevier},
pages = {93-118},
year = {2022},
isbn = {978-0-323-85851-9},
doi = {https://doi.org/10.1016/B978-0-323-85851-9.00018-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323858519000183},
author = {Beatriz Rivela and Brandon Kuczenski and Dolores Sucozhañay},
keywords = {Life Cycle Thinking, Life Cycle Assessment, Life Cycle Costing, Social Life Cycle Assessment, Life Cycle Sustainability Assessment},
abstract = {This chapter establishes a baseline of ideas of what Life Cycle Thinking means: going beyond the traditional focus, understanding and including the whole environmental, social, and economic implications of decision-making processes to identify potential conflicts, synergies, and trade-offs. The life cycle methodologies for sustainability assessment are described, providing an overview of the tools and criteria currently applied, available software and databases, and ongoing challenges. While Environmental Life Cycle Assessment (LCA) is a consolidated tool, based on the ISO standards, Life Cycle Costing (LCC), the tool aimed at the assessment of the economic domain using a life cycle perspective, has not been widely integrated into sustainability assessment until the last decade. Concerning the social dimension, Social Life Cycle Assessment (S-LCA) is still at an early stage of development, but it is a promising methodology to face the challenge of integrating the social aspects towards a holistic approach to sustainable development.}
}
@article{MEEKINGS2025101262,
title = {What's the point of talking? Auditory targets and communicative goals},
journal = {Journal of Neurolinguistics},
volume = {75},
pages = {101262},
year = {2025},
issn = {0911-6044},
doi = {https://doi.org/10.1016/j.jneuroling.2025.101262},
url = {https://www.sciencedirect.com/science/article/pii/S0911604425000181},
author = {Sophie Meekings and Sophie K. Scott},
abstract = {Human speech production is a complex action requiring minute control over the articulators and sensitivity to the surrounding environment. Computational and empirical work has attempted to identify the specific neural mechanisms and cognitive processes that allow us to reliably produce speech sounds. This work has established that humans can use their perception of the auditory and somatosensory consequences of their actions to guide subsequent speech movements. However, speech predominantly takes place in a communicative context, and this context is also known to modulate the way that people speak: human voices are highly flexible. In this paper, we try to unite the traditional motor control conception of internally defined acoustic and somatosensory goals with linguistic research showing that talkers respond and entrain to their conversational partners. We provide an overview of the theoretical and empirical work surrounding the use of sensory feedback monitoring in speech production and discuss practical constraints that have limited more naturalistic investigations into dyadic interaction. To conclude, we argue that the variability of results seen in the speech motor control literature reflects a more complex underlying neural architecture, and an overarching communicative goal that supersedes specific phonetic targets.}
}
@article{GRAF2021100836,
title = {A cycle for validating a learning progression illustrated with an example from the concept of function},
journal = {The Journal of Mathematical Behavior},
volume = {62},
pages = {100836},
year = {2021},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2020.100836},
url = {https://www.sciencedirect.com/science/article/pii/S0732312320301000},
author = {Edith Aurora Graf and Peter W. {van Rijn} and Cheryl L. Eames},
keywords = {Learning progressions, Learning trajectories, Validation, Empirical recovery, Mathematics assessment},
abstract = {A learning progression, or learning trajectory, describes the evolution of student thinking from early conceptions to the target understanding within a particular domain. As a complex theory of development, it requires conceptual and empirical support. In earlier work, we proposed a cycle for the validation of a learning progression with four steps: 1) Theory Development, 2) Examination of Empirical Recovery, 3) Comparison to Competing Models, and 4) Evaluation of Instructional Efficacy. A group of experts met to discuss the application of learning sciences to the design, use, and validation of classroom assessment. Learning progressions, learning trajectories, and how they can support classroom assessment were the main focuses. Revisions to the cycle were suggested. We describe the adapted cycle and illustrate how the first third of it has been applied towards the validation of a learning progression for the concept of function.}
}
@article{DOOLITTLE2019889,
title = {Making Evolutionary Sense of Gaia},
journal = {Trends in Ecology & Evolution},
volume = {34},
number = {10},
pages = {889-894},
year = {2019},
issn = {0169-5347},
doi = {https://doi.org/10.1016/j.tree.2019.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169534719301417},
author = {W. Ford Doolittle},
keywords = {Gaia hypothesis, evolution, differential persistence, clade selection},
abstract = {The Gaia hypothesis in a strong and frequently criticized form assumes that global homeostatic mechanisms have evolved by natural selection favoring the maintenance of conditions suitable for life. Traditional neoDarwinists hold this to be impossible in theory. But the hypothesis does make sense if one treats the clade that comprises the biological component of Gaia as an individual and allows differential persistence – as well as differential reproduction – to be an outcome of evolution by natural selection. Recent developments in theoretical and experimental evolutionary biology may justify both maneuvers.}
}
@article{SZNYCER2025106652,
title = {Emotional tears: What they are and how they work},
journal = {Evolution and Human Behavior},
volume = {46},
number = {1},
pages = {106652},
year = {2025},
issn = {1090-5138},
doi = {https://doi.org/10.1016/j.evolhumbehav.2025.106652},
url = {https://www.sciencedirect.com/science/article/pii/S1090513825000017},
author = {Daniel Sznycer and Asmir Gračanin and Debra Lieberman},
keywords = {Tears, Value, Emotion, Motivation, Signaling},
abstract = {Although much is known about emotional tears from the perspectives of neurobiology and behavior, the production of emotional tears and the responses they elicit depend sensitively on a rich set of computations—one which has received little attention to date. This review article aims to close this gap. Emotional tearing occurs during negative events (e.g., injuries) and positive events (e.g., achievements). Episodes of tearing appear to be united by tearers' subjective imputation of negative or positive value to certain internal or external phenomena. Knowing the degree to which objects, organisms, events, and states of affairs enhance or diminish one's prospects—the value of things—is a pressing matter for humans and other organisms. Value information is produced for internal consumption, to be used by behavior-regulating mechanisms in the focal individual. But some evaluations are made available, in addition, to other people, through tearing and other forms of verbal and non-verbal communication. Tearing may function as an implicit plea for receivers (the tear targets) to minimize the costs imposed on the tearer by nature, by third-parties, or by the tear targets themselves—common when the tearer has lower formidability or wherewithal than tear targets do. In addition, tearing may exhort tear targets to infer and register which things the tearer values, positively or negatively. Here, we characterize tears, describe the game-theoretic logic of bargaining from a position of weakness, outline the computational systems that regulate the production of and responses to emotional tears, and review findings about emotional tearing that are relevant to the signaling hypothesis.}
}
@incollection{HLAVAEEK20041,
title = {Chapter I - Reality, Mathematics, and Computation},
editor = {Ivan Hlaváěek and Jan Chleboun and Ivo Babuška},
series = {North-Holland Series in Applied Mathematics and Mechanics},
publisher = {North-Holland},
volume = {46},
pages = {1-49},
year = {2004},
booktitle = {Uncertain Input Data Problems and the Worst Scenario Method},
issn = {0167-5931},
doi = {https://doi.org/10.1016/S0167-5931(04)80005-6},
url = {https://www.sciencedirect.com/science/article/pii/S0167593104800056},
author = {Ivan Hlaváěek and Jan Chleboun and Ivo Babuška}
}
@article{YANG201451,
title = {Reactivity of concurrent verbal reporting in second language writing},
journal = {Journal of Second Language Writing},
volume = {24},
pages = {51-70},
year = {2014},
issn = {1060-3743},
doi = {https://doi.org/10.1016/j.jslw.2014.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S1060374314000113},
author = {Chengsong Yang and Guangwei Hu and Lawrence Jun Zhang},
keywords = {Reactivity, Think-aloud, Second language acquisition (SLA), L2 writing, Argumentative writing, Chinese EFL writers},
abstract = {This paper reports an empirical study designed to explore whether concurrent verbal reporting has a reactive effect on the process of second language writing. Ninety-five Chinese EFL learners were randomly assigned to an argumentative writing task under three conditions: metacognitive thinking aloud (MTA), nonmetacognitive thinking aloud (NMTA), and no thinking aloud (NTA), after they completed a similar baseline writing task. Their essays were analyzed in terms of linguistic fluency, complexity, accuracy, and overall quality to examine if there were any significant between-group differences that could be taken as evidence of reactivity. After controlling for baseline differences, analyses revealed no traces of reactivity left on a majority of measures except that: (a) the two think-aloud conditions significantly increased dysfluencies in participants’ essays; (b) they also tended to reduce syntactic variety of the essays; and (c) MTA significantly prolonged time on task and retarded the speed of written production. These negative effects are interpreted in light of Kellogg's (1996) cognitive model of writing as suggesting no serious interference with L2 writing processes and are taken as cautions for, rather than counterevidence against, the use of the think-aloud method to obtain L2 writing process data.}
}
@incollection{CAGLAR2025389,
title = {Bioinformatic Applications in Neuroscience},
editor = {Shoba Ranganathan and Mario Cannataro and Asif M. Khan},
booktitle = {Encyclopedia of Bioinformatics and Computational Biology (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {389-406},
year = {2025},
isbn = {978-0-323-95503-4},
doi = {https://doi.org/10.1016/B978-0-323-95502-7.00282-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955027002827},
author = {Caner Çağlar and Beyza {Kinsiz Gürsoy}},
keywords = {ALS, Alzheimer disease, Bioinformatic, DNA sequencing, Metabolomics, Neuroscience, Parkinson’s disease, Proteomics, Psychiatric disorders, Radiomics, Transcriptomics},
abstract = {In the last decade of the 20th century, interest in neuroscience significantly increased, prioritizing the study of neurological diseases and treatments. This chapter examines how the integration of neuroscience and bioinformatics has enhanced our grasp of neurological and psychiatric disorders. Combining computational methods with neuroscience allows for the analysis of large datasets, uncovering the genetic, molecular, and neural foundations of diseases. Advances in genomics, transcriptomics, and proteomics, along with the integration of radiomics and metabolomics, have transformed our understanding of brain function and disease mechanisms. By enabling non-invasive, quantitative analysis of medical images, radiomics further enhances diagnostic precision and prognostic insights. Future research will focus on using AI and machine learning for personalized medicine and targeted therapeutic development.}
}
@article{CORTI19942717,
title = {A computational study of metastability in vapor—liquid equilibrium},
journal = {Chemical Engineering Science},
volume = {49},
number = {17},
pages = {2717-2734},
year = {1994},
issn = {0009-2509},
doi = {https://doi.org/10.1016/0009-2509(94)E0093-6},
url = {https://www.sciencedirect.com/science/article/pii/0009250994E00936},
author = {David S. Corti and Pablo G. Debenedetti},
abstract = {Computer simulations are ideally suited to study systems under arbitrary constraints; hence they are useful for the investigation of metastability. Different types of constraints were applied to the three-dimensional Lennard—Jones fluid in the vapor—liquid coexistence region. Constraining the magnitude of allowed density fluctuations (restricted ensemble) has little effect on the equation of state and on phase equilibrium predictions for reduced temperatures lower than 0.95. Thermodynamic integrations along constrained and unstable paths are in good agreement with chemical potential calculations, indicating that imposing the density constraint does not violate microscopic reversibility. Restricted ensemble calculations were also used to calculate the width of the transition region where the mechanism of phase separation in the superheated liquid changes from nucleation to spinodal decomposition. The width of this region decreases as the temperature is reduced away from criticality. Free energy barriers to isotropic compression were used to determine the width of the transition region from nucleation to spinodal decomposition in the supercooled vapor. This transition region also becomes narrower as the distance from the critical point increases. The pressure of the deeply superheated liquid was found to be sensitive to the maximum size of voids that are allowed to form.}
}
@article{MIYAMOTO2022231473,
title = {Data-driven optimization of 3D battery design},
journal = {Journal of Power Sources},
volume = {536},
pages = {231473},
year = {2022},
issn = {0378-7753},
doi = {https://doi.org/10.1016/j.jpowsour.2022.231473},
url = {https://www.sciencedirect.com/science/article/pii/S0378775322004803},
author = {Kaito Miyamoto and Scott R. Broderick and Krishna Rajan},
keywords = {Lithium-ion batteries, 3D miniature batteries, Optimization of 3D battery architecture, Machine learning, Multiobjective optimization},
abstract = {To power microelectronics for the internet-of-things applications, high-performance miniature batteries, called microbatteries, are critically important. Given their limited size, the three-dimensional design of microbatteries is key to maximizing their performance. Therefore, a computational strategy to identify the target battery architecture has major implications for performance improvement. In this paper, we propose a data-driven 3D battery optimization system at the full cell level that combines an automatic geometry generator based on Monte Carlo Tree Search and highly accurate machine-learning-based performance simulators. The performance of the proposed method is demonstrated by designing high-performance 3D batteries with more than 5.5 times efficiency compared with the approach based on a randomized algorithm. One of the designed geometries displayed greater power and energy densities due to more than 10% reduced internal resistance than the reported state-of-the-art geometry at the current density of higher than 15.8 mA/cm2. The results demonstrate the effectiveness of the method.}
}
@article{SOUSA2015113,
title = {Symmetry-based generative design and fabrication: A teaching experiment},
journal = {Automation in Construction},
volume = {51},
pages = {113-123},
year = {2015},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2014.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0926580514002283},
author = {José Pedro Sousa and João Pedro Xavier},
keywords = {Architecture, Geometry, Symmetry, Computational design, Digital fabrication, Design education},
abstract = {Throughout history, symmetry has been widely explored as a geometric strategy to conceive architectural forms and spaces. Nonetheless, its concept has changed and expanded overtime. Nowadays, it is understood as an ordering principle resulting from the application of isometric transformations that keep the original object invariant. Departing from this notion, scientists, philosophers and designers have extended it to embrace other geometric scenarios. Following this idea, exploring symmetry does not mean the generation of simple and predictable design solutions. On the contrary, it is a creative window to achieve geometric complexity based on very simple rules. In this context, this paper aims at discussing the relevance of exploring symmetry in architectural design today by means of digital technologies. It argues that the coupled use of computational design and digital fabrication processes allows designers to explore and materialize a higher level of design complexity in a structured and controlled way, especially when non-isometric transformations are involved. As the background for testing and illustrating its arguments, this paper describes a teaching experiment conducted in the Constructive Geometry course at the FAUP, following design-to-fabrication methodologies.}
}
@article{SUN20112118,
title = {How digital scaffolds in games direct problem-solving behaviors},
journal = {Computers & Education},
volume = {57},
number = {3},
pages = {2118-2125},
year = {2011},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2011.05.022},
url = {https://www.sciencedirect.com/science/article/pii/S036013151100128X},
author = {Chuen-Tsai Sun and Dai-Yi Wang and Hui-Ling Chan},
keywords = {Human–computer interface, Interactive learning environments, Secondary education, Teaching/learning strategies},
abstract = {Digital systems offer computational power and instant feedback. Game designers are using these features to create scaffolding tools to reduce player frustration. However, researchers are finding some unexpected effects of scaffolding on strategy development and problem-solving behaviors. We used a digital Sudoku game named Professor Sudoku to classify built-in critical features, frustration control and demonstration scaffolds, and to investigate their effects on player/learner behaviors. Our data indicate that scaffolding support increased the level at which puzzles could be solved, and decreased frustration resulting from excessive numbers of retries. However, it also reduced the number of unassisted placements (i.e., independently filled cells), and increased reliance on scaffolding tools, both of which are considered disadvantageous for learning. Among the three scaffold types, frustration control reduced the potential for players to feel stuck at certain levels, but also reduced the frequency of use of critical feature-making tools, which are thought to have greater heuristic value. We conclude that the simultaneous provision of critical feature and frustration control scaffolds may increase player reliance on available support, thereby reducing learning opportunities. Providing players with critical features and demonstration scaffolds at the same time increases reliance on available support for some players, but for most it encourages the development of solving strategies.}
}
@article{KAMPPINEN1998481,
title = {Evolution and culture: the Darwinian view on infosphere},
journal = {Futures},
volume = {30},
number = {5},
pages = {481-484},
year = {1998},
issn = {0016-3287},
doi = {https://doi.org/10.1016/S0016-3287(98)00050-0},
url = {https://www.sciencedirect.com/science/article/pii/S0016328798000500},
author = {Matti Kamppinen},
abstract = {This essay looks at the idea that human culture is an evolving system, a complex entity that undergoes evolutionary processes. This idea can also be expressed as follows: the cultural infosphere has the same mode of operation as the organic biosphere. There are three parts to the essay: it begins with some highlights from the history of evolutionary thinking; second, it explains the mechanisms of cultural selection; and third, it discusses the vision of the future provided by evolutionary thinking. The kind of evolutionary thinking focused upon is one that takes Charles Darwin seriously. The depth, reach and relevance of Darwinian thinking has been aptly exposed by Daniel C. Dennett,[1]and this essay assesses its worth in futures research.}
}
@article{PENG202484,
title = {Multi-perspective thought navigation for source-free entity linking},
journal = {Pattern Recognition Letters},
volume = {178},
pages = {84-90},
year = {2024},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2023.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167865523003677},
author = {Bohua Peng and Wei He and Bin Chen and Aline Villavicencio and Chengfu Wu},
keywords = {Information retrieval, Question generation, Entity linking, Chain-of-thought reasoning},
abstract = {Neural entity-linking models excel at bridging the lexical gap of multiple facets of facts, such as entity-related claims or evidence documents. Despite advancements in self-supervised learning and pretrained language models, challenges persist in entity linking, particularly in interpretability and transferability. Moreover, these models need many aligned documents to adapt to emerging entities, which may not be available due to data scarcity. In this work, we propose a novel Demonstrative Self-TrAining fRamework (D-STAR) that leverages multi-perspective thought navigation. D-STAR iteratively optimizes a question generator and an entity retriever by navigating thoughts on a dynamic graph reasoning across multiple perspectives for question generation. The generated question–answer pairs, along with hard negatives shared in the graph, enable adaptation with minimal computational overhead. Additionally, we introduce a new task, source-free entity linking, focusing on unsupervised transfer learning without direct access to original domain data. To demonstrate the feasibility of this task, we provide a generated question–answering dataset, FandomWiki, for novel entities. Our experiments show that D-STAR significantly improves baselines on SciFact, Zeshel, and FandomWiki.}
}
@article{LU2025121076,
title = {The neuroscientific basis of flow: Learning progress guides task engagement and cognitive control},
journal = {NeuroImage},
volume = {308},
pages = {121076},
year = {2025},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2025.121076},
url = {https://www.sciencedirect.com/science/article/pii/S1053811925000783},
author = {Hairong Lu and Dimitri {Van der Linden} and Arnold B. Bakker},
keywords = {Learning progress, Flow experience, Engagement, Cognitive control, Goal-directed behavior},
abstract = {People often strive for deep engagement in activities, a state typically associated with feelings of flow - full task absorption accompanied by a sense of control and enjoyment. The intrinsic factors driving such engagement and facilitating subjective feelings of flow remain unclear. Building on computational theories of intrinsic motivation, this study examines how learning progress predicts engagement and directs cognitive control. Results showed that task engagement, indicated by feelings of flow and low distractibility, is a function of learning progress. Electroencephalography data further revealed that learning progress is associated with enhanced proactive preparation (e.g., reduced pre-stimulus contingent negativity variance and parietal alpha desynchronization) and improved feedback processing (e.g., increased P3b amplitude and parietal alpha desynchronization). The impact of learning progress on cognitive control is observed at the task-block and goal-episode levels, but not at the trial level. This suggests that learning progress shapes cognitive control over extended periods as progress accumulates. These findings highlight the critical role of learning progress in sustaining engagement and cognitive control in goal-directed behavior.}
}
@incollection{MACHINMASTROMATTEO2025376,
title = {Literacy of the Future},
editor = {David Baker and Lucy Ellis},
booktitle = {Encyclopedia of Libraries, Librarianship, and Information Science (First Edition)},
publisher = {Academic Press},
edition = {First Edition},
address = {Oxford},
pages = {376-387},
year = {2025},
isbn = {978-0-323-95690-1},
doi = {https://doi.org/10.1016/B978-0-323-95689-5.00197-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323956895001978},
author = {Juan D. Machin-Mastromatteo},
keywords = {Adaptation, Collaboration, Critical engagement, Democratic engagement, Digital literacy, Educational integration, Ethical dimensions, Futures Literacy, Information literacy, Lifelong learning, Media literacy, Multiliteracies, Programming skills, Social participation, Technological advancements},
abstract = {This entry summarizes the development of the literacy concepts most commonly associated with LIS, namely information literacy, digital literacy, and media literacy, which frame a synthesis of the future perspectives of these and other literacies that have been proposed in the literature.11An alphabetical and non-exhaustive list could include: academic literacy, artificial intelligence or algorithmic literacy, civic literacy, context literacy, data literacy, emotional literacy, financial literacy, focus literacy, futures literacies, game literacy, graphic literacy, health literacy, literacy education, legal literacy, media literacy, multiliteracies, new literacies, new media literacies, navigation literacy, numerical literacy, participatory/participation literacy, personal literacy, psycho-literacy, scientific literacy, search engine literacy, skepticism literacy, statistical literacy, transliteracy, and visual literacy or visuacy. Note: not all of these are covered in this entry for space limitations. These future perspectives are organized in nine sections: the educational implications of literacy, information literacy, digital literacy, literacy education, multiliteracies and holistic perspectives, media literacy, futures literacy, algorithmic literacy and artificial intelligence implications, and other literacies. The purpose of this entry is to offer a brief overview and commentary on the types of literacies that we need to be aware of and competent in for the near future. As these future trends are derived from the specialized literature, they include some already occurring considerations. However, they might become more salient topics in the upcoming years, and they might entail many different implications for the future of LIS professionals, libraries, and even for education in general.}
}
@article{GAO2025102628,
title = {An organic artificial synaptic memristor for neuromorphic computing},
journal = {Applied Materials Today},
volume = {43},
pages = {102628},
year = {2025},
issn = {2352-9407},
doi = {https://doi.org/10.1016/j.apmt.2025.102628},
url = {https://www.sciencedirect.com/science/article/pii/S2352940725000472},
author = {Kaikai Gao and Bai Sun and Bo Yang and Zelin Cao and Yu Cui and Mengna Wang and Chuncai Kong and Guangdong Zhou and Sihai Luo and Xiaoliang Chen and Jinyou Shao},
keywords = {Artificial synaptic, Organic material, Neuromorphic computing, Dataset recognition, Artificial intelligence},
abstract = {Developing an artificial synaptic device that can simulate the learning and memory abilities of the human brain is a key step toward achieving neuromorphic computing. Although traditional transistors and emerging memristors are considered potential candidates for achieving these functions, their manufacturing often relies on non-renewable semiconductor materials. Here, we have successfully fabricated a flexible artificial synaptic device with a typical memristive sandwich structure (Ag/PMMA/SLE/Ti) utilizing the cost-effective organic material sodium lignosulfonate (SLE) as the dielectric layer. This device effectively achieves short-term plasticity (STP), spike-number-dependent plasticity (SNDP), and long-term potentiation/depression (LTP/LTD). Furthermore, the conductance of the designed artificial synapse corresponds to synaptic weights, which can be recognized by neuromorphic computation on CYCLE and MNIST datasets (small/large sizes) with an accuracy of 33.8 % and 89.3 %/91.0 %, respectively. Therefore, this artificial synaptic device exhibits the flexibility to serve in various wearable scenarios, including intelligent electronic skin (e-skin). Additionally, the excellent biocompatibility of SLE aligns well with the concept of green electronics.}
}
@incollection{VALLERO202151,
title = {Chapter 3 - Transitional and translational sciences},
editor = {Daniel A. Vallero},
booktitle = {Environmental Systems Science},
publisher = {Elsevier},
pages = {51-87},
year = {2021},
isbn = {978-0-12-821953-9},
doi = {https://doi.org/10.1016/B978-0-12-821953-9.00012-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012821953900012X},
author = {Daniel A. Vallero},
keywords = {Translational science, Geodesign, Landfill fires, Tire fires, Coal mine fires, Nuclear accidents, “As low as reasonably practicable” (ALARP), Rational methods, Modularity, Interoperability},
abstract = {This chapter introduces two aspects critical to environmental systems science. The first is attention to ways to move from reductionism to systems thinking. The second is the need to translate the methods, results, and meaning of scientific discoveries from one discipline to all those needed to address an environmental or public health problem. To aid in this discussion, several examples of environmental episodes are discussed with an eye toward root causes. Knowledgebase needs to support the transition to systems thinking are discussed, including modularity and interoperability of models and methods}
}
@article{ADENIJI2023,
title = {Draft genome sequence of active gold mine isolate Pseudomonas iranensis strain ABS_30},
journal = {Microbiology Resource Announcements},
volume = {12},
number = {12},
year = {2023},
issn = {2576-098X},
doi = {https://doi.org/10.1128/MRA.00849-23},
url = {https://www.sciencedirect.com/science/article/pii/S2576098X23009234},
author = {Adetomiwa A. Adeniji and Ayansina S. Ayangbenro and Olubukola O. Babalola and Julie C. {Dunning Hotopp}},
keywords = {bioremediation, biosynthetic clusters, genome sequence, gold mine, , secondary metabolites},
abstract = {ABSTRACT
Pseudomonas iranensis ABS_30, isolated from gold mining soil, exhibits metal-resistant properties valuable for heavy metal removal. We report the draft genome sequencing of the P. iranensis ABS_30 strain, which is 5.9 Mb in size.}
}
@article{OXMAN1994141,
title = {Precedents in design: a computational model for the organization of precedent knowledge},
journal = {Design Studies},
volume = {15},
number = {2},
pages = {141-157},
year = {1994},
issn = {0142-694X},
doi = {https://doi.org/10.1016/0142-694X(94)90021-3},
url = {https://www.sciencedirect.com/science/article/pii/0142694X94900213},
author = {Rivka E Oxman},
keywords = {case-based reasoning, design precedents, memory organization},
abstract = {A computational model for the organization of design precedent knowledge is developed. The model is composed of distinct chunks of knowledge called design stories. A formalism for the design story is proposed which represents the linkage between design issue, concept and form in designs. Stories are structured in memory according to a semantic network. The lexicon of the semantic network acts as a memory index. The memory structure and indexing system are demonstrated to enhance search and to support cross-contextual browsing and exploration in the precedent library. The approach is demonstrated in a pilot design aid system in the task domain of early conceptual design in architecture.}
}
@article{BRESSANELLI2024142512,
title = {Are digital servitization-based Circular Economy business models sustainable? A systemic what-if simulation model},
journal = {Journal of Cleaner Production},
volume = {458},
pages = {142512},
year = {2024},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2024.142512},
url = {https://www.sciencedirect.com/science/article/pii/S0959652624019607},
author = {Gianmarco Bressanelli and Nicola Saccani and Marco Perona},
keywords = {Circular economy, Digital servitization, Sustainability impact assessment, Electrical and electronics equipment, Life cycle thinking, Systemic perspective},
abstract = {Manufacturing companies are struggling with the implementation of Circular Economy, especially due to the uncertainty regarding its potential sustainability benefits. In particular, and despite digital servitization is advocated by several studies as a way to achieve environmental gains, circular business models based on digital servitization are not always sustainable due to burden shifting and unexpected consequences which are difficult to assess before implementation. This is particularly relevant for the Electrical and Electronics Equipment industry, which suffers structural weaknesses such as the dependance on critical raw materials and an increasing waste generation. However, literature lacks models and tools able to address the complexity inherent in the systemic micro-macro perspective envisioned by Circular Economy, while studies that quantitatively assess the sustainability impacts and trade-offs of digital servitization-based circular scenarios are limited. This article aims to develop a better understanding of how the sustainability impacts of circular and servitized scenarios can be assessed and quantified at the economic, environmental, and social level, adopting a systemic perspective through the development of a what-if simulation model. The model is implemented in a spreadsheet tool and applied to a digital servitization-based Circular Economy scenario inspired by the case of a company offering long-lasting, high-efficient washing machines as-a-service. Results show that digital servitization can actually lead to a win-win-win situation with net positive effects to the environment, the society, and the economy. This result is based on the joint application of product design for digitalization and life extension, pay-per-use business models, and product reuse. These results are robust within a significant range of key parameters values. Practitioners and policymakers may use the model to support the evaluation of different circular and servitized scenarios before implementation.}
}
@article{FARAJ2021100337,
title = {Unto the breach: What the COVID-19 pandemic exposes about digitalization},
journal = {Information and Organization},
volume = {31},
number = {1},
pages = {100337},
year = {2021},
issn = {1471-7727},
doi = {https://doi.org/10.1016/j.infoandorg.2021.100337},
url = {https://www.sciencedirect.com/science/article/pii/S1471772721000038},
author = {Samer Faraj and Wadih Renno and Anand Bhardwaj},
keywords = {COVID, Digitalization, Technology, Organizing, Breaching experiment},
abstract = {Much recent scholarly investigation has been focused on the promise of digitalization and the new ways of working and organizing it makes possible. In this paper, we analyze how the COVID-19 pandemic has acted as a natural breaching experiment that has challenged taken-for-granted expectations about digitalization and revealed four important issues: uneven access to digital infrastructures, the persistence of the analog in digitalization, the brittleness of unchecked digitalization, and panoptical surveillance. The sudden shift to digital work has exposed taken-for-granted assumptions about the universality of digital access. The crisis has also revealed that many highly digitalized processes still rely on analog elements. The pandemic has also exposed that many algorithms used in digitalized inter-organizational processes are brittle due to overreliance on historic patterns. Finally, the pandemic has breached fundamental expectations of privacy when organizational surveillance was extended into private and public spaces. Thus, the pandemic has laid bare fundamental challenges in digitalization and has exposed the limits of rose‑tinted thinking about the relation between technology and organizing.}
}
@article{BELLO2025101316,
title = {Self-control on the path toward artificial moral agency},
journal = {Cognitive Systems Research},
volume = {89},
pages = {101316},
year = {2025},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101316},
url = {https://www.sciencedirect.com/science/article/pii/S1389041724001104},
author = {Paul Bello and Will Bridewell},
keywords = {Self-control, Attention, Cognitive architecture},
abstract = {The ability of agents to commit to their plans and see them through is a core concept in the philosophy of action (Bratman, 1987, Holton, 2009) and is considered to be a defining feature of having an intention. Seeing plans through in the face of highly compelling opportunities for action that are incompatible with our current commitments requires self-control. In this review paper, we draw upon ancient and modern literature on self-control along with contemporary ideas about the cognitive architecture supporting intentional action to argue that any computational account of moral agency must include an approach to self-control. In addition, we extract and develop a list of necessary features of the phenomena against which individual modeling efforts can be compared. The ARCADIA cognitive system will be discussed in light of this list of features and used to demonstrate both success and failure in a highly simplified self-control dilemma. Finally, we end by discussing a path toward more functionally complete models of agency and control, along with offering perfunctory thoughts on some of the more conceptually challenging issues to address in the future.}
}
@article{ZHANG2025100889,
title = {Identification of Critical Phosphorylation Sites Enhancing Kinase Activity With a Bimodal Fusion Framework},
journal = {Molecular & Cellular Proteomics},
volume = {24},
number = {1},
pages = {100889},
year = {2025},
issn = {1535-9476},
doi = {https://doi.org/10.1016/j.mcpro.2024.100889},
url = {https://www.sciencedirect.com/science/article/pii/S1535947624001798},
author = {Menghuan Zhang and Yizhi Zhang and Keqin Dong and Jin Lin and Xingang Cui and Yong Zhang},
keywords = {kinase activity, critical phosphorylation site, deep learning, phosphorylation mass spectrometry, embedding},
abstract = {Phosphorylation is an indispensable regulatory mechanism in cells, with specific sites on kinases that can significantly enhance their activity. Although several such critical phosphorylation sites (phos-sites) have been experimentally identified, many more remain to be explored. To date, no computational method exists to systematically identify these critical phos-sites on kinases. In this study, we introduce PhoSiteformer, a transformer-inspired foundational model designed to generate embeddings of phos-sites using phosphorylation mass spectrometry data. Recognizing the complementary insights offered by protein sequence data and phosphorylation mass spectrometry data, we developed a classification model, CSPred, which employs a bimodal fusion strategy. CSPred combines embeddings from PhoSiteformer with those from the protein language model ProtT5. Our approach successfully identified 77 critical phos-sites on 58 human kinases. Two of these sites, T517 on PKG1 and T735 on PRKD3, have been experimentally verified. This study presents the first systematic and computational approach to identify critical phos-sites that enhance kinase activity.}
}
@article{FLAHERTY2022114546,
title = {The conspiracy of Covid-19 and 5G: Spatial analysis fallacies in the age of data democratization},
journal = {Social Science & Medicine},
volume = {293},
pages = {114546},
year = {2022},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2021.114546},
url = {https://www.sciencedirect.com/science/article/pii/S0277953621008789},
author = {Eoin Flaherty and Tristan Sturm and Elizabeth Farries},
keywords = {Conspiracy theories, Spatial data, Health geography, Public data, COVID-19, 5G},
abstract = {In a context of mistrust in public health institutions and practices, anti-COVID/vaccination protests and the storming of Congress have illustrated that conspiracy theories are real and immanent threat to health and wellbeing, democracy, and public understanding of science. One manifestation of this is the suggested correlation of COVID-19 with 5G mobile technology. Throughout 2020, this alleged correlation was promoted and distributed widely on social media, often in the form of maps overlaying the distribution of COVID-19 cases with the instillation of 5G towers. These conspiracy theories are not fringe phenomena, and they form part of a growing repertoire for conspiracist activist groups with capacities for organised violence. In this paper, we outline how spatial data have been co-opted, and spatial correlations asserted by conspiracy theorists. We consider the basis of their claims of causal association with reference to three key areas of geographical explanation: (1) how social properties are constituted and how they exert complex causal forces, (2) the pitfalls of correlation with spatial and ecological data, and (3) the challenges of specifying and interpreting causal effects with spatial data. For each, we consider the unique theoretical and technical challenges involved in specifying meaningful correlation, and how their discarding facilitates conspiracist attribution. In doing so, we offer a basis both to interrogate conspiracists’ uses and interpretation of data from elementary principles and offer some cautionary notes on the potential for their future misuse in an age of data democratization. Finally, this paper contributes to work on the basis of conspiracy theories in general, by asserting how – absent an appreciation of these key methodological principles – spatial health data may be especially prone to co-option by conspiracist groups.}
}
@article{ADENIJI2023,
title = {Draft genome sequence of Priestia megaterium AB-S79 strain isolated from active gold mine},
journal = {Microbiology Resource Announcements},
volume = {13},
number = {2},
year = {2023},
issn = {2576-098X},
doi = {https://doi.org/10.1128/mra.01055-23},
url = {https://www.sciencedirect.com/science/article/pii/S2576098X23010629},
author = {Adetomiwa A. Adeniji and Ayansina S. Ayangbenro and Olubukola O. Babalola},
keywords = {bioremediation, biosynthetic traits, genome analysis, , secondary metabolites, genomics},
abstract = {ABSTRACT
We screened and isolated Priestia megaterium strain AB-S79 from active gold mine soil, then sequenced its genome to unravel its biosynthetic traits. The isolate with a 5.7-Mb genome can be utilized as a reference in genome-guided strain selection for metabolic engineering and other biotechnological operations.}
}
@article{CHEN2010573,
title = {Generating ontologies with basic level concepts from folksonomies},
journal = {Procedia Computer Science},
volume = {1},
number = {1},
pages = {573-581},
year = {2010},
note = {ICCS 2010},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2010.04.061},
url = {https://www.sciencedirect.com/science/article/pii/S1877050910000621},
author = {Wen-hao Chen and Yi Cai and Ho-fung Leung and Qing Li},
keywords = {Folksonomy, Ontology, Basic level categories, Category utility},
abstract = {This paper deals with the problem of ontology generation. Ontology plays an important role in knowledge representation, and it is an artifact describing a certain reality with specific vocabulary. Recently many researchers have realized that folksonomy is a potential knowledge source for generating ontologies. Although some results have already been reported on generating ontologies from folksonomies, most of them do not consider what a more acceptable and applicable ontology for users should be, nor do they take human thinking into consideration. Cognitive psychologists find that most human knowledge is represented by basic level concepts which is a family of concepts frequently used by people in daily life. Taking cognitive psychology into consideration, we propose a method to generate ontologies with basic level concepts from folksonomies. Using Open Directory Project (ODP) as the benchmark, we demonstrate that the ontology generated by our method is reasonable and consistent with human thinking.}
}
@article{HUANG2022108818,
title = {Hippocampus-heuristic character recognition network for zero-shot learning in Chinese character recognition},
journal = {Pattern Recognition},
volume = {130},
pages = {108818},
year = {2022},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2022.108818},
url = {https://www.sciencedirect.com/science/article/pii/S0031320322002990},
author = {Guanjie Huang and Xiangyu Luo and Shaowei Wang and Tianlong Gu and Kaile Su},
keywords = {Chinese character recognition, Hippocampus thinking, Radical analysis, Zero-shot learning, Label embedding},
abstract = {The recognition of Chinese characters has always been a challenging task due to their huge variety and complex structures. The current radical-based methods fail to recognize Chinese characters without learning all of their radicals in the training stage. To this end, we propose a novel Hippocampus-heuristic Character Recognition Network (HCRN), which can recognize unseen Chinese characters only by training part of radicals. More specifically, the network architecture of HCRN is a new pseudo-siamese network designed by us, which can learn features from pairs of input samples and use them to predict unseen characters. The experimental results on the recognition of printed and handwritten characters show that HCRN is robust and effective on zero/few-shot learning tasks. For the printed characters, the mean accuracy of HCRN outperforms the state-of-the-art approach by 23.93% on recognizing unseen characters. For the handwritten characters, HCRN improves the mean accuracy by 11.25% on recognizing unseen characters.}
}
@article{BEATY201922,
title = {Network neuroscience of creative cognition: mapping cognitive mechanisms and individual differences in the creative brain},
journal = {Current Opinion in Behavioral Sciences},
volume = {27},
pages = {22-30},
year = {2019},
note = {Creativity},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2018.08.013},
url = {https://www.sciencedirect.com/science/article/pii/S2352154618301219},
author = {Roger E Beaty and Paul Seli and Daniel L Schacter},
abstract = {Network neuroscience research is providing increasing specificity on the contribution of large-scale brain networks to creative cognition. Here, we summarize recent experimental work examining cognitive mechanisms of network interactions and correlational studies assessing network dynamics associated with individual creative abilities. Our review identifies three cognitive processes related to network interactions during creative performance: goal-directed memory retrieval, prepotent-response inhibition, and internally-focused attention. Correlational work using prediction modeling indicates that functional connectivity between networks — particularly the executive control and default networks — can reliably predict an individual’s creative thinking ability. We discuss potential directions for future network neuroscience, including assessing creative performance in specific domains and using brain stimulation to test causal hypotheses regarding network interactions and cognitive mechanisms of creative thought.}
}
@article{CAMARGOJUNIOR2016190,
title = {Optimal economic result and risk of parallel development of concept options in dynamic markets},
journal = {RAI Revista de Administração e Inovação},
volume = {13},
number = {3},
pages = {190-198},
year = {2016},
issn = {1809-2039},
doi = {https://doi.org/10.1016/j.rai.2016.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S1809203916300377},
author = {Alceu Salles {Camargo Júnior} and Abraham Sin Oih Yu},
keywords = {New product development, Economic result and risk of projects, Option thinking},
abstract = {New product development is an essential competence to organizations. Launching success products requires elaborate and precise knowledge about the technological platforms, like the most important market needs and characteristics, and the project team have to employ information systems to support the project decisions, which must be rapid and accurate. However, when the market characteristics are much dynamic and change rapidly or the development project aims at a really new product, the levels of uncertainties are greater, and the project team must employ more robust strategies of risk management. Option thinking is useful to develop several concept alternatives of some crucial subsystems of the new product in order to achieve new technical and market knowledge by repeating cycles of design, built and tested by several and different prototypes in parallel. These different prototypes develop, test and can accumulate knowledge about each one, different technologies, architectures and quality attributes or the usability for potential customers. This study achieves the optimal number of concept options to develop in parallel in order to maximize the economic performance of the development project of a new product constituted of two important subsystems. Mathematical models simulating the sequential decision process are developed to determine the economic result and risk of a two-subsystem product innovation project. Our results point the parallel development of concept options as a robust strategy to manage new product development mostly in adverse conditions, that is, with greater levels of uncertainties.}
}
@incollection{GALLISTEL199235,
title = {Classical Conditioning as an Adaptive Specialization: A Computational Model},
editor = {Douglas L. Medin},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {28},
pages = {35-67},
year = {1992},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(08)60487-9},
url = {https://www.sciencedirect.com/science/article/pii/S0079742108604879},
author = {C.R. Gallistel},
abstract = {Publisher Summary
This chapter analyzes the results of some modern classical conditioning experiments from the perspective of a computational model based on the assumption that the underlying learning process is specifically adapted to the domain of multivariate, nonstationary time series. It focuses on the quantitative results from experiments on the effects of partial reinforcement on the rate of acquisition and extinction because the other predictions of the model have been discussed and associative models are conspicuously unsuccessful at making quantitative predictions in this area. The model gives a mathematical characterization of the learning process from which one can derive the results of conditioning experiments. It is unlike these models in the sense that it is not in the associative tradition. The model replaces the associative explanatory framework with a framework that treats the conditioning process as a computational mechanism adapted through evolution to the peculiarities of one domain-a mechanism that solves one and only one of the several fundamentally distinct learning problems that confront mobile, multicellular organisms.}
}
@article{HENRY2016119,
title = {Hofmeister series: The quantum mechanical viewpoint},
journal = {Current Opinion in Colloid & Interface Science},
volume = {23},
pages = {119-125},
year = {2016},
issn = {1359-0294},
doi = {https://doi.org/10.1016/j.cocis.2016.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S1359029416301108},
author = {Marc Henry},
keywords = {Quantum mechanics, Phase coherence, Living cells, Condensed matter, Hofmeister series, Water, Aqueous solutions, Harmonic ratios},
abstract = {It is suggested that electromagnetic quantum vacuum fluctuations are at the very deep root of the so-called “specific ions effects” in concentrated solutions or in living cells. A many-body quantum-mechanical frame of thinking is proposed based on the concept of quantum coherence taking into account explicitly density and excitation frequencies of molecules and/or ionic species. It is also proposed that Hofmeister phenomena could have a natural explanation in the harmonic relationships between sets of characteristic frequencies ruled by quantum mechanical laws. It then follows that physical chemistry of concentrated media and biology should be ruled more by a quantum “symphony” between indistinguishable constituents rather than localized two-body electrical interactions between molecular or ionic species.}
}
@incollection{KUMAR20031,
title = {1 - An introduction to computational development},
editor = {Sanjeev Kumar and Peter J. Bentley},
booktitle = {On Growth, Form and Computers},
publisher = {Academic Press},
address = {London},
pages = {1-43},
year = {2003},
isbn = {978-0-12-428765-5},
doi = {https://doi.org/10.1016/B978-012428765-5/50034-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780124287655500347},
author = {Sanjeev Kumar and Peter J. Bentley}
}
@incollection{ROWLAND2003341,
title = {Chapter 16 - Interpreting Analytical Spectra with Evolutionary Computation},
editor = {Gary B. Fogel and David W. Corne},
booktitle = {Evolutionary Computation in Bioinformatics},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {341-365},
year = {2003},
series = {The Morgan Kaufmann Series in Artificial Intelligence},
isbn = {978-1-55860-797-2},
doi = {https://doi.org/10.1016/B978-155860797-2/50018-4},
url = {https://www.sciencedirect.com/science/article/pii/B9781558607972500184},
author = {Jem J. Rowland},
abstract = {Publisher Summary
This chapter deals with analytical techniques that are used to probe the activity and chemical makeup of cells. Metabolomics, the study of the entire biochemical constituents of a cell at any one time, is found to provide a rich means of monitoring organism activity. It can reveal explanations for different characteristics of seemingly similar organisms and can be used to relate function with gene. Spectroscopies are well suited to the study and interpretation of the metabolome in functional genomics. Another important technique in functional genomics is the measurement of gene expression via transcriptome arrays. This chapter outlines the various ways in which evolutionary computation (EC) can provide the basis for powerful tools for spectral interpretation and thus for functional genomics. It mentions various methods of forming predictive models from multivariate, often quasi-continuous data. It also discusses ways in which the effectiveness of such conventional techniques may be enhanced by combining them with evolutionary techniques.}
}
@article{XU201766,
title = {Emerging Trends for Microbiome Analysis: From Single-Cell Functional Imaging to Microbiome Big Data},
journal = {Engineering},
volume = {3},
number = {1},
pages = {66-70},
year = {2017},
issn = {2095-8099},
doi = {https://doi.org/10.1016/J.ENG.2017.01.020},
url = {https://www.sciencedirect.com/science/article/pii/S2095809917301595},
author = {Jian Xu and Bo Ma and Xiaoquan Su and Shi Huang and Xin Xu and Xuedong Zhou and Wei E. Huang and Rob Knight},
keywords = {Microbiome, Method development, Single-cell analysis, Big data, China Microbiome Initiative},
abstract = {Method development has always been and will continue to be a core driving force of microbiome science. In this perspective, we argue that in the next decade, method development in microbiome analysis will be driven by three key changes in both ways of thinking and technological platforms: ① a shift from dissecting microbiota structure by sequencing to tracking microbiota state, function, and intercellular interaction via imaging; ② a shift from interrogating a consortium or population of cells to probing individual cells; and ③ a shift from microbiome data analysis to microbiome data science. Some of the recent method-development efforts by Chinese microbiome scientists and their international collaborators that underlie these technological trends are highlighted here. It is our belief that the China Microbiome Initiative has the opportunity to deliver outstanding “Made-in-China” tools to the international research community, by building an ambitious, competitive, and collaborative program at the forefront of method development for microbiome science.}
}
@article{GALBUSERA2022103109,
title = {Game-based training in critical infrastructure protection and resilience},
journal = {International Journal of Disaster Risk Reduction},
volume = {78},
pages = {103109},
year = {2022},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2022.103109},
url = {https://www.sciencedirect.com/science/article/pii/S2212420922003284},
author = {Luca Galbusera and Monica Cardarilli and Marina {Gómez Lara} and Georgios Giannopoulos},
keywords = {Critical infrastructure, Resilience, Preparedness, Training, Exercises, Serious games, Gamification},
abstract = {Several institutions worldwide are reflecting on the relevance of training and exercises to critical infrastructure protection and resilience. This is witnessed, for instance, by Council Directive 2008/114/EC in the EU and the Homeland Security Exercise and Evaluation Program in the US. Contributing to the research actions in the field, the present article discusses methodological approaches, tools, techniques, and technologies relevant to this domain. In particular, we report on a recent training initiative elaborated by the authors and involving a game-based, modelling-and-simulation-backed, computer-assisted exercise for critical infrastructure expert audiences. This was developed taking advantage of JRC's Geospatial Risk and Resilience Assessment Platform (GRRASP) and critical infrastructure analysis methodologies integrated therein. The overarching objective was to enhance system thinking and raise awareness of resilience aspects while familiarizing participants with specific analysis tools and scientific models.}
}
@incollection{MCKELVEY199687,
title = {Chapter 2 Computation of equilibria in finite games},
series = {Handbook of Computational Economics},
publisher = {Elsevier},
volume = {1},
pages = {87-142},
year = {1996},
issn = {1574-0021},
doi = {https://doi.org/10.1016/S1574-0021(96)01004-0},
url = {https://www.sciencedirect.com/science/article/pii/S1574002196010040},
author = {Richard D. McKelvey and Andrew McLennan},
abstract = {Publisher Summary
This chapter provides an overview of the latest state of the art of methods for numerical computation of Nash equilibria —and refinements of Nash equilibria —for general finite n-person games. The appropriate method for computing Nash equilibria for a game depends on a number of factors. The first and most important factor involves, whether it is required to simply find one equilibrium (a sample equilibrium), or find all equilibria. The problem of finding one equilibrium is a well studied problem, and there exist number of different methods for numerically computing a sample equilibrium. The problem of finding all equilibria has been addressed recently. While, there exist methods for computation of all equilibria, they are computationally intensive. With current methods, they are only feasible on small problems. The chapter overviews methods for computing sample equilibria in normal form games, and discusses the computation of equilibria on extensive form games.}
}
@article{KIM201828,
title = {Degree of satisfaction-difference (DOSD) method for measuring consumer acceptance: A signal detection measurement with higher reliability than hedonic scaling},
journal = {Food Quality and Preference},
volume = {63},
pages = {28-37},
year = {2018},
issn = {0950-3293},
doi = {https://doi.org/10.1016/j.foodqual.2017.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0950329317301738},
author = {Min-A Kim and Danielle {van Hout} and Jean-Marc Dessirier and Hye-Seong Lee},
keywords = {Acceptance test, Affective product discrimination, Range effects, Indirect scaling, Satisfaction, Reference framing},
abstract = {Predictions of consumer acceptance are often based on hedonic scores, but these are determined not only by the consumer level of product liking, but also by consumer scale usage, which in turn is affected by thinking style and experimental contexts. To improve the validity and reliability of consumer acceptance measurement, a new indirect scaling method, the ‘Degree of Satisfaction-Difference (DOSD)’, was developed using a reminder design and signal detection theory (SDT). In DOSD, a product-specified ‘cognitive warm-up’ was used to evoke the consumer personal context and the internal evaluative criteria prior to product evaluation. In DOSD, each test product was presented together with a fixed-reference (identified as such) and consumers were asked to evaluate their satisfaction with the reference first with a sureness rating, and then to evaluate the test product for both absolute satisfaction and comparative satisfaction to the reference. The reliability of DOSD was tested against traditional hedonic scaling using an independent samples design of two consumer groups with equivalent cognitive reflection test profiles, each including High Reflection Thinkers (HRTs) and Low Reflection Thinkers (LRTs) in equal proportion. Each group tested two sets of skin lotions differing in product range, either using DOSD or hedonic scaling. When examining the affective discriminations of the two common products in terms of d′ values between product sets, the LRT subjects generated inconsistent responses with hedonic scaling, but reproducible responses with DOSD. The HRT subjects performed consistently using both scaling methods. These results validate DOSD’s superior reliability in affective tests and demonstrate its potential as an alternative consumer acceptance measurement to hedonic scaling.}
}
@article{DO2020110730,
title = {Capturing creative requirements via requirements reuse: A machine learning-based approach},
journal = {Journal of Systems and Software},
volume = {170},
pages = {110730},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110730},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220301631},
author = {Quoc Anh Do and Tanmay Bhowmik and Gary L. Bradshaw},
keywords = {Requirements reuse, Requirements engineering, Creativity in RE, Boilerplate, Natural language processing, Machine learning},
abstract = {The software industry has become increasingly competitive as we see multiple software serving the same domain and striving for customers. To that end, modern software needs to provide creative features to improve sustainability. To advance software creativity, research has proposed several techniques, including multi-day workshops involving experienced requirements analysts, and semi-automated tools to support creative thinking in a limited scope. Such approaches are either useful only for software with already rich issue tracking systems, or require substantial engagement from analysts with creative minds. In a recent work, we have demonstrated a novel framework that is beneficial for both novel and existing software and allows end-to-end automation promoting creativity. The framework reuses requirements from similar software freely available online, utilizes advanced natural language processing and machine learning techniques, and leverages the concept of requirement boilerplate to generate candidate creative requirements. An application of our framework on software domains: Antivirus, Web Browser, and File Sharing followed by a human subject evaluation have shown promising results. In this invited extension, we present further analysis for our research questions and report an additional evaluation by human subjects. The results exhibit the framework’s ability in generating creative features even for a relatively matured application domain, such as Web Browser, and provoking creative thinking among developers irrespective of their experience levels.}
}
@article{ROSSITER202017604,
title = {Using interactive tools to facilitate student self-testing of dynamics and PI compensation},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {17604-17609},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2677},
url = {https://www.sciencedirect.com/science/article/pii/S240589632033439X},
author = {J.A. Rossiter},
keywords = {Virtual laboratories, staff efficiency, student engagement, independent learning},
abstract = {Virtual laboratories have become a common tool in recent years for supporting student learning and engagement. This paper presents a new tool for helping students self-assess their competence in basic dynamics for 1st and 2nd order systems alongside simple PI compensation techniques. The tools provide a supported environment for helping students work towards the correct answer by providing succinct feedback on incorrect responses and opportunities to try again, while displaying relevant information. A partner interactive tool is also provided which focuses solely on assessment with no feedback, so that students can assess their ability to get correct answers in a scenario that only the first attempt counts. This paper gives the thinking behind the tools, their coding and also accessibility for students.}
}
@incollection{VERSCHAFFEL2010401,
title = {Mathematics Learning},
editor = {Penelope Peterson and Eva Baker and Barry McGaw},
booktitle = {International Encyclopedia of Education (Third Edition)},
publisher = {Elsevier},
edition = {Third Edition},
address = {Oxford},
pages = {401-406},
year = {2010},
isbn = {978-0-08-044894-7},
doi = {https://doi.org/10.1016/B978-0-08-044894-7.00517-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780080448947005170},
author = {L. Verschaffel and B. Greer and E. {De Corte}},
keywords = {Adaptive expertise, Assessment, Collaboration, Competence, Constructivism, Design experiment, High-stakes testing, Mathematical, Mathematics education, Mathematics learning, Mathematics teaching, Prior knowledge, Routine expertise, Situated cognition, Standards},
abstract = {This article presents a review of important recent themes and developments in research on the learning and teaching of mathematical knowledge and thinking. As a framework, we use a model for designing a powerful environment for learning and teaching mathematics; this model is structured according to four interrelated components, namely competence, learning, intervention, and assessment (CLIA-model) (De Corte et al., 2004). We argue and illustrate that our empirically based knowledge of each of these four interconnected components has substantially advanced over the past decades, enabling a progressively better understanding of not only the components that constitute a mathematical disposition, but also the nature of the learning and developmental processes that should be induced in students to facilitate the acquisition of competence, the characteristics of learning environments that are powerful in initiating and evoking those processes, and finally, the kind of assessment instruments that are appropriate to help monitor and support learning and teaching.}
}
@article{SON2015120,
title = {The history of Western futures studies: An exploration of the intellectual traditions and three-phase periodization},
journal = {Futures},
volume = {66},
pages = {120-137},
year = {2015},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2014.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S0016328714002079},
author = {Hyeonju Son},
keywords = {History of Western futures studies, Intellectual tradition, Periodization, Rationalization of futures, Industrialization of futures, Fragmentation of futures.},
abstract = {The main purpose of this paper is to present a three-phase periodization of modern Western futures studies to construct historical classification. In order to reach this goal, the following intellectual traditions are introduced to review the philosophical and historical contexts that affect the very foundations of futures studies: (a) religions, (b) utopias, (c) historicism, (d) science fiction, and (e) systems thinking. The first phase (beginning in 1945 to the 1960s) was the era of scientific inquiry and rationalization of the futures characterized by the prevalence of technological forecasting, the rise of alternative futures in systematic ways, and the growth of professionalization of futures studies. In the first phase, futures had become objects of rationalization removed from the traditional approaches such as utopia, grandiose evolutionary ideas, naive prophecies, science fiction, religious attitudes, and mystical orientation. The second phase (the 1970s and the 1980s) saw the creation the global institution and industrialization of the futures. This era was marked by the rise of worldwide discourse on global futures, the development of normative futures, and the deep involvement of the business community in futures thinking. In the second phase, futures studies-industry ties were growing and the future-oriented thoughts extensively permeated the business decision-making process. The third phase (the 1990s – the present) reflects the current era of the neoliberal view and fragmentation of the futures. This phase is taking place in the time of neoliberal globalization and risk society discourses and is characterized by the dominance of foresight, the advance of critical futures studies, and the intensification of fragmentation. In the third phase, futures practice tends to be confined to the support of strategic planning, and hence is experiencing an identity crisis and loss of its earlier status of humanity-oriented futures.}
}
@article{AUMER2024100111,
title = {Impaired cognitive flexibility in schizophrenia: A systematic review of behavioral and neurobiological findings},
journal = {Biomarkers in Neuropsychiatry},
volume = {11},
pages = {100111},
year = {2024},
issn = {2666-1446},
doi = {https://doi.org/10.1016/j.bionps.2024.100111},
url = {https://www.sciencedirect.com/science/article/pii/S2666144624000297},
author = {Philipp Aumer and Geva A. Brandt and Dusan Hirjak and Florian Bähner},
keywords = {Schizophrenia, Cognitive flexibility, Set-shifting, Wisconsin card sorting test, Intra-extra dimensional set shift, Cambridge Neuropsychological Test Automated Battery},
abstract = {Background and hypothesis
Impaired cognitive flexibility in schizophrenia (SZ) is well documented and correlation with worse functional outcome indicates clinical relevance. Paradigms that assess cognitive flexibility include the Wisconsin Card Sorting Test (WCST) and the Cambridge Neuropsychological Test Automated Battery’s (CANTAB) Intra-Extra Dimensional Set Shift (IED). This systematic review provides an overview of the current state of research on cognitive flexibility in schizophrenia and points out relevant areas of non-consensus.
Methods
Two electronic databases (Embase and PubMed) were searched for records published from 1993 to 2024 on adult SZ patients that were assessed for cognitive flexibility/set-shifting ability using WCST and/or IED.
Results
38 studies were included in the review, most of which reported significantly worse performance of SZ patients in WCST and/or IED compared to healthy controls (HC). Most publications focused on the specific profile of cognitive inflexibility. Other aspects included progression of cognitive inflexibility over the course of the illness, neurobiological correlates, IQ as a possible confounder and whether cognitive inflexibility is a heritable trait.
Conclusion
Included studies show that cognitive inflexibility rather reflects a stable trait than a state, indicating a lasting prefrontal impairment in SZ. Further longitudinal studies are needed to clarify how these deficits evolve during progression of the disorder. Neither antipsychotic medication nor intelligence seem to explain impaired cognitive flexibility. However, a disease-specific cognitive phenotype has not yet been established and additional research on neuro-computational mechanisms is thus needed to identify possible targets for interventional studies.}
}
@article{COUVELAS2020326,
title = {Bioclimatic building design theory and application},
journal = {Procedia Manufacturing},
volume = {44},
pages = {326-333},
year = {2020},
note = {The 1st International Conference on Optimization-Driven Architectural Design (OPTARCH 2019)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.238},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920308258},
author = {Agnes Couvelas},
keywords = {Modern architecture, Cultural heritage, Sustainable architecture design, Bioclimatic Performance Optimization, Inter-locality},
abstract = {Ecological thinking is the recognition of the dialectic unity between natural and man-made environment, the respect to what exists around us, and the concomitant “openness” toward others. Here, I present examples from my own work to describe a number of passive bioclimatic approaches focused on the above principles. First, the use of the wind as an expressive element in building design, including the enhancement of air flow in the interior space, the moderation of wind and sand accumulation, the moderation of the sound carried by prevailing winds, and the conversion of the wind into a means of protection against its own force. Second, the use of adaptive building envelopes and shading systems to achieve control of natural light, ventilation and temperature of the inner space through their own transformability, surface openings and materials, including planting as a building material; in a sense, treating buildings as living organisms. Three of these examples have been included in the H2020-MSCA-RISE OptArch project, in which I am scientifically responsible for the work package WP5 entitled “Improvement of bioclimatic design through optimization of performance”.}
}
@article{ZHANG2024103588,
title = {Anonymous data sharing scheme for resource-constrained internet of things environments},
journal = {Ad Hoc Networks},
volume = {163},
pages = {103588},
year = {2024},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2024.103588},
url = {https://www.sciencedirect.com/science/article/pii/S1570870524001999},
author = {Zetian Zhang and Jingyu Wang and Lixin Liu and Yongfeng Li and Yun Hao and Hanqing Yang},
keywords = {Anonymity, Resource-constrained, Integrity verification, Data sharing, Accountability, Revocation},
abstract = {With the rapid development of Internet of Things (IoT) technology in industrial, agricultural, medical and other fields, IoT terminal devices face security and privacy challenges when sharing data. Among them, ensuring data confidentiality, achieving dual-side privacy protection, and performing reliable data integrity verification are basic requirements. Especially in resource-constrained environments, limitations in the storage, computing, and communication capabilities of devices increase the difficulty of implementing these security safeguards. To address this problem, this paper proposes a resource-constrained anonymous data-sharing scheme (ADS-RC) for the IoT. In ADS-RC, we use elliptic curve operations to replace computation-intensive bilinear pairing operations, thereby reducing the computational and communication burden on end devices. We combine an anonymous verifiable algorithm and an attribute encryption algorithm to ensure double anonymity and data confidentiality during the data-sharing process. To deal with potential dishonest behavior, this solution supports the revocation of malicious user permissions. In addition, we designed a batch data integrity verification algorithm and stored verification evidence on the blockchain to ensure the security and traceability of data during transmission and storage. Through experimental verification, the ADS-RC scheme achieves reasonable efficiency in correctness, security and efficiency, providing a new solution for data sharing in resource-constrained IoT environments.}
}
@article{REISS1967193,
title = {Individual thinking and family interaction—II. A study of pattern recognition and hypothesis testing in families of normals, character disorders and schizophrenics},
journal = {Journal of Psychiatric Research},
volume = {5},
number = {3},
pages = {193-211},
year = {1967},
issn = {0022-3956},
doi = {https://doi.org/10.1016/0022-3956(67)90002-7},
url = {https://www.sciencedirect.com/science/article/pii/0022395667900027},
author = {David Reiss},
abstract = {The present study investigated the relationship between family interaction and individual pattern recognition in five families of normals, five families of character disorders and five families of schizophrenics. Following a period of family interaction, members of normal families showed improvement in pattern recognition; members of families of schizophrenics showed deterioration or no change and members of character disorder families were in between. During the period of family interaction, members of normal families were independent and adventuresome in testing their pattern concepts whereas members of families of schizophrenics were cautios, copied each other's performance but showed little pooling of ideas. These findings support the hypothesis that family interaction can influence perceptual process in its individual members in a short time and points to some particular relationships between family interaction and individual perception.}
}
@article{LOCKWOOD2019100688,
title = {Computing as a mathematical disciplinary practice},
journal = {The Journal of Mathematical Behavior},
volume = {54},
pages = {100688},
year = {2019},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2019.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732312318300282},
author = {Elise Lockwood and Anna F. DeJarnette and Matthew Thomas},
keywords = {Computation, Mathematical disciplinary practices, Mathematicians},
abstract = {In this paper, we make a case for computing as a mathematical disciplinary practice. We present results from interviews with research mathematicians in which they reflected on the use of computing in their professional work. We draw on their responses to present evidence that computing is an inherent part of doing mathematics and is a practice they want their students to develop. We also discuss the mathematicians’ perspectives on how they learned and teach computing, and we suggest that much needs to be explored about how to teach computing effectively. Our overarching goal is to draw attention to the importance of the teaching and learning of computing, and we argue that it is an imperative topic of study in mathematics education research.}
}
@article{SHUBBAR2024382,
title = {Bridging Qatar's food demand and self-sufficiency: A system dynamics simulation of the energy–water–food nexus},
journal = {Sustainable Production and Consumption},
volume = {46},
pages = {382-399},
year = {2024},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2024.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S2352550924000423},
author = {Haya Talib Shubbar and Furqan Tahir and Tareq Al-Ansari},
keywords = {Carbon emissions, Energy-water-food nexus, Food self-sufficiency, Food security, Qatar, System dynamics},
abstract = {The food sector in Qatar is confronted with formidable challenges due to its harsh environmental conditions. Striving for total food self-sufficiency in such an environment would inevitably exert pressure on the energy and water sectors. This heightened demand for energy and water translates into increased costs and escalates environmental impacts. Consequently, this study embarks on an in-depth analysis of food production within the context of Qatar's energy-water-food nexus, aiming to demonstrate how varying degrees of food self-sufficiency may impact the demand on Qatar's water and energy sectors, as well as on greenhouse gas (GHG) emissions. Moreover, this study demonstrates to what extent specific subsystems within the nexus can be modified to enhance sustainability. An energy-water-food nexus is meticulously crafted within the proposed framework to elucidate the intricate interdependencies among these sectors, incorporating pertinent external variables. These interconnections are then transmuted into a system dynamics model (SDM), facilitating a nuanced exploration of potential transformations and their ripple effects. Furthermore, a life-cycle thinking approach explicitly tailored to Qatar was implemented to estimate GHG emissions accurately. Four distinct scenarios are rigorously examined using the SDM, spanning from a status quo perspective to ambitious transitions toward full food self-sufficiency. The findings of the scenarios indicate that scenario 4, which partially provides the country with its food demands locally using desalinated water, treated wastewater, and groundwater and satisfies 20 % of its energy demand from solar energy, is the most ideal with an annual 5.36 × 1010 kWh/year energy consumption, 1.73 × 1012 l/year water demand, and 3.26 × 1010 kg CO2 eq./year emissions. The outcomes underscore the imperative for prioritizing less energy-intensive resources to mitigate overall energy consumption. Additionally, achieving an optimal national scenario necessitates a judicious equilibrium between food imports and domestic production.}
}
@article{ASGARI20251631,
title = {Uncovering the role of big data analytics on the resilience of agri-food supply chains: a systematic literature review},
journal = {Procedia Computer Science},
volume = {253},
pages = {1631-1639},
year = {2025},
note = {6th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.01.225},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925002339},
author = {Alireza Asgari},
keywords = {Big Data Analytics, Resilience, Agri-food Supply Chain, systematic literature review},
abstract = {In an era marked by frequent disruptions, ensuring the resilience of agri-food supply chains is critical for maintaining food availability and accessibility. The significant increase in generation and complexity of data due to the advancement of computational power and relational databases, has made the discovery of new information through the process of data collection and manipulation possible. Big data analytics can contribute significantly to attaining agri-food supply chain and has many implications inside the food sector. This systematic literature review investigates the role of big data analytics in bolstering agri-food supply chain’s ability to anticipate, respond to, and recover from disruptions. The findings reveal a positive influence of predictive and prescriptive analytics on readiness and response phases of resilience, particularly on capabilities such as risk management, flexibility, and agility. Based on these findings, a comprehensive framework is proposed which maps the current state-of-the-art in addition to identifying gaps in the current literature, offering valuable insights for practitioners and guiding future research.}
}
@article{LIN20162176,
title = {New statistical analysis in marketing research with fuzzy data},
journal = {Journal of Business Research},
volume = {69},
number = {6},
pages = {2176-2181},
year = {2016},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2015.12.026},
url = {https://www.sciencedirect.com/science/article/pii/S0148296315006517},
author = {Hsin-Cheng Lin and Chen-Song Wang and Juei Chao Chen and Berlin Wu},
keywords = {Decision making, Fuzzy statistics, Fuzzy data, Marketing research},
abstract = {This research proposes new statistical methods for marketing research and decision making. The study employs a soft computing technique and a new statistical tool to evaluate people's thinking. Because the classical measurement system has difficulties in dealing with the non-real valued information, the study aims to find an appropriate measurement system to overcome this problem. The main idea is to decompose the data into a two-dimensional type, centroid and its length (area). The two-dimensional questionnaires this study proposes help reaching market information.}
}
@article{NYSTROM201077,
title = {Ontological musings on how nature computes},
journal = {Procedia Computer Science},
volume = {1},
number = {1},
pages = {77-86},
year = {2010},
note = {ICCS 2010},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2010.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S1877050910000116},
author = {J.F. Nystrom},
keywords = {Universe as computation, Quantum vacuum, Computational cosmography},
abstract = {Modern physical theory and modern computational techniques are used to provide conjecture on how nature computes. I utilize time-domain simulation of physical phenomena and build analogies between elements of computation and the “things” of Universe computation, resulting, for example, in the identification of the quantum vacuum as the power source for Universe computation. While reviewing how Universe can be viewed as a computation, we find the need for Negative Universe (which is a part of the quantum vacuum mechanism). This idea is compared with Penrose’s current model which utilizes a separate Platonic world outside of physical Universe. Lastly, in the Discussion, I present an updated version of computational cosmography as a model for Universe as computation.}
}
@article{DAS2025111780,
title = {ANN-based prediction of aerodynamic force coefficients and shape optimization using MOEO of a high-rise building with varying cross-section along height},
journal = {Journal of Building Engineering},
volume = {100},
pages = {111780},
year = {2025},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2025.111780},
url = {https://www.sciencedirect.com/science/article/pii/S2352710225000166},
author = {Arghyadip Das and Sujit Kumar Dalui},
keywords = {Tall building, Computational fluid dynamics (CFD), Corner recession, Force coefficient, Artificial neural network (ANN), Multi-objective equilibrium optimization (MOEO)},
abstract = {The main goal of this study is to find the effect of various geometrical configurations with varying cross-sections along the height of a corner recessed square tall building, considering the wind flow around the buildings from 0° to 360°. The design variables considered in this study are the amount of corner recessing (S), the height of the square cross-section (h) and the wind incidence angle (Ø). The design variables are generated using the simple random sampling technique in MATLAB. A series of numerical analyses have been performed for those randomly generated design variables using Computational Fluid Dynamics (CFD) in the ANSYS-CFX module to evaluate the along and across-wind force coefficients (Cfx and Cfy). The numerical simulations have been carried out using the k-ε turbulence model on a length scale of 1:300. The results of the CFD simulations are used to train the artificial neural network (ANN) of Cfx and Cfy. After simulating the networks of Cfx and Cfy, a shape optimization study has been carried out using Multi-Objective Equilibrium Optimization (MOEO) to find the optimal shapes of various building configurations considering different weights of the design function values (Cfx and Cfy). The results of the shape optimization study have been validated with separate CFD investigations and several wind tunnel experiments.}
}
@article{RAI201651,
title = {Fragmentary shape recognition: A BCI study},
journal = {Computer-Aided Design},
volume = {71},
pages = {51-64},
year = {2016},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2015.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0010448515001542},
author = {Rahul Rai and Akshay V. Deshpande},
keywords = {Brain–Computer Interfaces (BCI), Fragmentary shape recognition, User studies, Cognitive load in shape processing, Natural interactions},
abstract = {Recently, Brain–Computer Interface (BCI) has emerged as a potential modality that utilizes natural and intuitive human mechanisms of thinking process to enable interactions in CAD interfaces. Before BCI could become a mainstream mode of HCI for CAD interfaces; fundamental studies directed towards understanding how humans mentally represent and process the geometry are needed. The outlined work in this paper presents an objective user study to understand shape recognition process in the humans. Specifically, we focus on the fundamental task of fragmentary shape identification. The problem of fragmentary shape recognition can be defined as follows: given a partial and incomplete minimalistic representation of a given shape, can one recognize the actual complete shape or object? In user studies, each subject was progressively (in stages) shown more informative fragmented images of an object to be recognized. During each stage of the experiment, the brain activity of users in the form of electroencephalogram (EEG) signals was recorded with a BCI headset. The recorded signals are then processed to objectively study the fragmentary shape recognition process. The results of user studies conclusively show that the measured brain activities of subjects can serve as a very accurate proxy to estimate subjects fragmentary shape recognition process.}
}
@article{SCOLOZZI2017957,
title = {The anthroposphere as an anticipatory system: Open questions on steering the climate},
journal = {Science of The Total Environment},
volume = {579},
pages = {957-965},
year = {2017},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2016.10.086},
url = {https://www.sciencedirect.com/science/article/pii/S0048969716322604},
author = {Rocco Scolozzi and Davide Geneletti},
keywords = {System thinking, Future studies, Climate change, System dynamics, Anticipatory system, Transdisciplinary},
abstract = {Climate change research and action counteracting it affect everyone and would involve cross-societal transformations reshaping the anthroposphere in its entirety. Scrutinizing climate-related science and policies, we recognize attempts to steer the evolution of climate according to expected (or modelled) futures. Such attempts would turn the anthroposphere into a large “anticipatory system”, in which human society seeks to anticipate and, possibly, to govern climate dynamics. The chief aim of this discussion paper is to open a critical debate on the climate change paradigm (CCP) drawing on a strategic and systemic framework grounded in the concept of anticipatory system sensu Rosen (1991). The proposed scheme is ambitiously intended to turn an intricate issue into a complex but structured problem that is to say, to make such complexity clear and manageable. This framework emerges from concepts borrowed from different scientific fields (including future studies and system dynamics) and its background lies in a simple quantitative literature overview, relying upon a broad level of analysis. The proposed framework will assist researchers and policy makers in thinking of CCP in terms of an anticipatory system, and in disentangling its interrelated (and sometimes intricate) aspects. In point of fact, several strategic questions related to CCP were not subjected to an adequate transdisciplinary discussion: what are the interplays between physical processes and social-political interventions, who is the observer (what he/she is looking for), and which paradigm is being used (or who defines the desirable future). The proposed scheme allows to structure such various topics in an arrangement which is easier to communicate, highlighting the linkages in between, and making them intelligible and open to verification and discussion. Furthermore, ideally developments will help scientists and policy makers address the strategic gaps between the evidence-based climatological assessments and the plurality of possible answers as applied to the geopolitical contingencies.}
}
@incollection{MILLER2023203,
title = {Chapter 7 - The calculated uncertainty of scientific discovery: From Maths to Deep Maths},
editor = {Steven G. Krantz and Arni S.R. {Srinivasa Rao} and C.R. Rao},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {49},
pages = {203-226},
year = {2023},
booktitle = {Artificial Intelligence},
issn = {0169-7161},
doi = {https://doi.org/10.1016/bs.host.2023.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S016971612300024X},
author = {D. Douglas Miller},
keywords = {Mathematics, Philosophy, Statistics, Null hypothesis, Artificial intelligence, Data dimensionality, Machine learning, Algorithms, Deep learning, Stochastic gradient descent, Model optimization, Bias, Neural networks, Backpropagation, Large language models, Model generalizability},
abstract = {Throughout history, diverse Maths have underpinned numerous important natural and physical science discoveries. In their initial development and application, these Maths were often incompletely or imperfectly understood, with constants and “fudge factors” needed to account for statistical uncertainties to advance a scientific discipline. Some polymaths have acted as philosophers in support of new ways of thinking, based on their novel discoveries about the natural and physical world. Deep Maths integral to artificial intelligence (AI), machine learning and deep learning (DL), are also subject to human imperfections (i.e., computational errors, operator assumptions) and stochastic uncertainties (i.e., modeling biases, convergence optimizers). Mathematicians and domain experts can collaborate to increase AI model accuracy by improving training data quality (i.e., curating, reducing dimensionality), mitigating human and machine biases, and understanding data contexts prior to query. Since the advent of DL and through the design of multilayered feedforward neural networks then large language models, scientists have applied advanced AI computing capabilities to push the limits of this technology trend. Recently, AI's capacity to uncover newly modeled insights has been hyped beyond the proven limits of DL model accuracy. History has witnessed the acceptance of new knowledge (primarily by peers) based on the accuracy and/or reproducibility of empirical observations and on varied interpretations of mathematical proofs. Societal enthusiasm for science or technology insertion is often limited by the general public's understanding of the underlying Maths and Deep Maths, and related human fears and concerns of displacement (i.e., lost jobs, ecological impact, less privacy, etc.). Today's proponents of societal progress based on new discoveries and technologies are motivated by a range of influences (i.e., humanity, control, security, profit, etc.), creating additional uncertainties that can deflect initial scientific enthusiasm and/or delay widespread adoption.}
}
@article{BREIT2025102621,
title = {Mathematics achievement and learner characteristics: A systematic review of meta-analyses},
journal = {Learning and Individual Differences},
volume = {118},
pages = {102621},
year = {2025},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2024.102621},
url = {https://www.sciencedirect.com/science/article/pii/S1041608024002140},
author = {Moritz Breit and Michael Schneider and Franzis Preckel},
keywords = {Mathematics achievement, meta-analysis, Systematic review, Math talent, Predictors},
abstract = {Learners' individual differences in mathematics achievement are associated with individual differences in psychological characteristics. A number of meta-analyses have quantified the strengths of these correlations. However, these findings are scattered across different strands of the literature. The present systematic review aims to integrate these strands by providing an overview of meta-analyses of psychological correlates of mathematics achievement. We conducted a systematic literature search and included 30 meta-analyses, reporting correlations between mathematics achievement and 66 variables based on 13,853 effect sizes and an estimated 4,658,717 participants. The correlations are rank-ordered by size and complemented with information about the meta-analyses, their inclusion criteria, and methods. The results show strong associations of mathematics achievement with verbal skills and abilities, prior knowledge, intelligence, creativity, math-specific skills, math self-concept, self-regulation, meta-cognition, and executive functions. Relatively weaker relations were observed for emotional intelligence, achievement goals, academic emotions, and the Big Five personality traits.}
}
@article{AWD2023107403,
title = {A review on the enhancement of failure mechanisms modeling in additively manufactured structures by machine learning},
journal = {Engineering Failure Analysis},
volume = {151},
pages = {107403},
year = {2023},
issn = {1350-6307},
doi = {https://doi.org/10.1016/j.engfailanal.2023.107403},
url = {https://www.sciencedirect.com/science/article/pii/S1350630723003576},
author = {Mustafa Awd and Lobna Saeed and Frank Walther},
keywords = {Modeling, Simulation, Finite Element Analysis, Analytical Models, Multi-Physics, Data-Driven Modeling, Additive Manufacturing, Failure Mechanisms},
abstract = {This review discusses the feasibility of using microstructure- and defect-sensitive models to predict the fatigue behavior of additively generated materials through a non-exclusive qualitative assessment of the current literature on the structural integrity of additively manufactured structures. The time it takes to implement additively manufactured structures is reduced if a computational model can predict and enhance their mechanical performance. Computational modeling techniques can express nonlinear, multimodal functions in failure analysis of engineering materials, reducing environmental waste and providing sustainable technology. Machine learning is used in manufacturing and industrial sectors to optimize process parameters and model data-driven correlations between processes, structures, and properties. Machine learning and artificial intelligence can be combined to enable atomistic-scale damage tolerance design, which can be customized using computer programming. The main advantage of machine learning is that it can be well integrated with finite element, analytical, or empirical modeling with a significant increase in the yield of the model being used.}
}
@article{KIM201716,
title = {A study on metadata structure and recommenders of biological systems to support bio-inspired design},
journal = {Engineering Applications of Artificial Intelligence},
volume = {57},
pages = {16-41},
year = {2017},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2016.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0952197616301786},
author = {Sun-Joong Kim and Ji-Hyun Lee},
keywords = {Bio-inspired design, Biological system metadata modeling, Knowledge-based system, Recommendation system, Ontology},
abstract = {Bio-inspired design was introduced as an alternative method to encourage breakthrough innovations during design projects by stimulating analogical reasoning and thinking of designers. However, the method did not perform as well as researchers expected because most designers, who are novices in the fields of biology and ecology, cannot infer the proper analogue (i.e. biological system) from nature. To resolve this fundamental problem, a causal model based representation framework for ‘analogical reasoning’ – searching and selecting the biological systems to apply – have been developed. In addition, ontology based repository structures and retrieval systems have been proposed to support ‘analogical thinking’ of designers. Nevertheless, these systematic approaches still restrict the candidates and inevitably lose potential biological systems relevant to the design project, due to the ‘physical relation’ biased problem and the ambiguity of the indexing mechanism of both current representation frameworks and retrieval systems. For example, the causality based support system known as a robust representation framework for a single biological system, stores information of a biological system only by its internal ‘physical relations’ and retrieves biological systetabms only by the physical relevance. However, from the perspective of ecological thinking, the further relatedness of ‘physical, biological, and ecological relations’ composes the holistic concept used to identify an organism in the flow of evolution because the ‘biological and ecological relations’ are also involved in the traits that designers may be interested in. Therefore, the supplementary information for ‘biological and ecological relations’ must be added to index the biological and environmental interactions, and to use the connectivity among entire organisms in the retrieval process. In this research, a causality based holistic representation framework for biological systems and an ‘all-connected’ ontology based repository and retrieval system are developed as a knowledge-based recommendation system to support bio-inspired design. The knowledge-based system we developed allows engineering designers to search and select a particular biological system and extract design strategy without much biological knowledge. This effort provides more opportunities in a bio-inspired design process by adding potential biological systems that might previously not have been considered.}
}