@article{XU2024167,
title = {Towards carbon neutrality in China: A systematic identification of China's sustainable land-use pathways across multiple scales},
journal = {Sustainable Production and Consumption},
volume = {44},
pages = {167-178},
year = {2024},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2023.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S235255092300283X},
author = {Zhenci Xu},
keywords = {Carbon neutrality, Land use, Multi-scales, System thinking, China},
abstract = {Sustainable land use is crucial for achieving Carbon Neutrality goals, which requires a scientific identification of optimized pathways for land use patterns across multiple scales. Yet, current land use studies predominantly focus on single scales but lack system thinking and fail to establish complementary cross-regional carbon neutrality collaboration schemes. Applying life-cycle thinking to analyze land use sustainability and carbon neutrality potential at multiple scales could address this challenge. This study aims to present China's first multi-scale spatiotemporal optimization pathway for sustainable land use to improve carbon neutrality potential. It systematically integrates the complex spatial coupling relationships between land use intensity and efficiency. We integrate multi-scale sustainable land use pathways, spanning grid, basin, and administrative levels, and unveil significant variations in land use sustainability and carbon neutrality potential across China. Sixty-three percent of China's land is in low sustainability, and the overall carbon neutrality potential in China is relatively low, with regions accounting for <30 % facing more carbon neutrality missions. Implementing sequential and partitioned governance modes can effectively support China in achieving sustainable land use and advancing Carbon Neutrality goals. Our sustainable land use pathways for China provide valuable insights for systematically undertaking carbon neutrality actions across different scales.}
}
@article{FRANCIS2022103521,
title = {A framework for dynamic life cycle sustainability assessment and policy analysis of built environment through a system dynamics approach},
journal = {Sustainable Cities and Society},
volume = {76},
pages = {103521},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2021.103521},
url = {https://www.sciencedirect.com/science/article/pii/S2210670721007873},
author = {Ann Francis and Albert Thomas},
keywords = {Sustainability assessment, System dynamics, Dynamic life cycle sustainability assessment (D-LCSA), Computational modelling, Life cycle assessment},
abstract = {Sustainability is gaining attention, particularly in the building sector, owing to its significant influence on economy, society and environment. However, most assessment methods/frameworks available for this sector focus solely or dominantly on the environmental dimension of sustainability. Hence, a sustainability assessment framework for buildings that accounts for the interdependencies amongst social, economic and environmental aspects is essential. Further, buildings also undergo several time-induced changes in their characteristics, such as changes in electricity consumption, material properties, surrounding infrastructure and energy mix that can influence their sustainability. Therefore, this paper introduces a system dynamics-based methodological framework for Dynamic Life Cycle Sustainability Assessment (D-LCSA) capable of incorporating the dynamic changes in the building characteristics with time and capturing the interactions amongst different sustainability indicators. The usability and utility of the framework is demonstrated using a case study residential project in India. The case study results show that ignoring time-dependant dynamic aspects in sustainability assessment of buildings leads to underestimating the overall sustainability impacts by about 50 per cent and specific environmental impacts by about 12 per cent. Therefore, the study reinforces the need to adopt dynamic thinking through modelling and simulation to predict sustainability performance in the built environment.}
}
@article{CORCORAN2020158,
title = {Language as a biomarker for psychosis: A natural language processing approach},
journal = {Schizophrenia Research},
volume = {226},
pages = {158-166},
year = {2020},
note = {Biomarkers in the Attenuated Psychosis Syndrome},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2020.04.032},
url = {https://www.sciencedirect.com/science/article/pii/S0920996420302474},
author = {Cheryl M. Corcoran and Vijay A. Mittal and Carrie E. Bearden and Raquel {E. Gur} and Kasia Hitczenko and Zarina Bilgrami and Aleksandar Savic and Guillermo A. Cecchi and Phillip Wolff},
keywords = {Psychosis, Automated language analysis, Natural language processing, Machine learning, Semantic coherence, Discourse coherence, Referential coherence, Semantic density, Latent semantic analysis, Digital phenotyping, Psychosis risk, Clinical high risk, Ultra high risk, Schizophrenia},
abstract = {Human ratings of conceptual disorganization, poverty of content, referential cohesion and illogical thinking have been shown to predict psychosis onset in prospective clinical high risk (CHR) cohort studies. The potential value of linguistic biomarkers has been significantly magnified, however, by recent advances in natural language processing (NLP) and machine learning (ML). Such methodologies allow for the rapid and objective measurement of language features, many of which are not easily recognized by human raters. Here we review the key findings on language production disturbance in psychosis. We also describe recent advances in the computational methods used to analyze language data, including methods for the automatic measurement of discourse coherence, syntactic complexity, poverty of content, referential coherence, and metaphorical language. Linguistic biomarkers of psychosis risk are now undergoing cross-validation, with attention to harmonization of methods. Future directions in extended CHR networks include studies of sources of variance, and combination with other promising biomarkers of psychosis risk, such as cognitive and sensory processing impairments likely to be related to language. Implications for the broader study of social communication, including reciprocal prosody, face expression and gesture, are discussed.}
}
@incollection{CLEEREMANS200581,
title = {Computational correlates of consciousness},
editor = {Steven Laureys},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {150},
pages = {81-98},
year = {2005},
booktitle = {The Boundaries of Consciousness: Neurobiology and Neuropathology},
issn = {0079-6123},
doi = {https://doi.org/10.1016/S0079-6123(05)50007-4},
url = {https://www.sciencedirect.com/science/article/pii/S0079612305500074},
author = {Axel Cleeremans},
abstract = {Over the past few years numerous proposals have appeared that attempt to characterize consciousness in terms of what could be called its computational correlates: Principles of information processing with which to characterize the differences between conscious and unconscious processing. Proposed computational correlates include architectural specialization (such as the involvement of specific regions of the brain in conscious processing), properties of representations (such as their stability in time or their strength), and properties of specific processes (such as resonance, synchrony, interactivity, or information integration). In exactly the same way as one can engage in a search for the neural correlates of consciousness, one can thus search for the computational correlates of consciousness. The most direct way of doing is to contrast models of conscious versus unconscious information processing. In this paper, I review these developments and illustrate how computational modeling of specific cognitive processes can be useful in exploring and in formulating putative computational principles through which to capture the differences between conscious and unconscious cognition. What can be gained from such approaches to the problem of consciousness is an understanding of the function it plays in information processing and of the mechanisms that subtend it. Here, I suggest that the central function of consciousness is to make it possible for cognitive agents to exert flexible, adaptive control over behavior. From this perspective, consciousness is best characterized as involving (1) a graded continuum defined over quality of representation, such that availability to consciousness and to cognitive control correlates with properties of representation, and (2) the implication of systems of meta-representations.}
}
@article{USMANI20241044,
title = {The Digital Age: Exploring the Intersection of AI/CI and Human Cognition and Social Interactions},
journal = {Procedia Computer Science},
volume = {239},
pages = {1044-1052},
year = {2024},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2023},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.06.268},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924015114},
author = {Usman Ahmad Usmani and Ari Happonen and Junzo Watada},
keywords = {Artificial intelligence, Computational intelligence, Digitalization, Digital transformation, Human cognition, Social interaction, Industry 4.0, Digital capability, Social transformation, Human computer interaction},
abstract = {Although solutions based on artificial and computational intelligence have made life easier, the fast development of technology also raises questions about near future and log term human cognition and social interaction. Through a survey of the literature and qualitative analysis, our work examined current research on how the AI/CI affects human cognitive functions and social interactions. We discuss how AI and CI are influencing e.g. how we humans gather information, build relationships, and communicate with others, with and without the new frontline technologies. Additionally, proposals for future advances are discussed along with the ethical and societal ramifications these technologies have, could and might bring into our lives. We think that by developing a deeper knowledge of how AI/CI affects human cognition and social interaction, new contributions are made to a positive conversation and encourage a responsible approach to incorporating new technologies into our daily lives.}
}
@article{MARIS2022131391,
title = {Structure and dynamics of methacrylamide, a computational and free-jet rotational spectroscopic study},
journal = {Journal of Molecular Structure},
volume = {1248},
pages = {131391},
year = {2022},
issn = {0022-2860},
doi = {https://doi.org/10.1016/j.molstruc.2021.131391},
url = {https://www.sciencedirect.com/science/article/pii/S0022286021015192},
author = {Assimo Maris and Sonia Melandri and Luca Evangelisti and Annalisa Vigorito and Silvia Sigismondi and Camilla Calabrese and Imanol Usabiaga},
keywords = {Amide, Gas-phase structure, Large amplitude motions, Nuclear quadrupole hyperfine structure, Rotational spectroscopy, Quantum mechanical calculations},
abstract = {The conformational space of methacrylamide was explored by quantum mechanical modeling and surveyed in the 59.6–104.0 GHz frequency range using a millimeter-wave Stark-modulated free-jet absorption spectrometer. According to the relative orientation of the two unsaturated bonds, two conformers were observed, namely s-trans (A=5234.360(1), B=3364.9717(8) and C=2173.099(1) MHz) and s-cis (A=5207.292(1), B=3470.930(1) and C=2113.496(1) MHz). The s-trans conformation is the global minimum, with relative energy 4(2) kJ mol−1 and calculated isomerization barrier 15 kJ mol−1. Except for the methyl hydrogen atoms, s-cis-methacrylamide is planar and its methyl internal rotation barrier is 10.2(1) kJ mol−1. In s-trans-methacrylamide the allyl and amino frames form a dihedral angle of about 30° and the methyl internal rotation barrier is 7.4 kJ mol−1. This different behaviour is explained in terms of attractive and repulsive intramolecular interactions between groups: CH2/CO and CH3/NH2 for s-cis, CH2/NH2 and CH3/CO for s-trans. The tunneling splitting related to the double-well potential describing the interconversion between the two equivalent s-trans forms is 837.97(2) MHz and was reproduced by a one-dimensional flexible model using a 3.6 kJ mol−1 interconversion barrier.}
}
@article{LIN2025102895,
title = {Integrating generative AI into digital multimodal composition: A study of multicultural second-language classrooms},
journal = {Computers and Composition},
volume = {75},
pages = {102895},
year = {2025},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2024.102895},
url = {https://www.sciencedirect.com/science/article/pii/S8755461524000719},
author = {Chin-Hsi Lin and Keyi Zhou and Lanqing Li and Lanfang Sun},
keywords = {Generative AI, Multimodal composing, Multicultural education},
abstract = {This study examines the integration of generative AI tools into digital multimodal composition (DMC) within a multicultural context, examining their impact on students’ motivation, writing processes, and outcomes. Eleven culturally diverse students from two high schools in Hong Kong participated in the study. The study developed and employed a novel pedagogical framework, IDEA (Interpret, Design, Evaluate, and Articulate), to seamlessly incorporate generative AI into DMC practices. Data-collection methods included analysis of generative AI tool-usage history, classroom video observations, surveys, and interviews. The findings reveal that students leveraged generative AI’s capabilities across five key areas: content generation, feedback and revision, multilingual support, critical thinking, and visual representation. The integration of AI tools followed distinct stages in the composition process, resulting in enhancements to the vocabulary, grammar, and structural elements of students’ work. This research contributes to the growing body of knowledge on the intersection of generative AI, education, and multimodal literacy, with a particular emphasis on human-AI collaboration in multicultural settings. It also offers valuable insights for educators seeking to enhance students’ DMC skills through the thoughtful integration of generative AI tools, potentially increasing engagement, motivation, and creative expression among learners from diverse cultural backgrounds.}
}
@article{LOURIDAS1999517,
title = {Design as bricolage: anthropology meets design thinking},
journal = {Design Studies},
volume = {20},
number = {6},
pages = {517-535},
year = {1999},
issn = {0142-694X},
doi = {https://doi.org/10.1016/S0142-694X(98)00044-1},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X98000441},
author = {Panagiotis Louridas},
keywords = {aesthetics, design activity, design cognition, metaphor, psychology of design},
abstract = {We identify a metaphor for the design activity: we view design as bricolage. We start from describing bricolage, and we proceed to the relationship of design to art. We obtain a characterisation of design that enables us to show that both traditional and contemporary design are forms of bricolage. We examine the consequences of `design as bricolage' for the relationship between design and science and for the extent of the design activity.}
}
@article{HARTMANN2021112902,
title = {Model development for evidence-based prioritisation of policy action on emerging chemical and microbial drinking water risks},
journal = {Journal of Environmental Management},
volume = {295},
pages = {112902},
year = {2021},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2021.112902},
url = {https://www.sciencedirect.com/science/article/pii/S0301479721009646},
author = {Julia Hartmann and Juan Carlos Chacon-Hurtado and Eric Verbruggen and Jack Schijven and Emiel Rorije and Susanne Wuijts and Ana Maria {de Roda Husman} and Jan Peter {van der Hoek} and Lisa Scholten},
keywords = {Multi criteria analysis, MCA, Stakeholder consultation, Water contaminants, Pathogen},
abstract = {While the burden of disease from well-studied drinking water contaminants is declining, risks from emerging chemical and microbial contaminants arise because of social, technological, demographic and climatological developments. At present, emerging chemical and microbial drinking water contaminants are not assessed in a systematic way, but reactively and incidence based. Furthermore, they are assessed separately despite similar pollution sources. As a result, risks might be addressed ineffectively. Integrated risk assessment approaches are thus needed that elucidate the uncertainties in the risk evaluation of emerging drinking water contaminants, while considering risk assessors’ values. This study therefore aimed to (1) construct an assessment hierarchy for the integrated evaluation of the potential risks from emerging chemical and microbial contaminants in drinking water and (2) develop a decision support tool, based on the agreed assessment hierarchy, to quantify (uncertain) risk scores. A multi-actor approach was used to construct the assessment hierarchy, involving chemical and microbial risk assessors, drinking water experts and members of responsible authorities. The concept of value-focused thinking was applied to guide the problem-structuring and model-building process. The development of the decision support tool was done using Decisi-o-rama, an open-source Python library. With the developed decision support tool (uncertain) risk scores can be calculated for emerging chemical and microbial drinking water contaminants, which can be used for the evidence-based prioritisation of actions on emerging chemical and microbial drinking water risks. The decision support tool improves existing prioritisation approaches as it combines uncertain indicator levels with a multi-stakeholder approach and integrated the risk assessment of chemical and microbial contaminants. By applying the concept of value-focused thinking, this study addressed difficulties in evidence-based decision-making related to emerging drinking water contaminants. Suggestions to improve the model were made to guide future research in assisting policy makers to effectively protect public health from emerging drinking water risks.}
}
@incollection{PARRY2016255,
title = {Chapter Ten - Using Data Mining and Computational Approaches to Study Intermediate Filament Structure and Function},
editor = {M. Bishr Omary and Ronald K.H. Liem},
series = {Methods in Enzymology},
publisher = {Academic Press},
volume = {568},
pages = {255-276},
year = {2016},
booktitle = {Intermediate Filament Proteins},
issn = {0076-6879},
doi = {https://doi.org/10.1016/bs.mie.2015.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0076687915004152},
author = {David A.D. Parry},
keywords = {IF chain assembly, Sequence periodicities, Heptad and hendecad substructure, Interchain ionic interactions, IF secondary and tertiary structure, Structural/functional motifs, Mutations},
abstract = {Experimental and theoretical research aimed at determining the structure and function of the family of intermediate filament proteins has made significant advances over the past 20 years. Much of this has either contributed to or relied on the amino acid sequence databases that are now available online, and the data mining approaches that have been developed to analyze these sequences. As the quality of sequence data is generally high, it follows that it is the design of the computational and graphical methodologies that are of especial importance to researchers who aspire to gain a greater understanding of those sequence features that specify both function and structural hierarchy. However, these techniques are necessarily subject to limitations and it is important that these be recognized. In addition, no single method is likely to be successful in solving a particular problem, and a coordinated approach using a suite of methods is generally required. A final step in the process involves the interpretation of the results obtained and the construction of a working model or hypothesis that suggests further experimentation. While such methods allow meaningful progress to be made it is still important that the data are interpreted correctly and conservatively. New data mining methods are continually being developed, and it can be expected that even greater understanding of the relationship between structure and function will be gleaned from sequence data in the coming years.}
}
@article{ZHOU2022105384,
title = {Informed speculation with k-level reasoning},
journal = {Journal of Economic Theory},
volume = {200},
pages = {105384},
year = {2022},
issn = {0022-0531},
doi = {https://doi.org/10.1016/j.jet.2021.105384},
url = {https://www.sciencedirect.com/science/article/pii/S0022053121002015},
author = {Hang Zhou},
keywords = {Level- thinking, Investors' sophistication, Market instability},
abstract = {This paper investigates the effect of strategic reasoning on financial markets with a level-k thinking framework. A level-k speculator performs k rounds of iterative reasoning to infer information from asset prices. In contrast to the static rational expectations equilibrium, the level-k framework produces a unified theory of momentum and contrarian trading strategies. Besides, this paper discusses how the distribution of sophistication levels affects several market variables and it sheds new light on empirical patterns such as: (1) overreaction of asset prices, (2) the excess volatility puzzle, and (3) the excessive trading volume puzzle. Moreover, this paper explores whether the level-k strategy converges to the rational expectations equilibrium.}
}
@article{LEWIS2018491,
title = {How Memory Replay in Sleep Boosts Creative Problem-Solving},
journal = {Trends in Cognitive Sciences},
volume = {22},
number = {6},
pages = {491-503},
year = {2018},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2018.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S1364661318300706},
author = {Penelope A. Lewis and Günther Knoblich and Gina Poe},
keywords = {sleep, memory, creativity, reactivation, replay, consolidation},
abstract = {Creative thought relies on the reorganisation of existing knowledge. Sleep is known to be important for creative thinking, but there is a debate about which sleep stage is most relevant, and why. We address this issue by proposing that rapid eye movement sleep, or ‘REM’, and non-REM sleep facilitate creativity in different ways. Memory replay mechanisms in non-REM can abstract rules from corpuses of learned information, while replay in REM may promote novel associations. We propose that the iterative interleaving of REM and non-REM across a night boosts the formation of complex knowledge frameworks, and allows these frameworks to be restructured, thus facilitating creative thought. We outline a hypothetical computational model which will allow explicit testing of these hypotheses.}
}
@article{SNYDER2022100852,
title = {The role of heat resistance in yeast spoilage of thermally processed foods: highlighting the need for a probabilistic, systems-based approach to microbial quality},
journal = {Current Opinion in Food Science},
volume = {46},
pages = {100852},
year = {2022},
issn = {2214-7993},
doi = {https://doi.org/10.1016/j.cofs.2022.100852},
url = {https://www.sciencedirect.com/science/article/pii/S2214799322000546},
author = {Abigail B Snyder},
abstract = {The relationship between stress-tolerance mechanisms (cell-wall structure, metabolism, morphology, etc.) of individual fungi and the physiochemistry of their food environment selects for a small group of specific spoilage organisms (SSOs). However, common process deviations and post-processing contamination widen the lens of potentially relevant spoilage fungi. For example, although heat-resistant molds are considered the SSOs in thermally processed foods, unintended events (deviations, post-processing contamination) lead to spoilage by other propagules, notably yeast. The frequency of these unintended events changes our assessments of which spoilage fungi are relevant to a given food system. Consequently, a framework using probabilistic and systems-based thinking is needed to understand spoilage risk. Toward that goal, simple molecular tools for identification and subtyping are required.}
}
@article{SONOBE2022101560,
title = {Development and validation of machine learning prediction model for post-rehabilitation functional outcome after intracerebral hemorrhage},
journal = {Interdisciplinary Neurosurgery},
volume = {29},
pages = {101560},
year = {2022},
issn = {2214-7519},
doi = {https://doi.org/10.1016/j.inat.2022.101560},
url = {https://www.sciencedirect.com/science/article/pii/S2214751922000743},
author = {Shinya Sonobe and Tetsuo Ishikawa and Kuniyasu Niizuma and Eiryo Kawakami and Takuya Ueda and Eichi Takaya and Carlos {Makoto Miyauchi} and Junya Iwazaki and Ryuzaburo Kochi and Toshiki Endo and Arun Shastry and Vijayananda Jagannatha and Ajay Seth and Atsuhiro Nakagawa and Masahiro Yoshida and Teiji Tominaga},
keywords = {Intracerebral hemorrhage, Machine learning prediction, Post-rehabilitation functional outcome, Design thinking},
abstract = {Objective
Predicting outcomes after intracerebral hemorrhage (ICH) may help improve patient outcomes. We developed and validated a machine learning prediction model for post-rehabilitation functional outcomes after ICH. Patient selection and explanatory variable settings were based on clinical significance. Functional outcomes were predicted using ternary classification.
Methods
The subjects were patients aged > 18 years without pre-onset severe disability who developed primary putaminal and/or thalamic hemorrhage and underwent an inpatient rehabilitation program. As explanatory variables, 43 values related to patient background, imaging-related findings, systemic conditions, neurological findings, and blood tests were acquired within 10 days of onset. As an objective variable, the functional outcome at discharge to home or nursing home was acquired using a ternary classification. The dataset consisting of the collected information was split into a training dataset and a test dataset with a ratio of 2:1. A predictive model using a balanced random forest algorithm was created using supervised learning from the training dataset. The predictive performance was validated using a test dataset.
Results
Between January 2018 and June 2019, 100 consecutive patients were included in the study. The areas under the receiver operating characteristic curves for predictions of good, moderate, and poor outcomes were 0.952, 0.790, and 0.921, respectively.
Conclusions
The predictive performance of the model was comparable to that of previous models. Patient selection and variable settings from a clinical perspective may contribute to accurate and detailed predictions. These study designs are based on design thinking and may meet the needs of clinical practice.}
}
@article{MUZAFFAR20224912,
title = {Analysing the Causes of Design Generated Waste through System Dynamics},
journal = {KSCE Journal of Civil Engineering},
volume = {26},
number = {12},
pages = {4912-4925},
year = {2022},
issn = {1226-7988},
doi = {https://doi.org/10.1007/s12205-022-1896-1},
url = {https://www.sciencedirect.com/science/article/pii/S1226798824013461},
author = {Sidra Muzaffar and Khurram Iqbal Ahmad Khan and Muhammad Bilal Tahir and Hamna Bukhari},
keywords = {Construction & demolition waste, Design generated waste, Causal loop diagram, Systems thinking, System dynamics},
abstract = {A drastic rise in construction waste observed has elicited a radical impact on the environment and economy of the world. It is, therefore, necessary to come up with waste minimization management strategies that reflect in-depth review of sources of waste. This in depth review demands understanding the intricacy of causative factors triggering generation of “waste at source” which is the main motive of study and is done through System Dynamics for design phase in context of developing countries. 8 most important causative factors in design phase were shortlisted along with their interrelationships via literature and questionnaire survey. Followed by system thinking approach that addressed the complexities caused by those factors in 2 stages. Firstly, a Causal loop diagram was developed that illustated interrelationship between factors in the form of loops. Later SD model built, evaluated the combinatorial effect of 3 evolved stocks over the fourth stock Design Generated Waste-an emanating phenomenon. Simulation result revealed increasing trend of the stock DGW over a course of time. Therefore, increase in effect of complexities of behavior of design waste causes, will consequently lead to increase in DGW. Managing the complex behavior of these design causes will help control over the DGW w.r.t. time.}
}
@article{LLOYD2019167,
title = {You make it and you try it out: Seeds of design discipline futures},
journal = {Design Studies},
volume = {65},
pages = {167-181},
year = {2019},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2019.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X19300675},
author = {Peter Lloyd},
keywords = {design methods, design studies, design research, design process, design thinking},
abstract = {This paper takes a narrative seam through the design discipline, attempting to explain how design methodology, one of the three types of Nigel Cross' designerly ways of knowing, has changed over the 40 years of Design Studies. Specifically, the paper identifies the point when a ‘social turn’ in the discipline occurred, allowing more nuanced and critical studies of designing, and shifting the balance from an objective (‘scientific’) perspective to one more based on relativist approaches. The paper concludes by noting the plurality of present-day study, arguably enabled by design thinking, and sketches what this holds for the future of the discipline. The references in the paper are mainly restricted to those published in, or strongly relating to, Design Studies.}
}
@article{TESFATSION2001281,
title = {Introduction to the special issue on agent-based computational economics},
journal = {Journal of Economic Dynamics and Control},
volume = {25},
number = {3},
pages = {281-293},
year = {2001},
note = {Agent-based Computational Economics (ACE)},
issn = {0165-1889},
doi = {https://doi.org/10.1016/S0165-1889(00)00027-0},
url = {https://www.sciencedirect.com/science/article/pii/S0165188900000270},
author = {Leigh Tesfatsion},
keywords = {Agent-based computational economics},
abstract = {A brief overview of agent-based computational economics (ACE) is given, followed by a synopsis of the articles included in this special issue on ACE and in a companion special issue on ACE scheduled to appear in Computational Economics.}
}
@article{SALMON2022105511,
title = {Bicycle crash contributory factors: A systematic review},
journal = {Safety Science},
volume = {145},
pages = {105511},
year = {2022},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2021.105511},
url = {https://www.sciencedirect.com/science/article/pii/S0925753521003544},
author = {Paul M. Salmon and Mitch Naughton and Adam Hulme and Scott McLean},
keywords = {Cyclists, Cyclist crashes, Systems thinking, Road safety, Crash causation},
abstract = {There is a growing body of road safety research that seeks to identify crash contributory factors beyond road users, their vehicles, and the immediate road environment. Although cyclist safety represents a critical research area, this ‘systems thinking’ approach has received less attention in bicycle crash analysis. This article presents the findings from a systematic literature review which aimed to synthesise the peer reviewed literature regarding bicycle crash contributory factors (defined as factors which play a contributory role in bicycle crashes, as opposed to risk factors which are factors which may increase the probability of crashes). Crash contributory factors were extracted from included articles and mapped onto a systems thinking framework comprising seven hierarchical road transport system levels. The findings show that a majority of the included studies identified contributory factors relating to the road environment, cycling infrastructure, and cyclist and driver behaviour. No studies identified contributory factors outside of cyclists and road users, bicycles and vehicles, and the road environment and few specifically examined causal relationships between contributory factors. It is concluded that there are gaps in the knowledge base regarding the broader transport system features that play a role in bicycle crashes and how contributory factors interact to create crashes. We argue that more expansive research into the systemic factors involved in bicycle crashes is required and that initial work should focus on the development of new data sources and analysis methods.}
}
@article{YIN2015655,
title = {Automating design with intelligent human–machine integration},
journal = {CIRP Annals},
volume = {64},
number = {2},
pages = {655-677},
year = {2015},
issn = {0007-8506},
doi = {https://doi.org/10.1016/j.cirp.2015.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S000785061500147X},
author = {Yue H. Yin and Andrew Y.C. Nee and S.K. Ong and Jian Y. Zhu and Pei H. Gu and Lien J. Chen},
keywords = {Design automation, Human–machine integration, Intelligent design, Imaginal thinking, Ontology},
abstract = {This paper reviews the state-of-the-art methodologies for automating design with intelligent human–machine integration from the perspectives of ontology and epistemology. The human–machine integrated automating design paradigm is reviewed systematically based on a proposed prototype of human–machine integrated design, from the aspects of ontology-based knowledge management with local-to-global ontology transitions, and epistemology-based upward-spiral cognitive process of coupled design ideation. Particularly, imaginal thinking frame is proposed as the foundation of intelligent human–machine interaction that puts human and machine on an equal platform. Further, this paper presents implementations and applications of the automating design paradigm and concludes with the identification of future trend.}
}
@article{GEORGIEV20181,
title = {Enhancing user creativity: Semantic measures for idea generation},
journal = {Knowledge-Based Systems},
volume = {151},
pages = {1-15},
year = {2018},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2018.03.016},
url = {https://www.sciencedirect.com/science/article/pii/S0950705118301394},
author = {Georgi V. Georgiev and Danko D. Georgiev},
keywords = {Creativity, Divergence, Semantic networks, Similarity, WordNet},
abstract = {Human creativity generates novel ideas to solve real-world problems. This thereby grants us the power to transform the surrounding world and extend our human attributes beyond what is currently possible. Creative ideas are not just new and unexpected, but are also successful in providing solutions that are useful, efficient and valuable. Thus, creativity optimizes the use of available resources and increases wealth. The origin of human creativity, however, is poorly understood, and semantic measures that could predict the success of generated ideas are currently unknown. Here, we analyze a dataset of design problem-solving conversations in real-world settings by using 49 semantic measures based on WordNet 3.1 and demonstrate that a divergence of semantic similarity, an increased information content, and a decreased polysemy predict the success of generated ideas. The first feedback from clients also enhances information content and leads to a divergence of successful ideas in creative problem solving. These results advance cognitive science by identifying real-world processes in human problem solving that are relevant to the success of produced solutions and provide tools for real-time monitoring of problem solving, student training and skill acquisition. A selected subset of information content (IC Sánchez–Batet) and semantic similarity (Lin/Sánchez–Batet) measures, which are both statistically powerful and computationally fast, could support the development of technologies for computer-assisted enhancements of human creativity or for the implementation of creativity in machines endowed with general artificial intelligence.}
}
@article{RADU2023100008,
title = {Charting opportunities and guidelines for augmented reality in makerspaces through prototyping and co-design research},
journal = {Computers & Education: X Reality},
volume = {2},
pages = {100008},
year = {2023},
issn = {2949-6780},
doi = {https://doi.org/10.1016/j.cexr.2023.100008},
url = {https://www.sciencedirect.com/science/article/pii/S2949678023000028},
author = {Iulian Radu and Josia Yuan and Xiaomeng Huang and Bertrand Schneider},
keywords = {Augmented reality, Makerspaces, Co-design, STEM, Classroom integration},
abstract = {Makerspace environments are becoming popular project-based learning spaces where students interact with physical objects and peer collaboration, while developing 21st century skills and engaging with science, technology, engineering, and math (STEM) topics. At the same time, augmented reality (AR) technology, which combines physical objects with digital visualizations, is becoming increasingly applicable for makerspace activities and has potential to address challenges for student learning in makerspaces. However, there is a lack of understanding of how to use and integrate AR in real makerspace environments. In this research we use a co-design methodology to address the following questions: (1) How can AR be useful for education in makerspaces? (2) How are students impacted by the process of co-designing AR technology? and (3) What are practical considerations for integrating AR in makerspaces? We engaged in a co-design process in a semester-long makerspace course attended by 18 students in a graduate school of education. Through this process, we generated six prototypes with seven student co-designers, exploring AR use in design, fabrication, programming, electronics, and training. We also identified areas where AR technology can benefit makerspaces, such as teaching STEM skills, facilitating construction activities, enhancing contextualization of learning, and debugging. We observed that students participating in co-design demonstrated improved understanding of technology design, enthusiasm for engaging with makerspaces and AR, and increased critical thinking about AR technology. These results suggest considerations and guidelines for integrating AR technology into makerspace environments.}
}
@article{VELIZ2025115299,
title = {Modeling the interconnected drivers of power sector decarbonization in Chile},
journal = {Renewable and Sustainable Energy Reviews},
volume = {211},
pages = {115299},
year = {2025},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2024.115299},
url = {https://www.sciencedirect.com/science/article/pii/S1364032124010256},
author = {Karina D. Véliz and Jeffrey P. Walters and Carlos Fica and Carolina Busco},
keywords = {Decarbonization, Chile, Renewable energy, Systems thinking, Participatory modeling},
abstract = {This study sought to model the interconnected and multidimensional factors influencing the decarbonization of Chile's electricity sector. Factors were identified through a structured review of articles found in the Web of Science. Factor interactions were then characterized through a survey and participatory systems modeling workshop with stakeholders from various fields in the Chilean energy sector. The model emerging from the workshop was structurally analyzed to identify and evaluate system leverage points used to inform recommendations for future policy and practice. A key leverage point identified in this analysis underscores the importance of stakeholder awareness regarding the benefits of renewable energy projects, serving as a crucial catalyst towards decarbonization by fostering citizen support and driving the implementation of favorable public policies. Conversely, the model showed that public opposition to transmission line construction, stemming from health, environmental, and property value concerns, can potentially lead to project delays, increased costs, and challenges in modernizing electrical grids. These findings emphasize the need for public engagement and effective communication to prioritize decarbonization while balancing short-term impacts with long-term benefits. The systemic and process-oriented insights gained from the application of the participatory modeling approach presented in this study, highlight the value of utilizing systems thinking and modeling approaches to inform future decarbonization strategies on a global scale.}
}
@article{ALON2025104829,
title = {Leveraging natural language processing to elucidate real-world clinical decision-making paradigms: A proof of concept study},
journal = {Journal of Biomedical Informatics},
pages = {104829},
year = {2025},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2025.104829},
url = {https://www.sciencedirect.com/science/article/pii/S1532046425000589},
author = {Yaniv Alon and Etti Naimi and Chedva Levin and Hila Videl and Mor Saban},
keywords = {Clinical decision-making, Natural language processing (NLP), Heuristics-based reasoning, Shared decision-making, Healthcare informatics, AI in medicine},
abstract = {Background
Understanding how clinicians arrive at decisions in actual practice settings is vital for advancing personalized, evidence-based care. However, systematic analysis of qualitative decision data poses challenges.
Methods
We analyzed transcribed interviews with Hebrew-speaking clinicians on decision processes using natural language processing (NLP). Word frequency and characterized terminology use, while large language models (ChatGPT from OpenAI and Gemini by Google) identified potential cognitive paradigms.
Results
Word frequency analysis of clinician interviews identified experience and knowledge as most influential on decision-making. NLP tentatively recognized heuristics-based reasoning grounded in past cases and intuition as dominant cognitive paradigms. Elements of shared decision-making through individualizing care with patients and families were also observed. Limited Hebrew clinical language resources required developing preliminary lexicons and dynamically adjusting stopwords. Findings also provided preliminary support for heuristics guiding clinical judgment while highlighting needs for broader sampling and enhanced analytical frameworks.
Conclusions
This study represents the first use of integrated qualitative and computational methods to systematically elucidate clinical decision-making. Findings supported experience-based heuristics guiding cognition. With methodological enhancements, similar analyses could transform global understanding of tailored care delivery. Standardizing interdisciplinary collaborations on developing NLP tools and analytical frameworks may advance equitable, evidence-based healthcare by elucidating real-world clinical reasoning processes across diverse populations and settings.}
}
@article{RONG20121462,
title = {Computational performance of basic state reduction based dynamic programming algorithms for bi-objective 0–1 knapsack problems},
journal = {Computers & Mathematics with Applications},
volume = {63},
number = {10},
pages = {1462-1480},
year = {2012},
issn = {0898-1221},
doi = {https://doi.org/10.1016/j.camwa.2012.03.057},
url = {https://www.sciencedirect.com/science/article/pii/S0898122112002623},
author = {Aiying Rong and José Rui Figueira},
keywords = {Multi-objective optimization, Bi-objective knapsack problem, Dynamic programming, Basic state reduction techniques},
abstract = {This paper studies a group of basic state reduction based dynamic programming (DP) algorithms for the multi-objective 0–1 knapsack problem (MKP), which are related to the backward reduced-state DP space (BRDS) and forward reduced-state DP space (FRDS). The BRDS is widely ignored in the literature because it imposes disadvantage for the single objective knapsack problem (KP) in terms of memory requirements. The FRDS based DP algorithm in a general sense is related to state dominance checking, which can be time consuming for the MKP while it can be done efficiently for the KP. Consequently, no algorithm purely based on the FRDS with state dominance checking has ever been developed for the MKP. In this paper, we attempt to get some insights into the state reduction techniques efficient to the MKP. We first propose an FRDS based algorithm with a local state dominance checking for the MKP. Then we evaluate the relative advantage of the BRDS and FRDS based algorithms by analyzing their computational time and memory requirements for the MKP. Finally different combinations of the BRDS and FRDS based algorithms are developed on this basis. Numerical experiments based on the bi-objective KP instances are conducted to compare systematically between these algorithms and the recently developed BRDS based DP algorithm as well as the existing FRDS based DP algorithm without state dominance checking.}
}
@article{AMEMIYA2024105836,
title = {Children use disagreement to infer what happened},
journal = {Cognition},
volume = {250},
pages = {105836},
year = {2024},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2024.105836},
url = {https://www.sciencedirect.com/science/article/pii/S0010027724001227},
author = {Jamie Amemiya and Gail D. Heyman and Tobias Gerstenberg},
keywords = {Disagreement, Inference, Prediction, Theory of mind, Ambiguous speech},
abstract = {In a rapidly changing and diverse world, the ability to reason about conflicting perspectives is critical for effective communication, collaboration, and critical thinking. The current pre-registered experiments with children ages 7 to 11 years investigated the developmental foundations of this ability through a novel social reasoning paradigm and a computational approach. In the inference task, children were asked to figure out what happened based on whether two speakers agreed or disagreed in their interpretation. In the prediction task, children were provided information about what happened and asked to predict whether two speakers will agree or disagree. Together, these experiments assessed children's understanding that disagreement often results from ambiguity about what happened, and that ambiguity about what happened is often predictive of disagreement. Experiment 1 (N = 52) showed that children are more likely to infer that an ambiguous utterance occurred after learning that people disagreed (versus agreed) about what happened and found that these inferences become stronger with age. Experiment 2 (N = 110) similarly found age-related change in children's inferences and also showed that children could reason in the forward direction, predicting that an ambiguous utterance would lead to disagreement. A computational model indicated that although children's ability to predict when disagreements might arise may be critical for making the reverse inferences, it did not fully account for age-related change.}
}
@article{BLACKBURNE2025105969,
title = {Communicated priors tune the perception of control},
journal = {Cognition},
volume = {254},
pages = {105969},
year = {2025},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2024.105969},
url = {https://www.sciencedirect.com/science/article/pii/S0010027724002555},
author = {George Blackburne and Chris D. Frith and Daniel Yon},
keywords = {Agency, Control, Expectation, Prediction, Communication},
abstract = {Action allows us to shape the world around us. But to act effectively we need to accurately sense what we can and cannot control. Classic theories across cognitive science suppose that this ‘sense of agency’ is constructed from the sensorimotor signals we experience as we interact with our surroundings. But these sensorimotor signals are inherently ambiguous, and can provide us with a distorted picture of what we can and cannot influence. Here we investigate one way that agents like us might overcome the inherent ambiguity of these signals: by combining noisy sensorimotor evidence with prior beliefs about control acquired through explicit communication with others. Using novel tools to measure and model control decisions, we find that explicit beliefs about the controllability of the environment alter both the sensitivity and bias of agentic choices; meaning that we are both better at detecting and more biased to feel control when we are told to expect it. These seemingly paradoxical effects on agentic choices can be captured by a computational model where expecting to be in control exaggerates the sensitivity or ‘gain’ of the mechanisms we use to detect our influence over our surroundings – making us increasingly sensitised to both true and illusory signs of agency. In combination, these results reveal a cognitive and computational mechanism that allows public communication about what we can and cannot influence to reshape our private sense of control.}
}
@article{BUCKER20031309,
title = {Parallel programming in computational science: an introductory practical training course for computer science undergraduates at Aachen University},
journal = {Future Generation Computer Systems},
volume = {19},
number = {8},
pages = {1309-1319},
year = {2003},
note = {Selected papers from the Workshop on Education in Computational Sciences held at the International Conference on Computational Science},
issn = {0167-739X},
doi = {https://doi.org/10.1016/S0167-739X(03)00089-X},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X0300089X},
author = {H.M. Bücker and B. Lang and C.H. Bischof},
keywords = {Parallel programming, Java, Computational science and engineering, Education},
abstract = {Parallel programming of high-performance computers has emerged as a key technology for the numerical solution of large-scale problems arising in computational science and engineering (CSE). The authors believe that principles and techniques of parallel programming are among the essential ingredients of any CSE as well as computer science curriculum. Today, opinions on the role and importance of parallel programming are diverse. Rather than seeing it as a marginal beneficial skill optionally taught at the graduate level, we understand parallel programming as crucial basic skill that should be taught as an integral part of the undergraduate computer science curriculum. A practical training course developed for computer science undergraduates at Aachen University is described. Its goal is to introduce young computer science students to different parallel programming paradigms for shared and distributed memory computers as well as to give a first exposition to the field of computational science by simple, yet carefully chosen sample problems.}
}
@article{GUR2015207,
title = {Space reconstruction by primary visual cortex activity: a parallel, non-computational mechanism of object representation},
journal = {Trends in Neurosciences},
volume = {38},
number = {4},
pages = {207-216},
year = {2015},
issn = {0166-2236},
doi = {https://doi.org/10.1016/j.tins.2015.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0166223615000351},
author = {Moshe Gur},
keywords = {vision, object representation, recognition, conscious perception, parallel processing},
abstract = {The current view posits that objects, despite changes in appearance, are uniquely encoded by ‘expert’ cells. This view is untenable. First, even if cell ensemble responses are invariant and unique, we are consciously aware of all of the objects’ details. Second, in addition to detail preservation, data show that the current hypothesis fails to account for uniqueness and invariance. I present an alternative view whereby objects’ representation and recognition are based on parallel representation of space by primary visual cortex (V1) responses. Information necessary for invariance and other attributes is handled in series by other cortical areas through integration, interpolation, and hierarchical convergence. The parallel and serial mechanisms combine to enable our flexible space perception. Only in this alternative view is conscious perception consistent with the underlying architecture.}
}
@incollection{ROMO2020150,
title = {Metaphor},
editor = {Mark Runco and Steven Pritzker},
booktitle = {Encyclopedia of Creativity (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Oxford},
pages = {150-156},
year = {2020},
isbn = {978-0-12-815615-5},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.23529-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093245235293},
author = {Manuela Romo},
keywords = {Analogical thought, Combinatory process, Computational creativity, Constitutive metaphor, Darwin, Einstein, Lorca, Pedagogical metaphor, Poetry, Scientific discovery},
abstract = {The subject of metaphors is introduced with a definition, stressing its role as a universal process in the development of thinking and language in human beings. A discussion follows on the differences between metaphor and analogy in this thinking process. The classification of metaphors is then addressed, while the body of the text is dedicated to reviewing explanations given from the standpoint of Psychology of Creativity on the nature of this process and its role in creative output. Lastly, the usefulness of metaphors and their dependence on domain specificity is analysed.}
}
@article{PLATZ2024e4563,
title = {Dichlorocarbene: From Jack Hine to Robert Moss},
journal = {Journal of Physical Organic Chemistry},
volume = {37},
number = {1},
pages = {e4563},
year = {2024},
issn = {0894-3230},
doi = {https://doi.org/10.1002/poc.4563},
url = {https://www.sciencedirect.com/science/article/pii/S089432302300259X},
author = {Matthew S. Platz},
keywords = {carbene, dichlorocarbene, Jack Hine, Robert Moss},
abstract = {A select history of dichlorocarbene chemistry between 1950 and 2010 will be presented. This is not a comprehensive review; rather, it is a personal perspective on the contributions of two respected colleagues, the reactive intermediate that spanned their research efforts, and their important contributions to organic synthesis and mechanistic thinking.}
}
@article{ZHANG2023100528,
title = {Foreign language effect in accounting uncertainty expressions: Interpretation and probabilistic estimation},
journal = {Journal of International Accounting, Auditing and Taxation},
volume = {50},
pages = {100528},
year = {2023},
issn = {1061-9518},
doi = {https://doi.org/10.1016/j.intaccaudtax.2023.100528},
url = {https://www.sciencedirect.com/science/article/pii/S1061951823000071},
author = {Yuqian Zhang and Anura {De Zoysa} and Corinne Cortese},
keywords = {Foreign language effect, Uncertainty expressions, Probability estimation, Accounting judgement, Interpretation},
abstract = {The foreign language effect, or thinking in a foreign language, reduces judgment bias under uncertainty. This study investigates how language use (native versus foreign) affects accounting judgment on uncertainty expressions. We conducted two separate experiments: between-subjects and within-subjects, both of which included tasks requiring interpretations and probability estimations based on accounting standard uncertainty expressions. The results demonstrated that foreign language use affected the interpretation of uncertainty expressions and reduced judgment bias in probability estimation, particularly in the context of asset recognition. These findings have important implications for accounting research and reporting.}
}
@incollection{KARACA2022149,
title = {Chapter 9 - Computational fractional-order calculus and classical calculus AI for comparative differentiability prediction analyses of complex-systems-grounded paradigm},
editor = {Yeliz Karaca and Dumitru Baleanu and Yu-Dong Zhang and Osvaldo Gervasi and Majaz Moonis},
booktitle = {Multi-Chaos, Fractal and Multi-Fractional Artificial Intelligence of Different Complex Systems},
publisher = {Academic Press},
pages = {149-168},
year = {2022},
isbn = {978-0-323-90032-4},
doi = {https://doi.org/10.1016/B978-0-323-90032-4.00006-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323900324000067},
author = {Yeliz Karaca and Dumitru Baleanu},
keywords = {Complexity, Artificial neural network, Classical calculus, Computational complexity, Data-driven fractional modeling, Differentiability prediction analyses, Fractional calculus, Mathematical biology and neuroscience, Mittag-Leffler function, Optimized fractional-order calculus},
abstract = {Modern science having embarked on the thorough and accurate interpretation of natural and physical phenomena has proven to provide successful models for the analysis of complex systems and harnessing of control over the various processes therein. Computational complexity, in this regard, comes to the foreground by providing applicable sets of ideas or integrative paradigms to recognize and understand the complex systems' intricate properties. Thus, while making the appropriate, adaptable and evolutive decisions in complex dynamic systems, it is essential to acknowledge different degrees of acceptance of the problems and construct the model it to account for its inherent constraints or limits. In this respect, while hypothesis-driven research has its inherent limitations regarding the investigation of multifactorial and heterogeneous diseases, a data-driven approach enables the examination of the way variables impact one another, which paves the way for the interpretation of dynamic and heterogeneous mechanisms of diseases. Fractional Calculus (FC), in this scope characterized by complexity, provides the applicable means and methods to solve integral, differential and integro-differential equations so FC enables the generalization of integration and differentiation possible in a flexible and consistent manner owing to its capability of reflecting the systems' actual state properties, which exhibit unpredictable variations. The fractional integration and differentiation of fractional-order is capable of providing better characterization of nonstationary and locally self-similar attributes in contrast to constant-order fractional calculus. It becomes possible to model many complex systems by fractional-order derivatives based on fractional calculus so that related syntheses can be realized in a robust and effective way. To this end, our study aims at providing an intermediary facilitating function both for the physicians and individuals by establishing accurate and robust model based on the integration of fractional-order calculus and Artificial Neural Network (ANN) for the diagnostic and differentiability predictive purposes with the diseases which display highly complex properties. The integrative approach we have proposed in this study has a multistage quality the steps of which are stated as follows: first of all, the Caputo fractional-order derivative, one of the fractional-order derivatives, has been used with two-parametric Mittag-Leffler function on the stroke dataset and cancer cell dataset, manifesting biological and neurological attributes. In this way, new fractional models with varying degrees have been established. Mittag-Leffler function, with its distributions of extensive application domains, can address irregular and heterogeneous environments for the solution of dynamic problems; thus, Mittag-Leffler function has been opted for accordingly. Following this application, the new datasets (mlf_stroke dataset and mlf_cancer cell dataset) have been obtained by employing Caputo fractional-order derivative with the two-parametric Mittag-Leffler function (α,β). In addition, classical derivative (calculus) was applied to the raw datasets; and cd_stroke dataset and cd_cancer cell dataset were obtained. Secondly, the performance of the new datasets as obtained from the Caputo fractional derivative with the two-parametric Mittag-Leffler function, the datasets obtained from the classical derivative application and the raw datasets have been compared by using feed forward back propagation (FFBP) algorithm, one of the algorithms of ANN (along with accuracy rate, sensitivity, precision, specificity, F1-score, multiclass classification (MCC), ROC curve). Based on the accuracy rate results obtained from the application with FFBP, the Caputo fractional-order derivative model that is most suitable for the diseases has been generated. The experimental results obtained demonstrate the applicability of the complex-systems-grounded paradigm scheme as proposed through this study, which has no existing counterpart. The integrative multi-stage method based on mathematical-informed framework with comparative differentiability prediction analyses can point toward a new direction in the various areas of applied sciences to address formidable challenges of critical decision making and management of chaotic processes in different complex dynamic systems.}
}
@article{CHIEN2009965,
title = {An efficient computational procedure for determining the container-loading pattern},
journal = {Computers & Industrial Engineering},
volume = {56},
number = {3},
pages = {965-978},
year = {2009},
note = {Intelligent Manufacturing and Logistics},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2008.09.019},
url = {https://www.sciencedirect.com/science/article/pii/S036083520800226X},
author = {Chen-Fu Chien and Chia-Yen Lee and Yi-Chao Huang and Wen-Ting Wu},
keywords = {Global logistics, Container-loading, Cutting and packing, Three-dimension knapsack, Decision support system},
abstract = {Supply chain and global logistics are driven by strategically focusing on core competences, outsourcing manufacturing to pursue higher value proposition in the supply chain, radically improving the return of capital investments and providing total solutions to targeted customers. The container-loading research has important industrial and commercial application for global logistics. In practice, loading pooled shipment into containers is a complex procedure that has relied largely on the workers’ experience. We developed an efficient computational procedure involving three-dimensional cutting for determining near-optimal container-loading patterns to minimize the waste of container space. We used numerical examples from a motor company that imports key components from Japan, produces parts in Taiwan, and assembles cars in China to estimate its validity and discussed the effectiveness of the proposed solution. This study concludes with a discussion of future research.}
}
@incollection{HANSKORTELING2022610,
title = {Cognitive Biases},
editor = {Sergio {Della Sala}},
booktitle = {Encyclopedia of Behavioral Neuroscience, 2nd edition (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {610-619},
year = {2022},
isbn = {978-0-12-821636-1},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.24105-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093245241059},
author = {J.E. {(Hans) Korteling} and Alexander Toet},
keywords = {Cognitive biases, Cognitive neuroscience, Decision making, Dual process theory, Expertise, Evolutionary psychology, Heuristics, Information processing capacity, Intuition, Neural networks, Rationality},
abstract = {Cognitive biases are systematic cognitive dispositions or inclinations in human thinking and reasoning that often do not comply with the tenets of logic, probability reasoning, and plausibility. These intuitive and subconscious tendencies are at the basis of human judgment, decision making, and the resulting behavior. Psychological frameworks consider biases as resulting from the use of (inappropriate) cognitive heuristics that people apply to deal with data-limitations, from information processing limitations, or from a lack of expertise. Neuro-evolutionary frameworks provide a more profound explanation of biases as originating from the inherent design characteristics of our brain as a neural network that was primarily developed to perform basic physical, perceptual and motor functions, and which also had to promote the survival of our hunter-gatherer ancestors.}
}
@article{LU2024,
title = {Methods for Calculating Building-Embodied Carbon Emissions for the Whole Design Process},
journal = {Fundamental Research},
year = {2024},
issn = {2667-3258},
doi = {https://doi.org/10.1016/j.fmre.2022.07.015},
url = {https://www.sciencedirect.com/science/article/pii/S266732582400092X},
author = {Mei Lu and Zhixing Luo and Yujie Cang and Nan Zhang and Liu Yang},
keywords = {Design process, embodied carbon emissions, calculation methods, conceptual design, scheme design, construction drawing design},
abstract = {Energy conservation and emissions reduction in the construction industry are important steps in achieving China's goals of peak carbon emissions by 2030 and carbon neutrality by 2060. The premise for building carbon emission (CE) reduction is to produce accurate CE calculations. Existing calculation methods for building CEs have many problems, such as complicated calculations, large data demands, time-consuming and laborious processes, weak design orientation of results, and poor feedback on emission reduction. At the same time, the calculation of CEs during the process of architectural design faces obstacles such as uncertainty of information, incomplete data, and difficulty in obtaining a bill of quantities based on design information. To resolve these obstacles, this study, based on a designer's vocabulary and thinking mode, describes the construction of a “design-oriented” calculation methods for building-embodied carbon emissions (ECEs). The prediction and assessment of the impact on the building environment during the architectural design process were helpful for identifying the key areas for carbon reduction, exploring potential emission reduction hotspots, and providing timely feedback for design optimization, which can have important theoretical value and practical significance in promoting the construction of low-carbon buildings.}
}
@article{CEKMIS2014115,
title = {A computational model for accommodating spatial uncertainty: Predicting inhabitation patterns in open-planned spaces},
journal = {Building and Environment},
volume = {73},
pages = {115-126},
year = {2014},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2013.11.023},
url = {https://www.sciencedirect.com/science/article/pii/S0360132313003430},
author = {Aslı Çekmiş and Işıl Hacıhasanoğlu and Michael J. Ostwald},
keywords = {Fuzzy logic, Fuzzy set, Spatial uncertainty, Occupancy prediction, Open-planned spaces},
abstract = {In the past, a range of computational models have been developed for analysing the social implications of spatial patterns and types. While such models are typically focussed on macro-patterns, often in cellular or linearly-organised spaces, few models exist for predicting where people will cluster within complex environments. One reason for this relates to the inherent uncertainty associated with spatial attributes and consequently of human spatial behaviours. The present paper draws on the concept of fuzzy spatial objects to develop an approach to handle such uncertainty in architecture. Focussing on large, open plan spaces, where the configuration of space does not define strict patterns of usage, the paper proposes a computational model for predicting patterns of spatial inhabitation. This new model relies on the theory of fuzzy sets to propose the existence of a “fuzzy architectural spatial object, (FASO)” which is comprised of spatial units with degrees of membership that reflect the possibility of a person being present in a sub-space or involved in a sub-function within a larger space. This model calculates and visualises the FASOs using a fuzzy inference engine and represents the space as distributed possibilities of presence according to the given data. After describing the model the paper demonstrates its application in the prediction of patterns of usage within a major exhibition space, and then presents a check of the efficacy of this prediction against the actual inhabitation of the space.}
}
@article{STEPHENS200833,
title = {What “counts” as algebra in the eyes of preservice elementary teachers?},
journal = {The Journal of Mathematical Behavior},
volume = {27},
number = {1},
pages = {33-47},
year = {2008},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2007.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0732312307000594},
author = {Ana C. Stephens},
keywords = {Algebra, Elementary mathematics, Teacher conceptions},
abstract = {This study examined conceptions of algebra held by 30 preservice elementary teachers. In addition to exploring participants’ general “definitions” of algebra, this study examined, in particular, their analyses of tasks designed to engage students in relational thinking or a deep understanding of the equal sign as well as student work on these tasks. Findings from this study suggest that preservice elementary teachers’ conceptions of algebra as subject matter are rather narrow. Most preservice teachers equated algebra with the manipulation of symbols. Very few identified other forms of reasoning – in particular, relational thinking – with the algebra label. Several participants made comments implying that student strategies that demonstrate traditional symbol manipulation might be valued more than those that demonstrate relational thinking, suggesting that what is viewed as algebra is what will be valued in the classroom. This possibility, along with implications for mathematics teacher education, will be discussed.}
}
@article{MINOZZI2020101498,
title = {Direct response and the strategy method in an experimental cheap talk game},
journal = {Journal of Behavioral and Experimental Economics},
volume = {85},
pages = {101498},
year = {2020},
issn = {2214-8043},
doi = {https://doi.org/10.1016/j.socec.2019.101498},
url = {https://www.sciencedirect.com/science/article/pii/S2214804319300230},
author = {William Minozzi and Jonathan Woon},
keywords = {Strategic information transmission, Sender-receiver games, Strategy method, Laboratory experiment},
abstract = {In cheap talk games, equilibrium analysis predicts extreme limits on the information that can be transmitted when senders and receivers have different goals. Yet experimental evidence suggests that senders overcommunicate relative to this baseline, revealing more information than predicted in equilibrium. We propose that overcommunication may be due in part to limited cognitive engagement by subjects, captured by level-k thinking. To test this conjecture, we compare two elicitation methods, direct response and the strategy method, holding other elements of the game fixed. Existing experimental studies of cheap talk games use the standard direct response method, while the strategy method—in which subjects make selections for all contingent choices—is believed to encourage more thoughtful decisionmaking. We therefore expect senders to transmit less information with the strategy method than with direct response. In contrast, we find the reverse: the strategy method increased overcommunication. Further examination suggests that this occurred because senders played more naïvely with the strategy method than with direct response. Our findings suggest that the strategy method and direct response do not elicit the same choices in cheap talk games.}
}
@article{WIECHERT20031363,
title = {The role of modeling in computational science education},
journal = {Future Generation Computer Systems},
volume = {19},
number = {8},
pages = {1363-1374},
year = {2003},
note = {Selected papers from the Workshop on Education in Computational Sciences held at the International Conference on Computational Science},
issn = {0167-739X},
doi = {https://doi.org/10.1016/S0167-739X(03)00093-1},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X03000931},
author = {W. Wiechert},
keywords = {Computational science education, Modeling and simulation, Modeling education},
abstract = {Modeling and simulation skills are two core competences of computational science and thus should be a central part of any curriculum. While there is a well-founded methodology for the design of simulation algorithms today the teaching of modeling skills carries some intrinsic problems. The reason is that modeling is still partly an art and partly a science. As an important consequence for university education, the concepts for teaching modeling must be quite different from those for teaching simulation algorithms. Experiences made with the courses on ‘Modeling and Simulation’ at the University of Siegen are summarized and some general concepts for the teaching of modeling skills are presented. In particular, three practical approaches to modeling education are discussed with several examples.}
}
@article{MCCREADY2010274,
journal = {Journal of Pragmatics},
volume = {42},
number = {1},
pages = {274-278},
year = {2010},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2009.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S0378216609001532},
author = {Elin McCready}
}
@article{FERNANDEZCABALLERO2003341,
title = {Lateral interaction in accumulative computation: a model for motion detection},
journal = {Neurocomputing},
volume = {50},
pages = {341-364},
year = {2003},
issn = {0925-2312},
doi = {https://doi.org/10.1016/S0925-2312(02)00571-4},
url = {https://www.sciencedirect.com/science/article/pii/S0925231202005714},
author = {Antonio Fernández-Caballero and José Mira Mira and Ana E. Delgado and Miguel A. {Fernández Graciani}},
keywords = {Accumulative computation, Lateral interaction, Double time scale, Motion detection, Image sequences},
abstract = {Some of the major computer vision techniques make use of neural nets. In this paper we present a novel model based on neural networks denominated lateral interaction in accumulative computation (LIAC). This model is based on a series of neuronal models in one layer, namely the local accumulative computation model, the double time scale model and the recurrent lateral interaction model. The LIAC model usefulness in the general task of motion detection may be appreciated by means of some significant examples of object detection in indefinite sequences of synthetic and real images.}
}
@article{FORTHMANN201959,
title = {Creative ideation, broad retrieval ability, and processing speed: A confirmatory study of nested cognitive abilities},
journal = {Intelligence},
volume = {75},
pages = {59-72},
year = {2019},
issn = {0160-2896},
doi = {https://doi.org/10.1016/j.intell.2019.04.006},
url = {https://www.sciencedirect.com/science/article/pii/S0160289618301211},
author = {Boris Forthmann and David Jendryczko and Jana Scharfen and Ruben Kleinkorres and Mathias Benedek and Heinz Holling},
keywords = {Divergent thinking, Broad retrieval ability, Processing speed, Structural equation modeling},
abstract = {Divergent thinking (DT) ability (i.e., the ability to come up with creative ideas) is a complex cognitive construct that has been associated with several specific components of the Cattel-Horn-Carroll (CHC) model. In this study, we employed a nested latent variable approach to examine the specific role of mental speed (Gs) and general retrieval ability (Gr) in DT ability, which was assessed by DT tasks that instructed to be creative and were scored for creative quality. Specifically, Gs was assumed to facilitate both Gr and DT, and Gr was assumed to contribute to DT. Successive latent variable models with orthogonal factors were tested to reflect these nested cognitive basic abilities. The proposed model of nested factors fit the data well: Latent Gs accounted for variation in Gs, Gr, and DT creative quality scores, latent Gr predicted performance in Gr and DT scores beyond Gs, and latent DT explained variation in DT scores beyond Gs and Gr. In addition, we related the resulting orthogonal latent variables to the external criteria of school grades to illustrate the explanatory power of the modeling approach. This study provides evidence that divergent thinking performance relies on mental speed and retrieval ability, as well as cognitive abilities unique to divergent thinking. We discuss consequences for the understanding of divergent thinking ability in the context of the CHC model.}
}
@article{LOU2022100247,
title = {Two-additive fuzzy measure-based information integration approach to product design alternative evaluation},
journal = {Journal of Industrial Information Integration},
volume = {25},
pages = {100247},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100247},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000467},
author = {Shanhe Lou and Yixiong Feng and Zhiwu Li and Jianrong Tan},
keywords = {Multi-criteria decision-making, Two-additive fuzzy measure, Information integration, Intuitionistic linguistic number},
abstract = {Conceptual design is a pivotal stage of new product development in manufacturing industries. Since multiple design alternatives are put forward at this stage, developing advanced evaluation methods is of great importance. Existing methods adopt additive models to integrate evaluation data. They face some inconsistency issues, e.g. inconsistency in the independent assumption and interdependent data, since evaluation criteria are interactional. Fuzzy measure that replaces the additivity with monotonicity has enabled advances in addressing such issues. This work proposes a two-additive fuzzy measure-based information integration approach to product design alternative evaluation for the first time. The evaluation data given by experts are in the form of intuitionistic linguistic numbers. They are more in accordance with the thinking habits of experts because the hesitation degree in linguistic assessment can be revealed. In order to reduce the subjective bias, the decision-making trial and evaluation laboratory method combining with grey relational analysis is applied to adjust evaluation data. Then monotonous two-additive fuzzy measure is identified by nonlinear programming using these data. It makes a good trade-off between computational complexity and presentation capability. Hence, evaluation data can be integrated by non-additive Choquet integral for ranking design alternatives. In comparison to additive model-based methods, the extra effect on the simultaneous satisfaction of criteria can be effectively revealed by the proposed approach. And the robustness of it is demonstrated by the sensitivity analysis. A case study on an elevator's design alternative evaluation is conducted to illustrate the feasibility and practicability of the proposed approach.}
}
@article{MATSUDA2005275,
title = {Functional competency and cognitive ability in mild Alzheimer's Disease: relationship between ADL assessed by a relative/ carer-rated scale and neuropsychological performance},
journal = {International Psychogeriatrics},
volume = {17},
number = {2},
pages = {275-288},
year = {2005},
issn = {1041-6102},
doi = {https://doi.org/10.1017/S1041610205001304},
url = {https://www.sciencedirect.com/science/article/pii/S1041610224046805},
author = {Osamu Matsuda and Masahiko Saito},
keywords = {Alzheimer's disease, cognitive deficits, functional competency},
abstract = {ABSTRACT
Background: Alzheimer's disease (AD) is characterized by multiple cognitive deficits and affects functional competency to perform daily activities (ADL). As this may contribute to the patient's overall disability, it is important to identify factors that compromise competency. Objective: The relationship between different cognitive domains and functional activities in AD was studied. Methods: The functional competency of 73 Japanese AD patients, most with mild dementia, was assessed using a 27-item relative/carer-rating scale covering 7 ADL: managing finances, using transportation, taking precautions, self-care, housekeeping, communication and taking medicine. Cognitive assessment used 16 neuropsychological tests from the Japanese version of the WAIS-R and COGNISTAT, covering 9 cognitive domains: orientation, attention, episodic memory, semantic memory, language, visuoperceptual and construction abilities, computational ability, abstract thinking, and psychomotor speed. Results: Multiple regression analysis by the stepwise method indicated that functional competency could, for the most part, be predicted from test scores for orientation, abstract thinking and psychomotor speed. Discussion: The results of this study suggest that impairment of these three cognitive domains plays an important role in the functional deterioration of AD.}
}
@article{BEHARA2009195,
title = {Parallel finite element computation of incompressible flows},
journal = {Parallel Computing},
volume = {35},
number = {4},
pages = {195-212},
year = {2009},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2008.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167819108001348},
author = {Suresh Behara and Sanjay Mittal},
keywords = {Navier–Stokes equations, Parallel computing, Superlinear speedup, Wake, Transition, Wake instabilities},
abstract = {A stabilized finite element formulation for three-dimensional unsteady incompressible flows is implemented on a distributed memory parallel computer. A matrix-free version of the GMRES algorithm is utilized to solve the equation systems in an implicit manner. The scalability of the computations on a 64-processor Linux cluster is evaluated for moderate to large size problems. A method for estimating the speedup for large-scale problems, where computations on a single processor is not possible, is proposed. Superlinear speedup is observed, perhaps for the first time, for a large-scale problem that is associated with more than 44 million nodes and 176 million equations. The performance of the various subactivities of the program is monitored to investigate the cause. It is found that the formation of the RHS vector and the preconditioner achieves a very high level of superlinear speedup as the number of processors increase. As a result, even though the network time for interprocessor communication increases with increase in processors, an overall superlinear speedup is realized for large-scale problems. The superlinear speedup is attributed to cache related effects. A comparison between the performance of matrix and matrix-free versions of the GMRES algorithm is carried out. It is found that for large-scale applications the matrix-free version outperforms its counterpart for reasonable dimensions of the Kyrylov subspace. The effect of mesh partitioning on the scalability is also studied. A significant reduction in communication time is observed with partitioning that leads to an overall improvement of speedup. The parallel implementation is utilized to study the wake instabilities in flow past a stationary circular cylinder at Re=150, 200 and 300. The Re=150 flow is found to be two-dimensional while mode-A and mode-B instabilities are observed at Re=200 and 300, respectively. The Re=300 flow is associated with a low frequency modulation in addition to the vortex shedding frequency.}
}
@article{GUO2025112955,
title = {Multimodal fine-grained reasoning for post quality evaluation},
journal = {Applied Soft Computing},
volume = {174},
pages = {112955},
year = {2025},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2025.112955},
url = {https://www.sciencedirect.com/science/article/pii/S1568494625002662},
author = {Xiaoxu Guo and Siyan Liang and Yachao Cui and Juxiang Zhou and Lei Wang and Han Cao},
keywords = {Multimodal data analysis, Forum posts, Post evaluation},
abstract = {Accurate assessment of post quality frequently necessitates complex relational reasoning skills that emulate human cognitive processes, thereby requiring the modeling of nuanced relationships. However, existing research on post-quality assessment suffers from the following problems: (1) They are often categorization tasks that rely solely on unimodal data, which inadequately captures information in multimodal contexts and fails to differentiate the quality of students’ posts finely. (2) They ignore the noise in the multimodal deep fusion between posts and topics, which may produce misleading information for the model. (3) They do not adequately capture the complex and fine-grained relationships between post and topic, resulting in an inaccurate evaluation, such as relevance and comprehensiveness. Based on the above challenges, the Multimodal Fine-grained Topic-post Relational Reasoning(MFTRR) framework is proposed for modeling fine-grained cues by simulating the human thinking process. It consists of the local–global semantic correlation reasoning module and the multi-level evidential relational reasoning module. Specifically, MFTRR addresses the challenge of unimodal and categorization task limitations by framing post-quality assessment as a ranking task and integrating multimodal data to more effectively distinguish quality differences. To capture the most relevant semantic relationships, the Local–Global Semantic Correlation Reasoning Module enables deep interactions between posts and topics at both local and global scales. It is complemented by a topic-based maximum information fusion mechanism to filter out noise. Furthermore, to model complex and subtle relational reasoning, the Multi-Level Evidential Relational Reasoning Module analyzes topic-post relationships at both macro and micro levels by identifying critical cues and delving into granular relational cues. MFTRR is evaluated using three newly curated multimodal topic-post datasets, in addition to the publicly available Lazada-Home dataset. Experimental results indicate that MFTRR outperforms state-of-the-art baselines, achieving a 9.52% improvement in the NDCG@3 metric compared to the best text-only method on the Art History course dataset.}
}
@article{REGGIO2002459,
title = {Computational analysis of the process for manufacturing seamless tubes},
journal = {Applied Thermal Engineering},
volume = {22},
number = {4},
pages = {459-470},
year = {2002},
issn = {1359-4311},
doi = {https://doi.org/10.1016/S1359-4311(01)00093-X},
url = {https://www.sciencedirect.com/science/article/pii/S135943110100093X},
author = {M. Reggio and F. McKenty and Luc Gravel and J. Cortes and G. Morales and M.-A. {Ladron de Guevara}},
keywords = {Seamless tube, Heat transfer, Computational simulation, CFD},
abstract = {A computer simulation of the transient three-dimensional heat transfer process occurring during the manufacturing of seamless tubes carried out by TAMSA, Tubos de Acero de Mexico, is reported. The work was performed by a team which combines Canadian and Mexican researchers and comprises both experimental and computational aspects. The Mexican team concentrated its efforts on experimentally investigating the metallurgical pattern of the mandrel, while the Canadian team devoted its time to the computer simulation and analysis of heat transfer and flow processes. In this paper, only the latter part is presented. The numerical simulation uses the Star-CD commercial CFD software package which is based on the finite volume methodology. The results show the importance of the cooling water channel configuration in relation to the mandrel temperature distribution and resulting metallurgical structure.}
}
@incollection{ROZINAJOVA201823,
title = {Chapter 2 - Computational Intelligence in Smart Grid Environment},
editor = {Arun Kumar Sangaiah and Michael Sheng and Zhiyong Zhang},
booktitle = {Computational Intelligence for Multimedia Big Data on the Cloud with Engineering Applications},
publisher = {Academic Press},
pages = {23-59},
year = {2018},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-12-813314-9},
doi = {https://doi.org/10.1016/B978-0-12-813314-9.00002-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128133149000025},
author = {Viera Rozinajová and Anna Bou Ezzeddine and Marek Lóderer and Jaroslav Loebl and Róbert Magyar and Petra Vrablecová},
keywords = {Smart grid, Intelligent data analysis, Computational intelligence, Power load prediction, Optimization, Bio-inspired algorithms, Ensemble models, Support vector regression},
abstract = {This chapter presents one way of incorporating computational intelligence into smart grid environment. We introduce an energy ecosystem, where contemporary technologies are used and by involving advanced methods of data analysis and optimization, we aim to ensure its effective operation. In order to schedule reliable energy supply, the prediction models for power load consumption and for energy spot prices are inevitable. We provide an overview of forecasting and optimization methods and propose solutions, which deal with stream and online processing as well as adaptivity of the proposed solutions. Several different prediction methods including statistical methods and computational intelligence methods, as well as our proposed ensemble and online SVR method are compared. We take into account the current trends of distributed energy generation from renewable sources and anticipate massive usage of electro vehicles in the near future, where the optimization of the whole environment is needed.}
}
@article{SHAHID2019638,
title = {Computational intelligence techniques for medical diagnosis and prognosis: Problems and current developments},
journal = {Biocybernetics and Biomedical Engineering},
volume = {39},
number = {3},
pages = {638-672},
year = {2019},
issn = {0208-5216},
doi = {https://doi.org/10.1016/j.bbe.2019.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0208521619300452},
author = {Afzal Hussain Shahid and M.P. Singh},
keywords = {Computational intelligence, Disease diagnosis, Prediction, Detection, Uncertainty, Medical data},
abstract = {Diagnosis, being the first step in medical practice, is very crucial for clinical decision making. This paper investigates state-of-the-art computational intelligence (CI) techniques applied in the field of medical diagnosis and prognosis. The paper presents the performance of these techniques in diagnosing different diseases along with the detailed description of the data used. This paper includes basic as well as hybrid CI techniques that have been used in recent years so as to know the current trends in medical diagnosis domain. The paper presents the merits and demerits of different techniques in general as well as application specific context. This paper discusses some critical issues related to the medical diagnosis and prognosis such as uncertainties in the medical domain, problems in the medical data especially dealing with time-stamped (temporal) data, and knowledge acquisition. Moreover, this paper also discusses the features of good CI techniques in medical diagnosis. Overall, this review provides new insight for future research requirements in the medical diagnosis domain.}
}
@incollection{ADAMS2016283,
title = {Chapter 16 - Brain Computations in Schizophrenia},
editor = {Ted Abel and Thomas Nickl-Jockschat},
booktitle = {The Neurobiology of Schizophrenia},
publisher = {Academic Press},
address = {San Diego},
pages = {283-295},
year = {2016},
isbn = {978-0-12-801829-3},
doi = {https://doi.org/10.1016/B978-0-12-801829-3.00024-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128018293000240},
author = {R.A. Adams and K.J. Friston},
keywords = {Schizophrenia, Bayesian brain, precision, aberrant salience, reversal, predictive coding, NMDAR, GABA, delusions},
abstract = {Because the brain performs Bayesian inference for the causes of its sensory data, the synaptic gain could encode the precision (inverse variance) of its beliefs using a hierarchical generative model and predictive coding. Several neurobiological risk factors for schizophrenia (NMDAR and GABAergic interneuron hypofunction) reduce both synaptic and “oscillatory” gain at high hierarchical areas. This could impair the encoding of precision at higher levels of the brain’s hierarchical model and increase expected precision at lower levels. This imbalance can account for many neurobiological and phenomenological findings in schizophrenia. Striatal D2R hyperactivity may increase the precision of current policies by inhibiting behavioral or cognitive switching. This could be a (dysfunctional) consequence of or even an attempt to compensate for prefrontal or hippocampal pathology. This D2R hyperactivity may also reduce learning from positive outcomes and affect the encoding of motivational (or informational) salience.}
}
@article{SHU2025117791,
title = {MSFPSO: Multi-algorithm integrated particle swarm optimization with novel strategies for solving complex engineering design problems},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {437},
pages = {117791},
year = {2025},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2025.117791},
url = {https://www.sciencedirect.com/science/article/pii/S0045782525000635},
author = {Bin Shu and Gang Hu and Mao Cheng and Cunxia Zhang},
keywords = {Particle swarm optimization, Cauchy variation, Joint adversarial selection, Differential creative search, Attraction-rejection, Algorithm fusion},
abstract = {Particle swarm optimization (PSO) is considered among the best seminal meta-heuristic algorithms,boasting merits of minimal parameter requirements, straightforward implementation, and highly accelerated convergence capacity, lower computational complexity, etc. Nevertheless, it also has drawbacks, for instance, it tends to converge prematurely at local optima, lack of diversity, and low accuracy. In order to effectively overcome these shortcomings, this paper presents a multi-strategy fusion enhanced PSO called MSFPSO algorithm. Firstly,It motivated by the black-winged kite algorithm, a migration mechanism based on Cauchy's variation is introduced. This mechanism contributes to the efficiency and effectiveness of the algorithm in exploiting the present search area. Also, it effectively balances the dynamics relationship between exploration and exploitation, boosting the algorithm's global and local search capabilities.Second, a joint-opposition selection strategy is introduced for expanding the solution search range. Our approach is designed to avoid getting stuck in local optima. Specifically, selective opposition obtains the proximity dimension of a candidate solution through a linearly decreasing threshold. Dynamic opposition further extends the process of investigating the solution space. The algorithm is fully incorporated with the differential creative search algorithm for dual-strategy scenarios to enhance the performance of the decision-making effectiveness, population diversity, exploitation capability of the PSO. Finally, an attraction-rejection optimization strategy is introduced to further obtain a good exploitation-exploration balance capability and avoid stagnation of the algorithm. In addition, the comparison results with eight advanced optimization algorithms and six improved particle swarm optimization algorithms on CEC2020 test sets, and the statistical analysis was conducted by Wilcoxon rank sum test. It illustrate the features of the MSFPSO developed within this research strong competitiveness. The convergence of the algorithm was verified at maximum iterations of 10000 on the CEC2017 test set. Meanwhile, the experimental outcomes of applying MSFPSO to 50 practical engineering design challenges prove its effectiveness and strong applicability. The test results and numerical computations manifest that the MSFPSO algorithm with strong competitiveness will become a preferred class of meta-heuristic algorithms to tackle issues within the realm of engineering optimization.}
}
@article{RIZZI20131,
title = {Introduction: Core computational principles in natural language syntax},
journal = {Lingua},
volume = {130},
pages = {1-13},
year = {2013},
note = {SI: Syntax and cognition: core ideas and results in syntax},
issn = {0024-3841},
doi = {https://doi.org/10.1016/j.lingua.2012.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0024384112002756},
author = {Luigi Rizzi}
}
@article{LIN20253,
title = {Multiple predictions of others’ actions in the human brain},
journal = {Trends in Neurosciences},
volume = {48},
number = {1},
pages = {3-4},
year = {2025},
issn = {0166-2236},
doi = {https://doi.org/10.1016/j.tins.2024.10.009},
url = {https://www.sciencedirect.com/science/article/pii/S0166223624002182},
author = {Yongling Lin and Marco K. Wittmann},
keywords = {social cognition, neuroimaging, decision making, prediction, theory of mind, computational modelling},
abstract = {The success of our actions often depends on what others are doing. How does the brain discern predictions of others’ actions when situations are ambiguous? Recent work by Ma and colleagues suggests that the brain solves this problem by entertaining multiple predictions of others’ actions, ranked by their likelihood.}
}
@incollection{JOHNSON2009137,
title = {Embodied cognition of movement decisions: a computational modeling approach},
editor = {Markus Raab and Joseph G. Johnson and Hauke R. Heekeren},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {174},
pages = {137-150},
year = {2009},
booktitle = {Mind and Motion: The Bidirectional Link between Thought and Action},
issn = {0079-6123},
doi = {https://doi.org/10.1016/S0079-6123(09)01312-0},
url = {https://www.sciencedirect.com/science/article/pii/S0079612309013120},
author = {Joseph G. Johnson},
keywords = {attention, decision making, motor system},
abstract = {This chapter presents a cognitive computational view of decision making as the search for, and accumulation of, evidence for options under consideration. It is based on existing models that have been successful in traditional decision tasks involving preferential choice. The model assumes shifting attention over time that determines momentary inputs to an evolving preference state. In this chapter, the cognitive model is extended to illustrate how links from the motor system may be incorporated. These links can basically be categorized into one of three influences: modifying the subjective evaluation of choice options, restricting attention, and altering the options that are to be found in the choice set. The implications for the formal model are introduced and preliminary evidence is drawn from the extant literature.}
}
@article{SAMARASINGHE2013188,
title = {Mixed-method integration and advances in fuzzy cognitive maps for computational policy simulations for natural hazard mitigation},
journal = {Environmental Modelling & Software},
volume = {39},
pages = {188-200},
year = {2013},
note = {Thematic Issue on the Future of Integrated Modeling Science and Technology},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2012.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S1364815212001909},
author = {Sandhya Samarasinghe and Graham Strickert},
keywords = {Fuzzy cognitive maps, Auto-Associative Neural Networks, Self-organizing maps, Natural hazard mitigation, Earthquakes, Mixed-method triangulation, Policy simulation},
abstract = {Human systems need to be adaptive to the consequences of natural hazards. Public policy decisions on natural hazard mitigation can benefit from computational models that embody a comprehensive view of the system. Such models need to be transparent and integrate both expert and lay expert knowledge and experience in an efficient manner. By integrating hard and soft sciences within an overall systems framework, scientists, policy makers and communities can better understand how to improve adaptive capacity. We present a fuzzy cognitive map based Auto-Associative Neural Networks framework generated from a development mixed method integration (triangulation) for adaptive policy formulations. The specific policies relate to preparation for, response to, and recovery from earthquakes in mountainous ski-field environments – a case study chosen to highlight the framework. Three different data collection techniques – expert geomorphic assessments, semi-structured qualitative interviews with three stakeholder groups (experts and lay experts), and fuzzy cognitive maps (FCM) (node and arc maps of stakeholder perceptions) were employed. FCM were first analysed using Graph theory indices to determine map structure. Special attention was paid to subsequent processing of fuzzy cognitive maps (e.g., condensation and aggregation) with qualitative followed by quantitative means to simplify the FCM from the original total of 300 variables to 5 high-level themes to improve the efficacy of subsequent policy simulations. Specifically, the use of Self Organising Maps (SOM) to group concepts (condensation) and individual stakeholders (aggregation) into social group FCMs is a novel contribution to advancing FCM. In the process, SOM also enabled the embedment of nonlinear relationships inherent in the system in the simplified FCM allowing a platform for realistic and meaningful policy simulations based on collective perceptions. Specifically, each of the three simplified stakeholder group FCM and a total social group FCM was represented by Auto-Associative Neural Networks (AANN) which converts an FCM into a dynamical system that allows policy scenario simulations based on input from both expert and lay expert stakeholders. A policy scenario is the level of importance given to a set of concepts and their effects on the system behaviour as revealed by the simulations. We present the results from one of several policy simulations to highlight the effectiveness of the mixed-method integration leading to simplified-FCM based ANNN simulations. Results revealed the similarities and differences between stakeholder group responses in relation to the scenario analysed and how these formed collective responses in the total social group map. Furthermore, outcomes of group and total social group simulations could be interpreted from individual and group stakeholder FCMs giving credibility to the mixed-method approach.}
}
@article{SAMUELSSON2023100173,
title = {A shape of play to come: Exploring children's play and imaginaries with robots and AI},
journal = {Computers and Education: Artificial Intelligence},
volume = {5},
pages = {100173},
year = {2023},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2023.100173},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X23000528},
author = {Robin Samuelsson},
keywords = {Early childhood, Robots, AI, Play, Playful learning, Sociotechnical imaginaries},
abstract = {We are rapidly moving into an era where AI and robots are part of everyday interactions in society and education, and there are immense discussions today about current and future technologies. Still, children are often not included in this discussion, while there is much to learn from current uses and children's understandings of AI and robotics. The study is based on a seven-month ethnographical work that details the implementation of a robot in two preschool groups of children aged 1–2 and 3–5 (n = 38). The study descriptively combines a framework for children's play analysis with explorative qualitative child interviews (n = 6) with the 3-5-year-olds to examine how children play with the robot and their thinking about a future with robots and AI. The results show how children's play with robots spans all of Hughes's (2011) sixteen play types and integrates robots into play in ways specific to child-robot interaction. The interviews indicate that children have well-formed knowledge about the current uses of robots and AI and elaborate imaginaries about a future with them, including critical boundaries toward robots and AI agents. The evidence shows emerging ways children relate to these. The potential of including children's actions and voices in the ongoing societal and educational debates on AI is discussed.}
}
@article{MAIRAL202262,
title = {What should the university of the future look like?},
journal = {On the Horizon},
volume = {31},
number = {1},
pages = {62-70},
year = {2022},
issn = {1074-8121},
doi = {https://doi.org/10.1108/OTH-08-2022-0050},
url = {https://www.sciencedirect.com/science/article/pii/S1074812122000239},
author = {Ricardo Mairal},
keywords = {Employment, Internationalization, Higher education, Quality, Artificial intelligence, Online and distance education},
abstract = {Purpose
In this paper, the author has tried to outline the main ideas in connection with what the author conceives to be the university of the future, a university that should not only educate people within the university system but also prepare them to fill specific job positions at both local and global levels, apart from necessarily providing them with the critical thinking and competences in autonomous learning that will make them flexible and capable of adapting to the job market and to a fast-changing world in general.
Design/methodology/approach
The author has revised some of the major issues that are going to determine the direction of the university of the future, i.e. the employment opportunities of tomorrow; the role of new technologies, especially the impact of artificial intelligence (AI); quality in higher education; and internationalization.
Findings
The author has also pointed out the importance of the technologies and the great role they indisputably play in present and future education at all levels, a fact that has been particularly and hugely enhanced and promoted by the COVID-19 pandemic situation, thereby facilitating and fostering distance learning. This is very much connected to the application of AI to higher education, another unavoidable issue of utmost importance for the university of the future. While these technological advances present a challenge to universities, which must determine which are necessary and desirable and how to implement them, it is, ultimately, our responsibility to use them, in an ethical way, to the benefit of our students. The university of the future also has to be of high quality, and this involves carrying out important and decisive action having to do with matters of inclusion, hiring policies and the expansion of international opportunities for all parties involved.
Originality/value
This paper outlines the main ideas in connection with what the author conceives to be the university of the future, a university that should not only educate people within the university system but also prepare them to fill specific job positions at both local and global levels, apart from necessarily providing them with the critical thinking and competences in autonomous learning that will make them flexible and capable of adapting to the job market and to a fast-changing world in general. Moreover, the role of new technologies (especially the impact of AI), quality and internationalization are also discussed as relevant factors in this view of the university of the future.}
}
@article{KUDARIYAWAR2016193,
title = {Computational study of instabilities in a rectangular natural circulation loop using 3D CFD simulation},
journal = {International Journal of Thermal Sciences},
volume = {101},
pages = {193-206},
year = {2016},
issn = {1290-0729},
doi = {https://doi.org/10.1016/j.ijthermalsci.2015.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S1290072915003440},
author = {Jayaraj Yallappa Kudariyawar and Abhijeet Mohan Vaidya and Naresh Kumar Maheshwari and Polepalle Satyamurthy},
keywords = {Natural circulation loop, 3D CFD simulation, Instability},
abstract = {Steady state and transient characteristics of a natural circulation loop working with water are obtained. For this purpose, 3D steady state and transient CFD simulations are performed. The CFD model includes pipe thickness as well as secondary side coolant passage apart from primary side. Steady state and transient characteristics are computed for various configurations i.e. Vertical Heater Vertical Cooler (VHVC), Horizontal Heater Horizontal Cooler (HHHC), etc. Steady state data was compared with available correlations. Flow initiation transients were compared with experimental data. Both the steady state and transient results are found to be in good agreement with previously published data. The reason for formation of unidirectional and bi-directional pulsing in HHHC configuration at different powers is explained with the help of temperature fields at different instants of time. Effect of sudden power rise/power step back on instability in HHHC configuration is estimated using CFD simulations.}
}
@article{KONOPKA200391,
title = {Selected dreams and nightmares about computational biology},
journal = {Computational Biology and Chemistry},
volume = {27},
number = {2},
pages = {91-92},
year = {2003},
issn = {1476-9271},
doi = {https://doi.org/10.1016/S1476-9271(03)00024-0},
url = {https://www.sciencedirect.com/science/article/pii/S1476927103000240},
author = {Andrzej K Konopka}
}
@article{THOMPSON1983161,
title = {Thinking about thinking},
journal = {Trends in Neurosciences},
volume = {6},
pages = {161-163},
year = {1983},
issn = {0166-2236},
doi = {https://doi.org/10.1016/0166-2236(83)90076-0},
url = {https://www.sciencedirect.com/science/article/pii/0166223683900760},
author = {I.D. Thompson},
abstract = {The Cognitive Neuroscience Institute held its first conference in September 1982, in Kusadasi, Turkey. The institute was recently established in New York to promote research in cognitive neuroscience, and in December 1982 it presented the Hermann von Helmholtz Award to Vernon Mountcastle (see TINS, January 1983, Vol. 6, p. 9). The meeting was attended by individuals whose specialities range from molecular biology to philosophy. Their common aim was to investigate the role of cognitive neuroscience in establishing a theory of mental processing which combines the knowledge derived from cognitive psychology and from neuroscience. How this synthesis is to be achieved, and indeed the extent to which it is possible, was the subject of wide-ranging and often vigorous debate. But, as many disciplines begin to converge on common problems, the prospects for cognitive neuroscience appear encouraging. Thus, as neuroscientists start to unravel the molecular mechanisms of learning and memory, it is interesting to consider what constraints such mechanisms might place on the operational rules for correlating single-neuron activity and behaviour in invertebrates, it has been argued that similar progress in understanding the mammalian brain will come from the application of models, derived from cognitive psychology, to neurophysiology. Artificial intelligence provides an opportunity to model many cognitive processes, but how close do the models come to reflecting underlying mental states? Indeed, the problem or non-problem of self-awareness dominated many conversations, tantalizing some participants by its intractability and accepted by others as a naturally emergent attribute of the mechanics of the mind.}
}
@incollection{JOHNSON201435,
title = {Chapter 3 - Computational and Process Models of Decision Making in Psychology and Behavioral Economics},
editor = {Paul W. Glimcher and Ernst Fehr},
booktitle = {Neuroeconomics (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {35-47},
year = {2014},
isbn = {978-0-12-416008-8},
doi = {https://doi.org/10.1016/B978-0-12-416008-8.00003-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124160088000036},
author = {Eric J. Johnson and Roger Ratcliff},
keywords = {Computation Process Models, Decision Neuroscience, Drift-Diffusion Models, economic theory, Intertemporal Choice, Riskless Choice, Risky Choice},
abstract = {This chapter reviews models of choice on two levels: The first concerns the descriptions of choice and their evolution from normative models of how choices should be make to more behaviorally realistic models, more consistent with data showing that choice depends heavily on context. We present brief overviews of risky and riskless choice models and data and for choice over time. We then turn to computational process models, a more recent class of models that make prediction for multiple properties of the decision process beyond simply what is chosen, including predicting the distribution of errors and decision times.These models are typically applied to simpler choices, but have found great use in contemporary neuroscience.}
}
@article{MARTINRAMOS201751,
title = {First exposure to Arduino through peer-coaching: Impact on students' attitudes towards programming},
journal = {Computers in Human Behavior},
volume = {76},
pages = {51-58},
year = {2017},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2017.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0747563217304193},
author = {Pablo Martín-Ramos and Maria João Lopes and M. Margarida {Lima da Silva} and Pedro E.B. Gomes and Pedro S. {Pereira da Silva} and José P.P. Domingues and Manuela {Ramos Silva}},
keywords = {Attitudes survey, Arduino, High school, Programming, Peer coaching},
abstract = {In this paper we report the work that jeKnowledge (Júnior Empresa da Faculdade de Ciências e Tecnologias da Universidade de Coimbra), a student-led initiative, has done in the ‘jeKnowledge academy’ courses to actively engage Portuguese high-school students in STEM education through hands-on projects based on the low-cost Arduino platform. F2F activities, based on a peer-assisted learning strategy, were complemented with tutorials and more advanced project suggestions in a blog. Pre and post surveys on students' attitudes towards programming and peer-coaching were administered to pre-university and first year college participants, finding an overall increase in the Likert scale for all the programming-related constructs under study (confidence, interest, gender, usefulness and professional) after the introductory course. As regards the peer-based learning approach, younger students seemed to be more eager to be taught in a less formal way than their older counterparts. The course resulted in high degrees of satisfaction for both the student tutors and their tutees.}
}
@article{XUE2024100156,
title = {Conceptual frameworks for the integration of genetic and social epidemiology in complex diseases},
journal = {Global Epidemiology},
volume = {8},
pages = {100156},
year = {2024},
issn = {2590-1133},
doi = {https://doi.org/10.1016/j.gloepi.2024.100156},
url = {https://www.sciencedirect.com/science/article/pii/S2590113324000221},
author = {Diane Xue and Anjum Hajat and Alison E. Fohner},
abstract = {Uncovering the root causes of complex diseases requires complex approaches, yet many studies continue to isolate the effects of genetic and social determinants of disease. Epidemiologic efforts that under-utilize genetic epidemiology methods and findings may lead to incomplete understanding of disease. Meanwhile, genetic epidemiology studies are often conducted without consideration of social and environmental context, limiting the public health impact of genomic discoveries. This divide endures despite shared goals and increases in interdisciplinary data due to a lack of shared theoretical frameworks and differing language. Here, we demonstrate that bridging epidemiological divides does not require entirely new ways of thinking. Existing social epidemiology frameworks including Ecosocial theory and Fundamental Cause Theory, can both be extended to incorporate principles from genetic epidemiology. We show that genetic epidemiology can strengthen, rather than detract from, efforts to understand the impact of social determinants of health. In addition to presenting theoretical synergies, we offer practical examples of how genetics can improve the public health impact of epidemiology studies across the field. Ultimately, we aim to provide a guiding framework for trainees and established epidemiologists to think about diseases and complex systems and foster more fruitful collaboration between genetic and traditional epidemiological disciplines.}
}
@article{JOHNSON1997721,
title = {Observations with regard to massively parallel computation for Monte Carlo simulation of stochastic dynamical systems},
journal = {International Journal of Non-Linear Mechanics},
volume = {32},
number = {4},
pages = {721-734},
year = {1997},
note = {Third International Stochastic Structural Dynamics Conference},
issn = {0020-7462},
doi = {https://doi.org/10.1016/S0020-7462(96)00097-2},
url = {https://www.sciencedirect.com/science/article/pii/S0020746296000972},
author = {E.A. Johnson and S.F. Wojtkiewicz and L.A. Bergman and B.F. Spencer},
abstract = {The evolution of stochastic dynamical systems is governed by Fokker-Planck equations if the response process is Markovian. Analytical solutions for the transient response of multidimensional systems exist only for the simplest dynamical systems. The evolution of the transition probability density function over the phase space has been solved numerically for various low dimensional systems subjected to additive and multiplicative white noise excitations using the finite element method. Systems of higher order, however, pose difficulty when using standard finite element formulations due to memory requirements and computational expense. Direct Monte Carlo simulation (MCS), while often regarded as less elegant than other methods, can be used to solve problems of significantly higher complexity. The number of realizations required to accurately produce the transition probability density function over the entire phase space, especially in the tails, is large, but since each realization is entirely independent of the others, the Monte Carlo simulation is easily and efficiently adapted to parallel computation. The advent of high-speed, massively-parallel computers permits a large number of realizations of a complex dynamical system to be simultaneously determined. Consequently, Monte Carlo simulation may be more efficient for higher-dimensional systems than other solution methods currently in use. This investigation will examine some of these observations and compare the performance of MCS on various platforms, in the context of a four-dimensional linear oscillator and a Duffing oscillator subjected to band-limited white noise.}
}
@article{EVERS2025180,
title = {Preliminaries to artificial consciousness: A multidimensional heuristic approach},
journal = {Physics of Life Reviews},
volume = {52},
pages = {180-193},
year = {2025},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2025.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S1571064525000028},
author = {K. Evers and M. Farisco and R. Chatila and B.D. Earp and I.T. Freire and F. Hamker and E. Nemeth and P.F.M.J. Verschure and M. Khamassi},
abstract = {The pursuit of artificial consciousness requires conceptual clarity to navigate its theoretical and empirical challenges. This paper introduces a composite, multilevel, and multidimensional model of consciousness as a heuristic framework to guide research in this field. Consciousness is treated as a complex phenomenon, with distinct constituents and dimensions that can be operationalized for study and for evaluating their replication. We argue that this model provides a balanced approach to artificial consciousness research by avoiding binary thinking (e.g., conscious vs. non-conscious) and offering a structured basis for testable hypotheses. To illustrate its utility, we focus on "awareness" as a case study, demonstrating how specific dimensions of consciousness can be pragmatically analyzed and targeted for potential artificial instantiation. By breaking down the conceptual intricacies of consciousness and aligning them with practical research goals, this paper lays the groundwork for a robust strategy to advance the scientific and technical understanding of artificial consciousness.}
}
@incollection{RUFFONI2017169,
title = {3.10 Finite Element Analysis in Bone Research: A Computational Method Relating Structure to Mechanical Function☆},
editor = {Paul Ducheyne},
booktitle = {Comprehensive Biomaterials II},
publisher = {Elsevier},
address = {Oxford},
pages = {169-196},
year = {2017},
isbn = {978-0-08-100692-4},
doi = {https://doi.org/10.1016/B978-0-12-803581-8.09798-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128035818097988},
author = {D. Ruffoni and G.H. {van Lenthe}},
keywords = {Bone imaging, Bone research, Computational modeling, Femur, Finite element analysis, Fracture, Hierarchical structure, Micro-computed tomography, Osteoporosis, Radius, Strength, Vertebra},
abstract = {Bone is probably the most frequently investigated biological material and finite element analysis (FEA) is the computational tool most commonly used for the analysis of bone biomechanical function. FEA has been used in bone research for more than 30 years and has had a substantial impact on our understanding of the complex behavior of bone. Bone is structured in a hierarchical way covering many length scales and this chapter reflects this hierarchical organization. In particular, the focus is on the applications of FEA for understanding the relationship between bone structure and its mechanical function at specific hierarchical levels. Depending on the hierarchical level, different issues have been investigated with FEA ranging from more clinically oriented topics related to bone quality (eg, predicting bone strength and fracture risk) to more fundamental problems dealing with the mechanical aspects of biological processes (eg, stress and strain around osteocyte lacunae) as well as with the micromechanical behavior of bone at its ultrastructure. A better understanding of the relationship between structure and mechanical function is expected to be important for the current trends in (bio)materials design, where the structure of biological materials is considered as a possible source of inspiration, as well as for more successful approaches in the prevention and treatment of age- and disease-related fractures.}
}
@article{MARTINRAMOS2018420,
title = {Reprint of ‘First exposure to Arduino through peer-coaching: Impact on students' attitudes towards programming’},
journal = {Computers in Human Behavior},
volume = {80},
pages = {420-427},
year = {2018},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2017.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S074756321730691X},
author = {Pablo Martín-Ramos and Maria João Lopes and M. Margarida {Lima da Silva} and Pedro E.B. Gomes and Pedro S. {Pereira da Silva} and José P.P. Domingues and Manuela {Ramos Silva}},
keywords = {Attitudes survey, Arduino, High school, Programming, Peer coaching},
abstract = {In this paper we report the work that jeKnowledge (Júnior Empresa da Faculdade de Ciências e Tecnologias da Universidade de Coimbra), a student-led initiative, has done in the ‘jeKnowledge academy’ courses to actively engage Portuguese high-school students in STEM education through hands-on projects based on the low-cost Arduino platform. F2F activities, based on a peer-assisted learning strategy, were complemented with tutorials and more advanced project suggestions in a blog. Pre and post surveys on students' attitudes towards programming and peer-coaching were administered to pre-university and first year college participants, finding an overall increase in the Likert scale for all the programming-related constructs under study (confidence, interest, gender, usefulness and professional) after the introductory course. As regards the peer-based learning approach, younger students seemed to be more eager to be taught in a less formal way than their older counterparts. The course resulted in high degrees of satisfaction for both the student tutors and their tutees.}
}
@article{COLLINS2023101585,
title = {Generative linguistics: ‘Galilean style’},
journal = {Language Sciences},
volume = {100},
pages = {101585},
year = {2023},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2023.101585},
url = {https://www.sciencedirect.com/science/article/pii/S0388000123000505},
author = {John Collins},
keywords = {Chomsky, Centre-embedding, Competence/performance, Computation, Galilean style, Galileo},
abstract = {Generative linguistics is often claimed by Chomsky to have a 'Galilean style', which is intended to position linguistics as a science continuous with standard practise in the natural sciences. These claims, however, are more suggestive than explanatory. The paper will, first, explain just what a Galilean style is. It will then be argued that its application to two key notions in generative linguistics - the competence/performance distinction (with reference to centre-embedding) and the notion of computation - demands a departure from what we might expect of a Galilean style. In this sense, the epithet is misleading. It will also be shown, however, that the 'Galilean' label is appropriate once we factor in the difference between a science concerned with kinematics (the relations between objects in space and time) and one concerned with language.}
}
@article{ZENG2024123400,
title = {Research on the application of knowledge mapping and knowledge structure construction based on adaptive learning model},
journal = {Expert Systems with Applications},
volume = {249},
pages = {123400},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.123400},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424002653},
author = {Xiyin Zeng and Shouqiang Liu},
keywords = {Personalized learing, Pedagogy, Interactive learning environments, Applications},
abstract = {This project has developed a geometry learning software that integrates multiple computer technologies to address the challenges of deep analysis of knowledge points and establishing connections in learning software. The software combines Long Short-Term Memory (LSTM) and Residual Neural Network (ResNet101) to encode text and image features. A self-attention mechanism is used to fuse information from both modalities, enabling decoding of geometric models and classification of corresponding knowledge points.This project uses LSTM and ResNet101 models to extract text and visual features for problem-solving using the Multi Mode Thinking Chain (CoT) method. Classification labels are utilized to generate text responses for problem-solving ideas. Furthermore, a recommendation module is proposed, which combines knowledge tracking and neural collaborative filtering algorithms to capture student behavior and knowledge point vectors. Implicit factors representing students' mastery of different knowledge points are used as inputs in neural collaborative filtering for personalized recommendations. The results demonstrate improvements in accuracy using the ResNet + LSTM multimodal algorithm, achieving a 13 % increase compared to single-modal classification. The multimodal CoT approach also outperforms language models like GPT3.5 and VisualBert by 10 %. Additionally, the combined algorithm of knowledge tracking and neural collaborative filtering shows a 13.3 % higher F1 value compared to ordinary algorithms, confirming the superiority of the adopted method in this project.}
}
@article{MCKELVEY2009476,
title = {Designing an electronic auction market for complex ‘smart parts’ logistics: Options based on LeBaron's computational stock market},
journal = {International Journal of Production Economics},
volume = {120},
number = {2},
pages = {476-494},
year = {2009},
note = {Special Issue on Introduction to Design and Analysis of Production Systems},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2009.03.006},
url = {https://www.sciencedirect.com/science/article/pii/S0925527309000899},
author = {Bill McKelvey and Christine Wycisk and Michael Hülsmann},
keywords = {Supply chain management, Electronic auction market, I&C technologies, Complexity theory, Neural networks},
abstract = {Modern technologies, such as RFID, offer never-before seen learning abilities to parts moving in supply chains. Logistics systems may be understood as complex adaptive logistics systems (CALS). They also may be conceived as electronic auction markets as ‘smart parts’ bid for the best routing and pricing from transportation firms. To ensure the world-wide functionality and efficiency of CALS transportation markets, we suggest the utility of an agent-based computational market design based on Blake LeBaron's stock-market model. Given that parts may be more or less smart, markets more or less complex, and self-organizing CALS systems probabilistically subject to the bullwhip effect, we suggest nine different computational CALS market-design options, offering more adaptivity to unexpected environmental contingencies.}
}
@article{RAJ2021474,
title = {Assessment of antiviral potencies of cannabinoids against SARS-CoV-2 using computational and in vitro approaches},
journal = {International Journal of Biological Macromolecules},
volume = {168},
pages = {474-485},
year = {2021},
issn = {0141-8130},
doi = {https://doi.org/10.1016/j.ijbiomac.2020.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S0141813020351783},
author = {Vinit Raj and Jae Gyu Park and Kiu-Hyung Cho and Pilju Choi and Taejung Kim and Jungyeob Ham and Jintae Lee},
keywords = {Cannabinols,  antiviral assay, SARS-CoV-2 and M enzyme},
abstract = {Effective treatment choices to the severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2) are limited because of the absence of effective target-based therapeutics. The main object of the current research was to estimate the antiviral activity of cannabinoids (CBDs) against the human coronavirus SARS-CoV-2. In the presented research work, we performed in silico and in vitro experiments to aid the sighting of lead CBDs for treating the viral infections of SARS-CoV-2. Virtual screening was carried out for interactions between 32 CBDs and the SARS-CoV-2 Mpro enzyme. Afterward, in vitro antiviral activity was carried out of five CBDs molecules against SARS-CoV-2. Interestingly, among them, two CBDs molecules namely Δ9 -tetrahydrocannabinol (IC50 = 10.25 μM) and cannabidiol (IC50 = 7.91 μM) were observed to be more potent antiviral molecules against SARS-CoV-2 compared to the reference drugs lopinavir, chloroquine, and remdesivir (IC50 ranges of 8.16–13.15 μM). These molecules were found to have stable conformations with the active binding pocket of the SARS-CoV-2 Mpro by molecular dynamic simulation and density functional theory. Our findings suggest cannabidiol and Δ9 -tetrahydrocannabinol are possible drugs against human coronavirus that might be used in combination or with other drug molecules to treat COVID-19 patients.}
}
@article{TAILLANDIER2025105121,
title = {The dynamic sketch map to support reflection on urban flooding},
journal = {International Journal of Disaster Risk Reduction},
volume = {116},
pages = {105121},
year = {2025},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2024.105121},
url = {https://www.sciencedirect.com/science/article/pii/S2212420924008835},
author = {Franck Taillandier and Patrick Taillandier and Pénélope Brueder and Noé Brosse},
keywords = {Urban flood, Game, Sketch map, Agent-based simulation},
abstract = {Flood risk management is a significant concern for many regions. To reduce the flood impact, it is essential to increase residents' knowledge about this risk and in its management. Despite the many tools and methods available to raise awareness of flood risk, none of them fully meet the challenges of effective communication on flood and flood management by: integrating the perspective of local people, by providing information that is clear and easy to understand, by encouraging debate, discussion and reflection and by positioning flood mitigation measure at the center (positive vision on the risk). To answer this need, this article proposes an innovative approach that combines several methods, including sketch maps, agent-based simulation, and serious games. This combination enables to benefit from these three approaches: the expressiveness of sketch maps and the ability to analyze participants' spatial representations, the capacity of agent-based simulations to aid users in comprehending complex phenomena and dynamics, and the experimental and motivational environment provided by games. To implement this approach, we developed the DYSMA model, which bridges the gap between sketch maps and agent-based simulations by integrating drawn elements as agents, providing a dynamic sketch map. Additionally, we developed the Draw and Flood game, designed to engage the general public in thinking about flood management through the use of dynamic sketch maps. This approach is applied to an illustrative application dedicated to flooding in a small French city.}
}
@article{YOUNG201719,
title = {Technology-enhanced mathematics instruction: A second-order meta-analysis of 30 years of research},
journal = {Educational Research Review},
volume = {22},
pages = {19-33},
year = {2017},
issn = {1747-938X},
doi = {https://doi.org/10.1016/j.edurev.2017.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1747938X1730026X},
author = {Jamaal Young},
keywords = {Meta-analysis, Mathematics achievement, Technology, Calculators, Computer-assisted instruction},
abstract = {It is important to assess the cumulative effects of technology on student achievement captured in the last 30 years of technologyenhanced mathematics instruction. Synthesizing the thousands of articles and gray literature on this subject is necessary but would require a considerable commitment of academic resources. A second-order metaanalysis or meta-analysis of meta-analyses is an alternative that is reasonable and effective. Thus, a second-order meta-analysis of 19 prior meta-analyses with minimum overlap between primary studies was conducted. The results represent 663 primary studies (approximately 141,733 participants) and 1,263 effect sizes. The random effects' mean effect size of .38 was statistically significantly different from zero. The results provide a historical and contextualized summary of 30 years of meta-analytic research, which supports meta-analytic thinking and better interpretation of future effect sizes. Results indicate that technology function and study quality are major contributors to effect size variation. Specifically, computation enhancement technologies were most effective, while studies that examine combinations of enhancements were least effective. Implications for technology-enhanced mathematics instruction and meta-analytic research are provided.}
}
@article{LEOPOLD2024102913,
title = {The big mixup: Neural representation during natural modes of primate visual behavior},
journal = {Current Opinion in Neurobiology},
volume = {88},
pages = {102913},
year = {2024},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2024.102913},
url = {https://www.sciencedirect.com/science/article/pii/S0959438824000758},
author = {David A. Leopold},
abstract = {The primate brain has evolved specialized visual capacities to navigate complex physical and social environments. Researchers studying cortical circuits underlying these capacities have traditionally favored the use of simplified tasks and brief stimulus presentations in order to isolate cognitive variables with tight experimental control. As a result, operational theories about visual brain function have come to emphasize feature detection, hierarchical stimulus encoding, top-down task modulation, and functional segregation in distinct cortical areas. Recently, however, experimental paradigms combining natural behavior with electrophysiological recordings have begun to offer a distinctly different portrait of how the brain takes in and analyzes its visual surroundings. The present article reviews recent work in this area, highlighting some of the more surprising findings in domains of social vision and spatial navigation along with shifts in thinking that have begun to emanate from this approach.}
}
@incollection{RAMACHANDRAN2025,
title = {Energy efficiency sustainability framework for cloud and quantum data centres in the era of 6G},
series = {Advances in Computers},
publisher = {Elsevier},
year = {2025},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2025.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S0065245825000476},
author = {Muthu Ramachandran},
keywords = {Green software engineering, Cloud computing, Sustainability, Software engineering framework, Energy efficiency},
abstract = {The advent of advanced computational and communication technologies, notably 6G, cloud computing and quantum computing, has transformed our daily lives and work processes. 6G network will be much faster than the current 5G network and can connect seamlessly both physical and virtual world for more computation requirements with the cloud and quantum computing. In addition, 6G also has promised to half the carbon emission for building a sustainable future. Nonetheless, these technologies are associated with substantial energy consumption, contributing to a noteworthy 5.5 % of carbon emissions. This research presents a comprehensive sustainability framework aimed at guiding the development of large-scale applications. The framework emphasizes the incorporation of energy-efficient requirements, integration of energy efficiency in the design phase, and the implementation of energy-efficient algorithms leveraging Green Function (utilizing 3rd order differential equations to analyse the energy consumption of cloud applications). The experimental findings demonstrate the feasibility of achieving a remarkable 99 % energy efficiency in both cloud and quantum applications.}
}
@article{SPEISER2011271,
title = {Models for products},
journal = {The Journal of Mathematical Behavior},
volume = {30},
number = {4},
pages = {271-290},
year = {2011},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2011.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0732312311000307},
author = {Bob Speiser and Chuck Walter},
keywords = {Model, Representation, Presentation, Operator product, Controlled variable, Frame, Core knowledge, Analog magnitude, Parallel individuation, Shared intentionality},
abstract = {This paper explores how models can support productive thinking. For us a model is a thing, a tool to help make sense of something. We restrict attention to specific models for whole-number multiplication, hence the wording of the title. They support evolving thinking in large measure through the ways their users redesign them. They assume new forms, come to be seen and understood in different ways. We show how work that learners do with models can help them to transform, not simply their understanding of key concepts, but also how they come to view themselves as thinkers and learners, as collaborators in a social process that their work and thinking help to constitute. We draw on recent research on core knowledge, especially by Carey, Spelke, and Tomasello, to clarify how models, as we view them here, can underpin specific actions that support emerging understanding.}
}
@article{CAI2023101087,
title = {Impact of prompts on students’ mathematical problem posing},
journal = {The Journal of Mathematical Behavior},
volume = {72},
pages = {101087},
year = {2023},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2023.101087},
url = {https://www.sciencedirect.com/science/article/pii/S0732312323000573},
author = {Jinfa Cai and Hua Ran and Stephen Hwang and Yue Ma and Jaepil Han and Faith Muirhead},
keywords = {Problem posing, Problem-posing prompt, Problem-posing processes, Task variables, Task characteristics, Teaching mathematics through problem posing, P-PBL},
abstract = {This study used three pairs of problem-posing tasks to examine the impact of different prompts on students’ problem posing. Two kinds of prompts were involved. The first asked students to pose 2–3 different mathematical problems without specifying other requirements for the problems, whereas the second kind of prompt did specify additional requirements. A total of 2124 students’ responses were analyzed to examine the impact of the prompts along multiple dimensions. In response to problem-posing prompts with more specific requirements, students tended to engage in more in-depth mathematical thinking and posed much more linguistically and semantically complex problems with more relationships or steps required to solve them. The findings from this study not only contribute to our understanding of problem-posing processes but also have direct implications for teaching mathematics through problem posing.}
}
@article{YANG2000103,
title = {Computational verb systems: a new paradigm for artificial intelligence},
journal = {Information Sciences},
volume = {124},
number = {1},
pages = {103-123},
year = {2000},
issn = {0020-0255},
doi = {https://doi.org/10.1016/S0020-0255(99)00135-8},
url = {https://www.sciencedirect.com/science/article/pii/S0020025599001358},
author = {Tao Yang},
keywords = {Verbs, Computational verbs, Computational verb systems, Chaos, Artificial intelligence, Reasoning, Knowledge representation},
abstract = {Computational verb systems can help machines to implement, understand and use verbs as human perception of dynamics. By using computational verbs we can embed dynamical experiences of human experts into artificial intelligence. Computational verbs, which are models of verbs in nature languages, are basic building blocks of computational verb systems. In this paper, computational verbs are used to represent dynamical knowledge embedded by verbs as a new framework of knowledge representation. BE-transformations are used to transform statements containing dynamical verbs into statements only containing static verb BE; namely, BE-propositions. Based on BE-transformations, the computational verb logic can be built. Furthermore, reasoning with computational verbs can be built based on BE-transformations and basic verb logic operations.}
}
@article{MARINIER200948,
title = {A computational unification of cognitive behavior and emotion},
journal = {Cognitive Systems Research},
volume = {10},
number = {1},
pages = {48-69},
year = {2009},
note = {Modeling the Cognitive Antecedents and Consequences of Emotion},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2008.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S1389041708000302},
author = {Robert P. Marinier and John E. Laird and Richard L. Lewis},
abstract = {Existing models that integrate emotion and cognition generally do not fully specify why cognition needs emotion and conversely why emotion needs cognition. In this paper, we present a unified computational model that combines an abstract cognitive theory of behavior control (PEACTIDM) and a detailed theory of emotion (based on an appraisal theory), integrated in a theory of cognitive architecture (Soar). The theory of cognitive control specifies a set of required computational functions and their abstract inputs and outputs, while the appraisal theory specifies in more detail the nature of these inputs and outputs and an ontology for their representation. We argue that there is a surprising functional symbiosis between these two independently motivated theories that leads to a deeper theoretical integration than has been previously obtained in other computational treatments of cognition and emotion. We use an implemented model in Soar to test the feasibility of the resulting integrated theory, and explore its implications and predictive power in several task domains.}
}
@article{SCHMALZL20031021,
title = {Using standard image compression algorithms to store data from computational fluid dynamics},
journal = {Computers & Geosciences},
volume = {29},
number = {8},
pages = {1021-1031},
year = {2003},
issn = {0098-3004},
doi = {https://doi.org/10.1016/S0098-3004(03)00098-0},
url = {https://www.sciencedirect.com/science/article/pii/S0098300403000980},
author = {Jörg Schmalzl},
keywords = {Computational fluid dynamics, Data compression, Visualization, Post-processing},
abstract = {Three-dimensional numerical modeling of fluid flows is an important research tool to understand many fluid dynamical effects observed in nature. With the strong growth of available computational resources the use of such models has greatly increased over the last years. Because the available mass storage has not increased in the same order as the CPU speed many researchers nowadays face the problem of how to store and transfer the large data sets produced by the model calculations for post-processing. The use of lossy wavelet-based compression techniques on this data has been investigated in several publications. These techniques are often specialized to one problem and are not easy to implement. In the area of digital media, however, advances have been made for still image (JPEG, JPEG-2000) and motion image (MPEG) compression. In this paper we investigate the usefulness of these image compression algorithms for the storage of data from computational fluid dynamics on regular cartesian grids. We analyze both the compression ratios achieved and the error introduced by these lossy compression schemes. We found that, for our purposes, the JPEG compression scheme allows an easy-to-use, portable, robust, and computationally efficient lossy compression. For the easy use of these compression algorithms we present a simple wrapper library.}
}
@article{YAN20158006,
title = {Trustworthiness evaluation and retrieval-based revision method for case-based reasoning classifiers},
journal = {Expert Systems with Applications},
volume = {42},
number = {21},
pages = {8006-8013},
year = {2015},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2015.06.027},
url = {https://www.sciencedirect.com/science/article/pii/S0957417415004297},
author = {Aijun Yan and Dianhui Wang},
keywords = {Case-based reasoning classifiers, Classification accuracy, Case evaluation, Case revision},
abstract = {To achieve better classification performance using case-based reasoning classifiers, we propose a retrieval-based revision method with trustworthiness evaluation for problem solving. An improved case evaluation method is employed to evaluate the trustworthiness of the suggested solution after the reuse step, which will divide the target cases and its suggested solutions into a trustworthy set and an untrustworthy set in accordance with a threshold value of trustworthiness. The attribute weights are adjusted by running a genetic algorithm and are used in the second round of retrieval of the untrustworthy set to obtain the classification results. Experimental results demonstrate that our proposed method performs favorably compared with other methods. Also, the proposed method has less computation complexity for the trustworthiness evaluation, and enhances understanding on thinking and inference for case-based reasoning classifiers.}
}
@article{STRASS201534,
title = {Analyzing the computational complexity of abstract dialectical frameworks via approximation fixpoint theory},
journal = {Artificial Intelligence},
volume = {226},
pages = {34-74},
year = {2015},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2015.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0004370215000776},
author = {Hannes Strass and Johannes Peter Wallner},
keywords = {Abstract dialectical frameworks, Computational complexity, Approximation fixpoint theory},
abstract = {Abstract dialectical frameworks (ADFs) have recently been proposed as a versatile generalization of Dung's abstract argumentation frameworks (AFs). In this paper, we present a comprehensive analysis of the computational complexity of ADFs. Our results show that while ADFs are one level up in the polynomial hierarchy compared to AFs, there is a useful subclass of ADFs which is as complex as AFs while arguably offering more modeling capacities. As a technical vehicle, we employ the approximation fixpoint theory of Denecker, Marek and Truszczyński, thus showing that it is also a useful tool for complexity analysis of operator-based semantics.}
}
@article{DVIR20061233,
title = {Virtual Leashing: Creating a computational foundation for software protection},
journal = {Journal of Parallel and Distributed Computing},
volume = {66},
number = {9},
pages = {1233-1240},
year = {2006},
note = {Special Issue: Security in grid and distributed systems},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2006.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S074373150600092X},
author = {Ori Dvir and Maurice Herlihy and Nir N. Shavit},
keywords = {Digital rights management, Virtual leashing},
abstract = {We introduce Virtual Leashing,11The techniques described in this paper are protected by U.S. patents, both granted and pending. a new technique for software protection and control. The leashing process removes small fragments of code, pervasive throughout the application, and places them on a secure server. The secure server provides the missing functionality, but never the missing code. Reverse engineering the missing code, even with full tracing of the program's execution and its communication with the server, is computationally hard. Moreover, the server provides the missing functionality asynchronously: the application's performance is independent (within reason) of the secure server's speed. For example, the server might reside on a slow inexpensive chip or a remote Internet server. Leashing makes only modest demands on communication bandwidth, space, and computation.}
}
@article{MANSILHA2019190,
title = {Environmental externalities in broiler production: An analysis based on system dynamics},
journal = {Journal of Cleaner Production},
volume = {209},
pages = {190-199},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2018.10.179},
url = {https://www.sciencedirect.com/science/article/pii/S0959652618331950},
author = {Ricardo Brandão Mansilha and Dalila Cisco Collatto and Daniel Pacheco Lacerda and Maria Isabel {Wolf Motta Morandi} and Fabio Sartori Piran},
keywords = {Broiler, Environmental externalities, Energy sources, Systems thinking, System dynamics},
abstract = {Broiler represents approximately 1.5% of the Brazilian Gross Domestic Product (GDP). Brazil is one of the world's largest producers and exporters of chicken. Aiming to improve and sustain a competitive advantage, producers have invested in improvements in production systems in general, in particular aviary heating systems. However, producers need to choose the best among several alternatives of energy sources for heating. This decision impacts the environment to a greater or a lesser extent depending on the energy source chosen. The aim of this study is to develop a computational model to understand systemically and dynamically the environmental externalities based on the choice of energy source for aviary heating. The identification of criteria that influence the choice for heating systems was possible through a multiple case-study in the southern region of Brazil. By designing a computational model of system dynamics, it was possible to visualize scenarios using different energy sources and their respective negative environmental externalities. From the analysis of four scenarios, we sought to identify the one with the best relation to environmental and economic performance. It was evidenced that the scenario with the best relation was that using pellets as an energy source for aviary heating. The developed model may be applied to solve similar decision-making problems.}
}
@article{PARK20151,
title = {A Neuro-educational Study of the Development of the Creativity-based Teaching Program and its Effect},
journal = {Procedia - Social and Behavioral Sciences},
volume = {186},
pages = {1-8},
year = {2015},
note = {The Proceedings of 5th World Conference on Learning, Teaching and Educational Leadership},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2015.04.200},
url = {https://www.sciencedirect.com/science/article/pii/S187704281502460X},
author = {Sun-Hyung Park and Kwang-Ki Kim and Kyung-Hwa Lee},
keywords = {Creativity-based teaching programs, Divergent thinking, The TTCT, FMRI},
abstract = {This study aims at exploring a possibility of developing a creativity-based teaching program needed for enhancing prospective teachers’ creative potentials based on the theories of Sawyer and Renzulli. Neuroimaging tools such as fMRI were used to identify effects of the program on pre-service teachers’ neural activations on divergent thinking measured primarily by the Torrance Tests of Creative Thinking (TTCT). Since the research is still in progress, we present a theoretical model for the teaching program, and preliminary test results of comparing changes of neural recruitments in students’ brain participated in fMRI with the TTCT.}
}
@incollection{CLEMENTI200589,
title = {Chapter 6 - Computational chemistry: Attempting to simulate large molecular systems},
editor = {Clifford E. Dykstra and Gernot Frenking and Kwang S. Kim and Gustavo E. Scuseria},
booktitle = {Theory and Applications of Computational Chemistry},
publisher = {Elsevier},
address = {Amsterdam},
pages = {89-114},
year = {2005},
isbn = {978-0-444-51719-7},
doi = {https://doi.org/10.1016/B978-044451719-7/50049-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780444517197500494},
author = {Enrico Clementi},
abstract = {Publisher Summary
Computational chemistry is a very vast field dealing with atomic and molecular systems, considered at different complexity levels either as discretized quantum mechanical systems, or as statistical ensembles, amenable to Monte Carlo and Molecular Dynamic treatments, or as continuous matter fluid-dynamical distributions, modeled with Navier– Stokes equations. The mainstream computational chemistry was bent to fully solve the correlation problem with a single “technology.” Computational chemistry became a must for more and more chemists, even if the computer users had less and less awareness of the computational details of computer programs, and hardly understood that the computed answer could be incorrect, because of limitations of the selected method. In this computer generation and even more in the following years, internet, communications, commercial computer programs, computer servers, personal computers, desktop, graphics, Window, and Linux were common words, memory and disk space seemed unlimited, price/performance improved yearly, but faith in the computer replaced knowledge of the instrument and its software. Computational chemistry was becoming a part of the global economy.}
}
@article{GARCIASANCHO201216,
title = {From the genetic to the computer program: the historicity of ‘data’ and ‘computation’ in the investigations on the nematode worm C. elegans (1963–1998)},
journal = {Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences},
volume = {43},
number = {1},
pages = {16-28},
year = {2012},
note = {Data-Driven Research in the Biological and Biomedical Sciences On Nature and Normativity: Normativity, Teleology, and Mechanism in Biological Explanation},
issn = {1369-8486},
doi = {https://doi.org/10.1016/j.shpsc.2011.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S1369848611000781},
author = {Miguel García-Sancho},
keywords = {, Genetics, Computer, Program, Software, Data, Genomics, Model organism},
abstract = {This paper argues that the history of the computer, of the practice of computation and of the notions of ‘data’ and ‘programme’ are essential for a critical account of the emergence and implications of data-driven research. In order to show this, I focus on the transition that the investigations on the worm C. elegans experienced in the Laboratory of Molecular Biology of Cambridge (UK). Throughout the 1980s, this research programme evolved from a study of the genetic basis of the worm’s development and behaviour to a DNA mapping and sequencing initiative. By examining the changing computing technologies which were used at the Laboratory, I demonstrate that by the time of this transition researchers shifted from modelling the worm’s genetic programme on a mainframe apparatus to writing minicomputer programs aimed at providing map and sequence data which was then circulated to other groups working on the genetics of C. elegans. The shift in the worm research should thus not be simply explained in the application of computers which transformed the project from hypothesis-driven to a data-intensive endeavour. The key factor was rather a historically specific technology—in-house and easy programmable minicomputers—which redefined the way of achieving the project’s long-standing goal, leading the genetic programme to co-evolve with the practices of data production and distribution.}
}
@article{TRUBA2024101496,
title = {Psycholinguistic underpinnings of image formation: Suggestion and manipulation in the educational network discourse},
journal = {Thinking Skills and Creativity},
volume = {52},
pages = {101496},
year = {2024},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2024.101496},
url = {https://www.sciencedirect.com/science/article/pii/S1871187124000348},
author = {Hanna Truba and Sergii Khrapatyi and Kyrylo Harashchuk and Dmytro Shvets and Alina Proskurnia},
keywords = {Psycholinguistics, Image, Suggestion, Manipulation, Attraction, Fascination},
abstract = {This study delves into the intricate psycholinguistic mechanisms that underpin image formation within the educational network discourse, with a specific focus on the dynamics of suggestion and manipulation. In an era where digital communication reigns supreme, understanding how language shapes perceptions and influences behavior is paramount. This research seeks to unravel the complex interplay between suggestion, manipulation, and the formation of images within educational networks. Drawing from insights across disciplines such as psychology, linguistics, and communication studies, this study examines how linguistic cues and contextual factors interact to shape individuals' perceptions and responses within educational settings. Acknowledging the transformative power of language in shaping attitudes, beliefs, and actions, this study aims to shed light on the subtle yet profound ways in which educators employ linguistic strategies to influence discourse within educational networks. By employing a multifaceted approach that integrates theoretical frameworks with empirical analysis, this research endeavors to uncover the underlying mechanisms driving suggestion and manipulation within educational discourse. Through a meticulous examination of textual elements, discourse patterns, and communicative strategies employed by educators in digital environments, this study seeks to elucidate the intricate processes involved in image formation. By exploring the role of suggestion and manipulation in shaping perceptions, attitudes, and behaviors, this research contributes to a deeper understanding of the psycholinguistic underpinnings of educational network discourse. Furthermore, this study not only offers theoretical insights but also practical implications for educators, policymakers, and practitioners involved in educational communication. By highlighting the ethical considerations and implications of linguistic manipulation within educational networks, this research aims to empower stakeholders to navigate digital discourse with greater awareness and discernment. In conclusion, this study represents a significant contribution to the field of thinking skills and creativity by offering new insights into the psycholinguistic dynamics of image formation within educational networks. By unraveling the complexities of suggestion and manipulation, this research opens avenues for further inquiry and underscores the importance of critical thinking and creativity in navigating contemporary digital landscapes.}
}
@article{ACAR2006993,
title = {Endowing cognitive mapping with computational properties for strategic analysis},
journal = {Futures},
volume = {38},
number = {8},
pages = {993-1009},
year = {2006},
note = {Organisational Foresight},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2005.12.018},
url = {https://www.sciencedirect.com/science/article/pii/S0016328705002302},
author = {William Acar and Douglas Druckenmiller},
abstract = {A number of cognitive, causal mapping and simulation techniques exist for dealing with the growing importance of environmental uncertainty. After briefly commenting on some of the more salient extant approaches, this paper offers a new one for consideration by the scenario planning community. Comprehensive Situation Mapping (CSM) is a powerful analytical tool combined with a process for framing and debating strategic situations. The CSM approach combines the problem framing features of causal mapping with a dialectical inquiry process patterned after Churchman's. Like the better approaches to planning through cognitive mapping, it facilitates the “backward analysis” of the underlying strategic assumptions. Its novelty is that it also allows the “forward analysis” of a situation by computing the potential change scenarios. Initially developed for manual application, the principles of CSM were originally tested in appropriate case studies. The contribution of the present paper is to present its theory and point out that its future potential is even greater: in concluding we indicate that, by using recent distributed artificial intelligence (DAI) technology, a fully computerized and interactive prototype is now being set up for commercial applications.}
}
@article{ZHU2020102369,
title = {Sentiment and guest satisfaction with peer-to-peer accommodation: When are online ratings more trustworthy?},
journal = {International Journal of Hospitality Management},
volume = {86},
pages = {102369},
year = {2020},
issn = {0278-4319},
doi = {https://doi.org/10.1016/j.ijhm.2019.102369},
url = {https://www.sciencedirect.com/science/article/pii/S0278431918307333},
author = {Liang Zhu and Yan Lin and Mingming Cheng},
keywords = {Peer-to-peer accommodation, Guest satisfaction, Online ratings, Sentiment analysis, Analytical thinking, Authenticity},
abstract = {This study aims to decode guest satisfaction with peer-to-peer accommodations by analyzing the relationship between guests’ sentiment and online ratings and examining how analytical thinking and authenticity influence this relationship. Based on reviews of 4602 Airbnb listings in San Francisco, we empirically find that positive (negative) sentiment is linked to a high (low) rating. We further show that this link is stronger when guests manifest a higher extent of analytical thinking and authenticity. Both Tobit and ordered logit models yield consistent estimation results, showing the robustness of our findings. Our study contributes to the tourism and hospitality literature by theoretically explaining the association between sentiment and ratings. In addition, this paper enriches our knowledge regarding the trustworthiness of Airbnb ratings.}
}
@article{MORTON2019,
title = {Computer Programming: Should Medical Students Be Learning It?},
journal = {JMIR Medical Education},
volume = {5},
number = {1},
year = {2019},
issn = {2369-3762},
doi = {https://doi.org/10.2196/11940},
url = {https://www.sciencedirect.com/science/article/pii/S2369376219000011},
author = {Caroline E Morton and Susan F Smith and Tommy Lwin and Michael George and Matt Williams},
keywords = {coding, medical education, undergraduate curriculum},
abstract = {Background
The ability to construct simple computer programs (coding) is being progressively recognized as a life skill. Coding is now being taught to primary-school children worldwide, but current medical students usually lack coding skills, and current measures of computer literacy for medical students focus on the use of software and internet safety. There is a need to train a cohort of doctors who can both practice medicine and engage in the development of useful, innovative technologies to increase efficiency and adapt to the modern medical world.
Objective
The aim of the study was to address the following questions: (1) is it possible to teach undergraduate medical students the basics of computer coding in a 2-day course? (2) how do students perceive the value of learning computer coding at medical school? and (3) do students see computer coding as an important skill for future doctors?
Methods
We developed a short coding course to teach self-selected cohorts of medical students basic coding. The course included a 2-day introduction on writing software, discussion of computational thinking, and how to discuss projects with mainstream computer scientists, and it was followed on by a 3-week period of self-study during which students completed a project. We explored in focus groups (FGs) whether students thought that coding has a place in the undergraduate medical curriculum.
Results
Our results demonstrate that medical students who were complete novices at coding could be taught enough to be able to create simple, usable clinical programs with 2 days of intensive teaching. In addition, 6 major themes emerged from the FGs: (1) making sense of coding, (2) developing the students’ skill set, (3) the value of coding in medicine, research, and business, (4) role of teaching coding in medical schools, (5) the concept of an enjoyable challenge, and (6) comments on the course design.
Conclusions
Medical students can acquire usable coding skills in a weekend course. They valued the teaching and identified that, as well as gaining coding skills, they had acquired an understanding of its potential both for their own projects and in health care delivery and research. They considered that coding skills teaching should be offered as an optional part of the medical curriculum.}
}
@article{CARLOZZI2022263,
title = {Daily Variation in Sleep Quality is Associated With Health-Related Quality of Life in People With Spinal Cord Injury},
journal = {Archives of Physical Medicine and Rehabilitation},
volume = {103},
number = {2},
pages = {263-273.e4},
year = {2022},
issn = {0003-9993},
doi = {https://doi.org/10.1016/j.apmr.2021.07.803},
url = {https://www.sciencedirect.com/science/article/pii/S0003999321013630},
author = {Noelle E. Carlozzi and Jenna Freedman and Jonathan P. Troost and Traci Carson and Ivan R. Molton and Dawn M. Ehde and Kayvan Najarian and Jennifer A. Miner and Nicholas R. Boileau and Anna L. Kratz},
keywords = {Ecological momentary assessment, Quality of life, Rehabilitation, Sleep, Spinal cord injuries},
abstract = {Objective
Although sleep difficulties are common after spinal cord injury (SCI), little is known about how day-to-day fluctuations in sleep quality affects health-related quality of life (HRQOL) among these individuals. We examined the effect of sleep quality on same-day HRQOL using ecological momentary assessment methods over a 7-day period.
Design
Repeated-measures study involving 7 days of home monitoring; participants completed HRQOL measures each night and ecological momentary assessment ratings 3 times throughout the day; multilevel models were used to analyze data.
Setting
Two academic medical centers.
Participants
A total of 170 individuals with SCI (N=170).
Interventions
Not applicable.
Main Outcome Measures
Daily sleep quality was rated on a scale of 0 (worst) to 10 (best) each morning. Participants completed end-of-day diaries each night that included several HRQOL measures (Sleep Disturbance, Sleep-related Impairment, Fatigue, Cognitive Abilities, Pain Intensity, Pain Interference, Ability to Participate in Social Roles and Activities, Depression, Anxiety) and ecological momentary assessment ratings of HRQOL (pain, fatigue, subjective thinking) 3 times throughout each day.
Results
Multilevel models indicated that fluctuations in sleep quality (as determined by end-of-day ratings) were significantly related to next-day ratings of HRQOL; sleep quality was related to other reports of sleep (Sleep Disturbance; Sleep-related Impairment; Fatigue) but not to other aspects of HRQOL. For ecological momentary assessment ratings, nights of poor sleep were related to worse pain, fatigue, and thinking. Generally, sleep quality showed consistent associations with fatigue and thinking across the day, but the association between sleep quality and these ecological momentary assessment ratings weakened over the course of the day.
Conclusions
Findings highlight the important association between sleep and HRQOL for people with SCI. Future work targeting sleep quality improvement may have positive downstream effects for improving HRQOL in people with SCI.}
}
@incollection{LANDAUER200243,
title = {On the computational basis of learning and cognition: Arguments from LSA},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {41},
pages = {43-84},
year = {2002},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(02)80004-4},
url = {https://www.sciencedirect.com/science/article/pii/S0079742102800044},
author = {Thomas K Landauer},
abstract = {Publisher Summary
This chapter discusses the computational basis of learning and cognition. To deal with a continuously changing environment, living things have three choices: (1) evolve unvarying processes that usually succeed, (2) evolve genetically fixed effector, perceptual, and computational functions that are contingent on the environment, and (3) learn adaptive functions during their lifetimes. The theme of this chapter is the relation between (2) and (3): the nature of evolutionarily determined computational processes that support learning. The principal goal of this chapter has been to suggest that high-dimensional vector space computations based on empirical associations among very large numbers of components could be a close model of a fundamental computational basis of most learning in both verbal and perceptual domains. More powerful representational effects can be brought about by linear inductive combinations of the elements of very large vocabularies than has often been realized. Success of one such model to demonstrate many natural properties of language commonly assumed to be essentially more complex, nonlinear, and/or unlearned, along with evidence and argument that similar computations may serve similar roles in object recognition, are taken to reaffirm the possibility that a single underlying associational mechanism lies behind many more special and complex appearing cognitive phenomena.}
}
@incollection{KRETZER202153,
title = {Chapter 4 - Digital crafting: a new frontier for material design},
editor = {Owain Pedgley and Valentina Rognoli and Elvin Karana},
booktitle = {Materials Experience 2},
publisher = {Butterworth-Heinemann},
pages = {53-66},
year = {2021},
isbn = {978-0-12-819244-3},
doi = {https://doi.org/10.1016/B978-0-12-819244-3.00003-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012819244300003X},
author = {Manuel Kretzer},
keywords = {Digital, crafting, design, architecture, production, fabrication, parametric, generative, Industry 4.0},
abstract = {This chapter provides an overview of the potentials of employing computational design methods and digital fabrication tools for the creation of novel, material-based design. Just as in the early days of architecture, when the master builder was responsible for all areas of building, these new technologies allow a return to the exploration of experimental design methods and the direct exchange with different materials. Designing for and through digital production techniques thus shifts the focus from formal design representations toward the physically realized. As such, material and tectonic thinking are reintroduced as the very base of the design approach. Due to this a new type of design becomes possible with a formerly unknown degree of complexity—both on a formal and on a functional level. This chapter gives an overview of the history of design, speaks about the so-called “digital continuum,” highlights the benefits of customization and individual production, stresses the nuisance of new esthetic formalizations and the importance of education to mediate such understanding to students of design and architecture.}
}
@article{LI2024101590,
title = {Transforming maker mindsets: A case study of elementary students in a maker education context during lesson study},
journal = {Thinking Skills and Creativity},
volume = {53},
pages = {101590},
year = {2024},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2024.101590},
url = {https://www.sciencedirect.com/science/article/pii/S1871187124001287},
author = {Jiajia Li and Zhuang Li and Huixin Gao and Tianying Yun},
keywords = {Maker mindsets, STEM learning, Maker education, Lesson study},
abstract = {Utilizing a case study approach, this research investigates the transformation of elementary students' Maker mindsets within the context of Maker education through a lesson study cycle. The study focuses on the Maker mindsets transformation of three students with varying abilities, deliberately chosen as information-rich participants. A project-specific questionnaire, the Maker Mindsets Scale, was employed to facilitate self-assessment of Maker mindsets before and after intervention. Additionally, teachers' post-lesson discussion meetings were observed, and semi-structured interviews with participating teachers were conducted to gauge their perceptions of students' Maker mindsets transformation. The analysis encompassed students' semi-structured reflection logs and interviews to uncover the underlying factors driving Maker mindsets transformation. The results revealed distinct variations in how students of different abilities perceived their Maker mindsets transformation. Nonetheless, participant teachers consistently observed transformations in STEM (Science, Technology, Engineering, Mathematics) thinking skills, self-efficacy, motivation, and collaborative learning across all students. The study further identifies a collaborative convergence of multiple factors contributing to Maker mindsets transformation, spanning teacher, student, and pedagogical perspectives. These findings carry significant implications for educators, advocating for the implementation of customized strategies, authentic contextualization, structured methodologies, and collaborative frameworks to holistically nurture Maker mindsets evolution. Moreover, our study underscores the practicality of the LS approach in fostering collaborative development of innovative pedagogical strategies aimed at fostering Maker mindsets formation.}
}
@article{REYNANTE2024102287,
title = {Reducing the cognitive abstractness of climate change through an “engineering fiction” learning experience: A natural language processing study},
journal = {Journal of Environmental Psychology},
volume = {95},
pages = {102287},
year = {2024},
issn = {0272-4944},
doi = {https://doi.org/10.1016/j.jenvp.2024.102287},
url = {https://www.sciencedirect.com/science/article/pii/S0272494424000604},
author = {Brandon Reynante and Nicole M. Ardoin and Roy Pea},
keywords = {Artificial intelligence, Climate change education, Climate fiction},
abstract = {The lackluster societal response to the climate crisis is partially attributed to the abstractness of people's mental construals of climate change given its vast spatial and temporal dimensions, which fail to evoke urgency to act. Prior efforts to measure mental construal levels of climate change are inconsistent, insufficient, and labor-intensive. This study developed and implemented learning experiences for integrating engineering design and climate fiction writing to engage 48 high school students in concrete climate change thinking. A novel measure of cognitive abstractness overcomes previous methodological shortcomings by automatically quantifying the linguistic abstractness of participant-authored stories using natural language processing. Comparing participant stories written at the beginning and end of the intervention reveals a significant decrease in linguistic abstractness (Cohen's d = 1.01, p = 0.03). This study contributes to the nascent movement for greater use of narratives as data sources in environmental psychology research, which may uncover new insights into human behavior and decision making.}
}
@article{WU2025200125,
title = {Exploring a unified definition of ecological complexity towards restoration},
journal = {Total Environment Advances},
pages = {200125},
year = {2025},
issn = {2950-3957},
doi = {https://doi.org/10.1016/j.teadva.2025.200125},
url = {https://www.sciencedirect.com/science/article/pii/S2950395725000050},
author = {Haoran Wu and Jed Soleiman and Jamie Bolam and Joseph Scott Boyle},
keywords = {Ecological complexity, Complexity science, System science, Ecosystem, Biodiversity, Ecological modelling, Integrative ecology},
abstract = {The concept of ecological complexity transforms the way we conceptualise ecosystem entities, interactions, and processes. Restoring the complexity of ecosystems has been proposed as a principle for nature recovery activities. Yet, the use of the ‘complexity’ is inconsistent in different research, and lacks a generally accepted definition. This paper draws perspectives from various research fields to synthesise a cohesive concept of ecological complexity. We review a school of thought within the field of ecology, environment, and technical fields such as algorithmic complexity theory, information theory, chaos theory, and graph theory. We also summarise research methods framed by the complexity concept, and provide case studies on how restoration may benefit from the concept, philosophy, and methodology within the umbrella term. The complexity of an ecosystem includes the variety of species and the interactions among them or with environmental variables, with which the essential ecosystem functions, stability, and resilience are sustained. This concept indicates that species and ecological interactions are unified currencies of restoration. Meaningful assembly of species and interactions requires a deep understanding of each restoration project, and here we reflect on tree disease management, regenerative agriculture, species reintroduction, and seagrass systems. Complexity-framed research methods, ranging from systematic reviews to causal loop diagrams and individual-based models, help to integrate the best available information, understand how ecosystems operate, and examine alternative management policies.}
}
@article{COSTA20221810,
title = {Multicriteria analysis by PROMETHEE-SAPEVO-M1 method: choice of Brazilian sugar and ethanol plants for biomethane production},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {10},
pages = {1810-1815},
year = {2022},
note = {10th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.09.661},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322019796},
author = {Wallace L.T. Costa and Igor P.A. Costa and Adilson V. Terra and Miguel Â.L. Moreira and Carlos, F.S. Gomes and Marcos Santos},
keywords = {Multicriteria, PROMETHEE, SAPEVO, Energy, Biomethane},
abstract = {Given the need for cleaner energy sources associated with the ESG (Environmental, social and corporate governance) policy, the work in question is a case study referring to the project to expand biomethane production in national territory, especially for industrial commercialization. By applying the Value Focused Thinking (VFT) methodology, the study initially seeks the approach based on the values of decision-makers, these being three professionals in the energy sector. After the central objective of supporting decision-making, the hybrid method PROMETHEE-SAPEVO-M1 was used, characterized by the possibility of evaluating quantitative data and qualitative variables. To this end, the modeling occurred through the software of the PROMETHEE-SAPEVO-M1 method to clarify the best plants, because of the range of possibilities in the national territory, for project implementation and subsequent production of biomethane for industrial use. As a result, we verified that São Paulo is the best alternative for applying the investment in biomethane production.}
}
@article{2004263,
title = {Computational Statistics and Data Analysis},
journal = {Chemometrics and Intelligent Laboratory Systems},
volume = {73},
number = {2},
pages = {263},
year = {2004},
issn = {0169-7439},
doi = {https://doi.org/10.1016/j.chemolab.2004.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169743904001820}
}
@article{FELLOWS2013541,
title = {Towards fully multivariate algorithmics: Parameter ecology and the deconstruction of computational complexity},
journal = {European Journal of Combinatorics},
volume = {34},
number = {3},
pages = {541-566},
year = {2013},
note = {Combinatorial Algorithms and Complexity},
issn = {0195-6698},
doi = {https://doi.org/10.1016/j.ejc.2012.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0195669812001400},
author = {Michael R. Fellows and Bart M.P. Jansen and Frances Rosamond},
abstract = {The aim of this article is to motivate and describe the parameter ecology program, which studies how different parameters contribute to the difficulty of classical problems. We call for a new type of race in parameterized analysis, with the purpose of uncovering the boundaries of tractability by finding the smallest possible parameterizations which admit FPT-algorithms or polynomial kernels. An extensive overview of recent advances on this front is presented for the Vertex Cover problem. Moving even beyond the parameter ecology program we advocate the principle of model enrichment, which raises the challenge of generalizing positive results to problem definitions with greater modeling power. The computational intractability which inevitably emerges can be deconstructed by introducing additional parameters, leading towards a theory of fully multivariate algorithmics.}
}