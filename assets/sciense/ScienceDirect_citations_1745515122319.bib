@article{SUN2019104141,
title = {Formal system interactive failure analysis method based on systems theoretic process analysis model},
journal = {Engineering Failure Analysis},
volume = {106},
pages = {104141},
year = {2019},
issn = {1350-6307},
doi = {https://doi.org/10.1016/j.engfailanal.2019.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S1350630718314973},
author = {Rui Sun and Deming Zhong and Weigang Li},
keywords = {Engine failures, Aircraft failures, Failure analysis},
abstract = {Interactive failures are failures caused by two or more components that often occur in complex systems when the system is modified, upgraded, or simply designed inadequately. However, the official guideline provided, namely, common cause analysis, cannot discover these problems. It cannot establish a complex interactive system model, nor can it provide a unified analysis method for all parts of the system. Another method, systems theoretic process analysis, is limited to the control system. To solve this problem, a method called system theoretic formal analysis method (STFAM) is proposed in this paper. STFAM establishes a system-component-interactive model that provides an abundance of interactive information for failure analysis and presents a unified model to support the analysis of multiple components in the system. It is divided into three steps. First, a hierarchical system structure is built and then transformed into a formalized state machine. Next, the interactive failures are determined and converted into a linear temporal logic or computation tree logic model. Finally, NuSMV is used to verify the model and record the results. To evaluate the proposed method, a practical problem that occurred in full-authority digital engine control, in which in some cases, the valve closes for unknown reasons until the system is reset is presented. An analysis of the issue demonstrates the effectiveness of our method.}
}
@article{MOHAMED2010317,
title = {Investigating Number Sense Among Students},
journal = {Procedia - Social and Behavioral Sciences},
volume = {8},
pages = {317-324},
year = {2010},
note = {International Conference on Mathematics Education Research 2010 (ICMER 2010)},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2010.12.044},
url = {https://www.sciencedirect.com/science/article/pii/S187704281002149X},
author = {Mohini Mohamed and Jacinta Johnny},
keywords = {Number sense, Mental computation, Number sense test, Number sense framework},
abstract = {Number sense can be described as good intuition about numbers and their relationships. Individuals with good number sense tend to exhibit the following characteristics when performing mental computations; sense-making approach, planning and control, flexibility and appropriateness sense of reasonableness. This is a very important skill to be mastered by every individual to enable them to handle numerical problems in their daily life. Students rarely face problems with algorithms. Unfortunately, many studies have showed that students have poor understanding in making sense on numbers when tested on their competency in number sense component. This study aims to investigate if there is a relationship between student performance in number sense and mathematics achievement and to explore the components of number sense that students are weak in.}
}
@incollection{LACA2019994,
title = {2.68 - Life Cycle Assessment in Biotechnology☆},
editor = {Murray Moo-Young},
booktitle = {Comprehensive Biotechnology (Third Edition)},
publisher = {Pergamon},
edition = {Third Edition},
address = {Oxford},
pages = {994-1006},
year = {2019},
isbn = {978-0-444-64047-5},
doi = {https://doi.org/10.1016/B978-0-444-64046-8.00109-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780444640468001099},
author = {Adriana Laca and Amanda Laca and Mónica Herrero and Mario Díaz},
keywords = {Biofuels, Biopolymers, Biotechnology, Environmental impacts, LCA, Life cycle assessment, Sustainability},
abstract = {The life cycle assessment (LCA) is a technique for assessing environmental impacts in order to identify the key points of a process/product/service as well as for suggesting alternatives to improve its environmental performance. The LCA methodology allows a "cradle to grave" perspective, considering that all the stages involved in the life cycle of a product or activity have a responsibility on the environmental consequences of it. Related terms as Life Cycle Engineering, Social Life Cycle and Life Cycle Thinking are emerging with a wide perspective. In this article, an overview of the main aspects of the used methodology is provided. Applications of LCA in food biotechnology, pharmaceuticals, biopolymers, biofuels and waste management of the biodegradable fractions are reviewed. Not only the utility of LCA to the biotechnological processes but also its main limitations are presented. Current tendencies in the LCA development highlight the need to update tools applicable to different areas with increasing demand of more accurate environmental information. Main footprints commonly used as LCA indicators have been recently combined with the final goal of obtaining a single indicator useful for decision-making. The interest of LCA for product design and the interactions of LCA with other environmental tools are also commented.}
}
@article{STEPHAN2024111077,
title = {EPiC grasshopper: A bottom-up parametric tool to quantify life cycle embodied environmental flows of buildings and infrastructure assets},
journal = {Building and Environment},
volume = {248},
pages = {111077},
year = {2024},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2023.111077},
url = {https://www.sciencedirect.com/science/article/pii/S0360132323011046},
author = {André Stephan and Fabian Prideaux and Robert H. Crawford},
keywords = {Hybrid life cycle assessment, Embodied energy, Embodied carbon, Grasshopper, Buildings, Infrastructure, LCA, Python, Rhinoceros, Design},
abstract = {Reducing the embodied environmental flows of built assets is becoming increasingly important and is a key priority for actors in the built environment to improve life cycle environmental performance. Policies and related targets for embodied environmental flow reductions are emerging. Despite this, tools for quantifying the life cycle embodied environmental flows of built assets are limited in variety and scope. Parametric life cycle assessment (LCA) tools have emerged to address some of these limitations. These tools can enhance decision making, be embedded directly into CAD programs, and offer real-time LCA calculations across multiple design variations. Yet, existing parametric tools for LCA rely on process-based material environmental flow data, limited geometries, limited real-time data visualisation capacity, and often require specialised technical expertise to use. These gaps limit their ability to provide transparent, robust, and rapid assessments. This paper introduces EPiC Grasshopper, an open-source, open-access, bottom-up, parametric tool that enables the quantification of life cycle embodied environmental flows at the early stages of built asset design, bridging the aforementioned gaps. The key characteristics and functionalities of the tool are described, followed by verification (checking that calculations are correct), validation (checking that results are representative of reality), and demonstration of its application to two built asset case studies, i.e. parametrically-defined Australian house and road. The paper shows how the tool can be used to generate designs to meet specific embodied environmental flow targets as well as streamline and increase the uptake of embodied environmental flow assessment and considerations in built asset design workflows.}
}
@article{KHALID2025100900,
title = {Ludogenic innovation: How playing incentivizes technological innovation},
journal = {Entertainment Computing},
volume = {52},
pages = {100900},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100900},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124002684},
author = {Mohd Nor Akmal Khalid},
keywords = {Play, Incentives, Innovation, Games, Artificial intelligence},
abstract = {Play has been an essential element of culture. It comprises forms and parts that intertwine to form complex and meaningful interactions. As an artifact, in the form of games (or video games), it created a massive following and interests, both on personal and organizational levels. An introduction to the term Ludogenic, which blends “ludus” (Latin for play) with “genic” (producing or generating), suggesting the creation or generation of playful and innovative experiences, was proposed as a new paradigm for future technological development in the age of artificial intelligence (AI). This article explores how play and games influence and impact technological advancement and contemporary innovation, eliciting the transformative potential of ludogenic innovation in reshaping technological development, offering insights into future directions for research and application in addressing complex global challenges, and highlighting the enduring impact of play on technological innovation.}
}
@article{CONG2025103131,
title = {Enhancing novel product iteration: An integrated framework for heuristic ideation via interpretable conceptual design knowledge graph},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103131},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103131},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625000242},
author = {Yangfan Cong and Suihuai Yu and Jianjie Chu and Yuexin Huang and Ning Ding and Cong Fang and Stephen Jia Wang},
keywords = {Novel product iteration, Conceptual product design, Design knowledge, Interpretable knowledge graph, Heuristic product ideation},
abstract = {Novel products emerge over time to survive the competitive landscape as no existing product can perpetually satisfy all evolving customer expectations. These products are often characterized by groundbreaking solutions previously unavailable on the market. However, the swift imitation of successful novel products by competitors underscores the need for sustained iteration and continuous improvement. Designers increasingly face challenges in keeping up to date with the growing volume and fragmented nature of design information from diverse sources. While knowledge graphs show promise in structuring and organizing complex design information, their effective application in the ideation process remains limited due to difficulties in automatic knowledge extraction and the lack of interpretability aligned well with designers’ cognitive processes. This study proposes an integrated method to construct an interpretable conceptual design knowledge graph (I-CDKG) that features both inherent and acquired interpretability for heuristic product ideation. First, the schema layer models product design knowledge and governs the semantic connection of design information reinforced by design cognition principles to create a reasonable organizational framework to foster intuitive knowledge exploration. Second, the data layer mainly fulfills automatic and smooth design knowledge extraction for I-CDKG construction through the deep learning ERNIE-BiGRU-CRF model combined with BIESO labeling mode and triple-extracting algorithm. Third, the application layer empowers designers to visually delve into interpretable design knowledge to locate inspiration from cluster, relation, and nest levels and enable constant I-CDKG expansion as design schemes proliferate. A case study on the smart cat litter box demonstrates the feasibility of the proposed methodology. The evaluation results confirm the I-CDKG’s advantages as a productive design tool for inspiring creative, practical, and cost-effective product ideations, thereby empowering the iterative development of competitive novel products.}
}
@article{WANG2016377,
title = {ACP-based social computing and parallel intelligence: Societies 5.0 and beyond},
journal = {CAAI Transactions on Intelligence Technology},
volume = {1},
number = {4},
pages = {377-393},
year = {2016},
issn = {2468-2322},
doi = {https://doi.org/10.1016/j.trit.2016.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S246823221630083X},
author = {Xiao Wang and Lingxi Li and Yong Yuan and Peijun Ye and Fei-Yue Wang},
keywords = {Social computing, Societies 5.0, Parallel intelligence, Knowledge automation, Cyber-physical-social system, Artificial societies, Computational experiments, Parallel execution},
abstract = {Social computing, as the technical foundation of future computational smart societies, has the potential to improve the effectiveness of open-source big data usage, systematically integrate a variety of elements including time, human, resources, scenarios, and organizations in the current cyber-physical-social world, and establish a novel social structure with fair information, equal rights, and a flat configuration. Meanwhile, considering the big modeling gap between the model world and the physical world, the concept of parallel intelligence is introduced. With the help of software-defined everything, parallel intelligence bridges the big modeling gap by means of constructing artificial systems where computational experiments can be implemented to verify social policies, economic strategies, and even military operations. Artificial systems play the role of “social laboratories” in which decisions are computed before they are executed in our physical society. Afterwards, decisions with the expected outputs are executed in parallel in both the artificial and physical systems to interactively sense, compute, evaluate and adjust system behaviors in real-time, leading system behaviors in the physical system converging to those proven to be optimal in the artificial ones. Thus, the smart guidance and management for our society can be achieved.}
}
@article{RODRIGUEZPLANAS2022429,
title = {Gender norms in high school: Impacts on risky behaviors from adolescence to adulthood},
journal = {Journal of Economic Behavior & Organization},
volume = {196},
pages = {429-456},
year = {2022},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2022.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S016726812200021X},
author = {Nuria Rodríguez-Planas and Anna Sanz-de-Galdeano and Anastasia Terskaya},
keywords = {Gender norms, short-, medium- and long-run effects, risky behaviors and labor market outcomes, Add health},
abstract = {Engagement in risky behaviors is traditionally more prevalent among males than females, and the gap increases as youths move from adolescence to adulthood. Using the National Longitudinal Study of Adolescent to Adult Health, we identify a causal effect of exposure to high-school grade-mates with mothers who think that important skills for both boys and girls to possess are traditionally masculine ones (such as to think for oneself or work hard) as opposed to traditionally feminine ones (namely, to be well-behaved, popular, or help others) on the gender gap in teenagers’ engagement in risky behaviors. We find that a higher proportion of grade-mates’ mothers with non-traditional or non-stereotypical gender views who believe that independent thinking and working hard matter for either gender is associated with a reduction of the gender gap in risky behaviors both in the short and medium run. These results are driven by males curbing risky behaviors, suggesting that the relaxation of gender stereotypes results in boys behaving “more like girls”. In the long run, being exposed to grade-mates whose mothers have non-stereotypical gender beliefs reduces the gender gap in labor market outcomes by improving women's performance. This evidence, together with our exploration of several potential mechanisms, suggests that the transmission of gender norms is driving our results.}
}
@incollection{OKEKE2024,
title = {Responsible Consumption and Production},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-443-13701-3.00574-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443137013005740},
author = {Austin Okeke and Juila J. Nobari and Mahtab Morovat and Yashar Salamzadeh},
keywords = {Responsible consumption, Responsible production, SDG goals, Sustainability},
abstract = {This chapter tries to share a clear and easy to understand view of “responsible consumption and production”. The first section of the chapter defines the importance of responsible consumption and responsible production and links it to SDG goals and sustainability. The second section of the chapter shares some main models of responsible consumption and production including the SURESCOM model, circular Business Models, Soft Consumption Decision-Making Models, Behavioral Models of Responsible Consumption and finally, Environmental Collaboration Frameworks. This short and comprehensive review helps both practitioners and researchers to get a comprehensive understanding of this important topic.}
}
@article{UTHAMACUMARAN2020759,
title = {Cancer: A turbulence problem},
journal = {Neoplasia},
volume = {22},
number = {12},
pages = {759-769},
year = {2020},
issn = {1476-5586},
doi = {https://doi.org/10.1016/j.neo.2020.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S1476558620301548},
author = {Abicumaran Uthamacumaran},
keywords = {Cancer, Complexity, Chaos, Nonlinear dynamics, Fractals, Chemical turbulence},
abstract = {Cancers are complex, adaptive ecosystems. They remain the leading cause of disease-related death among children in North America. As we approach computational oncology and Deep Learning Healthcare, our mathematical models of cancer dynamics must be revised. Recent findings support the perspective that cancer-microenvironment interactions may consist of chaotic gene expressions and turbulent protein flows during pattern formation. As such, cancer pattern formation, protein-folding and metastatic invasion are discussed herein as processes driven by chemical turbulence within the framework of complex systems theory. To conclude, cancer stem cells are presented as strange attractors of the Waddington landscape.}
}
@article{PALMIERI2020106074,
title = {Dataset of active avoidance in Wistar-Kyoto and Sprague Dawley rats: Experimental data and reinforcement learning model code and output},
journal = {Data in Brief},
volume = {32},
pages = {106074},
year = {2020},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2020.106074},
url = {https://www.sciencedirect.com/science/article/pii/S2352340920309689},
author = {John Palmieri and Kevin M. Spiegler and Kevin C.H. Pang and Catherine E. Myers},
keywords = {Avoidance learning, Reinforcement learning, Neurosciences, Computational modelling, Computational biology, Strain differences, Wistar Kyoto rat},
abstract = {Data were collected from 40 Wistar-Kyoto (WKY) and 40 Sprague Dawley (SD) rats during an active escape-avoidance experiment. Footshock could be avoided by pressing a lever during a danger period prior to onset of shock. If avoidance did not occur, a series of footshocks was administered, and the rat could press a lever to escape (terminate shocks). For each animal, data were simplified to the presence or absence of lever press and stimuli in each 12-second time frame. Using the pre-processed dataset, a reinforcement learning (RL) model, based on an actor-critic architecture, was utilized to estimate several different model parameters that best characterized each rat's behaviour during the experiment. Once individual model parameters were determined for all 80 rats, behavioural recovery simulations were run using the RL model with each animal's “best-fit” parameters; the simulated behaviour generated avoidance data (percent of trials avoided during a given experimental session) that could be compared across simulated rats, as is customarily done with empirical data. The datasets representing both the experimental data and the model-generated data can be interpreted in various ways to gain further insight into rat behaviour during avoidance and escape learning. Furthermore, the estimated parameters for each individual rat can be compared across groups. Thus, possible between-strain differences in model parameters can be detected, which might provide insights into strain differences in learning. The software implementing the RL model can also be applied to or serve as a template for other experiments involving acquisition learning. Reference for Co-Submission: K.M. Spiegler, J. Palmieri, K.C.H. Pang, C.E. Myers, A reinforcement-learning model of active avoidance behavior: Differences between Sprague-Dawley and Wistar-Kyoto rats. Behav. Brain Res. (2020 Jun 22[epub ahead of print])  doi: 10.1016/j.bbr.2020.112784}
}
@article{HAVENS2020104571,
title = {Automated Water Supply Model (AWSM): Streamlining and standardizing application of a physically based snow model for water resources and reproducible science},
journal = {Computers & Geosciences},
volume = {144},
pages = {104571},
year = {2020},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2020.104571},
url = {https://www.sciencedirect.com/science/article/pii/S0098300420305598},
author = {Scott Havens and Danny Marks and Micah Sandusky and Andrew Hedrick and Micah Johnson and Mark Robertson and Ernesto Trujillo},
keywords = {Hydrology, Computational method, Software engineering, Data assimilation},
abstract = {Reproducible science requires a shift in thinking and application for how data, code and analysis are shared. Now, scientists must act more like software engineers to design models and perform analysis that use principles and techniques pioneered by software developers. Creating reproducible models that are easy to use and understand is in the best interest for the snow and hydrology community, enabling studies by other researchers and facilitating technology transfer to operational applications. Here, we present the Automated Water Supply Model (AWSM) that streamlines and standardizes the workflow of a physically based snow model to create fully reproducible model simulations that can be utilized by researchers and operational water resource managers. AWSM orchestrates four core components that historically required significant, ad-hoc modeler interaction to load the input data, spatially interpolate to the modeling domain, run the models and process the outputs. Because AWSM was developed using principles and techniques from software engineering, users can quickly perform reproducible simulations on any operating system, from a laptop to the cloud. The three fully reproducible example case studies showcase the simplicity and flexibility of using AWSM to perform simulations from small research catchments to simulations that aid in real time water management decisions.}
}
@article{BRIAN2023129074,
title = {Introducing mindset streams to investigate stances towards STEM in high school students and experts},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {626},
pages = {129074},
year = {2023},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2023.129074},
url = {https://www.sciencedirect.com/science/article/pii/S0378437123006295},
author = {Kieran Brian and Massimo Stella},
keywords = {Cognitive modelling, Network science, Cognitive network science, Mindset reconstruction, STEM learning},
abstract = {We introduce mindset streams for assessing ways of bridging two target concepts in concept maps. We focus on behavioural forma mentis networks (BFMN), which map the associative and affective dimensions of memory recalls. Inspired by trains of thoughts taking several paths to link ideas, mindset streams are defined as BFMN subgraphs induced by all shortest paths between two target concepts, e.g. all recalls in shortest paths bridging “math” and “learning”. These streams quantify the following features of the mindset encoded in a BFMN: (i) semantic content (i.e. which ideas mediate connections between targets?), (ii) valence coherence/conflict (i.e. are connections mediated by entwining ideas perceived negatively, positively or neutrally?), and (iii) semantic relevance (i.e. are the bridges between targets peripheral or central for the connectivity/betweenness of the BFMN?). We investigate mindset streams between ‘maths”/“physics” and key motivational aspects of learning (“fun”, “work”, “failure”) in two BFMNs, encoding how 159 students and 59 experts perceived and associated concepts about Science Technology Engineering and Maths (STEM), respectively. Statistical comparisons against configuration models show that high schoolers bridge “maths” and “fun” only through overabundant levels of valence-conflicting associations, contrasting negatively perceived domain knowledge with peer-related positive experiences. This conflict is absent in the researchers’ mindset stream, which rather bridges “math” and “fun” through positive, science-related associations. The mindset streams of both groups bridge “maths” and “physics” to “work” through mostly positive career-related jargon. Students’ mindset streams of “failure” and “math”/“physics” are dominated by negative associations with test anxiety, whereas researchers integrate “failure” and “math”/“physics” in semantically richer and more positive contexts, denoting failure itself as a cornerstone of STEM learning. We discuss our findings and future research directions in view of relevant psychology/education literature.}
}
@article{KASTELLAKIS201519,
title = {Synaptic clustering within dendrites: An emerging theory of memory formation},
journal = {Progress in Neurobiology},
volume = {126},
pages = {19-35},
year = {2015},
issn = {0301-0082},
doi = {https://doi.org/10.1016/j.pneurobio.2014.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0301008214001373},
author = {George Kastellakis and Denise J. Cai and Sara C. Mednick and Alcino J. Silva and Panayiota Poirazi},
keywords = {Plasticity, Active dendrites, Associative memory, Synapse clustering, Synaptic tagging and capture},
abstract = {It is generally accepted that complex memories are stored in distributed representations throughout the brain, however the mechanisms underlying these representations are not understood. Here, we review recent findings regarding the subcellular mechanisms implicated in memory formation, which provide evidence for a dendrite-centered theory of memory. Plasticity-related phenomena which affect synaptic properties, such as synaptic tagging and capture, synaptic clustering, branch strength potentiation and spinogenesis provide the foundation for a model of memory storage that relies heavily on processes operating at the dendrite level. The emerging picture suggests that clusters of functionally related synapses may serve as key computational and memory storage units in the brain. We discuss both experimental evidence and theoretical models that support this hypothesis and explore its advantages for neuronal function.}
}
@incollection{BOTTGE2010767,
title = {Math Instruction for Children with Special Needs},
editor = {Penelope Peterson and Eva Baker and Barry McGaw},
booktitle = {International Encyclopedia of Education (Third Edition)},
publisher = {Elsevier},
edition = {Third Edition},
address = {Oxford},
pages = {767-773},
year = {2010},
isbn = {978-0-08-044894-7},
doi = {https://doi.org/10.1016/B978-0-08-044894-7.01126-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008044894701126X},
author = {B.A. Bottge},
keywords = {Cognitive strategy instruction, Concrete-to-representations-to-abstract (CRA) sequence, Curriculum-based assessment, Enhanced anchored instruction, K-12 math instruction, Learning disabilities, Meta-cognition, Schema-based instruction},
abstract = {Students with learning disabilities in math (denoted MD) display difficulties in developing conceptual understanding of number, in computation, and in formulating correct strategies for solving problems. Often, these students have concomitant reading difficulty, which severely limits their understanding of text-based problems. While explicit instruction of basic computation skills remains important, a greater emphasis is placed on the ability to solve problems, especially as a growing number of students with MD are included in general education classrooms. This article summarizes a small sample and brief descriptions of instructional interventions that hold promise for educating students with MD.}
}
@article{MADORE2022707,
title = {Readiness to remember: predicting variability in episodic memory},
journal = {Trends in Cognitive Sciences},
volume = {26},
number = {8},
pages = {707-723},
year = {2022},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2022.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S1364661322001127},
author = {Kevin P. Madore and Anthony D. Wagner},
keywords = {episodic retrieval, attention lapsing, goal processing, arousal, locus coeruleus, posterior alpha},
abstract = {Learning and remembering are fundamental to our lives, so what causes us to forget? Answers often highlight preparatory processes that precede learning, as well as mnemonic processes during the act of encoding or retrieval. Importantly, evidence now indicates that preparatory processes that precede retrieval attempts also have powerful influences on memory success or failure. Here, we review recent work from neuroimaging, electroencephalography, pupillometry, and behavioral science to propose an integrative framework of retrieval-period dynamics that explains variance in remembering in the moment and across individuals as a function of interactions among preparatory attention, goal coding, and mnemonic processes. Extending this approach, we consider how a ‘readiness to remember’ (R2R) framework explains variance in high-level functions of memory and mnemonic disruptions in aging.}
}
@article{ZHU2020706,
title = {Cognitive-inspired Computing: Advances and Novel Applications},
journal = {Future Generation Computer Systems},
volume = {109},
pages = {706-709},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X20308384},
author = {Rongbo Zhu and Lu Liu and Maode Ma and Hongxiang Li},
keywords = {Cognitive-inspired computing, Systems, Intelligent health analysis, Security and privacy, Novel applications},
abstract = {Cognition is emerging as a new and promising methodology with the development of cognitive-inspired computing and interaction systems, which enables a large class of applications and has emerged with a significance to change our life. However, recent advances on artificial intelligence (AI), edge computing, big data, and cognitive computational theory show that multidisciplinary cognitive-inspired computing still struggles with fundamental, long-standing problems, such as computational models and decision-making mechanisms based on the neurobiological processes of the brain, cognitive sciences, and psychology. How to enhance human cognitive performance with machine learning (ML), common sense, intelligent interaction, privacy security and novel applications is worth exploring. The objective of this special issue is to report high-quality state-of-the-art research contributions that address these key aspects of cognitive-inspired computing and novel applications. By presenting a selection of papers on various topics related to cognitive-inspired computing and applications, we hope to shed light on the multiple aspects of this emerging multidisciplinary paradigm. The papers included in this issue propose solutions for cognitive-inspired systems, AI-assisted computing, intelligent health analysis, security and privacy issues, as well as novel applications.}
}
@article{ZHEN2025103,
title = {A stochastic programming model for designing bus bridging services under metro disruptions},
journal = {Transportation Letters},
volume = {17},
number = {1},
pages = {103-118},
year = {2025},
issn = {1942-7867},
doi = {https://doi.org/10.1080/19427867.2024.2327811},
url = {https://www.sciencedirect.com/science/article/pii/S1942786724000146},
author = {Lu Zhen and Xueqin Du and Haolin Li and Zanyang Wu},
keywords = {Urban metro system, bus bridging, schedule, passenger assignment, stochastic programming, tabu search algorithm},
abstract = {ABSTRACT
With the growing reliance on urban metro networks, any accidental disruption can lead to rapid degradation and significant economic losses. Bus bridging services are common and efficient ways to minimize such adverse impacts. In this study, we investigate the problem of designing bus bridging services in response to unexpected metro disruptions, and propose a routing strategy with multiple bridging routes. In particular, to respond to uncertain factors such as passenger arrivals and bus travel times in the disruption environment, we develop a two-stage stochastic programming model for the collaborative optimization of bus bridging routes, schedules, and passenger assignments. To solve the computational challenges arising with the proposed model, a tailored tabu search algorithm is developed. Finally, several sets of numerical experiments are conducted and experimental results reveal that our proposed routing strategy can effectively improve the service level for the affected passengers during metro disruptions.}
}
@article{VELICHKOVSKY201735,
title = {Consciousness and working memory: Current trends and research perspectives},
journal = {Consciousness and Cognition},
volume = {55},
pages = {35-45},
year = {2017},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2017.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S1053810017301654},
author = {Boris B. Velichkovsky},
keywords = {Consciousness, Working memory, Visual masking, Attentional blink, Implicit working memory},
abstract = {Working memory has long been thought to be closely related to consciousness. However, recent empirical studies show that unconscious content may be maintained within working memory and that complex cognitive computations may be performed on-line. This promotes research on the exact relationships between consciousness and working memory. Current evidence for working memory being a conscious as well as an unconscious process is reviewed. Consciousness is shown to be considered a subset of working memory by major current theories of working memory. Evidence for unconscious elements in working memory is shown to come from visual masking and attentional blink paradigms, and from the studies of implicit working memory. It is concluded that more research is needed to explicate the relationship between consciousness and working memory. Future research directions regarding the relationship between consciousness and working memory are discussed.}
}
@article{COONEY1992237,
title = {The influence of verbal protocol methods on children's mental computation},
journal = {Learning and Individual Differences},
volume = {4},
number = {3},
pages = {237-257},
year = {1992},
issn = {1041-6080},
doi = {https://doi.org/10.1016/1041-6080(92)90004-X},
url = {https://www.sciencedirect.com/science/article/pii/104160809290004X},
author = {John B. Cooney and Stephen F. Ladd},
abstract = {The purpose of this study was to investigate the validity of children's verbal reports about the cognitive processes underlying their mental arithmetic. A within-subject comparison was made with respect to the data that could be obtained with retrospective verbal report, concurrent verbal report, and no verbal report conditions. The results of the investigation indicated that children's verbal reports of strategy use may not be veridical. The source of the nonveridicality was incompleteness rather than fabrication. It was also found that immediately retrospective and concurrent verbal reports increased students' solution accuracy relative to a no verbal report condition. Thus, the primary mental operations underlying children's mental arithmetic are reactive to giving verbal reports. It was concluded that empirical checks for reactivity and refinements to protocol procedures to reveal the progression of strategy use are needed in future research.}
}
@incollection{GILLAM199523,
title = {Chapter 2 - The Perception of Spatial Layout from Static Optical Information},
editor = {William Epstein and Sheena Rogers},
booktitle = {Perception of Space and Motion},
publisher = {Academic Press},
address = {San Diego},
pages = {23-67},
year = {1995},
series = {Handbook of Perception and Cognition},
isbn = {978-0-12-240530-3},
doi = {https://doi.org/10.1016/B978-012240530-3/50004-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780122405303500043},
author = {Barbara Gillam},
abstract = {Publisher Summary
This chapter reviews the literature on absolute distance, relative distance, surface slant and curvature, and the perception of size and shape within the context of several broad issues that have influenced thinking and experimentation to varying degrees in recent years. One issue that has driven recent research is the way stimulus input is described that carries implicit assumptions about how it is encoded and represented. Euclidian and other conventional frameworks may be restricting and misleading as a basis for visual theory. Another issue raised by computational approaches is the relationship between the processing of different sources of information or cues underlying the perception of spatial layout. Machine vision has tended to treat these cues as separate modules or processing systems, a view that has also received support from psychophysics. Comparison of some seemingly separate processes, specifically perspective and stereopsis, may indicate common mechanisms.}
}
@article{ZHANG2023205,
title = {A survey for solving mixed integer programming via machine learning},
journal = {Neurocomputing},
volume = {519},
pages = {205-217},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222014035},
author = {Jiayi Zhang and Chang Liu and Xijun Li and Hui-Ling Zhen and Mingxuan Yuan and Yawen Li and Junchi Yan},
keywords = {Mixed integer programming, Machine learning, Combinatorial optimization},
abstract = {Machine learning (ML) has been recently introduced to solving optimization problems, especially for combinatorial optimization (CO) tasks. In this paper, we survey the trend of leveraging ML to solve the mixed-integer programming problem (MIP). Theoretically, MIP is an NP-hard problem, and most CO problems can be formulated as MIP. Like other CO problems, the human-designed heuristic algorithms for MIP rely on good initial solutions and cost a lot of computational resources. Therefore, researchers consider applying machine learning methods to solve MIP since ML-enhanced approaches can provide the solution based on the typical patterns from the training data. Specifically, we first introduce the formulation and preliminaries of MIP and representative traditional solvers. Then, we show the integration of machine learning and MIP with detailed discussions on related learning-based methods, which can be further classified into exact and heuristic algorithms. Finally, we propose the outlook for learning-based MIP solvers, the direction toward more combinatorial optimization problems beyond MIP, and the mutual embrace of traditional solvers and ML components. We maintain a list of papers that utilize machine learning technologies to solve combinatorial optimization problems, which is available at https://github.com/Thinklab-SJTU/awesome-ml4co.}
}
@article{YURKOVICH2018130,
title = {Quantitative -omic data empowers bottom-up systems biology},
journal = {Current Opinion in Biotechnology},
volume = {51},
pages = {130-136},
year = {2018},
note = {Systems biology • Nanobiotechnology},
issn = {0958-1669},
doi = {https://doi.org/10.1016/j.copbio.2018.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0958166917302276},
author = {James T Yurkovich and Bernhard O Palsson},
abstract = {The large-scale generation of ‘-omic’ data holds the potential to increase and deepen our understanding of biological phenomena, but the ability to synthesize information and extract knowledge from these data sets still represents a significant challenge. Bottom-up systems biology overcomes this hurdle through the integration of disparate -omic data types, and absolutely quantified experimental measurements allow for direct integration into quantitative, mechanistic models. The human red blood cell has served as a starting point for the application of systems biology approaches and has been the focus of a recent burst of generated quantitative metabolomics and proteomics data. Thus, the red blood cell represents the perfect case study through which to examine our ability to glean knowledge from the integration of multiple disparate data types.}
}
@article{CHEN199799,
title = {Towards designing sustainable urban wastewater infrastructures: A screening analysis},
journal = {Water Science and Technology},
volume = {35},
number = {9},
pages = {99-112},
year = {1997},
note = {Sustainable Sanitation},
issn = {0273-1223},
doi = {https://doi.org/10.1016/S0273-1223(97)00188-1},
url = {https://www.sciencedirect.com/science/article/pii/S0273122397001881},
author = {J. Chen and M.B. Beck},
keywords = {Sustainability, wastewater treatment technologies, screening analysis, uncertainty, urban drainage system},
abstract = {Whether sustainability can, or should, be defined in a practical operational sense, it is clear that the emergence of such a notion has prompted what seems to be a profound re-thinking of whether our society, economic system, and technology are as we would wish them to be. Sustainable development, clean technology, life-cycle analysis, pollution prevention, and so on, are expressions of a willingness to leave no stone unturned, as it were, in the search for what would be appropriate. With respect to the design and operation of a city's wastewater infrastructure, in particular, this search is characterised by a seeming explosion in the possible combinations of appropriate technologies, gross uncertainty about how novel technologies - only now emerging - might perform in the very long term, and a continuing absence of specific criteria of sustainability for determining the grounds on which any candidate technology might be preferred over another. The paper introduces a simple computational procedure for generating and screening candidate combinations of unit-process technologies for an urban wastewater infrastructure. This is based on the use of Monte Carlo simulation, with the identification of those specific technologies (and combinations thereof) that appear to have the greatest probability of being selected for use under different, possibly evolving, criteria of sustainability. Application of the procedure is illustrated with respect to just a part of this infrastructure, i.e., the wastewater treatment plant.}
}
@article{PLEBE20164,
title = {What is ‘wrong’ in a neural model},
journal = {Cognitive Systems Research},
volume = {39},
pages = {4-14},
year = {2016},
note = {From human to artificial cognition (and back): new perspectives of cognitively inspired AI systems},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2015.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S1389041716000085},
author = {Alessio Plebe},
keywords = {Moral cognition, Neural computation, Orbitofrontal cortex, Amygdala, Self-organization},
abstract = {Neural computation has an influential role in the study of human capacities and behaviors. It has been the dominant approach in the vision science of the last half century, and it is currently one of the fundamental methods of investigation for most higher cognitive functions. Yet, neurocomputational approaches to moral behavior are lacking. Computational modeling in general has been scarcely pursued in morality, and existent non-neural attempts have failed to account for the mental processes involved in morality. In this paper we argue that recently the situation has evolved in a way that subverted the insufficient knowledge on the basic organization of moral cognition in brain circuits, making the project of modeling morality in neurocomputational terms feasible. We will present an original architecture that combines reinforcement learning and Hebbian learning, aimed at simulating forms of moral behavior in a simple artificial context. The relationship between language and morality is controversial. In the analytic tradition of philosophy, morality is essentially the language of morals. On the other side, current cognitive ethology has shown how non human species display behaviors that are surprisingly similar to those prescribed by human ethics. Nevertheless, morality in humans is deeply entrenched with language, and the semantics of words like ‘wrong’ resists consensual explanations. The model here proposed includes an auditory processing pathway, with the purpose of showing how the coding of “wrong”, even if highly simplified with respect to its rich content in natural language, can emerge in the course of moral learning.}
}
@incollection{RAHI2025597,
title = {Chapter 56 - Crowdsourcing and artificial intelligence based modeling framework for effective Public Healthcare Informatics and Smart eHealth System},
editor = {M.A. Ansari and R.S. Anand and Pragati Tripathi and Rajat Mehrotra and Md Belal Bin Heyat},
booktitle = {Artificial Intelligence in Biomedical and Modern Healthcare Informatics},
publisher = {Academic Press},
pages = {597-608},
year = {2025},
isbn = {978-0-443-21870-5},
doi = {https://doi.org/10.1016/B978-0-443-21870-5.00056-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044321870500056X},
author = {Pankaj Rahi and Monika Dandotiya and Santi Basa and Souvik Sen and Mayur D. Jakhete and P. Vijayakumar},
keywords = {Artificial intelligence, Crowdsourcing, IoT, Machine learning, Public health informatics, Smart eHealth system},
abstract = {With the help of a multiagent interactive healthcare plan, healthy and independent aging is possible. Testing one's fitness and keeping tabs on one's health can both benefit from an awareness of one's regular routine. The Healthcare Strategies Partnership is introduced here. Geographic and economic factors, including the prevalence of infectious tropical diseases and an increasing number of chronic illnesses, have contributed to a shift in the region's medical requirements. This takes place in a world that is not only difficult but also intricate. This system employs a smartphone's sensor, a machine learning algorithm, multiple agents (including a doctor, fitness instructor, guardian, and intelligent ranker agent), and a smartphone itself to increase a user's sense of independence. A group of health professionals collaborate to assess the patient's day-to-day activities and offer suggestions for improvement. The algorithm figures out a typical day in the life of an adult. A smart autonomous agent using crowdsourced data recommends the best possible treatment plan. In contrast to crowdsourcing, which places value on the abilities of people to generate, aggregate, or filter original data, automatic tools make use of information retrieval techniques to analyze publicly available information. Crowdsourcing, which facilitates collaboration among numerous individuals, is increasingly being used in a variety of industries. Methodical crowdsourcing of useful data and human computation of interchangeable knowledge will aid future advancements in public health informatics. The disease burden on any country can and will be decreased by these efforts the share of healthcare costs borne by individuals and their families. The work also aids in achieving the most crucial objectives along the path to the Sustainable Development Goals. To improve public health surveillance for the prevention and control of communicable and noncommunicable diseases, this chapter presents the fundamental modeling for its design and development. So that a smart autonomous agent can recommend the best course of treatment, lowering the disease burden in any country. Crowdsourcing is defined, and how it can be used to better public health, in this chapter of a book. They are better able to achieve a healthy lifestyle exit, and the statistics and indicators that are tracked primarily under the Sustainable Development Goals Framework are improved as a result. The goal of this chapter is to provide a high-level overview of recent developments in applying artificial intelligence techniques to public health surveillance and response, which is the single most important step toward preventing the spread of disease and saving the lives of humans and other sentient beings.}
}
@article{OLIVEIRA202575,
title = {Quantitative and qualitative analysis in urban morphology: systematic legacy and latest developments},
journal = {Proceedings of the Institution of Civil Engineers - Urban Design and Planning},
volume = {178},
number = {2},
pages = {75-87},
year = {2025},
issn = {1755-0793},
doi = {https://doi.org/10.1680/jurdp.24.00047},
url = {https://www.sciencedirect.com/science/article/pii/S1755079325000058},
author = {Vitor Oliveira and Sergio Porta},
keywords = {built environment, quantitative versus qualitative, town & city planning, urban form, urban morphology},
abstract = {Urban morphology studies the physical forms of human settlements and how these change over time by the action of different processes and agents. The field of knowledge has developed several theories, concepts, and methods to describe and explain the phenomena at hands. As in many fields, urban morphology contains a few misconceptions. One of these is the idea that quantitative analysis is a feature of the present and the future, and qualitative analysis of the past. The paper addresses this fallacy. Our discussion of the main schools of thought in urban morphology and their influential researchers suggests that quantitative approaches are well rooted in it since at least the mid-twentieth century and that the dominance of quantitative or qualitative tools is subject to cycles, as it happens in other sciences. Demonstration of both statements leads to a focus on a line of approaches, historico-geographical, configurational, and lately morphometrics, which share a common interest in cross-cases regularities, hence practices of pattern recognition.}
}
@article{SUTTON2004503,
title = {Representation, levels, and context in integrational linguistics and distributed cognition},
journal = {Language Sciences},
volume = {26},
number = {6},
pages = {503-524},
year = {2004},
note = {Distributed cognition and integrational linguistics},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2004.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0388000104000452},
author = {John Sutton},
keywords = {Integrational linguistics, Distributed cognition, Mental representation, Reduction, Context, Memory},
abstract = {Distributed Cognition and Integrational Linguistics have much in common. Both approaches see communicative activity and intelligent behaviour in general as strongly context-dependent and action-oriented, and brains as permeated by history. But there is some tension between the two frameworks on three important issues. The majority of theorists of distributed cognition want to maintain some notions of mental representation and computation, and to seek generalizations and patterns in the various ways in which creatures like us couple with technologies, media, and other agents; many also want to offer explanations at subpersonal levels which may undercut the autonomy of personal-level accounts. In contrast, dominant views in integrational linguistics reject all invocation of representation, resist the explanatory search for similarity across contexts and moments, and see linguistics as a lay discipline which should not offer explanations in terms alien to ordinary agents. On each of these issues, I argue that integrationists could move closer to the distributed cognition framework without losing the most important aspects of their view: integrationist criticisms of mainstream or classical theories can be respected while alliances with revised cognitivist views about representation, context, and explanation are developed.}
}
@article{WOODWARD2006631,
title = {Does prior mathematics knowledge really lead to variation in elementary statistics performance? Evidence from a developing country},
journal = {International Journal of Educational Development},
volume = {26},
number = {6},
pages = {631-639},
year = {2006},
issn = {0738-0593},
doi = {https://doi.org/10.1016/j.ijedudev.2006.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0738059306000071},
author = {George Woodward and Don Galagedera},
keywords = {Curriculum, Mathematics, Educational policy, Elementary statistics},
abstract = {A model incorporating prerequisite mathematics performance and other variables deemed to be associated with learning elementary statistics (ES) is developed. The relationship between ES performance and the explanatory variables is well represented by the logistics form. Aptitude, effort and motivation are the only significant explanatory variables of ES performance. Since prerequisite mathematics is not significant, statistical thinking at the tertiary level may be mostly intuitive and non-mathematical. Students with low aptitude experience increasing returns to effort over the first half of the feasible effort interval, while high-aptitude students experience diminishing returns at all levels of effort. The levels of effort required to achieve a minimum pass are interpreted.}
}
@article{SOHAIL201947,
title = {A videographic assessment of ferrofluid during magnetic drug targeting: An application of artificial intelligence in nanomedicine},
journal = {Journal of Molecular Liquids},
volume = {285},
pages = {47-57},
year = {2019},
issn = {0167-7322},
doi = {https://doi.org/10.1016/j.molliq.2019.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S0167732219315399},
author = {Ayesha Sohail and Maryam Fatima and Rahamt Ellahi and Khush Bakhat Akram},
keywords = {Ferrofluids, Drug targeting, Artificial intelligence, Videographic footage},
abstract = {Forecasting the thresholds via the computational analysis of magnetic drug targeting, is a useful approach since it can help to design the nanoscale experiments to get the best results and efficiency. In such investigations, an artificial intelligence when interlinked with the computational techniques provide better insight specially for rheological problems. In the proposed model mathematical framework for the magnetic drug targeting is adopted while the flow of the ferrofluid, with different concentrations is taken into account. The flow without any obstruction is compared with the flow having obstruction. The nanoscale dynamics sensitive to such obstructions are documented by videographic footage. Nanaoscale approach and the response of the nanomedicine relative to external agents are used. The pressure gradient, the magnetic susceptibility and the velocity profile of the ferrofluid provides useful thresholds to identify the geometry of the obstacle, and to forecast the resulting dynamics.}
}
@article{WANG2023101451,
title = {Unpacking the essential tension of knowledge recombination: Analyzing the impact of knowledge spanning on citation impact and disruptive innovation},
journal = {Journal of Informetrics},
volume = {17},
number = {4},
pages = {101451},
year = {2023},
issn = {1751-1577},
doi = {https://doi.org/10.1016/j.joi.2023.101451},
url = {https://www.sciencedirect.com/science/article/pii/S1751157723000767},
author = {Cheng-Jun Wang and Lihan Yan and Haochuan Cui},
keywords = {Knowledge Recombination, Category spanning, Disruption, Citation, Team size},
abstract = {Drawing on the theories of knowledge recombination, we aim to unpack the essential tension between tradition and innovation in scientific research. Using the American Physical Society data and computational methods, we analyze the relationship between knowledge spanning, citation impact, and disruptive innovation. The findings show that there exists a U-shaped relationship between knowledge spanning and disruptive innovation. In contrast, there is an inverted U-shaped relationship between knowledge spanning and citation impact, and the inverted U-shaped effect is moderated by team size. This study contributes to the theories of knowledge recombination by suggesting that either intellectual conformism or knowledge recombination can lead to disruptive innovation. That is, when evaluating the quality of scientific research with disruptive innovation, the essential tension seems to disappear.}
}
@article{DANNENHAUER2014226,
title = {Toward Meta-level Control of Autonomous Agents},
journal = {Procedia Computer Science},
volume = {41},
pages = {226-232},
year = {2014},
note = {5th Annual International Conference on Biologically Inspired Cognitive Architectures, 2014 BICA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.11.107},
url = {https://www.sciencedirect.com/science/article/pii/S187705091401552X},
author = {Dustin Dannenhauer and Michael T. Cox and Shubham Gupta and Matt Paisner and Don Perlis},
keywords = {Computational metacognition, cognitive architecture, metareasoning, long-duration autonomy},
abstract = {Metareasoning is an important capability for autonomous systems, particularly for those being deployed on long duration missions. An agent with increased self-observation and the ability to control itself in response to changing environments will be more capable in achieving its goals. This is essential for long-duration missions where system designers will not be able to, theoretically or practically, predict all possible problems that the agent may encounter. In this paper we describe preliminary work that integrates the metacognitive architecture MIDCA with an autonomous TREX agent, creating a more self-observable and adaptive agent.}
}
@article{OITAVEM2011661,
title = {A recursion-theoretic approach to NP},
journal = {Annals of Pure and Applied Logic},
volume = {162},
number = {8},
pages = {661-666},
year = {2011},
issn = {0168-0072},
doi = {https://doi.org/10.1016/j.apal.2011.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S016800721100011X},
author = {I. Oitavem},
keywords = {Computational complexity, Implicit characterization, Recursion schemes, NP},
abstract = {An implicit characterization of the class NP is given, without using any minimization scheme. This is the first purely recursion-theoretic formulation of NP.}
}
@article{MA2019367,
title = {An efficient method to compute different types of generalized inverses based on linear transformation},
journal = {Applied Mathematics and Computation},
volume = {349},
pages = {367-380},
year = {2019},
issn = {0096-3003},
doi = {https://doi.org/10.1016/j.amc.2018.12.064},
url = {https://www.sciencedirect.com/science/article/pii/S0096300318311251},
author = {Jie Ma and Feng Gao and Yongshu Li},
keywords = {Generalized inverse, Linear transformation, Rational matrix, MATHEMATICA},
abstract = {In this paper, we present functional definitions of all types of generalized inverses related to the {1}-inverse, which is a continuation of the work of Campbell and Meyer (2009). According to these functional definitions, we further derive novel representations for all types of generalized inverses related to the {1}-inverse in terms of the bases for R(A*), N(A) and N(A*). Based on these representations, we present the corresponding algorithm for computing various generalized inverses related to the {1}-inverse of a matrix and analyze the computational complexity of our algorithm for a constant matrix. Finally, we implement our algorithm and several known algorithms for symbolic computation of the Moore-–Penrose inverse in the symbolic computational package MATHEMATICA and compare their running times. Numerical experiments show that our algorithm outperforms these known algorithms when applied to compute the Moore–Penrose inverse of one-variable rational matrices, but is not the best choice for two-variable rational matrices in practice.}
}
@article{KWON2024488,
title = {On knowing a gene: A distributional hypothesis of gene function},
journal = {Cell Systems},
volume = {15},
number = {6},
pages = {488-496},
year = {2024},
issn = {2405-4712},
doi = {https://doi.org/10.1016/j.cels.2024.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S2405471224001236},
author = {Jason J. Kwon and Joshua Pan and Guadalupe Gonzalez and William C. Hahn and Marinka Zitnik},
keywords = {lexical semantics, gene function, machine learning, artificial intelligence, distributed representations, word embeddings, large language models, transformers},
abstract = {Summary
As words can have multiple meanings that depend on sentence context, genes can have various functions that depend on the surrounding biological system. This pleiotropic nature of gene function is limited by ontologies, which annotate gene functions without considering biological contexts. We contend that the gene function problem in genetics may be informed by recent technological leaps in natural language processing, in which representations of word semantics can be automatically learned from diverse language contexts. In contrast to efforts to model semantics as “is-a” relationships in the 1990s, modern distributional semantics represents words as vectors in a learned semantic space and fuels current advances in transformer-based models such as large language models and generative pre-trained transformers. A similar shift in thinking of gene functions as distributions over cellular contexts may enable a similar breakthrough in data-driven learning from large biological datasets to inform gene function.}
}
@article{LATHA2019122052,
title = {Analysing exposure diversity in collaborative recommender systems—Entropy fusion approach},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {533},
pages = {122052},
year = {2019},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2019.122052},
url = {https://www.sciencedirect.com/science/article/pii/S0378437119311963},
author = {R. Latha and R. Nadarajan},
keywords = {Clustering, Quadratic entropy, Exposure diversity, Novelty, Concordance},
abstract = {Recommender Systems are considered as essential business tools to leverage the potential growth of on-line services. Neighbourhood based collaborative filtering, a successful recommendation approach has mainly focused on improving accuracy of predictions. From user point of view, it is more valuable to obtain novel and diverse recommendations rather than monotonic preferences. Ratings given by a user for different categories of items are considered as a tool to access user exposure diversity which signifies his creative and divergent thinking. On the other hand, pair of items is concordant if highly correlated users agree in rating the items. Based on the user exposure diversity and item concordance, the neighbourhood selection process of item based collaborative recommender systems is refined. Rating predictions are made based on the newly selected neighbours. The performance of the proposed approach is investigated for accuracy and diversity of predictions on Movielens data sets. The results demonstrate that the proposed approach outperforms the state of the art recommendation approaches which address accuracy–diversity trade off. Statistical analysis is done to prove the efficiency of the proposed approach.}
}
@article{FINOTTO201385,
title = {Hybrid fuzzy-genetic system for optimising cabled-truss structures},
journal = {Advances in Engineering Software},
volume = {62-63},
pages = {85-96},
year = {2013},
note = {Special Issue dedicated to Professor Zden ek Bittnar on the occasion of his Seventieth Birthday: Part I},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2013.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S0965997813000513},
author = {V.C. Finotto and W.R.L. {da Silva} and M. Valášek and P. Štemberk},
keywords = {Hybrid system, Structural optimisation, Cabled-truss, Fuzzy logic, Genetic algorithm, Nonlinear finite element analysis},
abstract = {This paper demonstrates an application of a hybrid fuzzy-genetic system in the optimisation of lightweight cabled-truss structures. These structures are described as a system of cables and triangular bar formations jointed at their ends by hinged connections to form a rigid framework. The optimised lightweight structure is determined through a stochastic discrete topology and sizing optimisation procedure that uses ground structure approach, nonlinear finite element analysis, genetic algorithm, and fuzzy logic. The latter is used to include expertise into the evolutionary search with the aim of filtering individuals with low survival possibility, thereby decreasing the total number of evaluations. This is desired because cables, which are inherently nonlinear elements, demand the use of iterative procedures for computing the structural response. Such procedures are computationally costly since the stiffness matrix is evaluated in each iteration until the structure is in equilibrium. Initially, the proposed system is applied to truss benchmarks. Next, the use of cables is investigated and the system’s performance is compared against genetic algorithms. The results indicate that the hybrid system considerably decreased the number of evaluations over genetic algorithms. Also, cabled-trusses showed a significant improvement in structural mass minimisation when compared with trusses.}
}
@article{LI2024101038,
title = {One-stop multi-sensor fusion and multimodal precise quantified traditional Chinese medicine imaging health examination technology},
journal = {Journal of Radiation Research and Applied Sciences},
volume = {17},
number = {4},
pages = {101038},
year = {2024},
issn = {1687-8507},
doi = {https://doi.org/10.1016/j.jrras.2024.101038},
url = {https://www.sciencedirect.com/science/article/pii/S168785072400222X},
author = {Chuanxue Li and Ping Wang and Meifang Zheng and Wenxiang Li and Jun Zhou and Lin Fu},
keywords = {Traditional Chinese medicine imaging, Large language model, Knowledge graphs, Multimodal imaging, Health examination technology, Image fusion, Imaging agent, Deep learning},
abstract = {Except for single-mode traditional Chinese medicine imaging techniques such as infrared thermal imaging, the one-stop multimodal whole-body imaging health examination technology and device is still blank. We focus on infrared thermal imaging as the main modality, integrated various modalities of medical imaging intelligent sensing agents such as terahertz imaging. The upper and lower computer and virtual instrument architecture are used, and the imaging data are collected by the lower computers that each is an intelligent sensing agent. The upper computer is used for image reconstruction with intelligent algorithms. Based on the core theory of traditional Chinese medicine, intelligent fusion imaging is achieved through various modalities to achieve the ‘observation, hearing, questioning, and palpation’ four diagnostic integration. We use fractional Fourier transform to filter imaging data, Laplacian pyramid for image fusion. We have proposed an implementation method and process for combining traditional Chinese medicine imaging large language model with knowledge graph, and based on deep learning, we have studied the image and report generation algorithm that combines traditional Chinese medicine pathology and four diagnostic methods with knowledge graph fusion, as well as the traditional Chinese medicine human physiological and pathological interpretation and evaluation system. We have achieved some results, and through further research and development, we can achieve commercial applications.}
}
@article{PFEIFER199547,
title = {Cognition — perspectives from autonomous agents},
journal = {Robotics and Autonomous Systems},
volume = {15},
number = {1},
pages = {47-70},
year = {1995},
note = {The Biology and Technology of Intelligent Autonomous Agents},
issn = {0921-8890},
doi = {https://doi.org/10.1016/0921-8890(95)00014-7},
url = {https://www.sciencedirect.com/science/article/pii/0921889095000147},
author = {Rolf Pfeifer},
keywords = {Cognition, Autonomous agents, Cheap designs, “New AI”},
abstract = {The predominant paradigm in cognitive science has been the cognitivistic one, exemplified by the “Physical Symbol Systems Hypothesis”. The cognitivistic approach generated hopes that one would soon understand human thinking — hopes that up till now have still not been fulfilled. It is well-known that the cognitivistic approach, in spite of some early successes, has turned out to be fraught with problems. Examples are the frame problem, the symbol grounding problem, and the problems of interacting with a real physical world. In order to come to grips with the problems of cognitivism the study of embodied autonomous systems has been proposed, for example by Rodney Brooks. Brooks' robots can get away with no or very little representation. However, this approach has often been criticized because of the limited abilities of the agents. If they are to perform more intelligent tasks they will need to be equipped with representations or cognition — is an often heard argument. We will illustrate that we are well-advised not to introduce representational or cognitive concepts too quickly. As long as we do not understand the basic relationships between simple architectures and behavior, i.e. as long as we do not understand the dynamics of the system-environment interaction, it is premature to spend our time with speculations about potentially useful architectures for so-called high-level processes. This paper has a tutorial and a review aspect. In addition to presenting our own research, we will review some of the pertinent literature in order to make it usable as an introduction to “New AI”.}
}
@article{DAI2025100019,
title = {Why students use or not use generative AI: Student conceptions, concerns, and implications for engineering education},
journal = {Digital Engineering},
volume = {4},
pages = {100019},
year = {2025},
issn = {2950-550X},
doi = {https://doi.org/10.1016/j.dte.2024.100019},
url = {https://www.sciencedirect.com/science/article/pii/S2950550X24000190},
author = {Yun Dai},
keywords = {Artificial intelligence, generative AI, engineering education, student concern, barrier, technology integration, higher education},
abstract = {Generative artificial intelligence (GenAI) technologies are believed to transform engineering education. However, it remains underexamined how engineering students choose to use GenAI or not, along with the reasons behind their choices. To fill this research gap, this study presents a natural experiment that examines student use or non-use of GenAI tools in engineering design tasks in an undergraduate course. In this experiment, the participants (n = 403) were provided with unconstrained access to a GPT 4.0-empowered chatbot and were allowed to use it for their design projects voluntarily. Overall, 59.80 % of the students reported substantial use of GenAI in their design projects, and 40.20 % showed limited or no use. Those adopters used GenAI to aid idea generation and brainstorming, mediate discussions with instructors/TA, overcome non-technical expertise gaps, and optimize their design solutions. Conversely, non-adopters attributed their reluctance and rejection to inherent limitations in GenAI outputs, misalignment between GenAI functionalities and project needs, a lack of adaptation and prompt skills, and unclear benefits of GenAI use for personal growth. This study challenges the popular assumption of naturally active GenAI adoption among university students. It identifies three major factors—task characteristics, decision-maker characteristics, and context characteristics—that shape students' adoption of and interaction with GenAI. The findings highlight the need of establishing a consensus across various stakeholders (i.e., students, instructors, curriculum developers, policymakers, and others), while calling for adaptation and evidence-based decision-making in integrating GenAI tools into engineering education.}
}
@article{OUYANG2024e29176,
title = {Unmasking the challenges in ideological and political education in China: A thematic review},
journal = {Heliyon},
volume = {10},
number = {8},
pages = {e29176},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e29176},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024052071},
author = {Sha Ouyang and Wei Zhang and Jian Xu and Abdullah {Mat Rashid} and Shwu Pyng How and Aminuddin {Bin Hassan}},
keywords = {Ideological and political education, China, Thematic review},
abstract = {China's distinctive educational approach, particularly its emphasis on ideological and political education, has garnered considerable academic attention for its impact on shaping individual values, fostering citizenship, and maintaining social stability. Despite the Chinese government's prioritization of ideological and political education, academic research in this field appears constrained, with existing studies predominantly focusing on normative and descriptive aspects. Normative research delineates how ideological and political education should be executed, while descriptive research illustrates its practical implementation. The effectiveness of these approaches is significantly diminished if they are not adequately interconnected—when only the current reality is explained without providing tools for improvement or when prescribed steps for improvement lack a basis in specific contexts. This paper conducts a comprehensive review of research on ideological and political education using ATLAS. ti 9 for thematic analysis. The review aims to unveil the intricate landscape of current research in China and address key questions: What are the primary trends in the literature on ideological and political education between 2021 and July 2023? What challenges does ideological and political education face? Through a direct exploration of these issues, this paper seeks to optimize the ideological and political education system, elevate its adaptability and effectiveness, and open avenues for research, fostering a more dynamic, inclusive, and resilient development of ideological and political education.}
}
@incollection{CELKO2008255,
title = {Chapter 13 - Turning Specifications into Code},
editor = {Joe Celko},
booktitle = {Joe Celko's Thinking in Sets},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {255-271},
year = {2008},
series = {The Morgan Kaufmann Series in Data Management Systems},
isbn = {978-0-12-374137-0},
doi = {https://doi.org/10.1016/B978-012374137-0.50014-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780123741370500146},
author = {Joe Celko},
abstract = {Publisher Summary
This chapter delineates the importance of unlearning the procedural thinking and moves to a pure SQL view. Programmers tend to make the same kinds of errors in their designs and their code over and over. They confuse RDBMS with the file systems and 3GL- or OO-oriented programming environments they first learned. Programmers from the C family of languages tend to put the entire program in lowercase as if they were still using a teletype on a UNIX system. Mainframe programmers tend to put the entire program in uppercase as if they were still using punch cards or a 3270 video monitor for input. Cohesion is how well a module of code does one and only one thing, that it is logically coherent. There are several types of cohesion. The original definitions have been extended from procedural code to include OO and class hierarchies. The symptom in DDL is a table with lots of NULL-able columns. It is probably two or more entities crammed into a single table. The symptom in DML is a query or other statement that tries to do too many things. When the same procedure or query checks inventory and build a personnel report, cohesion problems crop up. The table-valued function shows that the programmer still wants to see procedural coding complete with parameters. An SQL programmer would think in terms of VIEWS and CTES.}
}
@article{GARIRA2023122,
title = {The transmission mechanism theory of disease dynamics: Its aims, assumptions and limitations},
journal = {Infectious Disease Modelling},
volume = {8},
number = {1},
pages = {122-144},
year = {2023},
issn = {2468-0427},
doi = {https://doi.org/10.1016/j.idm.2022.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S2468042722001075},
author = {Winston Garira and Bothwell Maregere},
keywords = {Single scale modelling of infectious disease dynamics, Multiscale modelling of infectious disease dynamics, Scales of organization of infectious disease system, Transmission mechanism theory of disease dynamics, Levels of organization of infectious disease system, The replication-transmission relativity theory of disease dynamics},
abstract = {Most of the progress in the development of single scale mathematical and computational models for the study of infectious disease dynamics which now span over a century is build on a body of knowledge that has been developed to address particular single scale descriptions of infectious disease dynamics based on understanding disease transmission process. Although this single scale understanding of infectious disease dynamics is now founded on a body of knowledge with a long history, dating back to over a century now, that knowledge has not yet been formalized into a scientific theory. In this article, we formalize this accumulated body of knowledge into a scientific theory called the transmission mechanism theory of disease dynamics which states that at every scale of organization of an infectious disease system, disease dynamics is determined by transmission as the main dynamic disease process. Therefore, the transmission mechanism theory of disease dynamics can be seen as formalizing knowledge that has been inherent in the study of infectious disease dynamics using single scale mathematical and computational models for over a century now. The objective of this article is to summarize this existing knowledge about single scale modelling of infectious dynamics by means of a scientific theory called the transmission mechanism theory of disease dynamics and highlight its aims, assumptions and limitations.}
}
@article{LUCKRING2023100950,
title = {Model validation hierarchies for connecting system design to modeling and simulation capabilities},
journal = {Progress in Aerospace Sciences},
volume = {142},
pages = {100950},
year = {2023},
issn = {0376-0421},
doi = {https://doi.org/10.1016/j.paerosci.2023.100950},
url = {https://www.sciencedirect.com/science/article/pii/S0376042123000660},
author = {James M. Luckring and Scott Shaw and William L. Oberkampf and Rick E. Graves},
keywords = {Modeling and simulation, Validation hierarchy, Systems architecture, Physics taxonomy, Surface-to-air missile defense system},
abstract = {Hierarchical structures provide a means to systematically deconstruct an engineering system of arbitrary complexity into its subsystems, components, and physical processes. Model validation hierarchies can aid in understanding the coupling and interaction of subsystems and components, as well as improve the understanding of how simulation models are used to design and optimize the engineering system of interest. The upper tiers of the hierarchy address systems and subsystems architecture decompositions, while the lower tiers address physical processes that are both coupled and uncoupled. Recent work connects these two general sections of the hierarchy through a transition tier, which blends the focus of system functionality and physics modeling activities. This work also includes a general methodology for how a model validation hierarchy can be constructed for any type of engineering system in any operating environment, e.g., land, air, sea, or space. We review previous work on the construction and use of model validation hierarchies in not only the field of aerospace systems, but also from commercial nuclear power plant systems. Then an example of a detailed model validation hierarchy is constructed and discussed for a surface-to-air missile defense system with an emphasis on the missile subsystems.}
}
@incollection{PASCAL2022231,
title = {10 - A practical guide to paleostress analysis},
editor = {Christophe Pascal},
booktitle = {Paleostress Inversion Techniques},
publisher = {Elsevier},
pages = {231-245},
year = {2022},
isbn = {978-0-12-811910-5},
doi = {https://doi.org/10.1016/B978-0-12-811910-5.00008-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128119105000087},
author = {Christophe Pascal},
keywords = {Fieldwork, Measuring, Processing, Plotting, Interpretation, Reporting},
abstract = {After having presented extensively different theoretical and methodological aspects of paleostress reconstruction methods, the purpose of the final chapter is to introduce recommendations for their practical use in tectonic problems. The discussion focuses on paleostress inversion of fault slip data, the latter being both the most elaborated and the most employed method. Detailed practical advice is given to conduct a paleostress study efficiently, starting with data acquisition in the field, proceeding with subsequent computation of paleostress tensors and ending with the reporting of the results and of their subsequent interpretations.}
}
@article{ABDELHAMID2023101986,
title = {Discovering epistasis interactions in Alzheimer’s disease using integrated framework of ensemble learning and multifactor dimensionality reduction (MDR)},
journal = {Ain Shams Engineering Journal},
volume = {14},
number = {7},
pages = {101986},
year = {2023},
issn = {2090-4479},
doi = {https://doi.org/10.1016/j.asej.2022.101986},
url = {https://www.sciencedirect.com/science/article/pii/S2090447922002970},
author = {Marwa M. {Abd El Hamid} and Mohamed Shaheen and Yasser M.K. Omar and Mai S. Mabrouk},
keywords = {Epistasis Interactions, Alzheimer's disease, Personalized Medicine, Ensemble learning techniques},
abstract = {Alzheimer's disease (AD) is a complex disorder with strong genetic factors. The proposed framework is applied to Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. We present a novel framework integrating ensemble learning and MDR constructive induction algorithm to discover epistasis interactions associated with AD in a computationally efficient method. Discovering epistasis interactions is a big challenge and significantly impacts personalized medicine (PM). The applied ensemble learning algorithms are random forests (RF) with Gini index and permutation importance, Extreme Gradient Boosting (XGBoost), and classification and regression trees (CART). The classification accuracy of 5-way models varied between (0.8674–0.8758), whereas the accuracy of 2-way, 3-way, and 4-way models varied between (0.6515–0.6649), (0.7071–0.7170), and (0.7811–0.7878) respectively. The promising results of this proposed framework show high-ranked risk genes and up to 5-way epistasis models that contribute to the disease risk efficiently and at higher accuracy.}
}
@article{BEHESHTIANARDEKANI1988183,
title = {An empirical study of the use of business expert systems},
journal = {Information & Management},
volume = {15},
number = {4},
pages = {183-190},
year = {1988},
issn = {0378-7206},
doi = {https://doi.org/10.1016/0378-7206(88)90044-4},
url = {https://www.sciencedirect.com/science/article/pii/0378720688900444},
author = {Mehdi Beheshtian-Ardekani and Linda M. Salchenberger},
keywords = {Business expert systems, Artificial intelligence, Knowledge-based systems, Fifth-generation},
abstract = {The evolution of computers from computational tools to “thinking machines” is causing businesses to evaluate their views of the computer's role. The inevitable availability of smart computers leads to questions of how and when fifth generation hardware and software will be integrated into corporate culture. Here, we present the results of a survey given to information systems managers to determine the extent of expert systems development by data processing departments and expert systems usage in organizations. The attitudes of management toward the future of expert systems are also discussed using the survey data. It was discovered that, while computer managers are receptive toward this new tool, most have no definite plans to develop expert systems in the near future. These results seem to be in conflict with other evidence about the growing numbers of expert systems in business applications. One explanation is that this new technology is part of the continuing “grass roots” movement of end-user computing.}
}
@article{HUANG2025105310,
title = {Modeling student teachers’ self-regulated learning of complex professional knowledge: A sequential and clustering analysis with think-aloud protocols},
journal = {Computers & Education},
volume = {233},
pages = {105310},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2025.105310},
url = {https://www.sciencedirect.com/science/article/pii/S0360131525000788},
author = {Lingyun Huang and Ying Zhan and Shen Ba},
keywords = {Self-regulated learning, Technological Pedagogical Content Knowledge, Sequential clustering analysis, Tthink-aloud protocols},
abstract = {There has been much discussion regarding the positive relationship between self-regulated learning (SRL) and technological pedagogical content knowledge (TPACK) development for student teachers. This study continued this claim and adopted advanced analytical methods to explain how SRL influences TPACK learning. Think-aloud protocols from 39 participants were collected and transcribed when they were learning TPACK by designing technology-infused lessons with nBrowser, a computer-based learning environment. Based on models, nine critical SRL events were retrieved from participants‘ think-aloud protocols and analyzed through sequential clustering analysis. The results show two SRL groups indicating distinct self-regulatory sequential patterns. One group had a shorter sequence length and dominantly enacted elaboration activities (Low-SRL group), while the other had longer sequence lengths and engaged in diverse SRL activities (High-regulation group). Relating to TPACK performance indicated by the quality of lesson plans, the results reveal that the participants in the High-SRL group outperformed their counterparts in the Low-SRL group. The findings are consistent with previous evidence and provide implications for practitioners about the importance of student teachers’ self-regulation trajectories.}
}
@incollection{GHIORSO2015143,
title = {Chapter 6 - Chemical Thermodynamics and the Study of Magmas},
editor = {Haraldur Sigurdsson},
booktitle = {The Encyclopedia of Volcanoes (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Amsterdam},
pages = {143-161},
year = {2015},
isbn = {978-0-12-385938-9},
doi = {https://doi.org/10.1016/B978-0-12-385938-9.00006-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780123859389000067},
author = {Mark S. Ghiorso and Guilherme A.R. Gualda},
keywords = {Chemical thermodynamics, Crystallization, Geobarometers, Geothermometers, Heat transfer, Magma chambers, Minimization, Open system, Phase equilibria, Thermodynamic potential},
abstract = {This chapter gives a brief overview of the application of chemical thermodynamics to magmatic systems. Topics covered include thermodynamic potentials and their application to various magma chamber evolution scenarios, including open systems and boundary conditions dictated by external controls on heat transfer and chamber volume. In addition, methods of finding the global minima of thermodynamic potentials are discussed along with algorithms for stable phase detection. The chapter ends with a summary of available tools of computational chemical thermodynamics that implement (1) geothermometers, geobarometers, and geohygrometers, (2) mineral and melt thermodynamic properties, (3) solubility calculators for volatiles in magmatic systems, and (4) computation of phase equilibria.}
}
@article{ALEXOPOULOS20241466,
title = {AJGP Solicits Papers Aimed to Enrich Geriatric Psychiatry},
journal = {The American Journal of Geriatric Psychiatry},
volume = {32},
number = {12},
pages = {1466-1468},
year = {2024},
issn = {1064-7481},
doi = {https://doi.org/10.1016/j.jagp.2024.08.018},
url = {https://www.sciencedirect.com/science/article/pii/S1064748124004482},
author = {George S. Alexopoulos}
}
@article{BEVEN2025106431,
title = {On the future of hydroecological models of everywhere},
journal = {Environmental Modelling & Software},
volume = {188},
pages = {106431},
year = {2025},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2025.106431},
url = {https://www.sciencedirect.com/science/article/pii/S136481522500115X},
author = {Keith Beven},
keywords = {Digital twins, Learning about places, Stakeholder communication},
abstract = {This paper addresses the potential for hydroecological models of everywhere to be used, in conjunction with interaction with local stakeholders, as a way of learning about places as well as being used as predictive tools. The importance of facilitating stakeholder involvement in defining assumptions and uncertainties, and in model evaluation is stressed. The potential for using data science and real-time updating in using the internet of things to contribute to a models of everywhere framework is also discussed.}
}
@article{MARKOVSKY202142,
title = {Behavioral systems theory in data-driven analysis, signal processing, and control},
journal = {Annual Reviews in Control},
volume = {52},
pages = {42-64},
year = {2021},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2021.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S1367578821000754},
author = {Ivan Markovsky and Florian Dörfler},
keywords = {Behavioral systems theory, Data-driven control, Missing data estimation, System identification},
abstract = {The behavioral approach to systems theory, put forward 40 years ago by Jan C. Willems, takes a representation-free perspective of a dynamical system as a set of trajectories. Till recently, it was an unorthodox niche of research but has gained renewed interest for the newly emerged data-driven paradigm, for which it is uniquely suited due to the representation-free perspective paired with recently developed computational methods. A result derived in the behavioral setting that became known as the fundamental lemma started a new class of subspace-type data-driven methods. The fundamental lemma gives conditions for a non-parametric representation of a linear time-invariant system by the image of a Hankel matrix constructed from raw time series data. This paper reviews the fundamental lemma, its generalizations, and related data-driven analysis, signal processing, and control methods. A prototypical signal processing problem, reviewed in the paper, is missing data estimation. It includes simulation, state estimation, and output tracking control as special cases. The direct data-driven control methods using the fundamental lemma and the non-parametric representation are loosely classified as implicit and explicit approaches. Representative examples are data-enabled predictive control (an implicit method) and data-driven linear quadratic regulation (an explicit method). These methods are equally amenable to certainty-equivalence as well as to robust control. Emphasis is put on the robustness of the methods under noise. The methods allow for theoretical certification, they are computationally tractable, in comparison with machine learning methods require small amount of data, and are robustly implementable in real-time on complex physical systems.}
}
@incollection{CAPLETTE2017905,
title = {Chapter 36 - The Time Course of Object, Scene, and Face Categorization},
editor = {Henri Cohen and Claire Lefebvre},
booktitle = {Handbook of Categorization in Cognitive Science (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {San Diego},
pages = {905-930},
year = {2017},
isbn = {978-0-08-101107-2},
doi = {https://doi.org/10.1016/B978-0-08-101107-2.00036-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780081011072000361},
author = {Laurent Caplette and Éric McCabe and Caroline Blais and Frédéric Gosselin},
keywords = {Categorization, attention, vision, temporal processing, object recognition, scene recognition, face recognition},
abstract = {We first describe Strategy Length & Internal Practicability (SLIP), a formal model for thinking about categorization, in particular about the time course of categorization. We then discuss an early application of this model to basic-levelness. We then turn to aspects of the time course of categorization that have been neglected in the categorization literature: our limited processing capacities; the necessity of having a flexible categorization apparatus; and the paradox that this inexorably brings about. We propose a twofold resolution of this paradox, attempting, in the process, to bridge work done on categorization in vision, neuropsychology, and physiology.}
}
@article{MICHIE19931,
title = {Turing's test and conscious thought},
journal = {Artificial Intelligence},
volume = {60},
number = {1},
pages = {1-22},
year = {1993},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(93)90032-7},
url = {https://www.sciencedirect.com/science/article/pii/0004370293900327},
author = {Donald Michie},
abstract = {Over forty years ago A.M. Turing proposed a test for intelligence in machines. Based as it is solely on an examinee's verbal responses, the Test misses some important components of human thinking. To bring these manifestations within its scope, the Turing Test would require substantial extension. Advances in the application of AI methods in the design of improved human-computer interfaces are now focussing attention on machine models of thought and knowledge from the altered standpoint of practical utility.}
}
@article{DING2025102351,
title = {Meta-analysis examining the relationship between framing effect and risky decisions},
journal = {Journal of Behavioral and Experimental Economics},
volume = {116},
pages = {102351},
year = {2025},
issn = {2214-8043},
doi = {https://doi.org/10.1016/j.socec.2025.102351},
url = {https://www.sciencedirect.com/science/article/pii/S2214804325000187},
author = {Xiaoqian Ding and Menghan Li and Junyi Qiao},
keywords = {Framing effect, Risky decision, Meta-analysis, Moderating analysis, Age, Problem domains, Culture},
abstract = {This study employed a meta-analysis to investigate the relationship between the framing effect and risky decisions. A systematic searched was conducted for relevant literature published in 12 electronic databases: Google Scholar, ProQuest Dissertations, Springer, Web of Science, PubMed, EBSCO, Elsevier SDOL, Chongqing VIP Information Co., WANFANG DATA, Chinese Selected Doctoral Dissertations and Master's Theses Full-Text Databases, and the China National Knowledge Infrastructure. A total of 40 relevant studies were identified, comprising a sample of 17,416 participants. The analysis employing the random-effects model revealed a statistically significant main effect of the framing effect on risky decisions (OR = 2.467). The moderator effect analysis revealed that problem domains and age served as moderating factors in the relationship between risky decisions and the framing effect, respectively. Culture, however, did not exert a moderating influence on the framing effect or risky decision-making. Specifically, individuals exhibited heightened susceptibility to the framing effect when making risky decisions in the problem domain of life-death, as compared to the problem domains of study and money. Adolescents, in contrast, were more vulnerable to the framing effect in making risky decisions than adulthood.}
}
@article{MULDNER2015127,
title = {Utilizing sensor data to model students’ creativity in a digital environment},
journal = {Computers in Human Behavior},
volume = {42},
pages = {127-137},
year = {2015},
note = {Digital Creativity: New Frontier for Research and Practice},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2013.10.060},
url = {https://www.sciencedirect.com/science/article/pii/S074756321300410X},
author = {Kasia Muldner and Winslow Burleson},
keywords = {Creativity, Student modeling, Eye tracking, EEG, Skin conductance, Intelligent Tutoring Systems},
abstract = {While creativity is essential for developing students’ broad expertise in Science, Technology, Engineering, and Math (STEM) fields, many students struggle with various aspects of being creative. Digital technologies have the unique opportunity to support the creative process by (1) recognizing elements of students’ creativity, such as when creativity is lacking (modeling step), and (2) providing tailored scaffolding based on that information (intervention step). However, to date little work exists on either of these aspects. Here, we focus on the modeling step. Specifically, we explore the utility of various sensing devices, including an eye tracker, a skin conductance bracelet, and an EEG sensor, for modeling creativity during an educational activity, namely geometry proof generation. We found reliable differences in sensor features characterizing low vs. high creativity students. We then applied machine learning to build classifiers that achieved good accuracy in distinguishing these two student groups, providing evidence that sensor features are valuable for modeling creativity.}
}
@article{TOKAC2023101220,
title = {A programming grammar for robotic fabrication: Incorporating material agency into clay textures},
journal = {Design Studies},
volume = {88},
pages = {101220},
year = {2023},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2023.101220},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X23000613},
author = {Iremnur Tokac and Herman Bruyninckx and Andrew Vande Moere},
keywords = {human–computer interaction, computational model(s), design technology, reflective practice, making grammars},
abstract = {Material agency describes how material affordances and constraints have the inherent capacity to suggest formal transformations. Digital fabrication typically excludes material agency because it requires the final form is digitally modelled before it can be fabricated. To enrich the fabrication design space with material agency, we introduce 1) a programming grammar that relates the sensing of material states with the transformation of fabrication actions via explicit rule notations; 2) a grammatical compiler that translates these rule notations into a responsive robot executable program; 3) a set of critical reflections on how this grammar enhances the fabrication design space with material agency. Consequently, our contributions broaden digital fabrication to produce intricate material forms that cannot be simulated by geometrical definitions.}
}
@article{ACHARJYA2022100647,
title = {A rough set, formal concept analysis and SEM-PLS integrated approach towards sustainable wearable computing in the adoption of smartwatch},
journal = {Sustainable Computing: Informatics and Systems},
volume = {33},
pages = {100647},
year = {2022},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2021.100647},
url = {https://www.sciencedirect.com/science/article/pii/S221053792100130X},
author = {D.P. Acharjya and Gladys Gnana {Kiruba B}},
keywords = {Rough set, Wearable computing, Path diagram, Composite reliability, Convergence, Discriminant validity},
abstract = {The rapid growth of sustainable computing towards the energy, power and environment seize an immense attention from bigger organizations to an individual life. Besides the world is advancing towards the digital mode and smartwatch is gaining its popularity because of additional importance to improve lifestyle. Moreover, it is not restricted to only time viewer rather paves a way in user's daily life. Therefore, it is highly cardinal in identifying the factors among consumers influencing the adoption of smartwatch in sustainable wearable computing. Traditional data modelling tools limited to technology acceptance model is used to this end. After all the study deals with user's behaviour that includes uncertainties and thus studying such problems using computational intelligence techniques is pivotal. In this research work we hybridize rough set, partial least square, and formal concept analysis to study smartwatch users adoption in wearable computing. Initially, the reliability and validity of the proposed model is analysed using structural equation modelling along with partial least square. Further, decision rules are generated using the rough set. Finally, important factors affecting the user's behavioural adoption towards sustainable wearable computing is discovered using formal concept analysis.}
}
@article{CHOGA202491,
title = {Rapid dynamic changes of FL.2 variant: A case report of COVID-19 breakthrough infection},
journal = {International Journal of Infectious Diseases},
volume = {138},
pages = {91-96},
year = {2024},
issn = {1201-9712},
doi = {https://doi.org/10.1016/j.ijid.2023.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S1201971223007725},
author = {Wonderful T. Choga and Gobuiwang Khilly {Kurusa (Gasenna)} and James Emmanuel San and Tidimalo Ookame and Irene Gobe and Mohammed Chand and Badisa Phafane and Kedumetse Seru and Patience Matshosi and Boitumelo Zuze and Nokuthula Ndlovu and Teko Matsuru and Dorcas Maruapula and Ontlametse T. Bareng and Kutlo Macheke and Lesego Kuate-Lere and Labapotswe Tlale and Onalethata Lesetedi and Modiri Tau and Mpaphi B. Mbulawa and Pamela Smith-Lawrence and Mogomotsi Matshaba and Roger Shapiro and Joseph Makhema and Darren P. Martin and Tulio {de Oliveira} and Richard J. Lessells and Shahin Lockman and Simani Gaseitsiwe and Sikhulile Moyo},
keywords = {SARS-CoV-2, Evolution, FL.2, Immunocompromised, Botswana},
abstract = {We investigated intra-host genetic evolution using two SARS-CoV-2 isolates from a fully vaccinated (primary schedule x2 doses of AstraZeneca plus a booster of Pfizer), >70-year-old woman with a history of lymphoma and hypertension who presented a SARS-CoV-2 infection for 3 weeks prior to death due to COVID-19. Two full genome sequences were determined from samples taken 13 days apart with both belonging to Pango lineage FL.2: the first detection of this Omicron sub-variant in Botswana. FL.2 is a sub-lineage of XBB.1.9.1. The repertoire of mutations and minority variants in the Spike protein differed between the two time points. Notably, we also observed deletions within the ORF1a and Membrane proteins; both regions are associated with high T-cell epitope density. The internal milieu of immune-suppressed individuals may accelerate SARS-CoV-2 evolution; hence, close monitoring is warranted.}
}
@article{WANG2024121777,
title = {Deep learning-based flatness prediction via multivariate industrial data for steel strip during tandem cold rolling},
journal = {Expert Systems with Applications},
volume = {237},
pages = {121777},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121777},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423022790},
author = {Qinglong Wang and Jie Sun and Yunjian Hu and Wenqiang Jiang and Xinchun Zhang and Zhangqi Wang},
keywords = {Strip flatness, Tandem cold rolling, Deep learning, Multi-variate prediction, Industrial data},
abstract = {Flatness deviations in the tandem cold-rolling process of steel strips have a direct impact on product quality and shape, leading to strip breakage, reduced working speed, and equipment damage. However, conventional physics-based numerical models are inadequate for accurately predicting flatness in the complex operating conditions and variables of tandem rolling environments. To address this challenge, a novel approach is proposed that utilizes deep convolutional neural networks (DCNNs) based on real industrial data from tandem cold rolling. The multi-input and multi-output architecture of our DCNNs enables them to solve the multi-level nonlinear problem associated with flatness prediction in the tandem cold-rolling process. The flatness profiles are effectively predicted using the proposed method, incorporating multiple variables without requiring additional data pre-processing methods. Additionally, the effects of network width, depth, and topology on flatness prediction performance are thoroughly investigated. The developed Inception-ResNet demonstrates remarkable predictive performance while using fewer model parameters and exhibiting lower computational complexity compared to other network architectures. Specifically, the proposed Inception-ResNet-39 model, consisting of 39 layers of learnable parameters, achieves state-of-the-art predictive performance. Our deep learning-based approach accurately predicts flatness in tandem cold-rolling through end-to-end modeling and provides complete pipelines for model transfer construction to ensure efficient implementation.}
}
@incollection{GEWEKE20013463,
title = {Chapter 56 - Computationally Intensive Methods for Integration in Econometrics**The authors gratefully acknowledge financial support from National Science Foundation grants SBR-9511280, SBR-9731037, SES-9814342, and SES-9819444.},
editor = {James J. Heckman and Edward Leamer},
series = {Handbook of Econometrics},
publisher = {Elsevier},
volume = {5},
pages = {3463-3568},
year = {2001},
issn = {1573-4412},
doi = {https://doi.org/10.1016/S1573-4412(01)05009-7},
url = {https://www.sciencedirect.com/science/article/pii/S1573441201050097},
author = {John Geweke and Michael Keane},
keywords = {Bayesian inference, discrete choice, dynamic optimization, integration, Markov chain Monte Carlo, multinomial probit, normal mixtures, selection models, Primary C15, Secondary C11},
abstract = {Until recently, inference in many interesting models was precluded by the requirement of high dimensional integration. But dramatic increases in computer speed, and the recent development of new algorithms that permit accurate Monte Carlo evaluation of high dimensional integrals, have greatly expanded the range of models that can be considered. This chapter presents the methodology for several of the most important Monte Carlo methods, supplemented by a set of concrete examples that show how the methods are used. Some of the examples are new to the econometrics literature. They include inference in multinomial discrete choice models and selection models in which the standard normality assumption is relaxed in favor of a multivariate mixture of normals assumption. Several Monte Carlo experiments indicate that these methods are successful at identifying departures from normality when they are present. Throughout the chapter the focus is on inference in parametric models that permit rich variation in the distribution of disturbances. The chapter first discusses Monte Carlo methods for the evaluation of high dimensional integrals, including integral simulators like the GHK method, and Markov Chain Monte Carlo methods like Gibbs sampling and the Metropolis–Hastings algorithm. It then turns to methods for approximating solutions to discrete choice dynamic optimization problems, including the methods developed by Keane and Wolpin, and Rust, as well as methods for circumventing the integration problem entirely, such as the approach of Geweke and Keane. The rest of the chapter deals with specific examples: classical simulation estimation for multinomial probit models, both in the cross sectional and panel data contexts; univariate and multivariate latent linear models; and Bayesian inference in dynamic discrete choice models in which the future component of the value function is replaced by a flexible polynomial.}
}
@article{ZHANG2022108760,
title = {Causal discovery and inference-based fault detection and diagnosis method for heating, ventilation and air conditioning systems},
journal = {Building and Environment},
volume = {212},
pages = {108760},
year = {2022},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2022.108760},
url = {https://www.sciencedirect.com/science/article/pii/S0360132322000099},
author = {Chaobo Zhang and Yazhou Zhao and Yang Zhao and Tingting Li and Xuejun Zhang},
keywords = {Fault detection and diagnosis, Heating, Ventilation and air conditioning systems, Building energy conservation, Causal discovery and inference, Individual average causal effect estimation, Backward structural causal model},
abstract = {Data driven-based methods have aroused wide attention in the domain of fault detection and diagnosis of heating, ventilation and air conditioning systems. However, they are good at learning statistical relationships between faults and symptoms rather than their causal relationships, resulting in poor interpretability. This paper proposes a causal discovery and inference-based fault detection and diagnosis method to address this challenge. It applies a do-calculus-based individual average causal effect estimation approach to reveal causal relationships between faults and symptoms. Based on the causal relationships discovered, a backward structural causal model is developed for fault detection and diagnosis. Regression coefficients in the model are visualized using heatmaps to explain the model reasoning processes. The method is validated using the experimental data collected by the ASHARE project RP-1312. The individual average causal effect estimation approach reveals the causal relationships between eleven air handing unit faults and twenty-eight symptoms successfully. The diagnosis accuracy of the backward structural causal model (99.58%) is almost as high as that of k-nearest neighbors, support vector machine, classification and regression trees, deep neural networks and convolutional neural networks. Its hyper-parameter optimization time is reduced by 81.64% and 99.91%, respectively, compared with deep neural networks and convolutional neural networks. And its model training time is reduced by 38.61% and 92.01%, respectively. Based on the heatmap of regression coefficients of the model, it is demonstrated that the decision-making processes of the model are understandable and consistent with the domain knowledge in most cases.}
}
@article{KAPITANYFOVENY202466,
title = {EEG based depression detection by machine learning: Does inner or overt speech condition provide better biomarkers when using emotion words as experimental cues?},
journal = {Journal of Psychiatric Research},
volume = {178},
pages = {66-76},
year = {2024},
issn = {0022-3956},
doi = {https://doi.org/10.1016/j.jpsychires.2024.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0022395624004515},
author = {Máté Kapitány-Fövény and Mihály Vetró and Gábor Révy and Dániel Fabó and Danuta Szirmai and Gábor Hullám},
keywords = {EEG, Depression, Machine learning, Diagnostic approach},
abstract = {Background
Objective diagnostic approaches need to be tested to enhance the efficacy of depression detection. Non-invasive EEG-based identification represents a promising area.
Aims
The present EEG study addresses two central questions: 1) whether inner or overt speech condition result in higher diagnositc accuracy of depression detection; and 2) does the affective nature of the presented emotion words count in such diagnostic approach.
Methods
A matched case-control sample consisting of 10 depressed subjects and 10 healthy controls was assessed. An EEG headcap containing 64 electrodes measured neural responses to experimental cues presented in the form of 15 different words that belonged to three emotional categories: neutral, positive, and negative. 120 experimental cues was presented for every participant, each containing an “inner speech” and an “overt speech” segment. An EEGNet neural network was utilized.
Results
The highest diagnostic accuracy of the EEGNet model was observed in the case of the overt speech condition (i.e. 69.5%), while a an overall subject-wise accuracy of 80% was achieved by the model. Only a negligible difference in diagnostic accuracy could be found between aggregated emotion word categories, with the highest accuracy (i.e. 70.2%) associated with the presentation of positive emotion words. Model decision was primarily influenced by electrodes representing the regions of the left parietal, the left temporal lobe and the middle frontal areas.
Conclusions
While the generalizability of our results is limited by the small sample size and potentially uncontrolled confounders, depression was associated with sensitive and presumably network-like aspects of these brain areas, potentially implying a higher level of emotion regulation that increases primarily in open communication.}
}
@article{TARAPOULOUZI2022123410,
title = {Heavy metals detection at chemometrics-powered electrochemical (bio)sensors},
journal = {Talanta},
volume = {244},
pages = {123410},
year = {2022},
issn = {0039-9140},
doi = {https://doi.org/10.1016/j.talanta.2022.123410},
url = {https://www.sciencedirect.com/science/article/pii/S0039914022002065},
author = {Maria Tarapoulouzi and Vincenzo Ortone and Stefano Cinti},
keywords = {Multivariate analysis, Design of experiment, Artificial intelligence, Electroanalysis, Sensors, Heavy metals},
abstract = {Heavy metals represent a serious issue regarding both environmental and health status. Their monitoring is necessary and it is necessary the development of decentralized approaches that are able to enforce the risk assessment. Electrochemical sensors and biosensors, with the various architectures, represent a solid reality often involved for this type of analytical determination. Although these approaches offer easy-to-use and portable tools, some limitations are often highlighted in presence of multi-targets and/or real matrices. However, chemometrics- and artificial intelligence-based tools, both for designing and for data analyzing, display the capability in producing novel functionality towards the management of complex matrices which often contain more information than those that are visualized with sensor detection. Design of experiment, exploratory, predictive and regression analysis can push the world of electrochemical (bio)sensors beyond the state of the art, because is still too large the number of analytical chemists that do not deal with multivariate thinking. In this paper, the use of multivariate methods applied to electrochemical sensing of heavy metals is showed, and each approach is described in terms of efficacy and outputs.}
}
@article{YANG2022849,
title = {Creative problem solving in knowledge-rich contexts},
journal = {Trends in Cognitive Sciences},
volume = {26},
number = {10},
pages = {849-859},
year = {2022},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2022.06.012},
url = {https://www.sciencedirect.com/science/article/pii/S1364661322001565},
author = {Wenjing Yang and Adam E. Green and Qunlin Chen and Yoed N. Kenett and Jiangzhou Sun and Dongtao Wei and Jiang Qiu},
keywords = {creativity, creative problem solving, knowledge, analogy, transfer},
abstract = {Creative problem solving (CPS) in real-world contexts often relies on reorganization of existing knowledge to serve new, problem-relevant functions. However, classic creativity paradigms that minimize knowledge content are generally used to investigate creativity, including CPS. We argue that CPS research should expand consideration of knowledge-rich problem contexts, both in novices and experts within specific domains. In particular, paradigms focusing on creative analogical transfer of knowledge may reflect CPS skills that are applicable to real-world problem solving. Such paradigms have begun to provide process-level insights into cognitive and neural characteristics of knowledge-rich CPS and point to multiple avenues for fruitfully expanding inquiry into the role of crystalized knowledge in creativity.}
}
@article{RAJPAL2022119624,
title = {Psychedelics and schizophrenia: Distinct alterations to Bayesian inference},
journal = {NeuroImage},
volume = {263},
pages = {119624},
year = {2022},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2022.119624},
url = {https://www.sciencedirect.com/science/article/pii/S105381192200739X},
author = {Hardik Rajpal and Pedro A.M. Mediano and Fernando E. Rosas and Christopher B. Timmermann and Stefan Brugger and Suresh Muthukumaraswamy and Anil K. Seth and Daniel Bor and Robin L. Carhart-Harris and Henrik J. Jensen},
keywords = {Psychedelics, Schizophrenia, Information theory, Predictive processing},
abstract = {Schizophrenia and states induced by certain psychotomimetic drugs may share some physiological and phenomenological properties, but they differ in fundamental ways: one is a crippling chronic mental disease, while the others are temporary, pharmacologically-induced states presently being explored as treatments for mental illnesses. Building towards a deeper understanding of these different alterations of normal consciousness, here we compare the changes in neural dynamics induced by LSD and ketamine (in healthy volunteers) against those associated with schizophrenia, as observed in resting-state M/EEG recordings. While both conditions exhibit increased neural signal diversity, our findings reveal that this is accompanied by an increased transfer entropy from the front to the back of the brain in schizophrenia, versus an overall reduction under the two drugs. Furthermore, we show that these effects can be reproduced via different alterations of standard Bayesian inference applied on a computational model based on the predictive processing framework. In particular, the effects observed under the drugs are modelled as a reduction of the precision of the priors, while the effects of schizophrenia correspond to an increased precision of sensory information. These findings shed new light on the similarities and differences between schizophrenia and two psychotomimetic drug states, and have potential implications for the study of consciousness and future mental health treatments.}
}
@article{CHIU2024100171,
title = {What are artificial intelligence literacy and competency? A comprehensive framework to support them},
journal = {Computers and Education Open},
volume = {6},
pages = {100171},
year = {2024},
issn = {2666-5573},
doi = {https://doi.org/10.1016/j.caeo.2024.100171},
url = {https://www.sciencedirect.com/science/article/pii/S2666557324000120},
author = {Thomas K.F. Chiu and Zubair Ahmad and Murod Ismailov and Ismaila Temitayo Sanusi},
keywords = {AI literacy, AI competency, K-12 education, Machine learning, Data literacy, Generative AI},
abstract = {Artificial intelligence (AI) education in K–12 schools is a global initiative, yet planning and executing AI education is challenging. The major frameworks are focused on identifying content and technical knowledge (AI literacy). Most of the current definitions of AI literacy for a non-technical audience are developed from an engineering perspective and may not be appropriate for K–12 education. Teacher perspectives are essential to making sense of this initiative. Literacy is about knowing (knowledge, what skills); competency is about applying the knowledge in a beneficial way (confidence, how well). They are strongly related. This study goes beyond knowledge (AI literacy), and its two main goals are to (i) define AI literacy and competency by adding the aspects of confidence and self-reflective mindsets, and (ii) propose a more comprehensive framework for K–12 AI education. These definitions are needed for this emerging and disruptive technology (e.g., ChatGPT and Sora, generative AI). We used the definitions and the basic curriculum design approaches as the analytical framework and teacher perspectives. Participants included 30 experienced AI teachers from 15 middle schools. We employed an iterative co-design cycle to discuss and revise the framework throughout four cycles. The definition of AI competency has five abilities that take confidence into account, and the proposed framework comprises five key components: technology, impact, ethics, collaboration, and self-reflection. We also identify five effective learning experiences to foster abilities and confidences, and suggest five future research directions: prompt engineering, data literacy, algorithmic literacy, self-reflective mindset, and empirical research.}
}
@article{MADUABUCHI2023125889,
title = {Deep neural networks for quick and precise geometry optimization of segmented thermoelectric generators},
journal = {Energy},
volume = {263},
pages = {125889},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.125889},
url = {https://www.sciencedirect.com/science/article/pii/S036054422202775X},
author = {Chika Maduabuchi and Chibuoke Eneh and Abdulrahman Abdullah Alrobaian and Mohammad Alkhedher},
keywords = {Segmented thermoelectric generator, Solar energy, Deep neural networks, Geometry optimization, Thermo-mechanical analysis, Finite element method},
abstract = {To solve the problems of the current optimization methods for solar segmented thermoelectric generator performance based on numerical methods, this paper applied deep neural networks to optimize the device geometry for improved thermo-mechanical performance. The motivation for using the deep neural network is to overcome the lengthy computational time and very high computational energy required by the traditional numerical method in optimizing the segmented thermoelectric generator performance. The numerical model is built using ANSYS software and the effects of temperature dependency in the 4 thermoelectric materials are considered to ensure result accuracy. Furthermore, 16 possible geometry parameters which were previously not considered, encompassing the individual and combined segment's heights and cross-sectional areas are optimized to find which set of parameters are the best in maximizing the device performance. The deep neural network is a regressive multilayer perceptron with network hyperparameters comprising 2 hidden layers with 5 neurons per layer. The training process is governed by the Levenberg-Marquardt standard backpropagation algorithm to minimize the mean squared error and maximize the regression correlation between the neural network forecasted outputs and the numerical-generated dataset. The most significant contribution of the proposed deep neural network is that it was able to quickly and accurately forecast the device performance in just 10 s, which was 2880 times faster than the conventional numerical-based optimization approach. Additionally, the optimized device had a maximum efficiency of 18%, which was 78% higher than that of the unoptimized device. Also, the thermal stress of the optimized device was 73% less than that of the unoptimized device design, indicating an extension in the device mechanical reliability and service lifetime. The results reported in this paper will accelerate the ease at which efficient, long-lasting segmented thermoelectric generators are manufactured by harnessing the power of artificial intelligence.}
}
@incollection{SCHONER202471,
title = {Chapter 4 - Toward a neural theory of goal-directed reaching movements},
editor = {Mindy F. Levin and Maurizio Petrarca and Daniele Piscitelli and Susanna Summa},
booktitle = {Progress in Motor Control},
publisher = {Academic Press},
pages = {71-102},
year = {2024},
isbn = {978-0-443-23987-8},
doi = {https://doi.org/10.1016/B978-0-443-23987-8.00008-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443239878000080},
author = {Gregor Schöner and Lukas Bildheim and Lei Zhang},
keywords = {Dynamic field theory, Neural dynamics, Neural timers, Target selection, Movement initiation, Mouse-tracking paradigm, Degrees of freedom problem, Posture movement problem},
abstract = {How do we bring about goal-directed motor acts? Reaching for an object that offers a useful exemplary case around which the processes underlying human movement behavior can be studied. Such reaching entails processes from scene and object perception, target selection, and movement initiation, to timing and control. These processes are typically studied in different subdisciplines, using different methods based on different theoretical concepts. Yet they are continuously coupled online and evolve in a closed loop. Understanding how they work together thus requires an integrative theoretical framework. While abstract computational ideas are often invoked for such integration, we argue for a theoretical account that is grounded in neural principles. We review the key concepts of a neural theory of goal-directed reaching movements that draw on neural dynamic models of population activation in which recurrent connectivity provides stability. For each component process, we discuss the key issues and empirical constraints for a neural dynamic account. Although a complete neural architecture of goal-directed movement behavior is still under development, the outline we provide interfaces with a large set of empirical findings.}
}
@article{LANG2017298,
title = {Mesoscopic Simulation Models for Logistics Planning Tasks in the Automotive Industry},
journal = {Procedia Engineering},
volume = {178},
pages = {298-307},
year = {2017},
note = {RelStat-2016: Proceedings of the 16th International Scientific Conference Reliability and Statistics in Transportation and Communication October 19-22, 2016. Transport and Telecommunication Institute, Riga, Latvia},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.01.118},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817301182},
author = {Sebastian Lang and Tobias Reggelin and Toralf Wunder},
keywords = {automotive industry, logistics planning, production planning, mesoscopic simulation, discrete-rate simulation},
abstract = {The paper evaluates mesoscopic simulation models applied to logistics planning tasks in the automotive industry. In terms of level of detail, mesoscopic simulation models fall between object based discrete-event simulation models and flow based continuous simulation models. Mesoscopic models represent logistics flow processes on an aggregated level through piecewise constant flow rates instead of modeling individual flow objects. The results are not obtained by counting individual objects but by using mathematical formulas to calculate the results as continuous quantities in every modeling time step. This leads to a fast model creation and computation. The authors expect that mesoscopic simulation models can help to support decisions on the operational, tactical and strategic level of planning. The paper describes a mesoscopic simulation model of the goods receiving of an assembly plant and compares the simulation results and computation time with a discrete-event model.}
}
@article{ROBERTSON2008436,
title = {New frontiers in space propulsion sciences},
journal = {Energy Conversion and Management},
volume = {49},
number = {3},
pages = {436-452},
year = {2008},
note = {Space Nuclear Power and Propulsion},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2007.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S019689040700369X},
author = {Glen A. Robertson and P.A. Murad and Eric Davis},
keywords = {Space propulsion, Warp drive, Worm Holes, EM propulsion},
abstract = {Mankind’s destiny points toward a quest for the stars. Realistically, it is difficult to achieve this using current space propulsion science and develop the prerequisite technologies, which for the most part requires the use of massive amounts of propellant to be expelled from the system. Therefore, creative approaches are needed to reduce or eliminate the need for a propellant. Many researchers have identified several unusual approaches that represent immature theories based upon highly advanced concepts. These theories and concepts could lead to creating the enabling technologies and forward thinking necessary to eventually result in developing new directions in space propulsion science. In this paper, some of these theoretical and technological concepts are examined – approaches based upon Einstein’s General Theory of Relativity, spacetime curvature, superconductivity, and newer ideas where questions are raised regarding conservation theorems and if some of the governing laws of physics, as we know them, could be violated or are even valid. These conceptual ideas vary from traversable wormholes, Krasnikov tubes and Alcubierre’s warpdrive to Electromagnetic (EM) field propulsion with possible hybrid systems that incorporate our current limited understanding of zero point fields and quantum mechanics.}
}
@article{ZHANG2025100137,
title = {AI Linguistics},
journal = {Natural Language Processing Journal},
volume = {10},
pages = {100137},
year = {2025},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2025.100137},
url = {https://www.sciencedirect.com/science/article/pii/S2949719125000135},
author = {Guosheng Zhang},
keywords = {AI architecture, Embedding augmentation, Embedding dimensionality, Embedding domain, General embedding},
abstract = {This research investigates the development of a linguistics for artificial intelligence (AI) to demystify the ”black box” of AI. At its core, the language of AI is Embedding—a novel high-dimensional, intelligent language. Embedding exhibits dual characteristics: it operates both as a semantic domain and as a mathematical point. This duality enables Embedding to maintain the discrete, symbolic nature of human languages while facilitating continuous operations in high-dimensional spaces, unlocking significant potential for advanced intelligence. A series of specialized experiments were designed to explore Embedding’s intrinsic properties, including its behavior as a semantic cloud in high-dimensional space, its degrees of freedom, and spatial transformations. Key findings include the discovery of substantial redundant dimensions in embeddings, confirmation that embeddings lack critical dimensions, and the measurement of engineering dimensions in natural language. This research also establishes the linguistic foundations and application limits of techniques such as dropout strategies, AI model distillation, and scaling laws among others. Building on these insights, we propose innovative solutions across several fields, including AI architecture design, AI reasoning, domain-based embedding search, and the construction of a multi-intelligence spectrum for embeddings. Ultimately, we introduce a foundational methodology for embedding everything from real-world into the AI world, providing a comprehensive reference framework for the evolution of artificial general intelligence (AGI) and artificial superintelligence (ASI). Additionally, this research explores linguistic approaches to the co-evolution of human intelligence and artificial intelligence.}
}
@article{ALAYANDE2020e00436,
title = {Estimating effective rates of protection in Nigeria's protected cement industry},
journal = {Scientific African},
volume = {8},
pages = {e00436},
year = {2020},
issn = {2468-2276},
doi = {https://doi.org/10.1016/j.sciaf.2020.e00436},
url = {https://www.sciencedirect.com/science/article/pii/S2468227620301745},
author = {Folarin Alayande},
keywords = {Trade protection, Effective rate of protection, Trade policy, Nigerian cement},
abstract = {Trade protection for selected products is a key element of smart industrial policy in many developing countries. Understanding quantitative measures of trade protection for industrial and consumer commodities is therefore a key requisite to determining the effectiveness of export-led growth in particular, and overall industrial policy. However, accurate estimates for trade policy incentives provided to industrial products are few and comparable datasets are sparse, with the most recent published country datasets dated as far back as 2012, yet with limited sector indices. This study estimates the effective rate of protection (ERP), a key index of trade protection and industrial policy, for the cement industry, one of the largest manufacturing industries in Nigeria. Time series data for 16 years, on the actual cost of trade protection, including tariff barriers and import prohibition bans, from 2000 to 2015 is used. With the ERP, the true cost of protection of domestic manufactures from imported goods, is computed using input shares and tariff data from the United Nations COMTRADE database. The data show the basis for computation and provides a re-useable template for central planners in the computation of effective rate of protection for similar manufacturing industries and other African countries. The computed ERP for the cement industry in Nigeria show a relatively high protection rate, and the overwhelming impact of trade prohibition on the ERP after the implementation of the Federal Government's incentive-led Backward Integration Programme. The evidence is compared with earlier data on Africa. Preliminary findings and trend analysis indicate a high correlation between the ERP and value added to gross domestic product (GDP).}
}
@article{PAN2024102334,
title = {Novel blockchain deep learning framework to ensure video security and lightweight storage for construction safety management},
journal = {Advanced Engineering Informatics},
volume = {59},
pages = {102334},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102334},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623004627},
author = {Xing Pan and Luoxin Shen and Botao Zhong and Da Sheng and Fang Huang and Luhan Yang},
keywords = {Construction safety management, Video security storage, Blockchain, Deep learning, Video summarization, Pre-defined rule},
abstract = {In construction management, video data tampering behavior like manual forging and deletion can negatively impact on-site safety and accident accountability. Blockchain technology holds the potential to address this issue by leveraging distributed ledger characteristics. However, blockchain's limited storage capacity and block size make it difficult to upload large-sized construction data such as daily monitoring video. Furthermore, it is unnecessary to store all construction data in any case. Therefore, this study proposes a blockchain deep learning framework that focuses on how to efficiently extract and securely store key information (i.e., video summarization that involves worker’s unsafe behavior) on-blockchain for data traceability. The framework involves a novel data-driven and rule-based keyframe extraction (DRKE) model to lightweight large-sized construction video in the nascent field of deep learning and blockchain combination. To define parameters for the DRKE model, specific construction rules (e.g., people’s unsafe behavior-based and people-based rules) have been pre-defined. This framework has been evaluated, and the results demonstrate its capability for effective video security storage, facilitating practical needs in construction management. The study extends existing research and provides a practical solution for large-sized construction video storage with security and lightweight considerations. The proposed video security storage and data lightweight process offers substantial benefits to construction management, such as streamlined accident investigation and accountability and improved on-site work efficiency, contributing to the smooth progress of construction projects.}
}
@article{KLEINMUNTZ1994457,
title = {Toward intelligent computerized clinicians},
journal = {Computers in Human Behavior},
volume = {10},
number = {4},
pages = {457-466},
year = {1994},
issn = {0747-5632},
doi = {https://doi.org/10.1016/0747-5632(94)90040-X},
url = {https://www.sciencedirect.com/science/article/pii/074756329490040X},
author = {Benjamin Kleinmuntz},
abstract = {The current scientific interest in computer thinking is explored for constructing a simulated psychodiagnostician. Going beyond simply building an expert system, this paper proposes to use a self-learning and generalizing computer architecture. State Operator And Result, or the SOAR system, to model intelligent clinical behavior. Accordingly, it fosters the collection and analysis of process-tracing verbal protocols for building psychodiagnosticians. It also holds out the possibility of going beyond psychodiagnosis for functioning as a multitask clinical psychologist.}
}
@article{CAI201253,
title = {On fast and accurate block-based motion estimation algorithms using particle swarm optimization},
journal = {Information Sciences},
volume = {197},
pages = {53-64},
year = {2012},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2012.02.014},
url = {https://www.sciencedirect.com/science/article/pii/S002002551200117X},
author = {Jing Cai and W. {David Pan}},
keywords = {Particle swarm optimization, Motion estimation, Fast block-matching methods, Video sequences, Computational complexity},
abstract = {Both fast and accurate block-matching algorithms are critical to efficient compression of video frames using motion estimation and compensation. While the particle swarm optimization approach holds the promise of alleviating the local optima problem suffered typically by existing very fast block matching methods, motion estimation algorithms based on particle swarm optimization in the literature appear to be either much slower than some leading fast block-matching methods for a given accuracy of motion estimation, or less accurate for a given computational complexity. In this paper, we show that the conventional particle swarm optimization approach, which was originally designed to solve general optimization problems where fast convergence of the algorithm might not be a primary concern, could be modified appropriately so that it could provide accurate motion estimation with very low computational cost in the specific context of video motion estimation. To this end, we proposed a new block matching algorithm based on a set of strategies adapted from the standard particle swarm optimization approach. Extensive simulations showed that the proposed method could achieve significant improvements over leading fast block matching methods including the diamond search and the cross-diamond search methods, in terms of both estimation accuracy and computational cost. In particular, the proposed method based on particle swarm optimization is not only much faster, but also remarkably more accurate (about 2dB higher in terms of the Peak Signal-to-Noise-Ratio) than the competing methods on video sequences with large motion.}
}
@article{CHOI2025102535,
title = {Comparison study of PR curriculum and PR job posts},
journal = {Public Relations Review},
volume = {51},
number = {1},
pages = {102535},
year = {2025},
issn = {0363-8111},
doi = {https://doi.org/10.1016/j.pubrev.2024.102535},
url = {https://www.sciencedirect.com/science/article/pii/S0363811124001140},
author = {Minhee Choi and Baobao Song and Yani Zhao and Lauren Tortella},
keywords = {Public relations industry skills, Public relations education, KSAO, Natural language processing},
abstract = {This study undertakes a comprehensive examination by comparing requirements outlined in entry-level PR job postings with the curricula of 83 undergraduate programs accredited by ACEJMC. Although the comparative analysis underscores the consistent alignment of PR education with industry expectations, some discrepancies in degree requirements and required skills and abilities are found. By employing a natural language processing approach, this study not only investigates current industry needs in terms of human resources but also provides practical implications for PR education.}
}
@article{QIAN2024117679,
title = {A novel dataset and feature selection for data-driven conceptual design of offshore jacket substructures},
journal = {Ocean Engineering},
volume = {303},
pages = {117679},
year = {2024},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2024.117679},
url = {https://www.sciencedirect.com/science/article/pii/S0029801824010163},
author = {Han Qian and Emmanouil Panagiotou and Mengyan Peng and Eirini Ntoutsi and Chongjie Kang and Steffen Marx},
keywords = {Offshore jacket substructure, Conceptual design, Data-driven method, Machine learning, Dataset, Feature selection},
abstract = {Conceptual design is crucial for designing offshore jacket substructures because it sets the direction for the entire design process. Nevertheless, conventional simulation-based optimization methods for jacket conceptual design face challenges, such as high computational costs and restricted optimization objectives. This paper proposes a data-driven method for offshore jacket conceptual design using machine learning (ML). First, a novel dataset of completed and under-construction jackets worldwide was established as the cornerstone of ML. The dataset comprised “in-action” data capturing key structural parameters of jackets and information on design boundary conditions. Subsequently, different features were comprehensively selected to identify and visualize their correlations for an interpretable data-driven design, ensuring the effectiveness of the dataset for training the ML models. Finally, random forest and eXtreme gradient boosting models were trained on the data from the selected feature subsets and then employed to predict individual jacket structural parameters. The predictive performance of the models indicates that the dataset and feature selection can capture the fundamental and shared characteristics of well-designed jackets, thereby improving the accuracy and efficiency of the conceptual design process. This study suggests the potential of a data-driven conceptual design for offshore jacket substructures.}
}
@article{HINZEN2024110952,
title = {The ‘L-factor’: Language as a transdiagnostic dimension in psychopathology},
journal = {Progress in Neuro-Psychopharmacology and Biological Psychiatry},
volume = {131},
pages = {110952},
year = {2024},
issn = {0278-5846},
doi = {https://doi.org/10.1016/j.pnpbp.2024.110952},
url = {https://www.sciencedirect.com/science/article/pii/S0278584624000204},
author = {Wolfram Hinzen and Lena Palaniyappan},
keywords = {Thought, Psychosis, Neurocognition, Psychopathology, Brain networks},
abstract = {Thoughts and moods constituting our mental life incessantly change. When the steady flow of this dynamics diverges in clinical directions, the possible pathways involved are captured through discrete diagnostic labels. Yet a single vulnerable neurocognitive system may be causally involved in psychopathological deviations transdiagnostically. We argue that language viewed as integrating cortical functions is the best current candidate, whose forms of breakdown along its different dimensions are then manifest as symptoms – from prosodic abnormalities and rumination in depression to distortions of speech perception in verbal hallucinations, distortions of meaning and content in delusions, or disorganized speech in formal thought disorder. Spontaneous connected speech provides continuous objective readouts generating a highly accessible bio-behavioral marker with the potential of revolutionizing neuropsychological measurement. This argument turns language into a transdiagnostic ‘L-factor’ providing an analytical and mechanistic substrate for previously proposed latent general factors of psychopathology (‘p-factor’) and cognitive functioning (‘c-factor’). Together with immense practical opportunities afforded by rapidly advancing natural language processing (NLP) technologies and abundantly available data, this suggests a new era of translational clinical psychiatry, in which both psychopathology and language may be rethought together.}
}
@article{SUTOYO2015435,
title = {Dynamic Difficulty Adjustment in Tower Defence},
journal = {Procedia Computer Science},
volume = {59},
pages = {435-444},
year = {2015},
note = {International Conference on Computer Science and Computational Intelligence (ICCSCI 2015)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.07.563},
url = {https://www.sciencedirect.com/science/article/pii/S187705091502092X},
author = {Rhio Sutoyo and Davies Winata and Katherine Oliviani and Dedy Martadinata Supriyadi},
keywords = {dynamic game balancing, tower defence, dynamic difficulty adjustment, computational intelligence},
abstract = {When we play tower defence game, generally we repeat the same stages several times with the same enemies. Moreover, when the players play a stage that is ridiculously hard or way too easy, they would probably quit the game because it ismoderately frustrating or boring. The purpose of this research is to createa game that can adapt to the players’ ability so the difficulty of the game becomes dynamic. In other words, the game will have different difficultiesof levels according to the players’ ability. High difficulty levels will be set if the players use good strategy and low difficulty levels will be set if the players use bad strategy. In this work, we determine the difficulties based on players’ lives, enemies’ health, and passive skills (skill points) that are chosen by the player. With three of these factors, players will have varies experience of playing tower defence because different combination will give different results to the system and difficulties of the games will be different for each gameplay. The result of this research is a dynamic difficulty tower defence game, dynamic difficulty adjustment (DDA) document, and gameplay outputs for best, average, and worst strategy cases.}
}
@article{BARRETO20114206,
title = {Lumping the States of a Finite Markov Chain Through Stochastic Factorization},
journal = {IFAC Proceedings Volumes},
volume = {44},
number = {1},
pages = {4206-4211},
year = {2011},
note = {18th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20110828-6-IT-1002.00073},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016442680},
author = {André M.S. Barreto and Marcelo D. Fragoso},
abstract = {Abstract
In this work we show how the lumping of states of a finite Markov chain can be regarded as a special decomposition of its transition matrix called stochastic factorization. The idea is simple: when a transition matrix is factored into the product of two stochastic matrices, one can swap the factors of the multiplication to obtain another model, potentially much smaller than the original one. We prove in the paper that the smaller Markov chain has the same reducibility and the same number of closed sets as the original model. Additionally, the stationary distributions of both chains are related through a linear transformation. By interpreting the lumping of states as a particular case of stochastic factorization, we discuss in which circumstances the lumped transition matrix can be used in place of the original one to compute its stationary distribution. To illustrate our ideas we use the computation of Google's PageRank as an example.}
}
@incollection{BRUDER2017101,
title = {Chapter 5 - Infrastructural intelligence: Contemporary entanglements between neuroscience and AI},
editor = {Tara Mahfoud and Sam McLean and Nikolas Rose},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {233},
pages = {101-128},
year = {2017},
booktitle = {Vital Models},
issn = {0079-6123},
doi = {https://doi.org/10.1016/bs.pbr.2017.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0079612317300547},
author = {Johannes Bruder},
keywords = {Artificial intelligence, Computational neuroscience, Brain imaging, Google DeepMind technologies, Default mode network},
abstract = {In this chapter, I reflect on contemporary entanglements between artificial intelligence and the neurosciences by tracing the development of Google's recent DeepMind algorithms back to their roots in neuroscientific studies of episodic memory and imagination. Google promotes a new form of “infrastructural intelligence,” which excels by constantly reassessing its cognitive architecture in exchange with a cloud of data that surrounds it, and exhibits putatively human capacities such as intuition. I argue that such (re)alignments of biological and artificial intelligence have been enabled by a paradigmatic infrastructuralization of the brain in contemporary neuroscience. This infrastructuralization is based in methodologies that epistemically liken the brain to complex systems of an entirely different scale (i.e., global logistics) and has given rise to diverse research efforts that target the neuronal infrastructures of higher cognitive functions such as empathy and creativity. What is at stake in this process is no less than the shape of brains to come and a revised understanding of the intelligent and creative social subject.}
}
@incollection{ZOHURI2022121,
title = {Chapter 5 - Mathematical modeling driven predication},
editor = {Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia},
booktitle = {Knowledge is Power in Four Dimensions: Models to Forecast Future Paradigm},
publisher = {Academic Press},
pages = {121-163},
year = {2022},
isbn = {978-0-323-95112-8},
doi = {https://doi.org/10.1016/B978-0-323-95112-8.00005-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323951128000052},
author = {Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia},
keywords = {Data mining and data analytics, Forecasting and prediction, Modeling and mathematics},
abstract = {During the past decade, there has been a tremendous blast and progress in computation technology, and with it comes vast amounts of data in a variety of fields such as the economy, medicine, biology, banking services such as customer relation management and credit card fraud, finance, demographic population growth from a demographical point of view nationwide and worldwide, and the need for new lifestyles and growth in term of continuous renewable sources of energy and its production, as well as marketing are among the fields that can be mentioned. The challenge of understanding these data has led to the development of new tools such as predictive analytics in the field of statistics and spawned new areas such as data mining, machine learning, and bioinformatics to process these data and determine the integrity of their information for prediction analysis. Many of these tools have common underpinnings but are often expressed with different terminology. This chapter will summarize the important ideas in these areas in a common conceptual framework.}
}
@article{MARTIN2018105,
title = {On an inferential model construction using generalized associations},
journal = {Journal of Statistical Planning and Inference},
volume = {195},
pages = {105-115},
year = {2018},
note = {Confidence distributions},
issn = {0378-3758},
doi = {https://doi.org/10.1016/j.jspi.2016.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0378375816301537},
author = {Ryan Martin},
keywords = {Likelihood, Marginalization, Monte Carlo, Plausibility function, Random set, Validity},
abstract = {The inferential model (IM) approach, like fiducial and its generalizations, depends on a representation of the data-generating process. Here, a particular variation on the IM construction is considered, one based on generalized associations. The resulting generalized IM is more flexible in that it does not require a complete specification of the data-generating process and is provably valid under mild conditions. Computation and marginalization strategies are discussed, and two applications of this generalized IM approach are presented.}
}
@article{KAUFFMAN2017115,
title = {Combining machine-based and econometrics methods for policy analytics insights},
journal = {Electronic Commerce Research and Applications},
volume = {25},
pages = {115-140},
year = {2017},
issn = {1567-4223},
doi = {https://doi.org/10.1016/j.elerap.2017.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S1567422317300145},
author = {Robert J. Kauffman and Kwansoo Kim and Sang-Yong Tom Lee and Ai-Phuong Hoang and Jing Ren},
keywords = {Causality, Computational Social Science, Data analytics, Econometrics, E-commerce, Empirical research, Fintech, Fusion analytics, Music popularity, Stock trading, Policy analytics, TV viewing, Video-on-demand (VoD)},
abstract = {Computational Social Science (CSS) has become a mainstream approach in the empirical study of policy analytics issues in various domains of e-commerce research. This article is intended to represent recent advances that have been made for the discovery of new policy-related insights in business, consumer and social settings. The approach discussed is fusion analytics, which combines machine-based methods from Computer Science (CS) and explanatory empiricism involving advanced Econometrics and Statistics. It explores several efforts to conduct research inquiry in different functional areas of Electronic Commerce and Information Systems (IS), with applications that represent different functional areas of business, as well as individual consumer, social and public issues. Recent developments and shifts in the scientific study of technology-related phenomena and Social Science issues in the presence of historically-large datasets prompt new forms of research inquiry. They include blended approaches to research methodology, and more interest in the production of research results that have direct application to industry contexts. This article showcases the methods shifts and several contemporary applications. They discuss: (1) feedback effects in mobile phone-based stock trading; (2) sustainability of top-rank chart popularity of music tracks; (3) household TV viewing patterns; and (4) household sampling and purchases of video-on-demand (VoD) services. The range of applicability of the ideas goes beyond the scope of these illustrations, to include issues in public services, healthcare, product and service deployment, public opinion and elections, electronic auctions, and travel and tourism services. In fact, the coverage is as broad as for-profit and for-non-profit, private and public, and governmental and non-governmental institutions.}
}
@article{MERRICK2019511,
title = {On choosing the resolution of normative models},
journal = {European Journal of Operational Research},
volume = {279},
number = {2},
pages = {511-523},
year = {2019},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2019.06.017},
url = {https://www.sciencedirect.com/science/article/pii/S0377221719304928},
author = {James H. Merrick and John P. Weyant},
keywords = {Problem structuring, Validation of OR computations, Information theory, Strategic planning, OR in environment and climate change},
abstract = {Long time horizon normative models are frequently used for policy analysis, strategic planning, and system analysis. Choosing the granularity of the temporal or spatial resolution of such models is an important modeling decision, often having a first order impact on model results. This type of decision is frequently made by modeler judgment, particularly when the predictive power of alternative choices cannot be tested. In this paper, we show how the implicit tradeoffs modelers make in these formulation decisions, in particular in the tradeoff between the accuracy of representation enabled by the available data and model parsimony, may be addressed with established information theoretic ideas. The paper provides guidance for modelers making these tradeoffs or, in certain cases, enables explicit tests for assessing appropriate levels of resolution. We will mainly focus on optimization based normative models in the discussion here, and draw our examples from the energy and climate domain.}
}
@incollection{ZEYER2015235,
title = {11 - For the mutual benefit: Health information provision in the science classroom},
editor = {Catherine {Arnott Smith} and Alla Keselman},
booktitle = {Meeting Health Information Needs Outside Of Healthcare},
publisher = {Chandos Publishing},
pages = {235-261},
year = {2015},
isbn = {978-0-08-100248-3},
doi = {https://doi.org/10.1016/B978-0-08-100248-3.00011-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780081002483000111},
author = {Albert Zeyer and Daniel M. Levin and Alla Keselman},
keywords = {Health education, Health literacy, Information, Knowledge, Science education},
abstract = {In this chapter, the authors argue that the school science classroom should help students deal with complex real-life information about health and disease. They also discuss means by which curriculum and instruction in science education can be tied to these issues. The chapter reviews opportunities and challenges presented to individuals by the expectations of participatory health care, focusing on models of health literacy that can help understand and address the challenges. The authors argue that the problem of ensuring effective information use often lies in a transmission approach to health information provision. Transmitted knowledge is often not understood nor applied, as demonstrated in studies of human papillomavirus vaccination education. An alternative to knowledge transmission is the approach that aims to foster critical literacy, which is grounded in critical thinking essential to the practice of science. The chapter reviews a number of interdisciplinary science education activities that introduce health issues in the context of biology, physics, and chemistry education, ensuring deep understanding needed for developing critical literacy. It also discusses science education approaches and theories that encourage the development of deep, culturally meaningful science knowledge. Finally, the chapter reviews professional development and the role of various professionals, including teachers and librarians, in the collaborative endeavor of effective health information provision.}
}
@article{BOLAND2019195,
title = {The price of discretizing time: a study in service network design},
journal = {EURO Journal on Transportation and Logistics},
volume = {8},
number = {2},
pages = {195-216},
year = {2019},
note = {Special Issue: Advances in vehicle routing and logistics optimization: exact methods},
issn = {2192-4376},
doi = {https://doi.org/10.1007/s13676-018-0119-x},
url = {https://www.sciencedirect.com/science/article/pii/S2192437620300327},
author = {Natashia Boland and Mike Hewitt and Luke Marshall and Martin Savelsbergh},
keywords = {Service network design, Time-space network, Time expanded network, Approximation},
abstract = {Researchers and practitioners have long recognized that many transportation problems can be naturally and conveniently modeled using time-expanded networks. In such models, nodes represent locations at distinct points in time and arcs represent possible actions, e.g., moving from one location to another at a particular point of time, or staying in the same location for a period of time. To use a time-expanded network, time must be discretized, i.e., the planning horizon is partitioned into discrete time intervals. The length of these intervals, therefore, must be chosen, and the parameters involving time, e.g., travel duration and due times, need to be mapped to these discrete intervals. Short intervals yield a high-quality approximation to the continuous-time problem, but typically induce a computationally intractable model; whereas long intervals can yield a computationally tractable, but low-quality model. The loss of quality is due to the approximation introduced by the mapping of parameters involving time. To guide researchers and practitioners in their use of time-expanded networks, we explore the choice of time discretization and its impact, by means of an extensive computational study on the service network design problem. The empirical results show that in some cases the loss of quality, i.e., the relative gap between the discretized and continuous-time optimal values, can be greater than 20%. We also investigate metrics that characterize and help identify instances that are likely to be sensitive to discretization and could incur a large loss of solution quality.}
}
@incollection{NOVOTNY1996149,
title = {Computational Biochemistry of Antibodies and T-Cell Receptors},
editor = {Frederic M. Richards and David E. Eisenberg and Peter S. Kim},
series = {Advances in Protein Chemistry},
publisher = {Academic Press},
volume = {49},
pages = {149-260},
year = {1996},
booktitle = {Antigen Binding Molecules: Antibodies and T-cell Receptors},
issn = {0065-3233},
doi = {https://doi.org/10.1016/S0065-3233(08)60490-8},
url = {https://www.sciencedirect.com/science/article/pii/S0065323308604908},
author = {Jiri Novotny and Jürgen Bajorath}
}
@article{ZHOU2025112831,
title = {Modeling of milling force in multi-axis machining process for thin-walled sculptured surface},
journal = {Thin-Walled Structures},
volume = {208},
pages = {112831},
year = {2025},
issn = {0263-8231},
doi = {https://doi.org/10.1016/j.tws.2024.112831},
url = {https://www.sciencedirect.com/science/article/pii/S0263823124012709},
author = {Tianxiang Zhou and Caixu Yue and Xianli Liu and Shaocong Sun and Shiliang Wei and Anshan Zhang},
keywords = {Thin-walled sculptured surface, Multi-axis machining, Ball-end milling, Cutting force, Cutter workpiece engagement, Instantaneous undeformed chip thickness},
abstract = {Ball-end milling is the preferred machining method for machining thin-walled sculptured surfaces as a method with higher efficiency and accuracy. However, with the change of tool attitude during the machining process, the cutter workpiece engagement (CWE) area and the instantaneous undeformed chip thickness (IUCT) will change at the same time, which makes it more difficult to accurately predict the milling force. In this paper, in order to accurately predict the milling force for multi-axis machining, the real CWE area is determined using the upper and lower boundary ideas and based on the optimization criterion. Subsequently, the IUCT model based on the infinitesimal point outer normal vector is proposed by considering the factors of tool attitude change and tool run-out, which avoids complicated iterative calculations. Finally, relying on the thin-walled sculptured surface machining experiments, a series of validation experiments were carried out under different cutting conditions, and the results show that the experimental data and the simulated data have good consistency in shape and size, which proves the validity and accuracy of the model and achieves a better result in terms of applicability, and provides a new way of thinking for the machining of sculptured surface thin-walled parts in real industrial scenarios.}
}
@incollection{IRISH2024,
title = {Interactions between episodic and semantic memory},
booktitle = {Reference Module in Neuroscience and Biobehavioral Psychology},
publisher = {Elsevier},
year = {2024},
isbn = {978-0-12-809324-5},
doi = {https://doi.org/10.1016/B978-0-443-15754-7.00009-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443157547000092},
author = {Muireann Irish and Matthew D. Grilli},
keywords = {Aging, Alzheimer's disease, Amnesia, Autobiographical memory, Episodic memory, Gradients, Hippocampus, Prospection, Semantic dementia, Semantic memory},
abstract = {In the present chapter, we challenge the idea that episodic and semantic memory are distinct memory systems supported by dissociable brain networks. Drawing on converging findings from cognitive neuroscience and neuropsychology, we show how these forms of declarative memory share remarkably similar neural networks and interact to support an array of cognitive endeavors. We contend that these points of overlap and apparent dissociations can be reconciled by situating the representational content of declarative memory along an episodic-semantic gradient or continuum. This continuum perspective can account for several bodies of research showing that episodic and semantic memory are highly interdependent in natural states and functional contexts, from the way memories are reorganized over time to how humans mentally construct future scenarios. We discuss these points of synergy and highlight unresolved questions, with a view to orienting the field towards a more integrated perspective.}
}
@incollection{PINTARIC20162367,
title = {Towards Outcomes-Based Education of Computer-Aided Chemical Engineering},
editor = {Zdravko Kravanja and Miloš Bogataj},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {38},
pages = {2367-2372},
year = {2016},
booktitle = {26th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-63428-3.50399-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780444634283503994},
author = {Zorka Novak Pintarič and Zdravko Kravanja},
keywords = {Computer-Aided, Chemical Engineering, Education, Bologna process, Learning Outcomes},
abstract = {Chemical engineering education is nowadays increasingly supported by the use of various computational tools as the employers’ requirements for computing skills of graduates are growing too. However, students often acquire computational skills in an unsystematic manner due to a lack of defining and applying computer-based outcomes within the syllabuses suitable for the particular level of the Bologna three-cycle system. This paper bridges this gap by providing the review of the essential learning outcomes in the computer-aided chemical engineering education during all three cycles. The identified outcomes gradually progress from application-based competencies up to more advanced process modeling ones based on knowledge synthesis and creation. Accordingly, the educational strategies and curricula can be redesigned in order to integrate courses more efficiently both horizontally and vertically, and upgrade the use of computational tools.}
}
@article{WANG2025100738,
title = {Shaping the future of academic conferences},
journal = {The Innovation},
volume = {6},
number = {1},
pages = {100738},
year = {2025},
issn = {2666-6758},
doi = {https://doi.org/10.1016/j.xinn.2024.100738},
url = {https://www.sciencedirect.com/science/article/pii/S2666675824001760},
author = {Haijun Wang and Ji Dai and Yuanzheng Cui and Peijun Zhang and Fang Wang and Buxing Han and Erik Jeppesen}
}
@article{YURKOVICH2017431,
title = {A Padawan Programmer’s Guide to Developing Software Libraries},
journal = {Cell Systems},
volume = {5},
number = {5},
pages = {431-437},
year = {2017},
issn = {2405-4712},
doi = {https://doi.org/10.1016/j.cels.2017.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S2405471217303368},
author = {James T. Yurkovich and Benjamin J. Yurkovich and Andreas Dräger and Bernhard O. Palsson and Zachary A. King},
abstract = {With the rapid adoption of computational tools in the life sciences, scientists are taking on the challenge of developing their own software libraries and releasing them for public use. This trend is being accelerated by popular technologies and platforms, such as GitHub, Jupyter, R/Shiny, that make it easier to develop scientific software and by open-source licenses that make it easier to release software. But how do you build a software library that people will use? And what characteristics do the best libraries have that make them enduringly popular? Here, we provide a reference guide, based on our own experiences, for developing software libraries along with real-world examples to help provide context for scientists who are learning about these concepts for the first time. While we can only scratch the surface of these topics, we hope that this article will act as a guide for scientists who want to write great software that is built to last.}
}
@article{HACKENBERG2025101205,
title = {Decentering to support responsive teaching for middle school students},
journal = {The Journal of Mathematical Behavior},
volume = {77},
pages = {101205},
year = {2025},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2024.101205},
url = {https://www.sciencedirect.com/science/article/pii/S0732312324000828},
author = {Amy J. Hackenberg and Fetiye {Aydeniz Temizer} and Rebecca S. Borowski},
keywords = {Decentering, Units coordination, Classroom study, Proportional reasoning, Middle school students, Teaching practice},
abstract = {A classroom study was conducted to understand how to engage in responsive teaching with 18 seventh grade students at three stages of units coordination during a unit on proportional reasoning co-taught by the first author and classroom teacher. In the unit, students worked on making two cars travel the same speed. Students at all three stages of units coordination learned to do so, as reported elsewhere (Hackenberg et al., 2023). This paper focuses on the practice of inquiring responsively in small groups. We found that teacher-researcher decentering was a mechanism underlying this practice. Decentering involves adopting the perspective of another person by setting one’s own perspective to the side and using the other’s perspective as a basis for interaction. We found that two patterns of decentering actions and a type of question, leveraging questions, supported students across stages of units coordination to sustain challenges and learn.}
}
@article{MAKRIDIS201328,
title = {Offshore wind power resource availability and prospects: A global approach},
journal = {Environmental Science & Policy},
volume = {33},
pages = {28-40},
year = {2013},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2013.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S146290111300097X},
author = {Christos Makridis},
keywords = {Offshore wind energy, Renewable energy, Global perspective, Renewable energy investment},
abstract = {In the absence of structural incentives that price negative externalities, renewable energies rely primarily on investors’ expectations of future performance to succeed in the marketplace. While there have been many disparate regional analyses of the prospects for clean energy, in particular wind, there is yet a cohesive framework for thinking about global interactions. Using data from the National Renewable Energy Laboratory (NREL), the article addresses three shortcomings in empirical renewable policy literature. First, the article briefly synthesizes the current state of the offshore wind literature. Second, the article develops a linear programming model to assess the relative prospects of offshore wind energy throughout seven world regions: Organization for Economic Co-operation and Development (OECD) North America, OECD Europe, OECD Asia and Eurasia, Non-OECD Europe and Eurasia, Non-OECD Asia, Africa, and Central & South America. Third, the article applies the Interactive Agency Model (IAM) as a systems-level framework for thinking about offshore wind development in the presence of social, economic, and institutional attributes. Results suggest that OECD Asia and Eurasia, OECD Europe, and non-OECD Europe and Eurasia, respectively, have the highest potential for offshore wind sector performance. Despite simplifying assumptions, this article presents one of the first evaluations of global offshore wind energy potential for policymakers and industry to consider in crafting future renewable energy investment decisions.}
}
@article{GOEL2023,
title = {Users’ Concerns About Endometriosis on Social Media: Sentiment Analysis and Topic Modeling Study},
journal = {Journal of Medical Internet Research},
volume = {25},
year = {2023},
issn = {1438-8871},
doi = {https://doi.org/10.2196/45381},
url = {https://www.sciencedirect.com/science/article/pii/S1438887123006179},
author = {Rahul Goel and Vijayachitra Modhukur and Katrin Täär and Andres Salumets and Rajesh Sharma and Maire Peters},
keywords = {endometriosis, latent Dirichlet allocation, pain, Reddit, sentiment analysis, social media, surgery, topic modeling, user engagement},
abstract = {Background
Endometriosis is a debilitating and difficult-to-diagnose gynecological disease. Owing to limited information and awareness, women often rely on social media platforms as a support system to engage in discussions regarding their disease-related concerns.
Objective
This study aimed to apply computational techniques to social media posts to identify discussion topics about endometriosis and to identify themes that require more attention from health care professionals and researchers. We also aimed to explore whether, amid the challenging nature of the disease, there are themes within the endometriosis community that gather posts with positive sentiments.
Methods
We retrospectively extracted posts from the subreddits r/Endo and r/endometriosis from January 2011 to April 2022. We analyzed 45,693 Reddit posts using sentiment analysis and topic modeling–based methods in machine learning.
Results
Since 2011, the number of posts and comments has increased steadily. The posts were categorized into 11 categories, and the highest number of posts were related to either asking for information (Question); sharing the experiences (Rant/Vent); or diagnosing and treating endometriosis, especially surgery (Surgery related). Sentiment analysis revealed that 92.09% (42,077/45,693) of posts were associated with negative sentiments, only 2.3% (1053/45,693) expressed positive feelings, and there were no categories with more positive than negative posts. Topic modeling revealed 27 major topics, and the most popular topics were Surgery, Questions/Advice, Diagnosis, and Pain. The Survey/Research topic, which brought together most research-related posts, was the last in terms of posts.
Conclusions
Our study shows that posts on social media platforms can provide insights into the concerns of women with endometriosis symptoms. The analysis of the posts confirmed that women with endometriosis have to face negative emotions and pain daily. The large number of posts related to asking questions shows that women do not receive sufficient information from physicians and need community support to cope with the disease. Health care professionals should pay more attention to the symptoms and diagnosis of endometriosis, discuss these topics with patients to reduce their dissatisfaction with doctors, and contribute more to the overall well-being of women with endometriosis. Researchers should also become more involved in social media and share new science-based knowledge regarding endometriosis.}
}
@incollection{SNOWDEN2015572,
title = {Semantic Memory},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {572-578},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.51059-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868510599},
author = {Julie S. Snowden},
keywords = {Amodal, Anterior temporal lobe, Brain networks, Conceptual knowledge, Distributed representations, Multimodal, Object knowledge, Schema, Semantic dementia, Semantic features, Semantic memory, Word knowledge},
abstract = {Semantic memory refers to our conceptual knowledge of the world. Understanding of semantic memory has come from several sources: cognitive studies of healthy individuals, computational modeling, patients with disordered semantic memory due to brain disease, and brain imaging and stimulation. The converging evidence indicates that semantic memory involves distributed brain networks, which, at least in part, are linked to the sensory processes involved in perception, action, and language. Whether there is also representation in amodal format remains an area of contention. Knowledge of the world, beyond word and object meanings, is a challenge for future studies of semantic memory.}
}
@article{HART2023182,
title = {Riders on a storm – The evaluation and control of creative processes A comment on: “A systematic framework of creative metacognition” by Izabela Lebuda and Mathias Benedek},
journal = {Physics of Life Reviews},
volume = {47},
pages = {182-183},
year = {2023},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2023.10.029},
url = {https://www.sciencedirect.com/science/article/pii/S1571064523001689},
author = {Yuval Hart}
}
@article{OXENFELDT197783,
title = {The computation of costs for price decisions},
journal = {Industrial Marketing Management},
volume = {6},
number = {2},
pages = {83-90},
year = {1977},
issn = {0019-8501},
doi = {https://doi.org/10.1016/0019-8501(77)90045-1},
url = {https://www.sciencedirect.com/science/article/pii/0019850177900451},
author = {A.R. Oxenfeldt},
abstract = {Business should compute costs in a particular way of pricing purposes. The correct cost computation varies with its purpose, though most executives still believe that an item has a true cost regardless of why it is computed. Even cost estimates made for price decisions will differ according to the type of price decision that is at issue. One finds important differences depending on whether one is estimating costs for a one-shot bid, for a promotional price offer that is to last for a short period, or for a decision concerning long-term price. Although the same basic principles would apply in all three cases, their application is different. The appropriate concept is that of “decision cost”, a very simple but powerful idea that leads to different cost conclusions than are reached by prevailing costing methods.}
}