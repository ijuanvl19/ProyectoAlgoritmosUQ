@article{LEHRER200039,
title = {Developing Model-Based Reasoning in Mathematics and Science},
journal = {Journal of Applied Developmental Psychology},
volume = {21},
number = {1},
pages = {39-48},
year = {2000},
issn = {0193-3973},
doi = {https://doi.org/10.1016/S0193-3973(99)00049-0},
url = {https://www.sciencedirect.com/science/article/pii/S0193397399000490},
author = {Richard Lehrer and Leona Schauble},
abstract = {It is essential to base instruction on a foundation of understanding of children's thinking, but it is equally important to adopt the longer-term view that is needed to stretch these early competencies into forms of thinking that are complex, multifaceted, and subject to development over years, rather than weeks or months. We pursue this topic through our studies of model-based reasoning. We have identified four forms of models and related modeling practices that show promise for developing model-based reasoning. Models have the fortuitous feature of making forms of student reasoning public and inspectable—not only among the community of modelers, but also to teachers. Modeling provides feedback about student thinking that can guide teaching decisions, an important dividend for improving professional practice.}
}
@incollection{PERKINS2002187,
title = {Standard logic as a model of reasoning: The empirical critique},
editor = {Dov M. Gabbay and Ralph H. Johnson and Hans Jürgen Ohlbach and John Woods},
series = {Studies in Logic and Practical Reasoning},
publisher = {Elsevier},
volume = {1},
pages = {187-223},
year = {2002},
booktitle = {Handbook of the Logic of Argument and Inference},
issn = {1570-2464},
doi = {https://doi.org/10.1016/S1570-2464(02)80007-6},
url = {https://www.sciencedirect.com/science/article/pii/S1570246402800076},
author = {David N. Perkins},
abstract = {Publisher Summary
This chapter describes standard logic as a model of reasoning. The notion of formal logic has figured centrally in conceptions of human reasoning, rationality, and adaptiveness. The chapter reviews the evidence, appraises its weight, and offers a summative judgment of the place of logic in human thinking. "Standard logic," includes the canons of formal deduction, the special case of disconfirming hypotheses by finding counterevidence for their implications, and also the principles of probabilistic and statistical inference developed by mathematicians over the past couple of hundred years. It also examines deliberate or reflexive reasoning. The chapter argues that standard logic or subsets of it can be implemented in quite different ways and that human cognition incorporates more than one implementation. In addition, almost all the research on the role of standard logic in human thinking concerns deliberate rather than reflexive reasoning. Accordingly, the present analysis focuses on deliberate reasoning and the place of standard logic in it.}
}
@incollection{2007229,
title = {Chapter 6 - Computational Methods for Optimal Filtering of Stochastic Signals},
editor = {A. Torokhti and P. Howlett},
series = {Mathematics in Science and Engineering},
publisher = {Elsevier},
volume = {212},
pages = {229-290},
year = {2007},
booktitle = {Computational Methods for Modelling of Nonlinear Systems},
issn = {0076-5392},
doi = {https://doi.org/10.1016/S0076-5392(07)80049-X},
url = {https://www.sciencedirect.com/science/article/pii/S007653920780049X}
}
@article{KESBERG2021110458,
title = {Personal values as motivational basis of psychological essentialism: An exploration of the value profile underlying essentialist beliefs},
journal = {Personality and Individual Differences},
volume = {171},
pages = {110458},
year = {2021},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2020.110458},
url = {https://www.sciencedirect.com/science/article/pii/S0191886920306498},
author = {Rebekka Kesberg and Johannes Keller},
keywords = {Essentialist beliefs, Human values, Psychological essentialism},
abstract = {Essentialist lay-theories can reflect a belief in genetic, social, and metaphysical determinism. These three types of essentialist beliefs are similar as they can be linked to a set of motives and each of those beliefs is related to stereotyping and prejudice. Nevertheless, the available evidence indicates that the three types of essentialist thinking are largely unrelated and it is unclear why some individuals endorse one type of essentialism and reject another. Examining the association between the endorsement of essentialist beliefs and personal values, our results based on N = 348 respondents indicate that specific value profiles build the motivational basis for specific essentialist beliefs. Specifically, conservation values are associated with belief in genetic and metaphysical determinism, while self-transcendence and self-enhancement are associated with belief in social determinism.}
}
@article{YUAN2025112659,
title = {Experimental and numerical studies on elastic vibrations of a thin spinning gear with a novel tooth contact model},
journal = {Mechanical Systems and Signal Processing},
volume = {231},
pages = {112659},
year = {2025},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2025.112659},
url = {https://www.sciencedirect.com/science/article/pii/S0888327025003607},
author = {Yunbo Yuan and Chenxin Wang and Kedar S. Vaidya and Guang Zhao and Donghua Wang and Robert G. Parker},
keywords = {Experiment, Natural frequency splitting, Deformable gear, Harmonic finite element model, Tooth contact model, Correlation},
abstract = {This work experimentally and numerically investigates elastic vibrations of a spinning spur gear with thin web and rim. Experimental measurements of radial and axial vibrations of the gear rim in a downward speed sweep test demonstrate splitting of three pairs of natural frequencies with respect to speed. Resonances are observed at intersections between the natural frequency lines and mesh frequency harmonic lines. Different from conventional thinking that spur gears have negligible out-of-plane vibrations, the tested spur gear has significant axial vibrations comparable to or even larger than radial vibrations. The experimental results, including the natural frequency splitting behaviors, specific natural frequencies, slopes of the split natural frequencies, and modal behaviors, correlate well to those from a harmonic finite element model. Vibration modes corresponding to the three pairs of split natural frequencies are standing-wave modes at zero speed (i.e., without gyroscopic effect), and they transition to traveling-wave modes for nonzero speeds (i.e., with gyroscopic effects). The measured resonances qualitatively match those from dynamic simulations of a harmonic finite element model with time-varying mesh stiffness excitation that considers distributed contact along the tooth facewidth and changing contact locations along the tooth profile. Both of these effects can result in significant changes (increases in most cases) of the resonant amplitudes for the elastic modes. Inclusion of these two effects in the mesh excitation is necessary for elastically deformable gears.}
}
@article{GILBERT2018278,
title = {Decoding intentions of self and others from fMRI activity patterns},
journal = {NeuroImage},
volume = {172},
pages = {278-290},
year = {2018},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2017.12.090},
url = {https://www.sciencedirect.com/science/article/pii/S105381191731114X},
author = {Sam J. Gilbert and Hoki Fung},
keywords = {Intentions, MVPA, fMRI},
abstract = {Previous studies using multi-voxel pattern analysis have decoded the content of participants' delayed intentions from patterns of fMRI data. Here we investigate whether this technique can be used to decode not only participants' own intentions, but also their representation of the intentions held by other people. In other words: if Sam is thinking about Hoki, can we decode the content of Hoki's intention by scanning Sam's brain? We additionally distinguished two components of intentions: action-plans versus goals, and included novel control analyses that allowed us to distinguish intending an outcome from simply expecting it to occur or simulating its consequences. Regions of frontal, parietal, and occipital cortex contained patterns from which it was possible to decode intentions of both self and other. Furthermore, crossclasification between self and other was possible, suggesting overlap between the two. Control analyses suggested that these results reflected visuo-spatial processes by which intentions were generated in our paradigm, rather than anything special about intentions per se. There was no evidence for any representation of intentions as mental states distinct from visuospatial processes involved in generating their content and/or simulating their outcomes. These findings suggest that the brain activity patterns decoded in intention-decoding fMRI studies may reflect domain-general processes rather than being intention-specific.}
}
@article{BIALEK19901227,
title = {Temporal filtering in retinal bipolar cells. Elements of an optimal computation?},
journal = {Biophysical Journal},
volume = {58},
number = {5},
pages = {1227-1233},
year = {1990},
issn = {0006-3495},
doi = {https://doi.org/10.1016/S0006-3495(90)82463-2},
url = {https://www.sciencedirect.com/science/article/pii/S0006349590824632},
author = {W. Bialek and W.G. Owen},
abstract = {Recent experiments indicate that the dark-adapted vertebrate visual system can count photons with a reliability limited by dark noise in the rod photoreceptors themselves. This suggests that subsequent layers of the retina, responsible for signal processing, add little if any excess noise and extract all the available information. Given the signal and noise characteristics of the photoreceptors, what is the structure of such an optimal processor? We show that optimal estimates of time-varying light intensity can be accomplished by a two-stage filter, and we suggest that the first stage should be identified with the filtering which occurs at the first anatomical stage in retinal signal processing, signal transfer from the rod photoreceptor to the bipolar cell. This leads to parameter-free predictions of the bipolar cell response, which are in excellent agreement with experiments comparing rod and bipolar cell dynamics in the same retina. As far as we know this is the first case in which the computationally significant dynamics of a neuron could be predicted rather than modeled.}
}
@article{CUI2024101074,
title = {AI-enhanced collective intelligence},
journal = {Patterns},
volume = {5},
number = {11},
pages = {101074},
year = {2024},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2024.101074},
url = {https://www.sciencedirect.com/science/article/pii/S2666389924002332},
author = {Hao Cui and Taha Yasseri},
keywords = {AI, collective intelligence, hybrid intelligence, multi-agent systems, human-machine networks, human-machine intelligence},
abstract = {Summary
Current societal challenges exceed the capacity of humans operating either alone or collectively. As AI evolves, its role within human collectives will vary from an assistive tool to a participatory member. Humans and AI possess complementary capabilities that, together, can surpass the collective intelligence of either humans or AI in isolation. However, the interactions in human-AI systems are inherently complex, involving intricate processes and interdependencies. This review incorporates perspectives from complex network science to conceptualize a multilayer representation of human-AI collective intelligence, comprising cognition, physical, and information layers. Within this multilayer network, humans and AI agents exhibit varying characteristics; humans differ in diversity from surface-level to deep-level attributes, while AI agents range in degrees of functionality and anthropomorphism. We explore how agents’ diversity and interactions influence the system’s collective intelligence and analyze real-world instances of AI-enhanced collective intelligence. We conclude by considering potential challenges and future developments in this field.}
}
@article{YOSHIOKA2024114985,
title = {An escort replicator dynamic with a continuous action space and its application to resource management},
journal = {Chaos, Solitons & Fractals},
volume = {185},
pages = {114985},
year = {2024},
issn = {0960-0779},
doi = {https://doi.org/10.1016/j.chaos.2024.114985},
url = {https://www.sciencedirect.com/science/article/pii/S096007792400537X},
author = {Hidekazu Yoshioka},
keywords = {Evolutionary game, Escort replicator dynamic, Kaniadakis escort function, Numerical computation, Application to sustainable resource management},
abstract = {The escort replicator dynamic (ERD) is a version of the replicator dynamic in evolutionary games where the utility-driven decision-making process is modulated due to the information costs to be paid by players. The escort function as a coefficient to distort the decision-making determines the behavior of solutions to the ERD, whereas its investigations are still not sufficient. Particularly, the ERD was investigated in finite-action settings in the previous studies, while that with a continuum of actions has not been studied well. In this paper, we formulate and analyze the ERD with a continuum of actions represented by a bounded interval. Our ERD is a partial integro-differential equation whose well-posedness is nontrivial because of specific nonlocal terms arising from the escort function. The Kaniadakis escort function is chosen as a major example of the escort function, with which we obtain the unique existence of solutions to the ERD. We also discuss cases with the other escort functions, such as the power and constant ones, and suggest that the growth and regularity behaviors of the escort function are crucial. Finally, we computationally apply the ERD to problems related to sustainable environmental and resource management.}
}
@article{TONKS2021102036,
title = {How situational competence beliefs and task value relate to inference strategies and comprehension during reading},
journal = {Learning and Individual Differences},
volume = {90},
pages = {102036},
year = {2021},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2021.102036},
url = {https://www.sciencedirect.com/science/article/pii/S104160802100073X},
author = {Stephen M. Tonks and Joseph P. Magliano and John Schwartz and Ryan D. Kopatich},
keywords = {Reading motivation, Inference generation, Reading comprehension, College students},
abstract = {In two studies, we explored the associations among situational reading-related competence beliefs and task value, inference strategies, comprehension during reading, and foundational skills in college age students. In Study 1, 93 participants from a community college completed assessments of comprehension and two types of inference strategies (elaboration and bridging), each immediately followed by a survey of their competence beliefs and task value regarding the task. Results showed that competence beliefs and task value related positively to reading comprehension. In addition, task value was positively associated with both elaborating and bridging inferences, and competence beliefs correlated positively with bridging inferences. In Study 2, we investigated these associations further in a group of 418 students studying at three different colleges. Participants completed the same assessments for competence beliefs, task value, and inference strategies, as well as assessments of comprehension and foundational reading skills. Study 2 analyses revealed that foundational reading skills were a strong predictor of both types of inferencing and also comprehension. Further, when controlling for foundational reading skills, task value predicted elaboration and bridging inferences, whereas competence beliefs did not predict inferencing, but were trending as a predictor of comprehension. Finally, we created a path model to explore mediational effects, and found that task value positively predicted comprehension performance through increased elaborations while thinking aloud.}
}
@article{ZHOU2025121668,
title = {Sparse loss-aware ternarization for neural networks},
journal = {Information Sciences},
volume = {693},
pages = {121668},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121668},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524015822},
author = {Ruizhi Zhou and Lingfeng Niu and Dachuan Xu},
keywords = {Machine learning, Ternary neural networks, Loss-aware quantization, Sparse regularization, ADMM},
abstract = {Deep neural networks (DNNs) have shown great success in machine learning tasks and widely used in many fields. However, the substantial computational and storage requirements inherent to DNNs are usually high, which poses challenges for deploying deep learning models on resource-limited devices and hindering further applications. To address this issue, the lightweight nature of neural networks has garnered significant attention, and quantization has become one of the most popular approaches to compress DNNs. In this paper, we introduce a sparse loss-aware ternarization (SLT) model for training ternary neural networks, which encodes the floating-point parameters into {−1,0,1}. Specifically, we abstract the ternarization process as an optimization problem with discrete constraints, and then modify it by applying sparse regularization to identify insignificant weights. To deal with the challenges brought by the discreteness of the model, we decouple discrete constraints from the objective function and design a new algorithm based on the Alternating Direction Method of Multipliers (ADMM). Extensive experiments are conducted on public datasets with popular network architectures. Comparisons with several state-of-the-art baselines demonstrate that SLT always attains comparable accuracy while having better compression performance.}
}
@article{LIU2022121968,
title = {Comparison of coal-to-ethanol product separation strategies},
journal = {Separation and Purification Technology},
volume = {301},
pages = {121968},
year = {2022},
issn = {1383-5866},
doi = {https://doi.org/10.1016/j.seppur.2022.121968},
url = {https://www.sciencedirect.com/science/article/pii/S1383586622015234},
author = {Daoyan Liu and Hao Lyu and Jiahao Wang and Chengtian Cui and Jinsheng Sun},
keywords = {Coal-to-ethanol, Separation strategy, Differential evolution algorithm, Parallel computation, Heat integration},
abstract = {Given China's energy structure and the limitations of bioethanol, the coal-to-ethanol (CTE) pathway, from dimethyl ether to ethanol (DMTE) via carbonylation and hydrogenation, is highly anticipated. Ethanol, methanol, methyl acetate, and ethyl acetate are the crude hydrogenation products that need to be purified, requiring at least an eight-column scheme. However, the optimization of the existing separation strategy with ethanol as the priority is unfavorable in the following aspects: it is usually plagued by tedious rules of thumb and, due to the large scale of the process, is prone to falling into local minima; pre-designed heat integration inevitably neglects the interaction of parameter optimization and heat integration; reports on alternative feasible distillation sequences are scarce in publications, let alone comparisons amongst these counterparts. Therefore, four viable separation strategies are proposed in this paper to compare with this faulted separation strategy. A self-adapting dynamic differential evolution (SADDE) algorithm, which is accelerated by parallel computation, is used to search for optimal column parameters of all the configuration options and facilitates simultaneous heat integration structure synthesis. Two strategies stand out after 3000 generations of evolution. Splitting methanol outperforms in specific steam consumption (SSC) of ethanol (1.8177), much better than the benchmark (2.4840), and splitting ethyl acetate with ethyl acetate priority has the most competitive total annual cost (TAC), 23.98% lower than the benchmark. In summary, this paper provides a reference for optimizing complex distillation systems like CTE product separation, or more specifically, the DMTE route, before the appearance of the most suitable separation strategy in demand. Furthermore, it will also serve for the CTE superstructure to further explore the optimal distillation sequence.}
}
@article{LI20234116,
title = {A call for caution in the era of AI-accelerated materials science},
journal = {Matter},
volume = {6},
number = {12},
pages = {4116-4117},
year = {2023},
issn = {2590-2385},
doi = {https://doi.org/10.1016/j.matt.2023.10.027},
url = {https://www.sciencedirect.com/science/article/pii/S2590238523005283},
author = {Kangming Li and Edward Kim and Yao Fehlis and Daniel Persaud and Brian DeCost and Michael Greenwood and Jason Hattrick-Simpers},
abstract = {It is safe to state that the field of matter has successfully entered the fourth paradigm, where machine learning and artificial intelligence (AI) are universally seen as useful, if not truly intelligent. AI’s utilization is near-ubiquitous from the prediction of novel materials to reducing computational overhead for material simulations; its value has been demonstrated time and again by both theorists and experimentalists. There is, however, a worrying trend toward large datasets and overparameterized models being all we need to accelerate science through accurate and robust machine learning systems.}
}
@article{CHILMON2020106870,
title = {Modelling and simulation considerations for an end-to-end supply chain system},
journal = {Computers & Industrial Engineering},
volume = {150},
pages = {106870},
year = {2020},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2020.106870},
url = {https://www.sciencedirect.com/science/article/pii/S0360835220305659},
author = {Barbara Chilmon and Nicoleta S. Tipi},
keywords = {Simulation, End-to-end supply chain, Systematic literature review},
abstract = {The efforts of this review paper are twofold: to provide an insightful examination of various contributions to knowledge surrounding simulation methods within an end-to-end supply chain and to guide research agenda by indicating generic elements required to model such systems using simulation. The authors examined 255 publications from 21 peer-reviewed journals in the field of an end-to-end supply chain and simulation using a systematic literature review approach. Each publication was thoroughly reviewed to capture best practices and key characteristics relative to simulation modelling techniques used in the context of complex end-to-end supply chain systems. This allowed for identification of generic elements required to model such systems, which were grouped into Structural, Computational and System Organization pillars. This research contributes to the body of knowledge by defining generic aspects of simulation modelling techniques used to study properties and attributes of complex end-to-end supply chains. The paper advances the theoretical understanding of the simulation methods used and applicability of simulation methodology in modelling end-to-end supply chain systems. The research presents the key findings from the use of simulation in modelling end-to-end supply chains and the main ways in which this modelling technique has informed research and practise.}
}
@article{MUNEEPEERAKUL2012123,
title = {The effect of scaling and connection on the sustainability of a socio-economic resource system},
journal = {Ecological Economics},
volume = {77},
pages = {123-128},
year = {2012},
issn = {0921-8009},
doi = {https://doi.org/10.1016/j.ecolecon.2012.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S092180091200081X},
author = {Rachata Muneepeerakul and Murad R. Qubbaj},
keywords = {Sustainability, Scaling, Connection, Bifurcation, Population dynamics},
abstract = {Policy makers dealing with complex systems oftentimes rely on “linear thinking.” This is understandable due to the ease and convenience offered by the simplicity of such conceptualization. Although this line of thinking may help facilitate decision making processes, it is only as defensible as the degree at which the system under consideration behaves linearly. Recent work shows that diverse properties of cities exhibit power-law relationships with population size. Such relationships may invalidate the reliance on linear thinking. Furthermore, in the era of globalization, resources and people move virtually freely through bounds of any confines used to define a system. We incorporate into a simple resource-population model the power-law scaling behavior and the influence of import and immigration, and investigate their effects on sustainable growth of communities. We explore through bifurcation analysis the different scenarios of how an unsustainable system could be sustained. Import can be effective if: the import exceeds a critical level and a critical mass of people populates the system. In contrast, increasing immigration alone can rescue the intrinsically unsustainable system, both directly through people entering the system and indirectly by increasing its harvesting ability, although critical values exist that cause the population to sharply rise or shrink.}
}
@incollection{KIRWAN202047,
title = {Chapter 3 - Strategies, planning, and design},
editor = {Christopher Kirwan and Fu Zhiyong},
booktitle = {Smart Cities and Artificial Intelligence},
publisher = {Elsevier},
pages = {47-67},
year = {2020},
isbn = {978-0-12-817024-3},
doi = {https://doi.org/10.1016/B978-0-12-817024-3.00003-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128170243000039},
author = {Christopher Kirwan and Fu Zhiyong},
keywords = {Citizen Engagement, Co-design, Data Visualization, Design Thinking, Generative Design, Information Architecture, Living Lab, Metadesign, Real-time Data, Simulation, Transdisciplinary Methods, User Experience},
abstract = {Traditional planning and design methodologies can now be augmented by new innovative tools and processes enabled by AI and smart technologies. These facilitate a more open-ended, multi-dimensional approach that incorporates diverse stakeholders to shape the potential of a collective intelligent operating system — one that best reflects the inherent nature of each unique city and urban condition. The design of smart cities must incorporate and adapt a combination of universal standards and localized policies through global civil society organizations and public-private-people partnerships established to serve the user and citizen as participants of the living city. New methods including co-design, co-creation, citizen participation and user experience (UX) feedback foster inclusive cities. Living labs and innovation hubs provide opportunities and spaces to prototype such initiatives. Transdisciplinary approaches are needed more than ever to expand our scope of inclusion to all life forms, including the rights of animals and nature as stakeholders. By applying a new combination of human and AI-enabled methods such as design thinking, machine learning and generative design, cities can now augment and improve their current state seamlessly, integrating technologies and management as an autopoietic smart city operating system.}
}
@incollection{AKAL202471,
title = {Chapter Four - AI methods in microbial metabolite determination},
editor = {Akanksha Srivastava and Vaibhav Mishra},
series = {Methods in Microbiology},
publisher = {Academic Press},
volume = {55},
pages = {71-85},
year = {2024},
booktitle = {Artificial Intelligence in Microbiology: Scope and Challenges Volume 1},
issn = {0580-9517},
doi = {https://doi.org/10.1016/bs.mim.2024.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0580951724000023},
author = {H. Ceren Akal and Rumeysa Nur Kara-Aktaş and Sebnem Ozturkoglu-Budak},
keywords = {Metabolite, Microorganism-derived, Computational, Artificial intelligence},
abstract = {The multitude of microorganism species and the amount of data requiring examination is increasing day by day, which has made it very difficult to make informative determinations and analysis to be conducted by human labour. Artificial intelligence (AI) applications are crucial in mitigating these difficulties. AI is a multidisciplinary field that tries to imitate human-like abilities through learning, analysing, problem-solving and interpretation via digital systems. It can take part in many fields where human labour is required. It is widely used in various scientific disciplines and industries, including biotechnology, microbiology, medicine, etc. Machine learning, a subbranch of AI, is one of the most frequently used auxiliary methods. Critical topics are examined rapidly and meaningfully via machine-learning such as drug production, microbial detection, antimicrobial resistance, vaccine predictions, and disease diagnoses. The aim of this chapter is to highlight the relevance of computational methods for the determination of microbial metabolites which are mainly described in literatures. These computational methods are related with the advanced AI tools of data/genome mining, multivariate data analysis, molecular networking, mathematical modelling, and optimization. These novel methods create new perspectives to the isolation and/or determination of microbial metabolites which are unwanted or essential to human health.}
}
@article{ZANUY2006330,
title = {Computational Study of the Fibril Organization of Polyglutamine Repeats Reveals a Common Motif Identified in β-Helices},
journal = {Journal of Molecular Biology},
volume = {358},
number = {1},
pages = {330-345},
year = {2006},
issn = {0022-2836},
doi = {https://doi.org/10.1016/j.jmb.2006.01.070},
url = {https://www.sciencedirect.com/science/article/pii/S0022283606001112},
author = {David Zanuy and Kannan Gunasekaran and Arthur M. Lesk and Ruth Nussinov},
keywords = {protofibril conformation, polyglutamine repeats, β-helices, structural analysis, huntingtin protein},
abstract = {The formation of fibril aggregates by long polyglutamine sequences is assumed to play a major role in neurodegenerative diseases such as Huntington. Here, we model peptides rich in glutamine, through a series of molecular dynamics simulations. Starting from a rigid nanotube-like conformation, we have obtained a new conformational template that shares structural features of a tubular helix and of a β-helix conformational organization. Our new model can be described as a super-helical arrangement of flat β-sheet segments linked by planar turns or bends. Interestingly, our comprehensive analysis of the Protein Data Bank reveals that this is a common motif in β-helices (termed β-bend), although it has not been identified so far. The motif is based on the alternation of β-sheet and helical conformation as the protein sequence is followed from the N to the C termini (β-αR-β-polyPro-β). We further identify this motif in the ssNMR structure of the protofibril of the amyloidogenic peptide Aβ1-40. The recurrence of the β-bend suggests a general mode of connecting long parallel β-sheet segments that would allow the growth of partially ordered fibril structures. The design allows the peptide backbone to change direction with a minimal loss of main chain hydrogen bonds. The identification of a coherent organization beyond that of the β-sheet segments in different folds rich in parallel β-sheets suggests a higher degree of ordered structure in protein fibrils, in agreement with their low solubility and dense molecular packing.}
}
@article{STEFIK1989241,
title = {Computation and cognition: Toward a foundation of cognitive science: Z.W. Pylyshyn, (MIT Press, Cambridge, MA, 1986); 292 pages, $33.75 (hardcover), $9.95 (paperback)},
journal = {Artificial Intelligence},
volume = {38},
number = {2},
pages = {241-247},
year = {1989},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(89)90061-1},
url = {https://www.sciencedirect.com/science/article/pii/0004370289900611},
author = {Mark Stefik}
}
@article{PADGETT1994185,
title = {Computational intelligence standards: motivation, current activities and progress},
journal = {Computer Standards & Interfaces},
volume = {16},
number = {3},
pages = {185-203},
year = {1994},
issn = {0920-5489},
doi = {https://doi.org/10.1016/0920-5489(94)90011-6},
url = {https://www.sciencedirect.com/science/article/pii/0920548994900116},
author = {Mary Lou Padgett and Walter J Karplus and Steve Deiss and Robert Shelton},
keywords = {Terminology, Artificial neural networks, Specification, Virtual reality},
abstract = {Computational Intelligence is an emerging technology of keen interest to the developers of computer standards and interfaces. Coherent communications among the diverse set of users of computational AI is necessary for the protection of all parties and can help further the serious development of artificial neural networks, fuzzy systems, evolutionary programming and virtual reality. Current activities of the IEEE Neural Networks Council Standards Committee encompass all these areas, emphasizing the development of glossaries and symbologies, performance measures and interface standards for these interrelated fields. Progress toward these goals is described in this paper.}
}
@article{BRADLEY2016277,
title = {Pilot Testing the Debriefing for Meaningful Learning Evaluation Scale},
journal = {Clinical Simulation in Nursing},
volume = {12},
number = {7},
pages = {277-280},
year = {2016},
issn = {1876-1399},
doi = {https://doi.org/10.1016/j.ecns.2016.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S1876139916000104},
author = {Cynthia Sherraden Bradley and Kristina Thomas Dreifuerst},
keywords = {DML, debriefing, effective briefing, debriefing evaluation, measurement},
abstract = {Background
Debriefing for Meaningful Learning (DML), an evidence-based debriefing method, promotes thinking like a nurse through reflective learning. Despite widespread adoption of DML, little is known about how well it is implemented. To assess the effectiveness of DML implementation, an evaluative rubric was developed and tested.
Sample
Three debriefers who had been trained to use DML at least 1 year previously, submitted five recorded debriefings each for evaluation.
Methods
Three raters who were experts in DML scored each of the 15 recorded debriefing session using DML Evaluation Scale (DMLES). Observable behaviors were scored with binary options. These raters also assessed the items in the DMLES for content validity.
Results
Cronbach's alpha, intraclass correlation coefficients, and Content Validity Index scores were calculated to determine reliability and validity.
Conclusion
Use of DMLES could support quality improvement, teacher preparation, and faculty development. Future testing is warranted to investigate the relationship between DML implementation and clinical reasoning.}
}
@article{CASTROSCHEZ201465,
title = {Experience applying language processing techniques to develop educational software that allow active learning methodologies by advising students},
journal = {Journal of Network and Computer Applications},
volume = {41},
pages = {65-79},
year = {2014},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2013.10.011},
url = {https://www.sciencedirect.com/science/article/pii/S1084804513002166},
author = {J.J. Castro-Schez and M.A. Redondo and F. Jurado and J. Albusac},
keywords = {Environment for active learning, Formal languages techniques, Automatic assessment},
abstract = {This paper is focused on those systems that allow students to build their own knowledge by providing them with feedback regarding their actions while performing a problem based learning activity or while making changes to problem statements, so that a higher order thinking skill can be achieved. This feedback is the consequence of an automatic assessment. Particularly, we propose a method that makes use of Language Processor techniques for developing these kinds of systems. This method could be applied in subjects in which problem statements and solutions can be formalized by mean of a formal language and the problems can be solved in an algorithmic way. The method has been used to develop a number of tools that are partially described in this paper. Thus, we show that our approach is applicable in addressing the development of the aforementioned systems. One of these tools (a virtual laboratory for language processing) has been in use for several years in order to support home assignments. The data collected for these years are presented and analyzed in this paper. The results of the analysis confirm that this tool is effective in facilitating the achievement of learning outcomes.}
}
@article{UBAN2021480,
title = {An emotion and cognitive based analysis of mental health disorders from social media data},
journal = {Future Generation Computer Systems},
volume = {124},
pages = {480-494},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.05.032},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21001825},
author = {Ana-Sabina Uban and Berta Chulvi and Paolo Rosso},
keywords = {Mental health disorders, Early risk prediction, Emotions, Cognitive styles, Deep learning, Social media},
abstract = {Mental disorders can severely affect quality of life, constitute a major predictive factor of suicide, and are usually underdiagnosed and undertreated. Early detection of signs of mental health problems is particularly important, since unattended, they can be life-threatening. This is why a deep understanding of the complex manifestations of mental disorder development is important. We present a study of mental disorders in social media, from different perspectives. We are interested in understanding whether monitoring language in social media could help with early detection of mental disorders, using computational methods. We developed deep learning models to learn linguistic markers of disorders, at different levels of the language (content, style, emotions), and further try to interpret the behavior of our models for a deeper understanding of mental disorder signs. We complement our prediction models with computational analyses grounded in theories from psychology related to cognitive styles and emotions, in order to understand to what extent it is possible to connect cognitive styles with the communication of emotions over time. The final goal is to distinguish between users diagnosed with a mental disorder and healthy users, in order to assist clinicians in diagnosing patients. We consider three different mental disorders, which we analyze separately and comparatively: depression, anorexia, and self-harm tendencies.}
}
@article{MEICHENBAUM1969101,
title = {The effects of instructions and reinforcement on thinking and language behavior of schizophrenics},
journal = {Behaviour Research and Therapy},
volume = {7},
number = {1},
pages = {101-114},
year = {1969},
issn = {0005-7967},
doi = {https://doi.org/10.1016/0005-7967(69)90054-0},
url = {https://www.sciencedirect.com/science/article/pii/0005796769900540},
author = {Donald H. Meichenbaum},
abstract = {Six experimental groups and 2 control groups (N=48) were used to investigate the relative effectiveness of prolonged training of schizophrenics with contingent social and token reinforcement on (a) the level of abstraction as measured on a proverbs task, (b) the percentage of “sick talk” (% ST) emitted in a structured interview, (c) both verbal response classes of proverb abstraction and % ST. Prior to treatment, schizophrenic Ss compared with 20 nonpsychiatric hospitalized medical patients were significantly inferior on the proverbs task and emitted five times more ST in a structured interview. The results indicated that the experimental treatments were effective in decreasing % ST and increasing abstraction to proverbs with token reinforcement being most effective. Evidence for response and stimulus generalization was obtained.}
}
@article{CAMARGO2022496,
title = {Existence, Hypotheses and Categories in Knowledge Representation},
journal = {Procedia Computer Science},
volume = {213},
pages = {496-503},
year = {2022},
note = {2022 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: The 13th Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.096},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922017872},
author = {Eduardo Camargo and Eduardo Yuji Sakabe and Ricardo Gudwin},
keywords = {Knowledge Representation, Cognitive Architecture, Cognitive Semiotics, Artificial Intelligent Agent},
abstract = {Cognitive architectures employ different means for knowledge representation. In this work, we describe how the Cognitive Systems Toolkit (CST), a toolkit for the construction of cognitive architectures addresses the issue of knowledge representation, by introducing the notion of a computational idea, as being an abstract and generic building block for representing multiple different pieces of knowledge. We particularly address how computational ideas can be used to represent both facts that really happened at an environment and just hypothesis that are not to be considered as being a part of existence, explaining how these are instances of general categories. At the end, we provide different examples to illustrate the subtle differences that are possible to be represented using this knowledge representation scheme.}
}
@article{DUBEY2020118,
title = {Understanding exploration in humans and machines by formalizing the function of curiosity},
journal = {Current Opinion in Behavioral Sciences},
volume = {35},
pages = {118-124},
year = {2020},
note = {Curiosity (Explore vs Exploit)},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2020.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S2352154620301108},
author = {Rachit Dubey and Thomas L Griffiths},
abstract = {Recent work in machine learning has demonstrated the benefits of providing artificial agents with a sense of curiosity—a form of intrinsic reward that supports exploration. Two strategies have emerged for defining these rewards: favoring novelty and pursuing prediction errors. Psychological theories of curiosity have also emphasized these two factors. We show how these two literatures can be connected by understanding the function of curiosity, which requires thinking about the abstract computational problem that both humans and machines face as they explore their world.}
}
@article{LIN2024200448,
title = {Maximizing the spread of information through content optimization},
journal = {Intelligent Systems with Applications},
volume = {24},
pages = {200448},
year = {2024},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2024.200448},
url = {https://www.sciencedirect.com/science/article/pii/S2667305324001224},
author = {Lei Lin and Yihua Du and Shibo Zhao and Wenkang Jiang and Qirui Tang and Li Xu},
keywords = {Computational social science, Human-in-the-loop, System simulation and optimization, Computational journalism, Computational advertisement},
abstract = {As data-driven prediction models advance, an increasing number of people are enjoying news personalized to their interests. The primary problem such recommendation models solve is to precisely match information with users and, in so doing, ensure that news spreads with greater efficiency. However, these techniques only help the media platform; they do not help those who produce the news. Hence, we devised a propagation framework based on a human-in-the-loop simulation that helps content authors maximize the spread of their messages through social networks. The framework works by acting on feedback provided by the simulation model. Additionally, the spread of information is formulated as a multi-objective optimization problem in which propagation is data-driven and simulated with machine learning techniques that leverage data on the historical behaviors of users. We additionally describe an implementation for this framework as an example of how the framework might be used in real life. On the practical side, the implementation uses text data from a blog to simulate the message's propagation, while, from a technical point of view, the multi-objective optimization problem is divided into an information retrieval problem and an integer programming problem, the results of which are fed back into the content editor as content operation strategies. A case study with the Sina Weibo microblog site not only validates the framework but also provides practitioners with insights into how to maximize the spread of information through social networking platforms. The results show that the proposed propagation framework is capable of increasing retweets by 7.9575 %. As an interesting aside, our experiments also show that the Weibo retweet lottery is both popular and a highly effective mechanism for increasing reposts.}
}
@article{1995146,
title = {95/02164 Sequential pressure-based Navier-Stokes algorithms on SIMD computers: Computational issues},
journal = {Fuel and Energy Abstracts},
volume = {36},
number = {2},
pages = {146},
year = {1995},
issn = {0140-6701},
doi = {https://doi.org/10.1016/0140-6701(95)93829-X},
url = {https://www.sciencedirect.com/science/article/pii/014067019593829X}
}
@article{PATON200263,
title = {Process, structure and context in relation to integrative biology},
journal = {Biosystems},
volume = {64},
number = {1},
pages = {63-72},
year = {2002},
issn = {0303-2647},
doi = {https://doi.org/10.1016/S0303-2647(01)00176-9},
url = {https://www.sciencedirect.com/science/article/pii/S0303264701001769},
author = {Ray Paton},
keywords = {Ecology, Proteins, Category theory, Modelling, Function, Liver},
abstract = {This paper seeks to provide some integrative tools of thought regarding biological function related to ideas of process, structure, and context. The incorporation of linguistic and mathematical thinking is discussed within the context of managing thinking about natural systems as described by Robert Rosen. Examples from ecology, protein networks, and liver function are introduced to illustrate key ideas. It is hoped that these tools of thought, and the further work needed to mobilise such ideas, will continue to address a number of issues raised and pursued by Michael Conrad, such as the seed-germination model and vertical information processing.}
}
@article{ZIA2022108066,
title = {SoFTNet: A concept-controlled deep learning architecture for interpretable image classification},
journal = {Knowledge-Based Systems},
volume = {240},
pages = {108066},
year = {2022},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.108066},
url = {https://www.sciencedirect.com/science/article/pii/S095070512101145X},
author = {Tehseen Zia and Nauman Bashir and Mirza Ahsan Ullah and Shakeeb Murtaza},
keywords = {Interpretability, Concepts, KNN, Explanation satisfaction},
abstract = {Interpreting deep learning (DL)-based computer vision models is challenging due to the complexity of internal representations. Most recent techniques for rendering DL learning outcomes interpretable operate on low-level features rather than high-level concepts. Methods that explicitly incorporate high-level concepts do so through a determination of the relevancy of user-defined concepts or else concepts extracted directly from the data. However, they do not leverage the potential of concepts to explain model predictions. To overcome this challenge, we introduce a novel DL architecture – the Slow/Fast Thinking Network (SoFTNet) – enabling users to define/control high-level features and utilize them to perform image classification predicatively. We draw inspiration from the dual-process theory of human thought processes, decoupling low-level, fast & non-transparent processing from high-level, slow & transparent processing. SoFTNet hence uses a shallow convolutional neural network for low-level processing in conjunction with a memory network for high-level concept-based reasoning. We conduct experiments on the CUB-200-2011 and STL-10 datasets and also present a novel concept-based deep K-nearest neighbor approach for baseline comparisons. Our experiments show that SoFTNet achieves comparable performance to state-of-art non-interpretable models and outperforms comparable interpretative methods.}
}
@article{FORREST19901,
title = {Emergent computation: Self-organizing, collective, and cooperative phenomena in natural and artificial computing networks: Introduction to the proceedings of the ninth annual CNLS conference},
journal = {Physica D: Nonlinear Phenomena},
volume = {42},
number = {1},
pages = {1-11},
year = {1990},
issn = {0167-2789},
doi = {https://doi.org/10.1016/0167-2789(90)90063-U},
url = {https://www.sciencedirect.com/science/article/pii/016727899090063U},
author = {Stephanie Forrest}
}
@article{SCHINCKUS20094415,
title = {Economic uncertainty and econophysics},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {388},
number = {20},
pages = {4415-4423},
year = {2009},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2009.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0378437109005494},
author = {Christophe Schinckus},
keywords = {Econophysics, Uncertainty, Economics, Keynes, Knight, Hayek},
abstract = {The objective of this paper is to provide a methodological link between econophysics and economics. I will study a key notion of both fields: uncertainty and the ways of thinking about it developed by the two disciplines. After having presented the main economic theories of uncertainty (provided by Knight, Keynes and Hayek), I show how this notion is paradoxically excluded from the economic field. In economics, uncertainty is totally reduced by an a priori Gaussian framework—in contrast to econophysics, which does not use a priori models because it works directly on data. Uncertainty is then not shaped by a specific model, and is partially and temporally reduced as models improve. This way of thinking about uncertainty has echoes in the economic literature. By presenting econophysics as a Knightian method, and a complementary approach to a Hayekian framework, this paper shows that econophysics can be methodologically justified from an economic point of view.}
}
@article{SEYMOUR2020117212,
title = {Hierarchical models of pain: Inference, information-seeking, and adaptive control.},
journal = {NeuroImage},
volume = {222},
pages = {117212},
year = {2020},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2020.117212},
url = {https://www.sciencedirect.com/science/article/pii/S1053811920306984},
author = {Ben Seymour and Flavia Mancini},
keywords = {Pain, Nociception, Information theory, Reinforcement learning, Optimal control, Predictive coding, Epistemic value, Free energy principle, Endogenous modulation},
abstract = {Computational models of pain consider how the brain processes nociceptive information and allow mapping neural circuits and networks to cognition and behaviour. To date, they have generally have assumed two largely independent processes: perceptual inference, typically modelled as an approximate Bayesian process, and action control, typically modelled as a reinforcement learning process. However, inference and control are intertwined in complex ways, challenging the clarity of this distinction. Here, we consider how they may comprise a parallel hierarchical architecture that combines inference, information-seeking, and adaptive value-based control. This sheds light on the complex neural architecture of the pain system, and takes us closer to understanding from where pain ’arises’ in the brain.}
}
@article{KUNZE2024249,
title = {Bioinspired approaches for resource-efficient material flow in production – an innovative actuator concept for peristaltic-based transport},
journal = {Procedia CIRP},
volume = {125},
pages = {249-254},
year = {2024},
note = {CIRP BioM 2024},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.08.043},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124003974},
author = {Henriette Kunze and Marcel Lorenz},
keywords = {tensegrity, biotensegrity, assembly technologies},
abstract = {In automated material flow, in a wide variety of areas, the primary goal is usually to handle a wide spectrum of components as time- and cost-efficiently as possible. In view of the current and future challenges in industrial production, it is becoming apparent that ecological requirements are becoming increasingly important in automation solutions. For example, in form of resource efficiency, transformability and material efficiency. In this context, especially materials handling technology is subject of various optimization approaches, as no value is added to the part handled. The question: "How does material flow occur in nature?" thus offers biologically inspired approaches to thinking about transport in the industrial sector. This paper first presents a selection of concepts or existing mechanisms that are adaptable in materials- handling technology and have been developed based on a biological model. In the second part of this paper, a new concept is presented that is modeled on peristalsis as a transport mechanism. The approach presented here uses tensegrity-structures for assembly, which are characterized by their high material efficiency and flexibility. The transport movement is achieved by peristaltic typical contraction or relaxation of the respective structure parts.}
}
@article{BAICANG2025129857,
title = {Multi-modal information fusion for multi-task end-to-end behavior prediction in autonomous driving},
journal = {Neurocomputing},
volume = {634},
pages = {129857},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129857},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225005296},
author = {Guo Baicang and Liu Hao and Yang Xiao and Cao Yuan and Jin Lisheng and Wang Yinlin},
keywords = {Autonomous driving, Multi-modal fusion, Vehicle behavior prediction, End-to-End, Attention mechanism},
abstract = {Behavior prediction in autonomous driving is increasingly achieved through end-to-end frameworks that predict vehicle states from multi-modal information, streamlining decision-making and enhancing robustness in time-varying road conditions. This study proposes a novel multi-modal information fusion-based, multi-task end-to-end model that integrates RGB images, depth maps, and semantic segmentation data, enhancing situational awareness and predictive precision. Utilizing a Vision Transformer (ViT) for comprehensive spatial feature extraction and a Residual-CNN-BiGRU structure for capturing temporal dependencies, the model fuses spatiotemporal features to predict vehicle speed and steering angle with high precision. Through comparative, ablation, and generalization tests on the Udacity and self-collected datasets, the proposed model achieves steering angle prediction errors of MSE 0.012 rad, RMSE 0.109 rad, and MAE 0.074 rad, and speed prediction errors of MSE 0.321 km/h, RMSE 0.567 km/h, and MAE 0.373 km/h, outperforming existing driving behavior prediction models. Key contributions of this study include the development of a channel difference attention mechanism and advanced spatiotemporal feature fusion techniques, which improve predictive accuracy and robustness. These methods effectively balance computational efficiency and predictive performance, contributing to practical advancements in driving behavior prediction.}
}
@article{YUZGEC2025113169,
title = {Accelerated opposition learning based chaotic single candidate optimization algorithm: A new alternative to population-based heuristics},
journal = {Knowledge-Based Systems},
volume = {314},
pages = {113169},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113169},
url = {https://www.sciencedirect.com/science/article/pii/S0950705125002163},
author = {Ugur Yuzgec},
keywords = {Opposition learning, Chaotic, Single candidate, Engineering design, Benchmark},
abstract = {This study considers the Single Candidate Optimizer (SCO) as an alternative to population-based heuristics, that is faster than them. Although the SCO algorithm is a fast single-candidate-based heuristic, it has certain limitations. To overcome these limitations and enhance the search performance of SCO, several solutions were proposed in this study. First, owing to the single-candidate nature of the SCO, the initial solution position can play a critical role. To compensate for this, an accelerated opposition-learning mechanism was integrated into the SCO. In addition, instead of the equation that is active when the number of unsuccessful improvement attempts is reached in the SCO structure, a mutation operator including chaotic functions (Levy, Gauss, and Cauchy) has been incorporated into the algorithm. Again, equations based on new approaches were added to the SCO algorithm to update the position of the candidate solution during the exploration and exploitation phases. Finally, the standard boundary value control mechanism is replaced with a more effective one. The algorithm developed in this study is named Accelerated Opposition Learning based Chaotic Single Candidate Optimizer (AccOppCSCO), inspired by the accelerated opposition learning mechanism and the mutation operator involving chaotic behaviors. The search capability of the proposed AccOppCSCO algorithm was first analyzed using four different methods: convergence, search history, trajectory, and computational complexity. The effectiveness of the mechanisms used in the AccOppCSCO algorithm for four different two-dimensional benchmark problems from the IEEE Congress on Evolutionary Computation 2014 (CEC2014) package was demonstrated. Subsequently, the performance of the proposed AccOppCSCO algorithm was evaluated on the CEC2014 and IEEE Congress on Evolutionary Computation 2020 (CEC2020) benchmark problems with different dimensions. The results show that the AccOppCSCO algorithm works effectively in the CEC2014 and CEC2020 test sets and offers better optimization results than SCO. The AccOppCSCO algorithm ranked first in the overall evaluation of the 30-dimensional CEC2014 comparison results with State of the Art (SOTA) heuristics from the literature. Finally, for ten different engineering design problems, the AccOppCSCO algorithm was analyzed and compared with the original SCO and other SOTA heuristics. The results show that AccOppCSCO is effective for engineering design problems. This emphasizes that the algorithm can work effectively on a wide range of problems and can be used in various applications. The source code of the AccOppCSCO algorithm for the CEC2014 benchmark suite is publicly available at https://github.com/uguryuzgec/AccOppCSCO.}
}
@article{PIQUEIRA2016271,
title = {A comparison of LMC and SDL complexity measures on binomial distributions},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {444},
pages = {271-275},
year = {2016},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2015.10.040},
url = {https://www.sciencedirect.com/science/article/pii/S0378437115008882},
author = {José Roberto C. Piqueira},
keywords = {Binomial, Complexity, Information, Measure, Probability},
abstract = {The concept of complexity has been widely discussed in the last forty years, with a lot of thinking contributions coming from all areas of the human knowledge, including Philosophy, Linguistics, History, Biology, Physics, Chemistry and many others, with mathematicians trying to give a rigorous view of it. In this sense, thermodynamics meets information theory and, by using the entropy definition, López-Ruiz, Mancini and Calbet proposed a definition for complexity that is referred as LMC measure. Shiner, Davison and Landsberg, by slightly changing the LMC definition, proposed the SDL measure and the both, LMC and SDL, are satisfactory to measure complexity for a lot of problems. Here, SDL and LMC measures are applied to the case of a binomial probability distribution, trying to clarify how the length of the data set implies complexity and how the success probability of the repeated trials determines how complex the whole set is.}
}
@article{SAMPSON20052095,
title = {Comments on: “Pore network simulation of fluid inbibition into paper during coating: II. Characterization of paper's morphology and computation of its effective permeability tensor” by Ghassemzadeh and Sahimi [Chemical Engineering Science 59(2004) 2265–2280]},
journal = {Chemical Engineering Science},
volume = {60},
number = {7},
pages = {2095},
year = {2005},
issn = {0009-2509},
doi = {https://doi.org/10.1016/j.ces.2004.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0009250904009327},
author = {W.W. Sampson and C.T.J. Dodson}
}
@article{BESHKOV2024109370,
title = {Topological structure of population activity in mouse visual cortex encodes densely sampled stimulus rotations},
journal = {iScience},
volume = {27},
number = {4},
pages = {109370},
year = {2024},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2024.109370},
url = {https://www.sciencedirect.com/science/article/pii/S2589004224005911},
author = {Kosio Beshkov and Marianne Fyhn and Torkel Hafting and Gaute T. Einevoll},
keywords = {Neuroscience, Sensory neuroscience, Cognitive neuroscience},
abstract = {Summary
The primary visual cortex is one of the most well understood regions supporting the processing involved in sensory computation. Following the popularization of high-density neural recordings, it has been observed that the activity of large neural populations is often constrained to low dimensional manifolds. In this work, we quantify the structure of such neural manifolds in the visual cortex. We do this by analyzing publicly available two-photon optical recordings of mouse primary visual cortex in response to visual stimuli with a densely sampled rotation angle. Using a geodesic metric along with persistent homology, we discover that population activity in response to such stimuli generates a circular manifold, encoding the angle of rotation. Furthermore, we observe that this circular manifold is expressed differently in subpopulations of neurons with differing orientation and direction selectivity. Finally, we discuss some of the obstacles to reliably retrieving the truthful topology generated by a neural population.}
}
@article{JOO2024226,
title = {Teaching and Learning Model for Artificial Intelligence Education},
journal = {Procedia Computer Science},
volume = {239},
pages = {226-233},
year = {2024},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2023},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.06.166},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924014091},
author = {Kil Hong Joo and Nam Hun Park},
keywords = {Artificial intelligence education, Lower grades in elementary school},
abstract = {AI education aims to nurture convergence talents equipped with various knowledge and AI capabilities. However, the educational programs developed so far are designed for less than 10 sessions. This is insufficient time for students to understand artificial intelligence algorithms, utilize the learned principles of artificial intelligence, and expand by converging with various knowledge. Since the developmental characteristics of students in the lower grades of elementary school are different, it is difficult to apply them as they are. Therefore, it is necessary to study the following AI education teaching and learning methods suitable for lower grade students. In this study, we develop a teaching system design model for artificial intelligence-based subject convergence education for elementary school students in the lower grades, and based on this, design and apply an artificial intelligence-based subject convergence education program. Experiments were conducted with 47 second-year elementary school students, and students’ responses were better in the AI-based convergence education program than in general subject classes in terms of interest, understanding, and expectations for classes.}
}
@article{XU2025101288,
title = {Multi-criteria feature selection on maritime emission abatement alternatives},
journal = {Research in Transportation Business & Management},
volume = {59},
pages = {101288},
year = {2025},
issn = {2210-5395},
doi = {https://doi.org/10.1016/j.rtbm.2025.101288},
url = {https://www.sciencedirect.com/science/article/pii/S2210539525000033},
author = {Kaiqi Xu and Mario P. Brito and Patrick Beullens},
keywords = {Analytic hierarchy process, Multi-criteria decision making, Emission reduction, Technology selection, Sustainability port},
abstract = {To comply with MARPOL Annex VI, stakeholders face multi-criteria decision-making in technology selection. This study provides an Analytic Hierarchy Process (AHP)-based method to support stakeholders in selecting emission abatement technology aligned with their business demands, taking into account a range of sustainability criteria. The analysis reveals that there is no one-size-fits-all solution to technology selection. Low-sulfur fuel oil and LNG are preferable alternative fuels for large-size commercial (long-sea shipping) vessels due to their better capacity storage savings, while a dual-fuel engine offers flexibility in fuel changeover. Electrification offers zero-emission performance, lower noise levels, and peak energy solutions benefiting cruise ships and short-distance or harbor boats, but tugboats need greener diesel to meet performance criteria. From a policy perspective, our model provides insights into the effects of green transition processes in Norway and Singapore on stakeholders' decisions with respect to port infrastructure and land transport at the portside.}
}
@article{ALEXIOU2009623,
title = {Exploring the neurological basis of design cognition using brain imaging: some preliminary results},
journal = {Design Studies},
volume = {30},
number = {6},
pages = {623-647},
year = {2009},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2009.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X09000313},
author = {K. Alexiou and T. Zamenopoulos and J.H. Johnson and S.J. Gilbert},
keywords = {design cognition, problem solving, design problems, research methods, cognitive neuroscience},
abstract = {The paper presents a pilot interdisciplinary research study carried out as a step towards understanding the neurological basis of design thinking. The study involved functional magnetic resonance imaging (fMRI) of volunteers while performing design and problem-solving tasks. The findings suggest that design and problem solving involve distinct cognitive functions associated with distinct brain networks. The paper introduces the methodology, presents the findings, and discusses the potential role of brain imaging in design research.}
}
@article{BARNES2021,
title = {Gene Expression and Data Analysis Pipeline Using Cancer BioPortal in the Classroom},
journal = {Journal of Microbiology & Biology Education},
volume = {22},
number = {1},
year = {2021},
issn = {1935-7877},
doi = {https://doi.org/10.1128/jmbe.v22i1.2313},
url = {https://www.sciencedirect.com/science/article/pii/S1935787721000721},
author = {Chassidy N. Barnes and Blake P. Johnson and Stefanie W. Leacock and Ruben M. Ceballos and Lori L. Hensley and Nathan S. Reyna},
abstract = {At institutions with an emphasis on authentic research experiences as an integral part of the biology curriculum, COVID created a huge challenge for course instructors whose learning objectives were designed for such experiences. Moving such laboratory experiences online when remote learning became necessary has resulted in a new model for CUREs that utilizes free online databases to provide not only a novel research experience for students, but also the opportunity to engage in big data analysis.
ABSTRACT
At institutions with an emphasis on authentic research experiences as an integral part of the biology curriculum, COVID created a huge challenge for course instructors whose learning objectives were designed for such experiences. Moving such laboratory experiences online when remote learning became necessary has resulted in a new model for CUREs that utilizes free online databases to provide not only a novel research experience for students, but also the opportunity to engage in big data analysis. Cancer BioPortal (cBioPortal) is an open-access collective cancer research resource for storing and exploring clinical, genomic, proteomic, and transcriptomic data. cBioPortal eliminates the computational barrier of interpreting complex genomic data by providing easily understandable visualization that can be interpreted and translated into relevant biological insights. Because no prior computational knowledge is required, cBioPortal is an ideal educational tool for either in-person or distance learning environments. We developed a pedagogical approach, video tutorials, and data analysis workflows centered on using cBioPortal. Pedagogically, students develop an initial research outline that is continually updated and graded throughout the project. Progress during the project or course is assessed by a series of student presentations that are 5 to 15 min in length and are aimed at explaining the approach used in data acquisition, interpretation of the data, and relevance to the initial hypothesis. While cancer-specific, this analysis platform appeals to a wide range of classes and student interests. Further, the project has been successfully done both as an independent research experience and as part of a virtual class-based research project.}
}
@incollection{POULSEN201543,
title = {Chapter 3 - Better Concurrency and SIMD on HBM},
editor = {James Reinders and Jim Jeffers},
booktitle = {High Performance Parallelism Pearls},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {43-67},
year = {2015},
isbn = {978-0-12-802118-7},
doi = {https://doi.org/10.1016/B978-0-12-802118-7.00003-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128021187000030},
author = {Jacob Weismann Poulsen and Per Berg and Karthik Raman},
keywords = {HIROMB-BOOS-Model, Danish Meteorological Institute, Operational ocean models, Parallelization, Validation, Verification, OpenMP, MPI, Intel® VTune™ Amplifier, Performance Monitoring Unit, Vectorization, Nesting, Scaling, Cache layout},
abstract = {This chapter describes some work that is being performed at the Danish Meteorological Institute for optimization of a 3D ocean circulation model code with roots back to the 1990s and which is known as the HIROMB-BOOS-Model. The optimization of this large code is instructive agreeing with the authors’ strong belief that the best performance only comes with a focus on architecting for it starting with appropriate data structures. The thinking process and techniques used in this chapter have wide applicability: focus on data locality and then apply threading and vectorization techniques. This way of thinking should be on the mind of every programmer working to design a high-performance application.}
}
@article{DONALDSON1995301,
title = {Building object-oriented systems: An introduction from concepts to implementation in c++: R. E. Callan, Computational Mechanics Publications, Southampton, UK, 1994. ISBN 1-85312-340-4. 304 pp. £47.00},
journal = {Artificial Intelligence in Engineering},
volume = {9},
number = {4},
pages = {301},
year = {1995},
note = {Selected Papers from the 1994 Japan/Korea Joint Conference on Expert Systems},
issn = {0954-1810},
doi = {https://doi.org/10.1016/0954-1810(95)90016-0},
url = {https://www.sciencedirect.com/science/article/pii/0954181095900160},
author = {Iain Donaldson}
}
@article{GORDON2018273,
title = {Healthier Choices in School Cafeterias: A Systematic Review of Cafeteria Interventions},
journal = {The Journal of Pediatrics},
volume = {203},
pages = {273-279.e2},
year = {2018},
issn = {0022-3476},
doi = {https://doi.org/10.1016/j.jpeds.2018.07.031},
url = {https://www.sciencedirect.com/science/article/pii/S0022347618309363},
author = {Katelyn Gordon and Linda Dynan and Robert Siegel},
keywords = {school cafeteria, behavioral economics, childhood obesity, food selection},
abstract = {Objective
To describe school cafeteria interventions in terms of a behavioral economics scheme and to assess which system is more likely to be effective in improving food selection or consumption.
Study design
With this systematic review, we categorize cafeteria interventions using the behavioral economics theory of Kahneman into system 1 (fast and intuitive thinking) and system 2 (slow and cognitively demanding) or mixed (having elements of system 1 and system 2). Pertinent studies were identified from review of the literature of interventions performed in school and cafeteria settings in children grades K-12 within the past 5 years (2012-2017) at time of search.
Results
In all, 48 of 978 studies met inclusion criteria. By defining success as a 30% improvement in a desired outcome or statistically significant reduction in body mass index, 89% of system 1, 67% of mixed (had both system 1 and 2 elements), and only 33% of system 2 interventions were successful.
Conclusions
This review found successful system 1 type school cafeteria interventions to be more common than system 2 type interventions and system 2 type interventions are less effective than system 1.}
}
@article{KIM2024110348,
title = {Hierarchical aerial offload computing algorithm based on the Stackelberg-evolutionary game model},
journal = {Computer Networks},
volume = {245},
pages = {110348},
year = {2024},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2024.110348},
url = {https://www.sciencedirect.com/science/article/pii/S1389128624001804},
author = {Sungwook Kim},
keywords = {Aerial access networks, High-altitude platforms, Unmanned aerial vehicles, Stackelberg-evolutionary game, Multi-objective bargaining solution},
abstract = {Aerial access networks have been envisioned as a promising 6 G technology to enhance the service experience in underserved areas where terrestrial base stations do not exist. In such scenarios, a hierarchical model of high-altitude platforms (HAPs) and unmanned aerial vehicles (UAVs) is considered to provide aerial computing services for ground Internet of Things (IoT) devices. In this study, we investigate a hierarchical aerial computing system to optimally orchestrate the limited computation resources in both HAPs and UAVs. For offloading services, we formulate a joint resource allocation problem to maximize service satisfaction for terrestrial IoT devices. To solve this problem, we employ the ideas of game theory with centralized decision and decentralized execution. Through the Stackelberg-evolutionary game model, the HAP works as a leader, and selects its price strategy based on the evolutionary learning process. As followers, individual UAVs make decisions to partially offload their computing tasks by considering different objectives. According to the interactive control paradigm, our proposed method can get reciprocal advantages for HAPs, UAVs, and ground IoT devices while adaptively handling dynamic aerial network conditions. Finally, extensive simulation results verify the efficiency of our proposed algorithm to increase the usability of edge servers’ computational resources. Compared with other existing state-of-the-art aerial network offloading protocols, we can improve the profits of system throughput, resource usability and UAV fairness up to 10 %, 10 %, and 15 %, respectively.}
}
@article{HAAS2024110900,
title = {Models vetted against prediction error and parameter sensitivity standards can credibly evaluate ecosystem management options},
journal = {Ecological Modelling},
volume = {498},
pages = {110900},
year = {2024},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2024.110900},
url = {https://www.sciencedirect.com/science/article/pii/S0304380024002886},
author = {Timothy C. Haas},
keywords = {Model vetting, Model credibility, Ecosystem management, Parameter sensitivity, Robust statistical estimators, High performance computing},
abstract = {A new standard for assessing model credibility is developed. This standard consists of parameter estimation, prediction error assessment, and a parameter sensitivity analysis that is driven by outside individuals who are skeptical of the model’s credibility (hereafter, skeptics). Ecological/environmental models that have a one-step-ahead prediction error rate that is better than naive forecasting — and are not excessively sensitive to small changes in their parameter values are said here to be vetted. A procedure is described that can perform this assessment on any model being evaluated for possible participation in an ecosystem management decision. Uncertainty surrounding the model’s ability to predict future values of its output variables and in the estimates of all of its parameters should be part of any effort to vett a model. The vetting procedure described herein, Prediction Error Rate-Deterministic Sensitivity Analysis (PER-DSA), incorporates these two aspects of model uncertainty. DSA in particular, requires participation by skeptics and is the reason why a successful DSA gives a model sufficient credibility to have a voice in ecosystem management decision making. But these models need to be stochastic and represent the mechanistic processes of the system being modeled. For such models, performing a PER-DSA can be computationally expensive. A cluster computing algorithm to speed-up these computations is described as one way to answer this challenge. This new standard is illustrated through a PER-DSA of a population dynamics model of South African rhinoceros (Ceratotherium simum simum).}
}
@article{SENANAYAKE2024104705,
title = {Agent-based simulation for pedestrian evacuation: A systematic literature review},
journal = {International Journal of Disaster Risk Reduction},
volume = {111},
pages = {104705},
year = {2024},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2024.104705},
url = {https://www.sciencedirect.com/science/article/pii/S2212420924004679},
author = {Gayani P.D.P. Senanayake and Minh Kieu and Yang Zou and Kim Dirks},
keywords = {Pedestrian behaviour modelling, Agent-based modelling, Behavioural decision-making, Emergency evacuation},
abstract = {Agent-based models (ABMs) offer promise for realistically simulating human behaviours and interactions during emergency evacuations. This review aims to systematically assess the state of the art in ABM-based evacuation modelling with respect to methodologies, validation practices, and the associated challenges over the past decade. The review critically examines 134 studies from 2013 to 2023 that have applied ABMs for pedestrian evacuation simulation to synthesise current capabilities, limitations, and advancement pathways. Findings identify persistent challenges related to modeller bias, computational complexity, data scarcity for calibration and validation, and the predominance of simplistic rule-based decision-making models, while promise exists with the adoption of flexible behavioural frameworks, high-performance computing architectures, machine learning techniques for adaptive agent behaviours and surrogate modelling, and evolutionary computation methods for transparent rule generation. The findings underscore the importance of interdisciplinary collaboration among behavioural scientists, modellers, and emergency planners to enhance the realism and reliability of ABMs. By providing a critical synthesis of the state-of-the-art and proposing future research directions, this review aims to accelerate the development and application of ABMs that can meaningfully enhance the safety and resilience of communities facing emergencies.}
}
@article{CHAUHAN2023107757,
title = {Personalized optimal room temperature and illuminance for maximizing occupant's mental task performance using physiological data},
journal = {Journal of Building Engineering},
volume = {78},
pages = {107757},
year = {2023},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2023.107757},
url = {https://www.sciencedirect.com/science/article/pii/S235271022301937X},
author = {Hardik Chauhan and Youjin Jang and Surakshya Pradhan and Hyosoo Moon},
keywords = {Indoor environment quality, Physiological response, Occupant performance, Machine learning, Particle swarm optimization},
abstract = {Indoor room temperature and illuminance level are critical factors of indoor environment quality (IEQ), affecting human mental task performance. These effects are reflected in their physiological responses such as heart rate, electrodermal activity, and skin temperature. Occupants' individual preferences, sensitivity, and physiological responses to different combinations of room temperature and illuminance level can differ among individuals. Despite previous studies investigating the individual and combined effects of different IEQ parameters, the limited research on the cross-modal relationship between room temperature and illuminance level and its impact on mental task performance highlights its significance. Moreover, to achieve personalized insights, it is essential to incorporate individual physiological responses, and this necessitates the development of an optimization model to comprehensively examine their impact. To address these issues, this study proposes a personalized model that optimizes room temperature and illuminance levels to enhance mental task performance using occupants' physiological data. Having the random forest algorithm, this study first predicted mental task performance, which includes four mental abilities such as attention, perception, working memory, and thinking ability using the occupant's physiological data. Then, the particle swarm optimization algorithm was employed to optimize room temperature and illuminance level to maximize the predicted mental task performance. The results of the proposed model align with observed values of room temperature and illuminance level during experiments, validating the adoption of a personalized approach. The findings contribute to future insights and guidelines for the design and management of indoor environments to maximize occupants' performance.}
}
@incollection{NA2025259,
title = {Chapter Ten - Quantum computing research: An in-depth exploration},
editor = {Pethuru Raj and Kavita Saini and Brij B. Gupta},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {138},
pages = {259-292},
year = {2025},
booktitle = {Post-Quantum Cryptography Algorithms and Approaches for IoT and Blockchain Security},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2025.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0065245825000294},
author = {Natraj N.A. and Pethuru Raj Chelliah},
keywords = {Quantum, Cryptography, Machine learning, Applications, Quantum Mechanics, Qubits, Gates, Drug discovery, chemistry},
abstract = {The area of computation has undergone a paradigm change with the introduction of quantum computing, which makes use of the complexity of quantum mechanics to successfully tackle problems that were previously considered to be impossible for traditional computers to address. This chapter will give a comprehensive study of quantum computing by delving into its core principles, important algorithms, and future applications. The goal of this chapter is to provide a comprehensive analysis of quantum computing. Quantum computing encompasses not only fundamental concepts like qubits, superposition, entanglement, quantum gates, and algorithms, but it also expands its scope to include quantum hardware, software, programming languages, simulation, modelling, and the intersection of quantum computing and artificial intelligence. Quantum computing is a relatively new field that has been showing significant promise in recent years. The transformative potential of quantum computing reverberates throughout a broad variety of sectors, including materials science and chemistry, as well as finance, optimisation, drug discovery, healthcare, and national security. Quantum computing has the potential to drastically alter these fields. Researchers and enthusiasts interested in acquiring a more in-depth understanding of this dynamic and continuously growing subject matter will find this chapter to be an indispensable resource. In addition to providing a detailed understanding of the theoretical underpinnings of quantum computing, it also details the implications that this technology has in the real world.}
}
@article{PARR2025105984,
title = {Inferring when to move},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {169},
pages = {105984},
year = {2025},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2024.105984},
url = {https://www.sciencedirect.com/science/article/pii/S0149763424004536},
author = {Thomas Parr and Ashwini Oswal and Sanjay G. Manohar},
keywords = {Computational neuroscience, Generative, Bayesian, Active inference, Movement, Dynamical systems, Parkinson’s disease},
abstract = {Most of our movement consists of sequences of discrete actions at regular intervals—including speech, walking, playing music, or even chewing. Despite this, few models of the motor system address how the brain determines the interval at which to trigger actions. This paper offers a theoretical analysis of the problem of timing movements. We consider a scenario in which we must align an alternating movement with a regular external (auditory) stimulus. We assume that our brains employ generative world models that include internal clocks of various speeds. These allow us to associate a temporally regular sensory input with an internal clock, and actions with parts of that clock cycle. We treat this as process of inferring which clock best explains sensory input. This offers a way in which temporally discrete choices might emerge from a continuous process. This is not straightforward, particularly if each of those choices unfolds during a time that has a (possibly unknown) duration. We develop a route for translation to neurology, in the context of Parkinson’s disease—a disorder that characteristically slows down movements. The effects are often elicited in clinic by alternating movements. We find that it is possible to reproduce behavioural and electrophysiological features associated with parkinsonism by disrupting specific parameters—that determine the priors for inferences made by the brain. We observe three core features of Parkinson’s disease: amplitude decrement, festination, and breakdown of repetitive movements. Our simulations provide a mechanistic interpretation of how pathology and therapeutics might influence behaviour and neural activity.}
}
@article{YECKEL19971379,
title = {Parallel computation of incompressible flows in materials processing: Numerical experiments in diagonal preconditioning},
journal = {Parallel Computing},
volume = {23},
number = {9},
pages = {1379-1400},
year = {1997},
note = {Parallel computing methods in applied fluid mechanics},
issn = {0167-8191},
doi = {https://doi.org/10.1016/S0167-8191(97)00059-8},
url = {https://www.sciencedirect.com/science/article/pii/S0167819197000598},
author = {Andrew Yeckel and Jeffrey J. Derby},
keywords = {Incompressible flow, Finite element method, Preconditioning, Iterative solution, Linear systems},
abstract = {Massively parallel computing is enabling dramatic advances in the simulation of three-dimensional flows in materials processing systems. This study focuses on the efficiency and robustness of parallel algorithms applied to such systems. Specifically, various diagonal preconditioning schemes are tested for the iterative solution of the linear equations arising from Newton's method applied to finite element discretizations. Two finite element discretizations are considered — the classical Galerkin and the Galerkin/least-squares method. Results show that the choice of preconditioning method can greatly influence the rate of convergence, but that no type worked uniformly well in all cases.}
}
@article{SCHIFERL1997249,
title = {Evolution of plastic anisotropy for high-strain-rate computations},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {143},
number = {3},
pages = {249-270},
year = {1997},
issn = {0045-7825},
doi = {https://doi.org/10.1016/S0045-7825(96)01159-0},
url = {https://www.sciencedirect.com/science/article/pii/S0045782596011590},
author = {Sheila K. Schiferl and Paul J. Maudlin},
abstract = {A model for anisotropic material strength, and for changes in the anisotropy due to plastic strain, is described. This model has been developed for use in high-rate, explicit, Lagrangian multidimensional continuum-mechanics codes. The model handles anisotropies, in single-phase materials, in particular the anisotropies due to crystallographic texture—preferred orientations of the single-crystal grains. Textural anisotropies, and the changes in these anisotropies, depend overwhelmingly on the crystal structure of the material and on the deformation history. The changes, particularly for complex deformations, are not amenable to simple analytical forms. To handle this problem, the material model described here includes a texture code, or micromechanical calculation, coupled to a continuum code. The texture code updates grain orientations as a function of tensor plastic strain, and calculates the yield strength in different directions. A yield function is fitted to these yield ‘points’. For each computational cell in the continuum simulation, the texture code tracks a particular set of grain orientations. The orientations will change due to the tensor strain history, and the yield function will change accordingly. Hence, the continuum code supplies a tensor strain to the texture code, and the texture code supplies an updated yield function to the continuum code. Since significant texture changes require relatively large strains—typically, a few percent or more—the texture code is not called very often, and the increase in computer time is not excessive. The model was implemented, using a finite-element continuum code and a texture code specialized for hexagonal-close-packed crystal structures. The results for several uniaxial stress problems and an explosive-forming problem are shown.}
}
@article{FATAHI2016272,
title = {A fuzzy cognitive map model to calculate a user's desirability based on personality in e-learning environments},
journal = {Computers in Human Behavior},
volume = {63},
pages = {272-281},
year = {2016},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2016.05.041},
url = {https://www.sciencedirect.com/science/article/pii/S0747563216303685},
author = {Somayeh Fatahi and Hadi Moradi},
keywords = {Personality, Emotion, User's status, Desirability, E-learning},
abstract = {The recent research in artificial intelligence shows an increasing interest in the modeling of human behavior factors such as personality, mood, and emotion for developing human-friendly systems. That is why there is an interest in developing models and algorithms to determine a human's emotions while interacting with a system to improve the quality of the interaction. In this paper, we propose a computational model to calculate a user's desirability based on personality in e-learning environments. The desirability is one of the most important variables in determining a user's emotions. The model receives several e-learning environmental events and predicts the desirability of the events based on the user's personality and his/her goals. The proposed model has been evaluated in a simulated and real e-learning environment. The results show that the model formulates the relationship between personality and emotions with high accuracy.}
}
@article{BECKER201979,
title = {Two results on slime mold computations},
journal = {Theoretical Computer Science},
volume = {773},
pages = {79-106},
year = {2019},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2018.08.027},
url = {https://www.sciencedirect.com/science/article/pii/S0304397518305590},
author = {Ruben Becker and Vincenzo Bonifaci and Andreas Karrenbauer and Pavel Kolev and Kurt Mehlhorn},
keywords = {, Dynamical systems, Linear programming, Optimization, Approximation algorithms},
abstract = {We present two results on slime mold computations. In wet-lab experiments by Nakagaki et al. (2000) [1] the slime mold Physarum polycephalum demonstrated its ability to solve shortest path problems. Biologists proposed a mathematical model, a system of differential equations, for the slime's adaption process (Tero et al., 2007) [3]. It was shown that the process convergences to the shortest path (Bonifaci et al., 2012) [5] for all graphs. We show that the dynamics actually converges for a much wider class of problems, namely undirected linear programs with a non-negative cost vector. Combinatorial optimization researchers took the dynamics describing slime behavior as an inspiration for an optimization method and showed that its discretization can ε-approximately solve linear programs with positive cost vector (Straszak and Vishnoi, 2016) [14]. Their analysis requires a feasible starting point, a step size depending linearly on ε, and a number of steps with quartic dependence on opt/(εΦ), where Φ is the difference between the smallest cost of a non-optimal basic feasible solution and the optimal cost (opt). We give a refined analysis showing that the dynamics initialized with any strongly dominating point converges to the set of optimal solutions. Moreover, we strengthen the convergence rate bounds and prove that the step size is independent of ε, and the number of steps depends logarithmically on 1/ε and quadratically on opt/Φ.}
}
@article{XIN2025111135,
title = {Environmental system dynamics: Current development and applications},
journal = {Ecological Modelling},
volume = {506},
pages = {111135},
year = {2025},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2025.111135},
url = {https://www.sciencedirect.com/science/article/pii/S0304380025001206},
author = {Sihui Xin and Zhouyuan Li and Junsong Nong and Jiaxin Wu and Xinwei Zou and Ruijin Wu and Shikui Dong and Rongling Wu and Shaopeng Wang},
keywords = {System dynamics, Stock-flow analysis, Causal loop feedback, Modeling applications, SD community development},
abstract = {System Dynamics (SD) has emerged as a fundamental and powerful modeling methodology for understanding and simulating the behavior of both natural and social-economic systems. This review explores the historical development, key concepts, and broad applications of SD as a fundamental modeling approach. Tracing its evolution from the its inception since 1960s, the paper outlines the four major phases of SD's growth, from its initial conceptualization, through its methodological refinements, to its widespread application in ecosystems, and socio-economic systems in recent over the half century. The review highlights the development of SD communities across the globe, including the North American school, European clusters, and the growing body of work in China, and the progress in the global collaboration. We discuss and organize the foundational paired concepts of SD, including stock-flow relationships, the structure-behavior paradigm, the cause-effect process, and the loop-feedback paradigm. We summarize the SD modeling workflow protocol in the four stages, as conceptualization, visualization, quantification, and verification. It demonstrates good practices of SD modeling in various ecological contexts, spanning population, community, landscape, and macro-ecosystem levels, while emphasizing the method's adaptability and capacity for spatial modeling. Building on an extensive literature review and bibliometric analysis, the paper synthesizes key progress in SD modeling while offering insights into future perspectives and potential advancements. It concludes by reflecting on SD's ability to address multi-scale, multi-dimensional challenges and its compatibility with emerging novel approaches. Our goal is to bridge SD with contemporary ecological modeling practices by systematically reviewing the theoretical and practical advances of SD. This review provides insights for scholars and practitioners seeking to embed SD approach to the environmental and ecological systems simulation.}
}
@article{WU2023100739,
title = {The development of teacher feedback literacy in situ: EFL writing teachers’ endeavor to human-computer-AWE integral feedback innovation},
journal = {Assessing Writing},
volume = {57},
pages = {100739},
year = {2023},
issn = {1075-2935},
doi = {https://doi.org/10.1016/j.asw.2023.100739},
url = {https://www.sciencedirect.com/science/article/pii/S1075293523000478},
author = {Peisha Wu and Shulin Yu and Yanqi Luo},
keywords = {Teacher feedback literacy, Second language writing, Teacher feedback, Computer-mediated feedback, Writing assessment},
abstract = {While recent years have witnessed increasing theoretical and empirical elaboration on the construct of teacher feedback literacy in higher education and second language education, little research has investigated the development of teacher feedback literacy, especially when teachers collaborate in an attempt to improve feedback strategies with technology. To fill this gap, the present study examined two L2 writing teachers taking the initiative to create, update, and implement a human-computer-automatic writing evaluation (AWE) integral feedback platform, and how such a feedback innovation process impacted their feedback literacy development. The analysis of multiple sources of data, including semi-structured interviews, stimulated recalls, class observation, and artifacts, revealed that the two teachers approached the innovation by orchestrating mediating tools, interacting dialogically with social agents, reflecting critically, and crossing boundaries. Through this process, the development of teacher feedback literacy occurred at varying rates across different aspects. Specifically, positive changes were effected in the teachers’ feedback thinking as well as feedback giving and sharing practices. However, the teachers’ feedback literacy in classroom practice did not seem to have generated as salient a positive outcome. Possible reasons are discussed regarding the scope of the feedback innovation and contextual constraints, and implications are offered. The study underscored L2 writing teacher feedback literacy as a developmental phenomenon molded by situated social practice.}
}
@article{GARBEY1990293,
title = {Massively parallel computation of conservation laws},
journal = {Parallel Computing},
volume = {16},
number = {2},
pages = {293-304},
year = {1990},
issn = {0167-8191},
doi = {https://doi.org/10.1016/0167-8191(90)90067-J},
url = {https://www.sciencedirect.com/science/article/pii/016781919090067J},
author = {Marc Garbey and David Levine},
keywords = {Cellular automata, Partial differential equations, Method of characteristics, Parallel algorithms, Conservation laws},
abstract = {We present a new method for computing solutions of conservation laws based on the use of cellular automata with the method of characteristics. The method exploits the high degree of parallelism available with cellular automata and retains important features of the method of characteristics. It yields high numerical accuracy and extends naturally to adaptive meshes and domain decomposition methods for perturbed conservation laws. We describe the method and its implementation for a Dirichlet problem with a single conservation law for the one-dimensional case. Numerical results for the one-dimensional law with the classical Burgers nonlinearity or the Buckley-Leverett equation show good numerical accuracy outside the neighborhood of the shocks. The error in the area of the shocks is of the order of the mesh size. The algorithm is well suited for execution on both massively parallel computers and vector machines. We present timing results for an Alliant FX/8, Connection Machine Model 2, and CRAY X-MP.}
}
@incollection{FAVERO2023509,
title = {Chapter 25 - Raster objects},
editor = {Luiz Paulo Fávero and Patrícia Belfiore and Rafael {de Freitas Souza}},
booktitle = {Data Science, Analytics and Machine Learning with R},
publisher = {Academic Press},
pages = {509-519},
year = {2023},
isbn = {978-0-12-824271-1},
doi = {https://doi.org/10.1016/B978-0-12-824271-1.00011-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128242711000111},
author = {Luiz Paulo Fávero and Patrícia Belfiore and Rafael {de Freitas Souza}},
keywords = {Spatial analysis, Maps, Raster objects, R},
abstract = {At the end of this chapter, you will be able to:•Understand what a raster object is;•Load and use raster objects;•Combine raster objects with shapefiles;•Manipulate and cut out raster objects;•Use the raster objects in such a way as to demand less computational time for the execution of tasks;•View raster objects in R language.}
}
@incollection{HUTCHINS20012068,
title = {Cognition, Distributed},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {2068-2072},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/01636-3},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767016363},
author = {E. Hutchins},
abstract = {Distributed cognition is a framework for thinking about cognition which seeks to understand how the cognitive properties of aggregates emerge from the interactions of component parts. It can be applied to cognitive systems at many levels of complexity, from areas of an individual brain to communities of interacting persons. Distributed cognition is sometimes construed as a special kind of cognition that occurs when people are in interaction with one another or with material artifacts. This is only partly correct. Rather than being a kind of cognition, distributed cognition is a manner of thinking about cognition that permits one to examine the relationships between what is in the mind and the world the mind is in. When applied to groups of persons, distributed cognition provides a language for cognitive processes that are distributed across the members of a social group, between people and their material environments, and through time. It attempts to use an understanding of the social, cultural, and material context of cognitive practices to constrain models of cognitive processes within and among individual minds.}
}
@article{KUGURAKOVA2016217,
title = {Neurobiological Plausibility as Part of Criteria for Highly Realistic Cognitive Architectures},
journal = {Procedia Computer Science},
volume = {88},
pages = {217-223},
year = {2016},
note = {7th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2016, held July 16 to July 19, 2016 in New York City, NY, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.07.428},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916316842},
author = {Vlada Kugurakova and Maxim Talanov and Denis Ivanov},
keywords = {Lövheim cube, cognitive architectures, neurobiological realism},
abstract = {In this paper we analyze neurobiologically inspired approaches to implement emotions in computational systems. We propose the criteria for realistic cognitive architectures and analyze current architectures using aforementioned criteria. The analysis indicated several interesting architectures H-CogAff, BICA that inspired us to start the development of our own based on biological realistic approaches.}
}
@incollection{WANDELL2025360,
title = {Visual processing},
editor = {Jordan Henry Grafman},
booktitle = {Encyclopedia of the Human Brain (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {360-381},
year = {2025},
isbn = {978-0-12-820481-8},
doi = {https://doi.org/10.1016/B978-0-12-820480-1.00116-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204801001169},
author = {Brian A. Wandell and Jonathan Winawer},
keywords = {Physiological optics, Retinal circuits, Eye movements, Lateral geniculate nucleus, V1, Visual cortex, Functional specialization, Neural signaling, Visual field maps, Retinotopy, Receptive fields, Sparse representations, Asynchronous representation, Redundancy, Bayesian inference},
abstract = {The human visual system is a network of neural components that combine to create our perception of the world and guide our behavior. Deciphering the computational principles of this system is an important scientific challenge. We review measurements of these components, from the retinal encoding to cortical circuitry, and from molecules to circuits, focusing on measurements that are relevant to visual processing. We then delve into principles proposed to explain how this diverse collection of visual components enables us to interpret our surroundings.}
}
@article{BRANICKY199567,
title = {Universal computation and other capabilities of hybrid and continuous dynamical systems},
journal = {Theoretical Computer Science},
volume = {138},
number = {1},
pages = {67-100},
year = {1995},
note = {Hybrid Systems},
issn = {0304-3975},
doi = {https://doi.org/10.1016/0304-3975(94)00147-B},
url = {https://www.sciencedirect.com/science/article/pii/030439759400147B},
author = {Michael S. Branicky},
abstract = {We explore the simulation and computational capabilities of hybrid and continuous dynamical systems. The continuous dynamical systems considered are ordinary differential equations (ODEs). For hybrid systems we concentrate on models that combine ODEs and discrete dynamics (e.g., finite automata). We review and compare four such models from the literature. Notions of simulation of a discrete dynamical system by a continuous one are developed. We show that hybrid systems whose equations can describe a precise binary timing pulse (exact clock) can simulate arbitrary reversible discrete dynamical systems defined on closed subsets of Rn. The simulations require continuous ODEs in R2n with the exact clock as input. All four hybrid systems models studied here can implement exact clocks. We also prove that any discrete dynamical system in Zn can be simulated by continuous ODEs in R2n + 1. We use this to show that smooth ODEs in R3 can simulate arbitrary Turing machines, and hence possess the power of universal computation. We use the famous asynchronous arbiter problem to distinguish between hybrid and continuous dynamical systems. We prove that one cannot build an arbiter with devices described by a system of Lipschitz ODEs. On the other hand, all four hybrid systems models considered can implement arbiters even if their ODEs are Lipschitz.}
}
@article{RASTEIRO2009e9,
title = {LABVIRTUAL—A virtual platform to teach chemical processes},
journal = {Education for Chemical Engineers},
volume = {4},
number = {1},
pages = {e9-e19},
year = {2009},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2009.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1749772809000025},
author = {M.G. Rasteiro and L. Ferreira and J. Teixeira and F.P. Bernardo and M.G. Carvalho and A. Ferreira and R.Q. Ferreira and F. Garcia and C.M.S.G. Baptista and N. Oliveira and M. Quina and L. Santos and P.A. Saraiva and A. Mendes and F. Magalhães and A.S. Almeida and J. Granjo and M. Ascenso and R.M. Bastos and R. Borges},
keywords = {Chemical processes, E-learning, Virtual laboratories, Computational platform},
abstract = {The need to develop the capacity for autonomous and critical thinking in students and introduce practical approaches that complement the scientific background, have been acting as driving-forces that motivate engineering educators to develop new teaching methodologies. The Chemical Engineering Departments of both the Universities of Coimbra and Porto have been experimenting in this area and addressing these concerns. Recently, they have been engaged in a broader project, involving a large group of academics with complementary competencies. This project is aimed at developing a virtual platform directed towards the learning of Chemical Processes with a wide scope. From the functional point of view the platform is organized into four main areas: Chemical Engineering, Chemical Processes, Virtual Experiments and Simulators. The Chemical Processes area is further divided into four different sections: Unit Operations and Separations, Chemical Reaction, Process Systems Engineering and Biological Processes. These sections include simulators, applications and case studies to better understand the chemical/biochemical processes. The Virtual Experiments area considers both the laboratory visualization of the basic phenomena related to the processes in the other four sections, and the remote monitoring of laboratory experiments. This platform, constructed around a dynamic Web Portal, allows discussion forums and is also aimed at sharing experiences with other schools. This paper describes the different subjects included in the web platform, as well as the simulation strategies and the web methodologies used for its construction, and also presents examples of application in the classroom.}
}
@article{HICKMAN1995153,
title = {Advanced computational methods for spatial information extraction},
journal = {Computers & Geosciences},
volume = {21},
number = {1},
pages = {153-173},
year = {1995},
issn = {0098-3004},
doi = {https://doi.org/10.1016/0098-3004(94)00063-Z},
url = {https://www.sciencedirect.com/science/article/pii/009830049400063Z},
author = {Betty L. Hickman and Michael P. Bishop and Michael V. Rescigno},
keywords = {Spatial feature extraction, Texture features, Parallel processing, Spatial task partitioning},
abstract = {A variety of mathematical approaches for spatial information extraction using digitized aerial photography and satellite imagery have been developed and implemented on serial computers. However, because of data volume and scale, the computational demands of spatial analysis procedures frequently exceed the capacity of available serial processing technologies. One way of addressing this problem is through parallel processing in which the power of multiple computing units can be used on a single problem. In this study we investigate the utility of parallel processing for spatial feature extraction. Our testing in the situation of texture feature extraction using a cooccurrence matrix indicates that dramatic reductions in execution time are possible—an image that required about 34 min to process using one processor was solved in under 2 min using nineteen processors. The availability of additional processors could result in smaller execution times. This speedup potential is a critical element in future studies focusing on more complex spatial analysis procedures.}
}
@article{NAKAKOJI2000451,
title = {Computational support for collective creativity},
journal = {Knowledge-Based Systems},
volume = {13},
number = {7},
pages = {451-458},
year = {2000},
issn = {0950-7051},
doi = {https://doi.org/10.1016/S0950-7051(00)00069-1},
url = {https://www.sciencedirect.com/science/article/pii/S0950705100000691},
author = {K Nakakoji and Y Yamamoto and M Ohira},
keywords = {Computer support for collective creativity, Human–computer interaction, Visual images in creative insight, Knowledge-based approaches, Visualization},
abstract = {The goal of our research is to develop computer systems that support designers’ collective creativity; such systems support individual creative aspects in design through the use of representations created by others in the community. We have developed two systems, IAM-eMMa and EVIDII, that both aim at supporting designers in finding visual images that would be useful for their creative design task. IAM-eMMa uses knowledge-based rules, which are constructed by other designers, to retrieve images related to a design task, and infers the underlying “rationale” when a designer chooses one of the images. EVIDII allows designers to associate affective words and images, and then shows several visual representations of the relationships among designers, images and words. By observing designers interacting with the two systems, we have identified that systems for supporting collective creativity need to be based on design knowledge that: (1) is contextualized; (2) is respectable and trustful; and (3) enables “appropriation” of a design task.}
}
@article{BUEHLER20081101,
title = {Theoretical and computational hierarchical nanomechanics of protein materials: Deformation and fracture},
journal = {Progress in Materials Science},
volume = {53},
number = {8},
pages = {1101-1241},
year = {2008},
issn = {0079-6425},
doi = {https://doi.org/10.1016/j.pmatsci.2008.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0079642508000510},
author = {Markus J. Buehler and Sinan Keten and Theodor Ackbarow},
abstract = {Proteins constitute the building blocks of biological materials such as tendon, bone, skin, spider silk or cells. An important trait of these materials is that they display highly characteristic hierarchical structures, across multiple scales, from nano to macro. Protein materials are intriguing examples of materials that balance multiple tasks, representing some of the most sustainable material solutions that integrate structure and function. Here we review progress in understanding the deformation and fracture mechanisms of hierarchical protein materials by using a materials science approach to develop structure-process-property relations, an effort defined as materiomics. Deformation processes begin with an erratic motion of individual atoms around flaws or defects that quickly evolve into formation of macroscopic fractures as chemical bonds rupture rapidly, eventually compromising the integrity of the structure or the biological system leading to failure. The combination of large-scale atomistic simulation, multi-scale modeling methods, theoretical analyses combined with experimental validation provides a powerful approach in studying deformation and failure phenomena in protein materials. Here we review studies focused on the molecular origin of deformation and fracture processes of three types of protein materials. The review includes studies of collagen – Nature’s super-glue; beta-sheet rich protein structures as found in spider silk – a natural fiber that can reach the strength of a steel cable; as well as intermediate filaments – a class of alpha-helix based structural proteins responsible for the mechanical integrity of eukaryotic cells. The article concludes with a discussion of the significance of universally found structural patterns such as the staggered collagen fibril architecture or the alpha-helical protein motif.}
}
@article{VAITESSWAR2024210,
title = {Machine learning based feature engineering for thermoelectric materials by design††Electronic supplementary information (ESI) available: Details on methodology, Box–Cox transformations, machine learning models, and inverse design. See DOI: https://doi.org/10.1039/d3dd00131h},
journal = {Digital Discovery},
volume = {3},
number = {1},
pages = {210-220},
year = {2024},
issn = {2635-098X},
doi = {https://doi.org/10.1039/d3dd00131h},
url = {https://www.sciencedirect.com/science/article/pii/S2635098X24000305},
author = {U. S. Vaitesswar and Daniil Bash and Tan Huang and Jose Recatala-Gomez and Tianqi Deng and Shuo-Wang Yang and Xiaonan Wang and Kedar Hippalgaonkar},
abstract = {Availability of material datasets through high performance computing has enabled the use of machine learning to not only discover correlations and employ materials informatics to perform screening, but also to take the first steps towards materials by design. Computational materials databases are well-labelled and provide a fertile ground for predicting both ground-state and functional properties of materials. However, a clear design approach that allows prediction of materials with the desired functional performance does not yet exist. In this work, we train various machine learning models on a dataset curated from a combination of Materials Project as well as computationally calculated thermoelectric electronic power factor using a constant relaxation time Boltzmann transport equation (BoltzTrap). We show that simple random forest-based machine learning models outperform more complex neural network-based approaches on the moderately sized dataset and also allow for interpretability. In addition, when trained on only cubic material systems, the best performing machine learning model employs a perturbative scanning approach to find new candidates in Materials Project that it has never seen before, and automatically converges upon half-Heusler alloys as promising thermoelectric materials. We validate this prediction by performing density functional theory and BoltzTrap calculations to reveal accurate matching. One of those predicted to be a good material, NbFeSb, has been studied recently by the thermoelectric community; from this study, we propose four new half-Heusler compounds as promising thermoelectric materials – TiGePt, ZrInAu, ZrSiPd and ZrSiPt. Our approach is generalizable to extrapolate into previously unexplored material spaces and establishes an automated pipeline for the development of high-throughput functional materials.}
}
@article{TURNER2024,
title = {Old Strategies, New Environments: Reinforcement Learning on Social Media},
journal = {Biological Psychiatry},
year = {2024},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2024.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S0006322324018201},
author = {Georgia Turner and Amanda M. Ferguson and Tanay Katiyar and Stefano Palminteri and Amy Orben},
keywords = {Development, Mental health, Reinforcement learning, Reward learning, Social media, Social reward},
abstract = {The rise of social media has profoundly altered the social world, introducing new behaviors that can satisfy our social needs. However, it is not yet known whether human social strategies, which are well adapted to the offline world we developed in, operate as effectively within this new social environment. Here, we describe how the computational framework of reinforcement learning (RL) can help us to precisely frame this problem and diagnose where behavior-environment mismatches emerge. The RL framework describes a process by which an agent can learn to maximize their long-term reward. RL, which has proven to be successful in characterizing human social behavior, consists of 3 stages: updating expected reward, valuating expected reward by integrating subjective costs such as effort, and selecting an action. Specific social media affordances, such as the quantifiability of social feedback, may interact with the RL process at each of these stages. In some cases, affordances can exploit RL biases that are beneficial offline by violating the environmental conditions under which such biases are optimal, such as when algorithmic personalization of content interacts with confirmation bias. Characterizing the impact of specific aspects of social media through this lens can improve our understanding of how digital environments shape human behavior. Ultimately, this formal framework could help address pressing open questions about social media use, including its changing role across human development and its impact on outcomes such as mental health.}
}
@article{SHU2025111052,
title = {Optimal power flow in hybrid AC-DC systems considering N-k security constraints in the preventive-corrective control stage},
journal = {Electric Power Systems Research},
volume = {238},
pages = {111052},
year = {2025},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2024.111052},
url = {https://www.sciencedirect.com/science/article/pii/S0378779624009349},
author = {Hongchun Shu and Hongfang Zhao and Mengli Liao},
keywords = {Flexible DC transmission, AC-DC hybrid system, -, Safety constraints, Optimal power flow},
abstract = {The optimal power flow methods for AC-DC systems containing VSC-HVDC generally only consider the economy during normal operation, overlooking the distribution of line transmission power in fault conditions. As a result, lines that continue to operate after a fault may experience overloading or operate at full capacity. Thus, a method for optimal power flow calculation is proposed that incorporates N-k security constraints in the preventive-corrective control stage for secure and economic operation of hybrid AC-DC systems. This method ensures that the line transmission power in the system meets the limits in the normal, short-term fault, and long-term fault states. In addition to the optimal power flow in the normal state, the method incorporates the system's imbalance as an indicator to evaluate system resilience. It combines this indicator with the economic, network loss, and performance metrics of the system, forming a two-stage bi-level multi-objective optimization model. Furthermore, to address the curse of dimensionality in anticipating system fault sets, a method for generating the anticipated fault set using non-sequential Monte Carlo simulation is proposed, along with a fault scenario search approach based on robust thinking to identify the most severe faults. Finally, the traditional IEEE 30-bus system was improved, and simulation verification was conducted using examples of an AC/DC system with a three-terminal DC network and a wind-solar-storage hybrid AC/DC system with a three-terminal DC network. The simulation results indicate that the proposed optimal power flow method considering the preventive-corrective control stage with N-k security constraints can effectively enhance system resilience. Furthermore, it improves the economic efficiency while ensuring the secure operation of the system.}
}
@article{BRENTDANIEL2023105630,
title = {Extremely rapid, Lagrangian modeling of 2D flooding: A rivulet-based approach},
journal = {Environmental Modelling & Software},
volume = {161},
pages = {105630},
year = {2023},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2023.105630},
url = {https://www.sciencedirect.com/science/article/pii/S1364815223000166},
author = {W. {Brent Daniel} and Corinne Roth and Xue Li and Cindy Rakowski and Tim McPherson and David Judi},
keywords = {Modeling, Fluid flow, Flood modeling, Hydrodynamic model, Rivulet, Lagrangian},
abstract = {Estimates of potential flood inundation areas and depths are critical to informing the preparedness, response, and investment decisions of many government agencies and private sector organizations, especially under a changing climate. The standard modeling approaches, however, are often either computationally intensive or constrained in their accuracy or applicability. A novel, rivulet-based, 2D model of flooding is described in this article that is 10,000 to 10 million times less computationally complex than the full solution of the shallow water equations, yet achieves inundation area hit rates of between 0.8 and 0.9, relative absolute mean errors of 10%–20% across a wide range of flow depths, and comparable accuracy at forecasting empirical high-water marks. This combination of accuracy and efficiency will significantly enhance real-time depth estimates during flood events, support detailed sensitivity analyses, and allow for the generation of large ensembles to enable complex uncertainty analyses.}
}
@article{CARVALHO2016169,
title = {Origins and evolution of enactive cognitive science: Toward an enactive cognitive architecture},
journal = {Biologically Inspired Cognitive Architectures},
volume = {16},
pages = {169-178},
year = {2016},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2015.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X15000535},
author = {Leonardo Lana de Carvalho and Denis James Pereira and Sophia Andrade Coelho},
keywords = {Cognitive science, Enaction, Complex systems, Cognitive architecture},
abstract = {This paper presents a historical perspective on the origin of the enactive approach to cognitive science, starting chronologically from cybernetics, with the aim of clarifying its main concepts, such as enaction, autopoiesis, structural coupling and natural drift; thus showing their influences in computational approaches and models of cognitive architecture. Works of renowned authors, as well as some of their main commentators, were addressed to report the development of enactive approach. We indicate that the enactive approach transcends its original context within biology, and at a second moment within connectionism, changes the understanding of the relationships so far established between the body and the environment, and the ideas of conceptual relationships between the mind and the body. The influence on computational theories is of great importance, leading to new artificial intelligence systems as well as the proposition of complex, autopoietic and alive machines. Finally, the article stresses the importance of the enactive approach in the design of agents, understanding that previous approaches have very different cognitive architectures and that a prototypical model of enactive cognitive architecture is one of the largest challenges today.}
}
@article{YANG2020119,
title = {A multilevel neighborhood sequential decision approach of three-way granular computing},
journal = {Information Sciences},
volume = {538},
pages = {119-141},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.05.060},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520304734},
author = {Xin Yang and Tianrui Li and Dun Liu and Hamido Fujita},
keywords = {Three-way granular computing, Sequential three-way decision, Neighborhood, Multilevel},
abstract = {The fusion of three-way decision and granular computing provides powerful ideas and methods to understand and solve the problems of cognitive science by thinking and information processing in threes. As a typical representation of three-way granular computing, sequential three-way decision focuses on making a multiple stages of decisions by a sequence of trisecting-acting-outcome (TAO) models. To construct more general granules, levels, and hierarchies, we investigate an integrative multi-granularity approach to sequential three-way decision in a neighborhood system by the evolution mechanism of data and parameters. We employ the γ-cut similarity neighborhood relation based on Gaussian kernel function to the hierarchical granulation of universe. Subsequently, we propose the multilevel neighborhood granular structures by the combinations of horizontal granularity and vertical granularity, and discuss the monotonicity of level measurements associated with the uncertainty of decision. Based on such a neighborhood structured approach, a multilevel framework of sequential three-way decision is examined from coarser to finer concerning the granularity of neighborhood information. Finally, we report a series of experiments to demonstrate the performance of proposed models and algorithms.}
}
@article{THANKACHAN2024101283,
title = {A mathematical formulation of learner cognition for personalised learning experiences},
journal = {Cognitive Systems Research},
volume = {88},
pages = {101283},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101283},
url = {https://www.sciencedirect.com/science/article/pii/S1389041724000779},
author = {Jeena A. Thankachan and Bama Srinivasan},
keywords = {Virtual Learning Environment (VLE), Cognitive Evaluation Metrics (CEM), Multimode evaluation, Cognitive abilities, Learning tasks, Reinforcement learning},
abstract = {The paper focuses on the assessment of cognitive skills within Virtual Learning Environments (VLEs). In response to the global shift to remote learning amid the COVID-19 pandemic, VLEs, which include learning management systems (LMS) and online collaboration platforms, gained prominence. The proposed work leverages an established Cattell–Horn–Carroll (CHC) theory to propose eight metrics, which collectively form a part of Cognitive Evaluation Metrics (CEM). The proposed metrics introduce a novel computational approach for multimode evaluation of learners’ cognitive abilities for each learning task within a learning environment. The paper details the formalism for the evaluation of the metrics and makes a contribution towards the potential of the proposed methodology to evaluate cognitive abilities. Additionally, the work implements CEM integration into the learner module of a Game-Based Learning (GBL) environment. Analysis of simulations in the GBL environment, along with statistical analysis, provides insights into the normal distribution of cognitive metrics. This reveals diverse ranges in various abilities such as long or short term memory, working memory, reasoning, attention, and processing speed. The paper also explores the impact of virtual assistants, which highlights their limited relevance to enhance cognitive abilities but serve as valuable on-demand support resources.}
}
@article{SUN2024103771,
title = {Supply chain planning with free trade zone and uncertain demand},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {192},
pages = {103771},
year = {2024},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2024.103771},
url = {https://www.sciencedirect.com/science/article/pii/S1366554524003624},
author = {Haoying Sun and Manoj Vanajakumari and Chelliah Sriskandarajah and Subodha Kumar},
keywords = {Supply chain management, Robust optimization, Dynamic lot sizing},
abstract = {Our research is inspired by the subcontracting problem at a major oil field services company in North America. The company’s supply chain consists of suppliers bringing raw materials to a Free Trade Zone (FTZ). The FTZ receives raw materials in full containers from various suppliers, and then the company ships them to various plants (e.g. oil excavation sites) frequently via subcontractors. This allows the company to focus on managing only the inbound transportation and inventory at the FTZ. The demand for each raw material is stochastic. We derive an algorithm running at polynomial time for the stochastic programming formulation and perform μ− regret Robust Optimization to handle the demand uncertainty. We also use a Sample Average Approximation method to alleviate the high computational requirement of the robust optimization model. The modeling approach demonstrated by this paper not only meets the needs of this specific company and industry but also can be applied to other industries with similar supply chain structures.}
}
@article{GUPTA19941,
title = {On the principles of fuzzy neural networks},
journal = {Fuzzy Sets and Systems},
volume = {61},
number = {1},
pages = {1-18},
year = {1994},
issn = {0165-0114},
doi = {https://doi.org/10.1016/0165-0114(94)90279-8},
url = {https://www.sciencedirect.com/science/article/pii/0165011494902798},
author = {M.M. Gupta and D.H. Rao},
keywords = {Fuzzy logic, neural networks, fuzzy neural networks, confluence operation, synpatic and somatic operations},
abstract = {Over the last decade or so, significant advances have been made in two distinct technological areas: fuzzy logic and computational neutral networks. The theory of fuzzy logic provides a mathematical framework to capture the uncertainties associated with human cognitive processes, such as thinking and reasoning. Also, it provides a mathematical morphology to emulate certain perceptual and linguistic attributes associated with human cognition. On the other hand, the computational neural network paradigms have evolved in the process of understanding the incredible learning and adaptive features of neuronal mechanisms inherent in certain biological species. Computational neural networks replicate, on a small scale, some of the computational operations observed in biological learning and adaptation. The integration of these two fields, fuzzy logic and neural networks; has given birth to an emerging technological field — the fuzzy neural networks. The fuzzy neural networks have the potential to capture the benefits of the two fascinating fields, fuzzy logic and neural networks, into a single capsule. The intent of this tutorial paper is to describe the basic notions of biological and computational neuronal morphologies, and to describe the principles and architectures of fuzzy neural networks. Towards this goal, we develop a fuzzy neural architecture based upon the notion of T-norm and T-conorm connectives. An error-based learning scheme is described for this neural structure.}
}
@article{DECOUGNY1994157,
title = {Load balancing for the parallel adaptive solution of partial differential equations},
journal = {Applied Numerical Mathematics},
volume = {16},
number = {1},
pages = {157-182},
year = {1994},
issn = {0168-9274},
doi = {https://doi.org/10.1016/0168-9274(94)00039-5},
url = {https://www.sciencedirect.com/science/article/pii/0168927494000395},
author = {H.L. deCougny and K.D. Devine and J.E. Flaherty and R.M. Loy and C. Özturan and M.S. Shephard},
abstract = {An adaptive technique for a partial differential system automatically adjusts a computational mesh or varies the order of a numerical procedure with a goal of obtaining a solution satisfying prescribed accuracy criteria in an optimal fashion. Processor load imbalances will, therefore, be introduced at adaptive enrichment steps during the course of a parallel computation. We develop and describe three procedures for retaining and restoring load balance that have low unit cost and are appropriate for use in an adaptive solution environment. Tiling balances load by using local optimality criteria within overlapping processor neighborhoods. Elemental data are migrated between processors within the same neighborhoods to restore balance. Tiling is restricted to uniform two-dimensional meshes and provides limited control of communications volume by priority-based element selection criteria. These shortcomings can potentially be overcome by creating a dynamic partition graph connecting processors and their neighboring regions. After coloring the edges of the graph, elemental data are iteratively transferred between processors by pairwise exchange to permit a more global migration. Octree decomposition of a spatial domain is a successful three-dimensional mesh generation strategy. The octree structure facilities a rapid load balancing procedure by performing tree traversals that (i) appraise subtree costs and (ii) partition spatial regions accordingly. Computational results are reported for two- and three-dimensional problems using nCUBE/2 hypercube, MasPar MP-2, and Thinking Machines CM-5 computers.}
}
@article{ALDRIDGE1967155,
title = {A wave analogue as a guide to ultrasonic thinking},
journal = {Ultrasonics},
volume = {5},
number = {3},
pages = {155-162},
year = {1967},
issn = {0041-624X},
doi = {https://doi.org/10.1016/S0041-624X(67)80060-X},
url = {https://www.sciencedirect.com/science/article/pii/S0041624X6780060X},
author = {E.E. Aldridge}
}
@article{FOWLER20245,
title = {Will variants of uncertain significance still exist in 2030?},
journal = {The American Journal of Human Genetics},
volume = {111},
number = {1},
pages = {5-10},
year = {2024},
issn = {0002-9297},
doi = {https://doi.org/10.1016/j.ajhg.2023.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0002929723004007},
author = {Douglas M. Fowler and Heidi L. Rehm},
abstract = {Summary
In 2020, the National Human Genome Research Institute (NHGRI) made ten “bold predictions,” including that “the clinical relevance of all encountered genomic variants will be readily predictable, rendering the diagnostic designation ‘variant of uncertain significance (VUS)’ obsolete.” We discuss the prospects for this prediction, arguing that many, if not most, VUS in coding regions will be resolved by 2030. We outline a confluence of recent changes making this possible, especially advances in the standards for variant classification that better leverage diverse types of evidence, improvements in computational variant effect predictor performance, scalable multiplexed assays of variant effect capable of saturating the genome, and data-sharing efforts that will maximize the information gained from each new individual sequenced and variant interpreted. We suggest that clinicians and researchers can realize a future where VUSs have largely been eliminated, in line with the NHGRI’s bold prediction. The length of time taken to reach this future, and thus whether we are able to achieve the goal of largely eliminating VUSs by 2030, is largely a consequence of the choices made now and in the next few years. We believe that investing in eliminating VUSs is worthwhile, since their predominance remains one of the biggest challenges to precision genomic medicine.}
}
@article{ELLIOT2022112418,
title = {An expanded framing of ecosystem services is needed for a sustainable urban future},
journal = {Renewable and Sustainable Energy Reviews},
volume = {162},
pages = {112418},
year = {2022},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2022.112418},
url = {https://www.sciencedirect.com/science/article/pii/S1364032122003264},
author = {T. Elliot and J.A. Torres-Matallana and B. Goldstein and J. {Babí Almenar} and E. Gómez-Baggethun and V. Proença and B. Rugani},
keywords = {Urban ecosystem services, Urban metabolism, Life cycle thinking, Land cover change, System dynamics modelling, Urban land teleconnections},
abstract = {Urban activities are an important driver of ecosystem services decline. Sustainable urbanisation necessitates anticipating and mitigating these negative socio-ecological impacts, both within and beyond city boundaries. There is a lack of scalable, dynamic models of changes to ecosystems wrought by urban processes. We developed a system dynamics model, ESTIMUM, to predict locations, types, and magnitude of changes in ecosystem services. We tested the model in Lisbon (Portugal) under four specific urban development scenarios – a base case scenario and three local sustainability-driven scenarios – to the year 2050. Our results show that urban sustainability policies focused on reducing impacts within Lisbon can be undermined by increased impacts in the extended regions that supply resources to the city. In particular, carbon sequestration from urban greening pales in comparison to growing greenhouse gases from the consumption of food, energy and construction materials. We also find that policies targeted at these extended environmental impacts can be much more effective than those with a limited focus on the urban form. For example, dietary shifts could support positive changes outside that city to increase global climate regulation by 54% compared to a mere 1% increase through intensive urban greening. This highlights the urgent need for a reframing of urban sustainability in policy and scholarly circles from city-centric focus towards an expanded multi-scalar conceptualisation of urban sustainability that accounts for urban impacts beyond the city boundaries.}
}
@article{HUANG2025135963,
title = {Dynamic performance optimization of a floating offshore wind turbine based on fractal-inspired design principles},
journal = {Energy},
volume = {324},
pages = {135963},
year = {2025},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2025.135963},
url = {https://www.sciencedirect.com/science/article/pii/S0360544225016056},
author = {Haoda Huang and Qingsong Liu and Musa Bashir and Sean Malkeson and Chun Li and Minnan Yue and Weipao Miao and Jin Wang},
keywords = {Floating offshore wind turbine, Leaf-vein structure, Computational fluid dynamics, Fractal dimension, Fully coupled dynamic response},
abstract = {As the development of onshore and fixed offshore wind turbines approaches saturation, floating offshore wind turbines (FOWTs) are increasingly gaining attention due to their ability to operate in deeper waters and harness more stable wind resources. However, the dynamic responses of FOWTs are amplified significantly under the complex sea conditions, posing challenges to the overall system stability. This study proposes a novel semi-submersible platform featuring fractal structure inspired by Victoria Amazonica as solutions to the overall system stability of FOWTs. The computational fluid dynamics method, integrated with dynamic fluid-body interaction and volume of fluid wave model, is used to examine the aero, hydro, and mooring dynamics of the FOWT. A parametric model of the fractal structure with different branch levels is constructed by recursive method. Firstly, the hydrodynamic performance of the novel platforms with multi-level branch structures is examined under single wave conditions. The results show that vortices in fractal structures present higher velocity gradients and greater viscous dissipation, thereby effectively absorbing wave energy. The stability of the platform improves progressively as the branch levels increase. Subsequently, the dynamic responses of the full-configuration FOWT mounted on the platform with 8-level fractal structure (8LFS-FOWT) are further evaluated under wind-wave coupling conditions. The results reveal that 8LFS-FOWT achieves superior hydrodynamic performance with the most notable improvement in pitch amplitude of 25.22 % decrease. This enhancement also brings a 12.75 % reduction in the standard deviation of power output, forming positive feedback to ensure safe and stable operation of the system. The findings provide a valuable reference for promoting the innovative platform design of FOWTs.}
}
@incollection{TIRAPELLE20233465,
title = {Practical learning activities to increase the interest of university applicants in STEM careers in the era of Industry 4.0},
editor = {Antonios C. Kokossis and Michael C. Georgiadis and Efstratios Pistikopoulos},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {52},
pages = {3465-3470},
year = {2023},
booktitle = {33rd European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-443-15274-0.50553-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443152740505539},
author = {Monica Tirapelle and Dian Ning Chia and Fanyi Duanmu and Konstantinos Katsoulas and Alberto Marchetto and Eva Sorensen},
keywords = {Engineering Education, Hands-on activities, Active learning, Orientation to university, PSE computational tools},
abstract = {Inspiring young students, especially young girls, about STEM disciplines is crucial to address the current shortage of engineers. Since the engineering skills that are required by graduates are evolving in line with technological progress, there is now an even stronger need for graduates with strong Process Systems Engineering skills. In this work, we describe an effective way to promote the chemical engineering curriculum, with particular emphasis on computational tools, to a group of Year 12 high school students during a one-week course in our department. The course was designed to engage students in active learning through interactive sessions and practical hands-on activities. Through the course, the students gained a better understanding of the importance of STEM subjects and, in particular, of the challenges and opportunities that engineers encounter in the era of Industry 4.0 with ever-increasing use of digitalization in process design and operation.}
}
@article{CHATRABHUJ2024101045,
title = {Design of an iterative method for environmental-sustainable development: Integrating bioinspired computing techniques},
journal = {Environmental Development},
volume = {51},
pages = {101045},
year = {2024},
issn = {2211-4645},
doi = {https://doi.org/10.1016/j.envdev.2024.101045},
url = {https://www.sciencedirect.com/science/article/pii/S2211464524000836},
author = { Chatrabhuj and Kundan Meshram},
keywords = {Sustainable development, Bioinspired computing, Hybrid algorithms, Agent-based modelling, High-performance computing},
abstract = {The need for sustainable development has grown in response to global environmental, social, and economic challenges. Conventional computational methods frequently struggle to address the complex nature of the Sustainable Development Goals (SDGs), lacking the ability to balance global search with local optimization and failing to prioritize goals related to sustainability. To address these restrictions, this work introduces the Integrated Bioinspired Computing Model for Sustainable Development (IBCMSD). By combining Genetic Algorithms (GAs), Artificial Neural Networks (ANNs), and Ant Colony Optimization (ACO), a cohesive hybrid model is developed that improves exploration and exploitation, balance for increased efficiency, and solution quality. It is implemented on High-Performance Computing (HPC) clusters to ensure scalability and resilience when dealing with complicated optimization challenges. Furthermore, using a multidisciplinary co-design method completes the model with multiple views, increasing its relevance and applicability in real-world circumstances. IBCMSD makes a significant contribution to computational sustainability by leveraging bioinspired computing, potentially enabling informed decision-making and SDG accomplishment across multiple domains.}
}
@article{LI2024112467,
title = {Study on correlation between perioperative cognitive function and nutritional status in elderly patients with gastric cancer},
journal = {Experimental Gerontology},
volume = {193},
pages = {112467},
year = {2024},
issn = {0531-5565},
doi = {https://doi.org/10.1016/j.exger.2024.112467},
url = {https://www.sciencedirect.com/science/article/pii/S0531556524001098},
author = {Rong Li and Yuping Liu and Yingtao Meng and Xianlin Qu and Meimei Shang and Lihui Yang and Jie Chai},
keywords = {Elderly, Gastric cancer, Perioperative period, Cognitive function, Nutritional status, Correlation, Analysis},
abstract = {Objective: To investigate the cognitive function and nutritional status of elderly patients with gastric cancer during perioperative period, and to analyze their correlation. Methods: Aged patients undergoing gastric cancer surgery in The Affiliated Cancer Hospital of Shandong First Medical University from March to October 2021 were selected as the subjects of this study. The monitoring data of cognitive function and nutritional status were retrospectively analyzed from 1 to 3 days before surgery, 1 and 3 days after surgery, 7 days after surgery (before discharge) and 30 days after surgery to analyze the correlation between cognitive function and nutritional status in elderly patients with gastric cancer. Results: the incidence of mild cognitive impairment in elderly patients with gastric cancer was 52.43 %, the visual space of the two groups' (mild cognitive impairment) ability of execution, name, attention, language, abstract thinking, delayed memory and cognitive function scores were lower than 1 set of directional force (cognitive function in normal group), statistically significant difference (P < 0.05). The nutritional status of elderly patients with gastric cancer was lower than that of healthy elderly group at the same period (P < 0.05). The scores of visual spatial executive function, name, attention, delayed memory, orientation and total score of cognitive function in elderly gastric cancer patients were positively correlated with nutritional status (P < 0.05). Conclusions: The cognitive function and nutritional status of elderly patients with gastric cancer are both in a low state during treatment and a higher level of cognitive function can help patients maintain a more correct nutritional cognition, and the nutritional status of patients will be relatively better. There is a positive correlation between cognitive function and nutritional status in elderly patients with gastric cancer, which should be paid attention to in the treatment.}
}
@incollection{FROEMER2025234,
title = {Belief updates, learning and adaptive decision making},
editor = {Jordan Henry Grafman},
booktitle = {Encyclopedia of the Human Brain (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {234-251},
year = {2025},
isbn = {978-0-12-820481-8},
doi = {https://doi.org/10.1016/B978-0-12-820480-1.00059-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204801000590},
author = {Romy Froemer and Matthew R. Nassar},
keywords = {Reinforcement learning, Reward, Value, Action, Dopamine, Belief updating, Sequential sampling, Attention, Confidence, Context, Experience, Goal-directed behavior, Cost-benefit decision-making},
abstract = {People make decisions every day and the outcomes of those decisions often lead them to change their beliefs and in some cases shape their future behavior. How does the brain decide which meal to order at a restaurant, and how does it learn from the experience of eating that meal? Here we review work from neuroscience, psychology and economics that shapes our understanding of how the brain makes decisions and learns through experience. We focus on computational mechanisms that can explain core phenomena in learning and decision making as well as how such mechanisms are implemented in the brain. Our review highlights both the considerable progress made in the last decades elucidating mechanisms of learning and decision making as well as the vast territory of open questions that remain to be answered.}
}
@article{WOLLMANN2019278,
title = {Proposal for a model to hierarchize strategic decisions according to criteria of value innovation, sustainability and budgetary constraint},
journal = {Journal of Cleaner Production},
volume = {231},
pages = {278-289},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.05.190},
url = {https://www.sciencedirect.com/science/article/pii/S095965261931724X},
author = {Dewey Wollmann and Ubiratã Tortato},
keywords = {Complex adaptive systems, Analytic network process, BOCR analysis, Linear programming},
abstract = {Organizations need management models, which will enable their executives to develop systemic thinking. In addition, executives should keep in mind that: some decision-making processes should be shared; impose political influence according to their preferences; value innovation strategies may be present; it is essential to consider the environmental, economic and social dimensions of sustainability. In this context, this study describes a model to hierarchize strategic decisions according to criteria innovation value, sustainability and budgetary constraint, developed according to the methodology proposed by Mitroff et al. (1974). In addition to hierarchizing, the model allows identifying the degree of importance of each of the strategic decisions in the performance indicators defined as evaluation criteria and sub-criteria. In the conceptualization phase, the model is influenced by concepts that describe complex adaptive systems. Next, the Analytic Network Process with Benefits, Opportunities, Costs and Risks Analysis and Linear Programming techniques are used in order to define the mathematical structure that operationalizes the model. The use of a hypothetical example demonstrates the capacity of the model proposed in this work to support the decision-making process of an organization in the selection of its decision alternatives. Thus, the model can help the academic and business communities concerned with the progress of sustainable societies, insofar as it subsidizes decision-making for the development and implementation of new products and processes related to cleaner production.}
}
@article{GOECKE2020101470,
title = {Testing competing claims about overclaiming},
journal = {Intelligence},
volume = {81},
pages = {101470},
year = {2020},
issn = {0160-2896},
doi = {https://doi.org/10.1016/j.intell.2020.101470},
url = {https://www.sciencedirect.com/science/article/pii/S0160289620300489},
author = {B. Goecke and S. Weiss and D. Steger and U. Schroeders and O. Wilhelm},
keywords = {Overclaiming, Declarative knowledge, Self-reported knowledge, Creativity, Intelligence, Faking},
abstract = {Overclaiming has been described as people's tendency to overestimate their cognitive abilities in general and their knowledge in particular. We discuss four different perspectives on the phenomenon of overclaiming that have been proposed in the research literature: Overclaiming as a result of a) self-enhancement tendencies, b) as a cognitive bias (e.g., hindsight bias, memory bias), c) as proxy for cognitive abilities, and d) as sign of creative engagement. Moreover, we discuss two different scoring methods for an OCQ (signal detection theory vs. familiarity ratings). To distinguish between the different viewpoints of what overclaiming is, we juxtaposed overclaiming, as indicated by claiming familiarity with non-existent terms, with fluid and crystallized intelligence, self-reported knowledge, creativity, faking ability, and personality. Overclaiming was measured with a newly comprised overclaiming questionnaire. Results of several latent variable analyses based upon a multivariate study with 298 participants were: First, overclaiming is neither predicted by honesty-humility nor faking ability and therefore reflects something different than mere self-enhancement tendencies. Second, overclaiming is not predicted by crystallized intelligence, but is highly predictive of self-reported knowledge and, thus, not suitable as an index or a proxy for cognitive abilities. Finally, overclaiming is neither related to divergent thinking and originality, and only moderately predicted by self-reported openness creativity from the HEXACO which means that overclaiming does not reflect creative ability. In sum, our results favor an interpretation of overclaiming as a phenomenon that requires more than self-enhancement motivation, in contrast to the claim that was initially proposed in the literature.}
}
@article{MILLNER2020704,
title = {Advancing the Understanding of Suicide: The Need for Formal Theory and Rigorous Descriptive Research},
journal = {Trends in Cognitive Sciences},
volume = {24},
number = {9},
pages = {704-716},
year = {2020},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2020.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364661320301480},
author = {Alexander J. Millner and Donald J. Robinaugh and Matthew K. Nock},
keywords = {suicide, suicide theory, formal models},
abstract = {Suicide is a leading cause of death worldwide and perhaps the most puzzling and devastating of all human behaviors. Suicide research has primarily been guided by verbal theories containing vague constructs and poorly specified relationships. We propose two fundamental changes required to move toward a mechanistic understanding of suicide. First, we must formalize theories of suicide, expressing them as mathematical or computational models. Second, we must conduct rigorous descriptive research, prioritizing direct observation and precise measurement of suicidal thoughts and behaviors and of the factors posited to cause them. Together, theory formalization and rigorous descriptive research will facilitate abductive theory construction and strong theory testing, thereby improving the understanding and prevention of suicide and related behaviors.}
}
@article{LITT1993459,
title = {Single neuron computation: T. McKenna, J. Davis and S.F. Zornetzer (Eds.) (Academic Press, San Diego, CA, 1992, 664 p., Price US $59.95)},
journal = {Electroencephalography and Clinical Neurophysiology},
volume = {87},
number = {6},
pages = {459-460},
year = {1993},
issn = {0013-4694},
doi = {https://doi.org/10.1016/0013-4694(93)90160-W},
url = {https://www.sciencedirect.com/science/article/pii/001346949390160W},
author = {Brian Litt}
}
@article{KNOWLTON2012373,
title = {A neurocomputational system for relational reasoning},
journal = {Trends in Cognitive Sciences},
volume = {16},
number = {7},
pages = {373-381},
year = {2012},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2012.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1364661312001283},
author = {Barbara J. Knowlton and Robert G. Morrison and John E. Hummel and Keith J. Holyoak},
abstract = {The representation and manipulation of structured relations is central to human reasoning. Recent work in computational modeling and neuroscience has set the stage for developing more detailed neurocomputational models of these abilities. Several key neural findings appear to dovetail with computational constraints derived from a model of analogical processing, ‘Learning and Inference with Schemas and Analogies’ (LISA). These include evidence that (i) coherent oscillatory activity in the gamma and theta bands enables long-distance communication between the prefrontal cortex and posterior brain regions where information is stored; (ii) neurons in prefrontal cortex can rapidly learn to represent abstract concepts; (iii) a rostral-caudal abstraction gradient exists in the PFC; and (iv) the inferior frontal gyrus exerts inhibitory control over task-irrelevant information.}
}
@article{OFFENHUBER2023264,
title = {Reconsidering Representation in College Design Curricula},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {9},
number = {2},
pages = {264-282},
year = {2023},
note = {The Future of Design Education: Rethinking Design Education for the 21st Century},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2023.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S2405872623000394},
author = {Dietmar Offenhuber and Joy Mountford},
keywords = {Representation, Data, Models, Maps, Visualization, Sensory modalities},
abstract = {The Future of Design Education working group on representation addressed the roles of data, maps, models, and interfaces as a continuum from representation to action. The article traces historical ideas of representation grounded by a linguistic paradigm to more recent approaches based on performance, embodiment, and sensory modalities other than vision. Discussions include the use of representations in the design process. Designers are able to use traditional forms of representation in the design of artifacts, such as sketches. These forms of representation are not sufficient for the design of systems. System design requires models that allow stakeholders to negotiate their view of a situation and design teams to iterate how things might work. Core ideas in the working group recommendations address issues of, substitution, formal rules, motivation, context dependency, materiality, provisionality, latency, performance, externalization, facilitation and negotiation, mediation, and measurement and evaluation. Discussions address the socio-political implications of representation and the expanding role of computing and data that call for a systems view.}
}
@article{BATTISTELLI2022,
title = {Online Strategies To Improve Quantitative Skills in Microbiology Laboratory Classes},
journal = {Journal of Microbiology & Biology Education},
volume = {23},
number = {1},
year = {2022},
issn = {1935-7877},
doi = {https://doi.org/10.1128/jmbe.00333-21},
url = {https://www.sciencedirect.com/science/article/pii/S1935787722000995},
author = {Joseph M. Battistelli and Rima B. Franklin},
keywords = {quantitative literacy, quantitative biology, problem solving, word problems, math skills, formula question, Canvas, spreadsheets, algebra, formula questions},
abstract = {Biology is an increasingly quantitative science. Thus, it is important that undergraduate biology curricula include frequent opportunities for students to practice their quantitative skills.
ABSTRACT
Biology is an increasingly quantitative science. Thus, it is important that undergraduate biology curricula include frequent opportunities for students to practice their quantitative skills. This can create a substantial grading burden for faculty teaching online and/or large enrollment courses, but the “formula question” feature present in many learning management systems (LMS) offers a solution. Using this feature, faculty set up a basic scaffold for an algebraic word problem, and the LMS can then automatically generate and grade many different versions of the question. In this paper, we describe the use of “formula questions” in an undergraduate microbiology course and specifically focus on how the strategic use of algebraic word problems at multiple points throughout the semester can help build quantitative literacy. Key to the success of this approach is that faculty provide a review of foundational mathematical skills early in the semester, even in upper-level classes. This should include reacquainting students with formatting conventions (e.g., rounding and scientific notation), familiarizing them with any idiosyncrasies of the technology platforms, and demonstrating how to solve math problems using spreadsheets. This initial effort increases student success when more complex problems are introduced later in the semester. Though the tips summarized in this paper focus on undergraduate microbiology teaching laboratories using Canvas, the approach can easily be modified to help students develop their critical thinking and quantitative reasoning skills at other levels and in other disciplines.}
}
@article{MOAVENI2018452,
title = {Modified Hankel Interaction Index Array for Input-Output Pairing with Improved Characteristics},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {18},
pages = {452-457},
year = {2018},
note = {10th IFAC Symposium on Advanced Control of Chemical Processes ADCHEM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.09.342},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318320251},
author = {Bijan Moaveni and Wolfgang Birk},
keywords = {Control configuration selection, Interaction measure, Hankel Interaction Index Array, System Gramians},
abstract = {In this study, a modified version of Hankel Interaction Index Array (HIIA) for control configuration selection is presented which can overcome some of its shortcomings, like e.g. scaling dependency, or not relating to closed loop system properties. Inspired by the relative gain array approach, the HIIA is reformulated in the relative gain thinking by considering the effect of closing loops. The ratio of the Hankel norm of the subsystems in closed and open loop are used to state a modified version of HIIA, which has improved characteristics compared to the original HIIA. Properties of the modified HIIA are discussed and benchmarked with established methods on three example cases.}
}
@article{FEMIMOL2025104019,
title = {A comprehensive review of blockchain with artificial intelligence integration for enhancing food safety and quality control},
journal = {Innovative Food Science & Emerging Technologies},
volume = {102},
pages = {104019},
year = {2025},
issn = {1466-8564},
doi = {https://doi.org/10.1016/j.ifset.2025.104019},
url = {https://www.sciencedirect.com/science/article/pii/S1466856425001031},
author = {R. Femimol and L. Nalini Joseph},
keywords = {Blockchain technology, Artificial intelligence, Food safety, Quality control, Supply chain traceability, Internet of things},
abstract = {Food safety and quality control are some of the biggest hurdles in today's global food supply chain, with complex networks, contamination risks, and limited transparency undermining efficiency. This review introduces a novel hybrid framework that uniquely integrates blockchain technology, Artificial Intelligence (AI), and the Internet of Things (IoT) to address these persistent issues. An analysis of 525 studies conducted between 2019 and has led to the selection of 70 papers that focus on the review of AI applications utilizing blockchain technology. The proposed model does not take the conventional route in that it solves critical issues regarding scalability, computational requirements, and integration complexities with IoT-based data collection and energy-efficient algorithms applied in supply chain logistic processes and operations. This method enhances the real-time processing of supply chain logistics while respecting privacy and conforming to standard practices. The model promotes unprecedented transparency, trust, and operational efficiency in food safety by integrating AI, blockchain, and IoT, thus generating new and scalable solutions to protect consumer interests and safety in logistics. This combination improves the trust of consumers also, enhances the safety of food and logistics, improving novel safety solutions for food.}
}
@article{CHONG2016257,
title = {A generalized cognitive hierarchy model of games},
journal = {Games and Economic Behavior},
volume = {99},
pages = {257-274},
year = {2016},
issn = {0899-8256},
doi = {https://doi.org/10.1016/j.geb.2016.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0899825616300847},
author = {Juin-Kuan Chong and Teck-Hua Ho and Colin Camerer},
keywords = {Cognitive hierarchy, Level- model, Level- model, Generalized cognitive hierarchy, Non-equilibrium structural models, Behavioral game theory},
abstract = {Subjects in simple games frequently exhibit non-equilibrium behaviors. Cognitive hierarchy (CH) and level k (LK) are two prevailing structural models that capture such behaviors well. This paper proposes a generalized CH (GCH) model that nests a variant of the LK model, called LM. GCH differs from CH in two ways. First, each lower level's actual frequency is exponentially weighted with α to form level-k's belief on relative proportions; α captures stereotype bias. CH assumes no stereotype bias (α=1) and LM assumes extreme bias (α=∞). Second, GCH replaces random choice with minimum aversion for level 0. Level 0s are more likely to choose strategies that never yield the minimum payoff for any of the opponent's strategies. GCH captures behaviors better than CH and LK in fifty-five n×m games from four datasets. Robustness tests using three new games further validate GCHs descriptive strength over CH and LK.}
}
@article{KAZEMI2002203,
title = {Exploring test performance in mathematics: the questions children’s answers raise},
journal = {The Journal of Mathematical Behavior},
volume = {21},
number = {2},
pages = {203-224},
year = {2002},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(02)00118-9},
url = {https://www.sciencedirect.com/science/article/pii/S0732312302001189},
author = {Elham Kazemi},
keywords = {Children’s thinking, Mathematical performance, Interpreting problems, Testing},
abstract = {This article investigates children’s mathematical performance on test items, specifically multiple-choice questions. Using interviews with 90 fourth-graders, it reveals why particular kinds of items are more or less difficult for students. By using multiple-choice questions and juxtaposing them with similar open-ended problems, the findings underscore the costs of not attending to children’s thinking in designing and interpreting problems. The data from this study suggest that when answering multiple-choice questions, students’ attention is drawn to the choices themselves. They do not necessarily think through the problem first and thus make their choices based on (often incorrect) generalizations they have made about problem-solving. Whether students answered a multiple-choice question or a similar open-ended problem first impacted both their performance and their reasoning. Moreover, children draw on their life experiences when the context of the problem is salient, thus ignoring important parameters of the stated problem. Implications for investigating children’s thinking, instruction, and test design are discussed.}
}
@incollection{MAMATHA2024259,
title = {Chapter Eleven - Bio-intelligent computing and optimization techniques for developing computerized solutions},
editor = {Anupam Biswas and Alberto Paolo Tonda and Ripon Patgiri and Krishn Kumar Mishra},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {135},
pages = {259-288},
year = {2024},
booktitle = {Applications of Nature-Inspired Computing and Optimization Techniques},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2023.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S006524582300089X},
author = {G.S. Mamatha and Haripriya V. Joshi and R. Amith},
keywords = {Bio-intelligent, Bio-inspired, Computing, Optimization technique, Bio-engineering},
abstract = {Bio-inspired computing is a field of study that Lois lee knits together subfields related to the connectionism, social behavior and emergence. It is often closely related to the field of artificial intelligence as many of its pursuits can be linked to machine learning. It relies heavily on fields of biology, computer science and mathematics. Briefly it is the use of computers to model the living phenomena and simultaneously the study of life to improve the usage of computer. Biologically inspired computation is a major subset of natural computation areas of research. Some areas of study encompassed under the canon of biologically inspired computing and their biological counterparts are, genetic algorithms, evolution, biodegradability prediction, biodegradation, cellular automata, life emergent system ants, termites, bees, wasps, neural networks, artificial immune systems rendering patterning and animal skins, bird feathers, mollusk shells and bacterial colonies. Linder Mayer systems, plant structures, communication networks and protocol, epidemiology and the spared of disease, intra membrane molecular processes in living cells, excitable media forest fires the wave heart conditions axons and sensor networks sensory organs. Optimization techniques takes more bottom-up decentralized approach and often involves the methods of specifying a set of simple rules, a set of simple organisms which adhere to those rules and method of iteratively applying those rules for example, training virtual insect to investigate to an unknown terrain for finding food includes six simple rules which can be adopted. After several generations of rules application, it is usually the case where some forms of complex behavior get built upon complexity until the end results is something markedly complex and quite often completely counterintuitive from what the original rules would be expected to produce. For this reason, most technology-oriented solutions like neural network models, algorithms and other techniques came in to existence for accurate measurements and analysis that can be used to refine statistical inference and extrapolation as system complexity increases. The rules of nature inspired computing are the principle simple rules yet after being used for over millions of years have produced remarkably complex optimization techniques. All these techniques for developing software applications along with optimization techniques are discussed in the chapter.}
}
@incollection{DORFMAN1998395,
title = {Chapter 8 Problem solving, inhibition, and frontal lobe function},
editor = {Naftali Raz},
series = {Advances in Psychology},
publisher = {North-Holland},
volume = {125},
pages = {395-448},
year = {1998},
booktitle = {The Other Side of the Error Term},
issn = {0166-4115},
doi = {https://doi.org/10.1016/S0166-4115(98)80010-1},
url = {https://www.sciencedirect.com/science/article/pii/S0166411598800101},
author = {Jennifer Dorfman},
abstract = {Traditionally, cognitive models of problem solving have not incorporated inhibitory mechanisms, conceiving of human thinking as similar to the computations carried out by a serial computer (e.g., Newell & Simon, 1972). This chapter seeks to demonstrate the importance of inhibition in problem solving by examining subject populations with selective impairments of cognitive inhibition associated with frontal lobe pathology. Studies of three groups with putative frontal lobe dysfunction are reviewed: patients with focal lesions of the prefrontal cortex; schizophrenics; and the normal elderly. It is argued that the basic deficits observed in these groups reflect breakdowns in a supervisory attentional system that modulates problem-solving activity and that is subserved by the frontal cortex (Shallice, 1982). It is concluded that it is time to abandon the computer metaphor of human problem solving and adopt a brain metaphor.}
}
@article{DEAN2020482,
title = {Deep into that darkness peering: A computational analysis of the role of depression in Edgar Allan Poe's life and death},
journal = {Journal of Affective Disorders},
volume = {266},
pages = {482-491},
year = {2020},
issn = {0165-0327},
doi = {https://doi.org/10.1016/j.jad.2020.01.098},
url = {https://www.sciencedirect.com/science/article/pii/S0165032719322554},
author = {Hannah J. Dean and Ryan L. Boyd},
keywords = {Edgar Allan Poe, LIWC, Depression, Suicide, Digital humanities},
abstract = {Background
To help shed light on the peculiar circumstances surrounding the death of the famed macabre and mystery writer, poet, editor, and literary critic, we explored the potential role of depression in the life and death of Edgar Allan Poe via his written language.
Method
Using computerized language analysis, we analyzed works from Poe's corpora of personal letters (N = 309), poems (N = 49), and short stories (N = 63), and investigated whether a pattern of linguistic cues consistent with depression and suicidal cognition were discernible throughout the writer's life, particularly in his final years. Building on past work, language scores were collapsed into a composite depression metric for each text. Data from each work type was subsequently compiled and graphed into a single plot by year, with scores exceeding the 95th percentile (p < 0.05) considered statistically significant and treated as potential depressive episodes.
Results
Significant, consistent patterns of depression were not found and do not support suicide as a cause of death. However, linguistic evidence was found suggesting the presence of several potential depressive episodes over the course of Poe's life – these episodes were the most pronounced during years of Poe's greatest success, as well as those following the death of his late wife.
Limitations
Given the sampling method, it is not possible to establish direct causality; results should be considered informed but tentative.
Conclusion
This investigation demonstrates the utility of language analysis for capturing disruptive/maladaptive emotional responses to life events.}
}