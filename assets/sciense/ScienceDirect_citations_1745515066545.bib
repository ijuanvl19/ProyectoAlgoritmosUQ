@article{MIYAMOTO2023869,
title = {An evaluation of homeostatic plasticity for ecosystems using an analytical data science approach},
journal = {Computational and Structural Biotechnology Journal},
volume = {21},
pages = {869-878},
year = {2023},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2023.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S2001037023000016},
author = {Hirokuni Miyamoto and Jun Kikuchi},
keywords = {Biodiversity, Environmental analysis, Network, Machine learning, Statistical inference},
abstract = {The natural world is constantly changing, and planetary boundaries are issuing severe warnings about biodiversity and cycles of carbon, nitrogen, and phosphorus. In other views, social problems such as global warming and food shortages are spreading to various fields. These seemingly unrelated issues are closely related, but it can be said that understanding them in an integrated manner is still a step away. However, progress in analytical technologies has been recognized in various fields and, from a microscopic perspective, with the development of instruments including next-generation sequencers (NGS), nuclear magnetic resonance (NMR), gas chromatography-mass spectrometry (GC/MS), and liquid chromatography-mass spectrometry (LC/MS), various forms of molecular information such as genome data, microflora structure, metabolome, proteome, and lipidome can be obtained. The development of new technology has made it possible to obtain molecular information in a variety of forms. From a macroscopic perspective, the development of environmental analytical instruments and environmental measurement facilities such as satellites, drones, observation ships, and semiconductor censors has increased the data availability for various environmental factors. Based on these background, the role of computational science is to provide a mechanism for integrating and understanding these seemingly disparate data sets. This review describes machine learning and the need for structural equations and statistical causal inference of these data to solve these problems. In addition to introducing actual examples of how these technologies can be utilized, we will discuss how to use these technologies to implement environmentally friendly technologies in society.}
}
@article{JOLY2017133,
title = {Corruption: The shortcut to disaster},
journal = {Sustainable Production and Consumption},
volume = {10},
pages = {133-156},
year = {2017},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2016.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S2352550916300288},
author = {Marcel Joly},
keywords = {Climate, Complex systems, Ethics, Petroleum, Politics, Zika virus},
abstract = {I invite readers to briefly explore the utility of mathematical modeling and systems thinking to properly address the impact of causal relationships associated with the higher levels of scattered corruption in the public administration on the constrained-based analysis of high-profile concerns for a sustainable economy. A recent and disastrous, but very educational, Brazilian experience is taken as the motivating example: the unequaled governmental corruption scandal hitherto known, which its epicenter has publicly being associated with the Brazilian state-owned energy company, Petrobras. Few, but remarkable, socio-environmental consequences thought to have been triggered by (or related to) the resulting political crisis, currently devastating the socioeconomic scenario in Brazil, are didactically selected for the in silico analysis proposed. Major findings show that: (a) a reductionist perspective may be illusive when comparing the distinct real-life production and consumption scenarios under corruption, and (b) nonlinear dynamics can efficiently provide theoretical plausibility for the emergent behaviors of the system, which are typically scenario-dependent and can drastically be altered when corruption evolves from a circumscribed into a scattered condition. In conclusion, these results corroborate the need for far greater attention to the issue of corruption–or more generally ethics–to aptly cope with a new array of complex global sustainability challenges that would have been unthinkable just a few decades, or years, ago.}
}
@article{THEOFILIDIS2024219,
title = {Mental Imagery: Investigating the Limits of Mental Partitioning},
journal = {Revista Colombiana de Psiquiatría},
volume = {53},
number = {3},
pages = {219-228},
year = {2024},
issn = {0034-7450},
doi = {https://doi.org/10.1016/j.rcp.2024.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S003474502400009X},
author = {Antonios Theofilidis and Maria-Valeria Karakasi and Filippos Kargopoulos},
keywords = {Mental imagery, Mental partitioning, Memory, Cognition, Neuroscience, Imaginería mental, Partición mental, Memoria, Cognición, Neurociencia},
abstract = {Introduction
Do we form mental models which bear an analogical relation to the real world like those of a photograph? Has the language of thought an analogue nature (it makes use of mental imagery) or whether it is exclusively of digital nature like language?
Objectives
The basic aim of the present study is to contribute to the ongoing work on mental imagery by extending the research to an unexplored area that of mental partitioning.
Methods
The present research sample consisted of 498 participants (234 males and 264 females). We used the SPSS software package in order to analyze our data.
Results
According to our results, we detected significant peculiarities in the cognitive performance of the participants in the tasks of mental partitioning of the Moebius strip, indicating certain limitations inherent in human thinking.
Conclusions
The position we are led to adopt is closer to that of Pylyshyn (2003), who maintained that visual mental imagery depends on abstract form of thought and on previous knowledge. Specifically, it rests on previous abstract propositional thought and knowledge rather than on concrete perceptual processes like the ones proposed by Kosslyn and Sheppard. The present work investigates a potentially valuable theoretical basis in imagery research for understanding maladaptive imagery across various related clinical disorders, while encouraging multidisciplinary approaches among cognitive psychological/neuroscientific and clinical domains.
Resumen
Introducción
¿Formamos modelos mentales que guardan una relación analógica con el mundo real como los de una fotografía? ¿Tiene el lenguaje del pensamiento una naturaleza analógica (hace uso de imágenes mentales) o es exclusivamente de naturaleza digital como el lenguaje?
Objetivos
El objetivo básico del presente estudio es contribuir al trabajo en curso sobre la imaginería mental extendiendo la investigación a un área inexplorada que es la partición mental.
Métodos
La muestra de la presente investigación estuvo compuesta por 498 participantes (234 varones y 264 mujeres). Usamos el paquete de software SPSS® para analizar nuestros datos.
Resultados
De acuerdo con nuestros resultados, detectamos peculiaridades significativas en el desempeño cognitivo de los participantes en las tareas de partición mental de la tira de Moebius, indicando ciertas limitaciones inherentes al pensamiento humano.
Conclusiones
La posición a la que nos vemos llevados a adoptar se acerca más a la de Pylyshyn (2003), quien sostenía que la imaginería mental visual depende de formas abstractas de pensamiento y de conocimientos previos. Específicamente, se basa en el pensamiento y el conocimiento proposicionales abstractos previos más que en procesos de percepción concretos como los propuestos por Kosslyn y Sheppard. El presente trabajo investiga una base teórica potencialmente valiosa en la investigación de imágenes para comprender las imágenes desadaptativas en varios trastornos clínicos relacionados, al tiempo que fomenta enfoques multidisciplinarios entre los dominios cognitivos psicológicos/neurocientíficos y clínicos.}
}
@article{HANSEN2022101637,
title = {From newspaper supplement to data company: Tracking rhetorical change in the Times Higher Education’s rankings coverage},
journal = {Poetics},
volume = {92},
pages = {101637},
year = {2022},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2021.101637},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X21001352},
author = {Morten Hansen and Astrid {Van den Bossche}},
keywords = {Computational hermeneutics, Rhetoric, Ratio, University rankings, Times Higher Education},
abstract = {Despite their importance, little is known about the companies behind global university rankings and how they have legitimized the use of league tables as structuring devices in the higher education sector. Taking a computational approach to Burke's dramatistic pentad, we analyse a corpus of 3,296 articles printed between 1994 and 2020 in the Times Higher Education magazine, publisher of the World University Rankings. We show how coverage of the rankings is subject to shifts in rhetorical strategy as Times Higher Education has developed into a ranking powerhouse. Over time, the magazine has spectacularized higher education by making changes in the rankings newsworthy, and has thereby cemented the company's position as an arbiter, reporter, and consultant in the sector.}
}
@article{CHIHAB2023106810,
title = {Thermal performance and energy efficiency of the composite clay and hemp fibers},
journal = {Journal of Building Engineering},
volume = {73},
pages = {106810},
year = {2023},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2023.106810},
url = {https://www.sciencedirect.com/science/article/pii/S2352710223009890},
author = {Yassine Chihab and Najma Laaroussi and Mohammed Garoum},
keywords = {Dynamic thermal, Composite materials, Clay bricks, Hemp fibers, Energy-savings},
abstract = {The purpose of this research is to demonstrate that incorporating hemp fibers into earth bricks can provide an adequate level of thermal comfort by enhancing the material's dynamic thermal characteristics. The main goal is to find the optimal thickness of a clay-hemp wall to attain the highest thermal inertia values. First, the flash method was used to estimate thermal diffusivity, and the state hot plate method was used to measure the thermal conductivity of a clay brick. Using the experimental data as input, computational analysis is performed to investigate the relationship between the thermal performance of composite materials and their microstructures, with the goal of predicting the effective thermal conductivity of the composite clay-hemp. Using the Random Sequential Addition algorithm, a two-phase, three-dimensional composite clay-hemp microstructure was produced. The effective thermal conductivity of these composites was assessed using the finite volume method. The predicted thermophysical characteristics were then utilized to simulate the transient heat transfer across clay-hemp walls. The results demonstrate that when the hemp volume fraction increased, the thermal conductivity, thermal diffusivity, and thermal volumetric capacity all decreased by approximately 52%, 27%, and 35%, respectively. Furthermore, incorporating hemp fibers improved the bricks' dynamic thermal characteristics. Finally, this study has revealed that a wall composed of clay-hemp bricks of 22 cm thick and an insulating layer of 6 cm thick allows for limiting the risk of overheating during the summer months (time lag between 10 and 12 h) while also satisfying the Moroccan Thermal Construction Regulation requirement for Marrakech city.}
}
@article{KOWALCZUK2019206,
title = {The impact of the temperament model on the behavior of an autonomous driver},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {8},
pages = {206-210},
year = {2019},
note = {10th IFAC Symposium on Intelligent Autonomous Vehicles IAV 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.08.072},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319304033},
author = {Z. Kowalczuk and O. Piechowski and M. Czubenko},
keywords = {Learning, adaptation, autonomous vehicles, artificial intelligence, cognitive aspects},
abstract = {Because it is generally believed that the personality and temperament of a human driver influence his/her behavior on the road, the article presents a computational model of the temperament of an autonomous agent - a driver. First, a short review of the four ideas of Galen’s temperament in psychology is presented. Temperament traits are grouped into four other sets, one of which is chosen for implementation in the project of integration of the temperament model with the target autonomous agent. On the basis of this selection, it is proposed to modify, by introducing additional useful mechanisms of temperament, the existing model (ISD) of an autonomous robot and/or driver. In addition, other ways of extending the ISD model are indicated, as well as possible applications of the proposed system. The developed model may also be interesting for other research purposes in which the description of the human personality is important.}
}
@article{JI2024109648,
title = {Research on 3D printed titanium alloy scaffold structure induced osteogenesis: Mechanics and in vitro testing},
journal = {Materials Today Communications},
volume = {40},
pages = {109648},
year = {2024},
issn = {2352-4928},
doi = {https://doi.org/10.1016/j.mtcomm.2024.109648},
url = {https://www.sciencedirect.com/science/article/pii/S2352492824016295},
author = {Yuchen Ji and Huiming Zhang and Zhixiu Jiang and Danyu Liu and Yuhao Yang and Chenxu Guan and Yucheng Su and Xinyu Wang and Feng Duan},
keywords = {Selective laser melting, Structural titanium alloy scaffold, Finite element analysis, Mechanical experiment, Osteogenesis},
abstract = {Several methods exist for repairing mandibular segmental bone defects, typically employing the implant method to accomplish the repair. Following a comparative analysis, the Ti6Al4V structural scaffold implant was chosen for bone defect repair. Triply periodic minimal surface (TPMS) is characterized by a high surface area to volume ratio, an average curvature of zero, and other notable advantages, providing a new line of thinking for bone tissue scaffolds. In this work, the in vitro osteogenesis of a cell unit measuring 4 mm was investigated. First, the finite element analysis (FEA) method and the mechanical experiment method were employed to screen the elasticity modulus of the cancellous bone of the mandible. Subsequently, the selective laser melting (SLM) technique was adopted to prepare three different structures precisely - Gyroid, octahedron, and cube - each with wall thicknesses of 0.3 mm, 0.4 mm, and 0.5 mm. In the in vitro osteogenic experiments, it was observed through confocal laser scanning microscopy (CLSM) that each scaffold displayed favorable cell spreading at 1 day and 3 days. Moreover, osteoblast cell adhesion and proliferation assays revealed improved cell adhesion and proliferation with prolonged co-cultivation time, signifying the excellent biocompatibility of the structural titanium alloy scaffolds. Furthermore, findings from cell differentiation and bioactivity assays indicated that the Gyroid structure exhibited superior osteogenesis compared to the cube and octahedron structures. However, no statistically significant difference was noted between varying wall thicknesses within the same structure.}
}
@article{FIRMIN2024128483,
title = {Parallel hyperparameter optimization of spiking neural networks},
journal = {Neurocomputing},
volume = {609},
pages = {128483},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128483},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224012542},
author = {Thomas Firmin and Pierre Boulet and El-Ghazali Talbi},
keywords = {Spiking neural networks, Hyperparameter optimization, Parallel asynchronous optimization, Bayesian optimization, STDP, SLAYER},
abstract = {Hyperparameter optimization of spiking neural networks (SNNs) is a difficult task which has not yet been deeply investigated in the literature. In this work, we designed a scalable constrained Bayesian based optimization algorithm that prevents sampling in non-spiking areas of an efficient high dimensional search space. These search spaces contain infeasible solutions that output no or only a few spikes during the training or testing phases, we call such a mode a “silent network”. Finding them is difficult, as many hyperparameters are highly correlated to the architecture and to the dataset. We leverage silent networks by designing a spike-based early stopping criterion to accelerate the optimization process of SNNs trained by spike timing dependent plasticity and surrogate gradient. We parallelized the optimization algorithm asynchronously, and ran large-scale experiments on heterogeneous multi-GPU Petascale architecture. Results show that by considering silent networks, we can design more flexible high-dimensional search spaces while maintaining a good efficacy. The optimization algorithm was able to focus on networks with high performances by preventing costly and worthless computation of silent networks.}
}
@article{BERKE2017222,
title = {Optimizing trauma-informed intervention for intimate partner violence in veterans: The role of alexithymia},
journal = {Behaviour Research and Therapy},
volume = {97},
pages = {222-229},
year = {2017},
issn = {0005-7967},
doi = {https://doi.org/10.1016/j.brat.2017.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0005796717301651},
author = {Danielle S. Berke and Alexandra Macdonald and Gina M. Poole and Galina A. Portnoy and Savannah McSheffrey and Suzannah K. Creech and Casey T. Taft},
keywords = {Veteran, Trauma, Alexithymia, Randomized control trial, Intimate partner violence},
abstract = {Recent research supports the efficacy of Strength at Home-Men's Program (SAH-M), a trauma-informed group intervention designed to reduce use of intimate partner violence (IPV) in veterans (Taft, Macdonald, Creech, Monson, & Murphy, 2016). However, change-processes facilitating the effectiveness of SAH-M have yet to be specified. Alexithymia, a deficit in the cognitive processing of emotional experience characterized by difficulty identifying and distinguishing between feelings, difficulty describing feelings, and use of an externally oriented thinking style, has been shown to predict PTSD severity and impulsive aggression; however, no studies have investigated the relationship between alexithymia and IPV. As such, the current study examined the role of improvements in alexithymia as a potential facilitator of treatment efficacy among 135 male veterans/service members, in a randomized control trial SAH-M. After an initial assessment including measures of IPV and alexithymia, participants were randomized to an Enhanced Treatment as Usual (ETAU) condition or SAH-M. Participants were assessed three and six months after baseline. Results demonstrated a statistically significant association between alexithymia and use of psychological IPV at baseline. Moreover, participants in the SAH-M condition self-reported significantly greater reductions in alexithymia over time relative to ETAU participants. Findings suggest that a trauma-informed intervention may optimize outcomes, helping men who use IPV both limit their use of violence and improve deficits in emotion processing.}
}
@article{OZTOP201343,
title = {Mirror neurons: Functions, mechanisms and models},
journal = {Neuroscience Letters},
volume = {540},
pages = {43-55},
year = {2013},
note = {The Mirror Neuron System},
issn = {0304-3940},
doi = {https://doi.org/10.1016/j.neulet.2012.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0304394012013183},
author = {Erhan Oztop and Mitsuo Kawato and Michael A. Arbib},
keywords = {Mirror neuron, Computational model, Action recognition, imitation, language evolution, Mirror neuron development, Direct matching},
abstract = {Mirror neurons for manipulation fire both when the animal manipulates an object in a specific way and when it sees another animal (or the experimenter) perform an action that is more or less similar. Such neurons were originally found in macaque monkeys, in the ventral premotor cortex, area F5 and later also in the inferior parietal lobule. Recent neuroimaging data indicate that the adult human brain is endowed with a “mirror neuron system,” putatively containing mirror neurons and other neurons, for matching the observation and execution of actions. Mirror neurons may serve action recognition in monkeys as well as humans, whereas their putative role in imitation and language may be realized in human but not in monkey. This article shows the important role of computational models in providing sufficient and causal explanations for the observed phenomena involving mirror systems and the learning processes which form them, and underlines the need for additional circuitry to lift up the monkey mirror neuron circuit to sustain the posited cognitive functions attributed to the human mirror neuron system.}
}
@article{YANG2024102469,
title = {Prosumer data center system construction and synergistic optimization of computing power, electricity and heat from a global perspective},
journal = {Thermal Science and Engineering Progress},
volume = {49},
pages = {102469},
year = {2024},
issn = {2451-9049},
doi = {https://doi.org/10.1016/j.tsep.2024.102469},
url = {https://www.sciencedirect.com/science/article/pii/S2451904924000878},
author = {Dongfang Yang and Xiaoyuan Wang and Rendong Shen and Yang Li and Lei Gu and Ruifan Zheng and Jun Zhao},
keywords = {Data center, Prosumer, Global optimization, Waste heat, Computing power},
abstract = {In the context of achieving carbon neutrality, the imperative to save energy and reduce emissions in data centers (DCs) has become paramount. Current research predominantly concentrates on internal systems of DCs, lacking the perspective of treating DCs as prosumers and subsequent global optimization, resulting in limited results. In this paper, a novel prosumer DC integrated energy system is constructed and a globally coordinated optimization strategy for the synergy of computing power, electricity, and heat is proposed. This is achieved through meticulous adjustments to the battery charge–discharge processes on the supply side, computational workloads within the DC's internal systems, and the heating temperature for waste heat utilization on the load side. The optimization objectives are centered around minimizing the renewable energy waste and operation cost. Compared to the non-optimized system, the optimized system exhibits reductions of 11.39 % in renewable energy waste, 6.96 % in operation cost, 8.89 % in grid electricity consumption, and 4.18 % in total electricity consumption. Furthermore, the strategy effectively reduces renewable energy waste and operation cost at different occupancy rates by 8.02 %–12.21 % and 6.61 %–10.44 %, respectively. Under varying battery capacities, the system demonstrates reductions in renewable energy waste and operation cost by 9.42 %–26.57 % and 6.89 %–10.70 %, confirming the effectiveness of the proposed strategy across different occupancy rates and battery capacities. The research findings further highlight the potential of globally coordinated optimization in the synergy of computing power, electricity, and heat, providing valuable insights for the sustainable development of DCs.}
}
@article{SU2024121016,
title = {Detecting anomalies with granular-ball fuzzy rough sets},
journal = {Information Sciences},
volume = {678},
pages = {121016},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121016},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524009307},
author = {Xinyu Su and Zhong Yuan and Baiyang Chen and Dezhong Peng and Hongmei Chen and Yingke Chen},
keywords = {Granular computing, Fuzzy rough sets, Granular-ball, Anomaly detection, Outlier detection},
abstract = {Most of the existing anomaly detection methods are based on a single and fine granularity input pattern, which is susceptible to noisy data and inefficient for detecting anomalies. Granular-ball computing, as a novel multi-granularity representation and computation method, can effectively compensate for these shortcomings. We utilize the fuzzy rough sets to mine the potential uncertainty information in the data efficiently. The combination of granular-ball computing and fuzzy rough sets takes into account the benefits of both methods, providing great application and research value. However, this novel combination still needs to be explored, especially for unsupervised anomaly detection. In this study, we first propose the granular-ball fuzzy rough set model, and the relevant definitions in the model are given. Subsequently, we pioneeringly present an unsupervised anomaly detection method based on granular-ball fuzzy rough sets called granular-ball fuzzy rough sets-based anomaly detection (GBFRD). Our method introduces the granular-ball fuzzy rough granules-based outlier factor to characterize the outlier degree of an object effectively. The experimental results demonstrate that GBFRD exhibits superior performance compared to the state-of-the-art methods. The code is publicly available at https://github.com/Mxeron/GBFRD.}
}
@incollection{HOLLAN199733,
title = {Chapter 2 - Information Visualization},
editor = {Marting G. Helander and Thomas K. Landauer and Prasad V. Prabhu},
booktitle = {Handbook of Human-Computer Interaction (Second Edition)},
publisher = {North-Holland},
edition = {Second Edition},
address = {Amsterdam},
pages = {33-48},
year = {1997},
isbn = {978-0-444-81862-1},
doi = {https://doi.org/10.1016/B978-044481862-1.50068-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780444818621500686},
author = {James D. Hollan and Benjamin B. Bederson and Jonathan I. Helfman},
abstract = {Publisher Summary
Computation provides the most plastic representational medium and for that it can be employed to mimic successful mechanisms of earlier media and also enables novel techniques that were not previously possible. Computation-based information presentations promise to dramatically enrich the understandings as well as assist in navigating and effectively exploiting rapidly growing and increasingly complex information collections. This chapter surveys a sample of recent information visualization research. Information visualization has a long history, dating to the earliest forms of symbolic representation, and can be approached from multiple perspectives, ranging across psychology, epistemology, graphic design, linguistics, and semiology to newer perspectives emerging from cognitive science. The goal of this chapter is to provide a glimpse of current research as it attempts to communicate the exciting potential of new dynamic representations. To accomplish this, profiling is done for selected recent work from the research group. This chapter further discusses the beginnings of a paradigm shift for thinking about information, one that starts viewing information as being much more dynamic and reactive to the nature of people's tasks, activities, and even relationships with others.}
}
@article{ADABALA2005896,
title = {From virtualized resources to virtual computing grids: the In-VIGO system},
journal = {Future Generation Computer Systems},
volume = {21},
number = {6},
pages = {896-909},
year = {2005},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2003.12.021},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X03002899},
author = {Sumalatha Adabala and Vineet Chadha and Puneet Chawla and Renato Figueiredo and José Fortes and Ivan Krsul and Andrea Matsunaga and Mauricio Tsugawa and Jian Zhang and Ming Zhao and Liping Zhu and Xiaomin Zhu},
keywords = {Virtual machines, Grid-computing, Middleware, Virtual data, Network computing},
abstract = {This paper describes the architecture of the first implementation of the In-VIGO grid-computing system. The architecture is designed to support computational tools for engineering and science research In Virtual Information Grid Organizations (as opposed to in vivo or in vitro experimental research). A novel aspect of In-VIGO is the extensive use of virtualization technology, emerging standards for grid-computing and other Internet middleware. In the context of In-VIGO, virtualization denotes the ability of resources to support multiplexing, manifolding and polymorphism (i.e. to simultaneously appear as multiple resources with possibly different functionalities). Virtualization technologies are available or emerging for all the resources needed to construct virtual grids which would ideally inherit the above mentioned properties. In particular, these technologies enable the creation of dynamic pools of virtual resources that can be aggregated on-demand for application-specific user-specific grid-computing. This change in paradigm from building grids out of physical resources to constructing virtual grids has many advantages but also requires new thinking on how to architect, manage and optimize the necessary middleware. This paper reviews the motivation for In-VIGO approach, discusses the technologies used, describes an early architecture for In-VIGO that represents a first step towards the end goal of building virtual information grids, and reports on first experiences with the In-VIGO software under development.}
}
@incollection{HOWARD2025291,
title = {Types of AI},
editor = {David Baker and Lucy Ellis},
booktitle = {Encyclopedia of Libraries, Librarianship, and Information Science (First Edition)},
publisher = {Academic Press},
edition = {First Edition},
address = {Oxford},
pages = {291-300},
year = {2025},
isbn = {978-0-323-95690-1},
doi = {https://doi.org/10.1016/B978-0-323-95689-5.00274-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323956895002741},
author = {Katherine Howard},
keywords = {AI, Artificial intelligence, Deep learning, Machine learning, Types of AI},
abstract = {The world of artificial intelligence (AI) is complex. As a relatively young discipline that is continually evolving, our understanding of how we categorize the various facets of AI also must evolve and adapt. AI falls into just two categories: AI based on capability, and AI based on functionality. AI based on capability has three sub-categories, two of which remain as purely theoretical concepts. AI that is based on functionality is distinguished by four sub-categories, with each one targeted at different problems, and solving them.}
}
@article{WESTERBERG2004447,
title = {A retrospective on design and process synthesis},
journal = {Computers & Chemical Engineering},
volume = {28},
number = {4},
pages = {447-458},
year = {2004},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2003.09.029},
url = {https://www.sciencedirect.com/science/article/pii/S0098135403002564},
author = {Arthur W Westerberg},
keywords = {Design, Process, Synthesis},
abstract = {We discuss the impact over the past 40 years of process systems thinking on the design of chemical processes. We first explore the rich set of issues related to process design, only some of which are technical. We then briefly examine simulation, optimization and more extensively process synthesis ideas as they relate to design. Throughout we note that this progress is inextricably linked with the development of computer technology.}
}
@article{QIAN2023110898,
title = {A novel granular ball computing-based fuzzy rough set for feature selection in label distribution learning},
journal = {Knowledge-Based Systems},
volume = {278},
pages = {110898},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110898},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123006482},
author = {Wenbin Qian and Fankang Xu and Jintao Huang and Jin Qian},
keywords = {Feature selection, Label distribution learning, Granular ball, Fuzzy rough set, Granular computing},
abstract = {Label distribution learning is a widely studied supervised learning diagram that can handle the problem of label ambiguity. The increasing size of datasets is accompanied by the disaster of dimensionality, which implies that the arrival of redundant and noisy features undermines the effect of label distribution learning. As a crucial data-preprocessing technique, feature selection is capable of choosing discriminative features. However, due to the complex issue of label ambiguity, traditional feature selection approaches for datasets with logical labels cannot be applied to label distribution data. In this paper, a novel granular ball computing-based fuzzy rough set (GBFRS) is proposed for label distribution feature selection. Specifically, the proposed method is first introduced at the finest granularity, i.e., calculating similarity relations between single data points. Considering that the label ambiguity issue is exacerbated by the label imbalance phenomenon, the relative similarity in label distribution space among samples is computed for better generalization of the model. Then, a robust approximation strategy is devised for the target sample by using its true different and partially different class samples. Finally, with the concept of granular balls, the method explores the similarity relations between balls and samples, and the granular ball computing-based fuzzy rough set method is developed , which is endowed with the ability to simulate the characteristics of large-scale priorities in human thinking and considers local consistency. Extensive experiments conducted on twenty-two datasets show that GBFRS can effectively select more significant features than seven state-of-the-art feature selection algorithms.}
}
@article{ZAHOOR2025100931,
title = {Child computer interactions: Cognitive development and segmenting unsafe video contents: A review},
journal = {Entertainment Computing},
volume = {53},
pages = {100931},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2025.100931},
url = {https://www.sciencedirect.com/science/article/pii/S1875952125000114},
author = {Irwa Zahoor and Sajaad Ahmed Lone},
keywords = {Child Computer Interaction (CCI), Artificial Intelligence (AI) based CCI, Cognitive development, Segmentation of inappropriate video content, and Systematic review on Child Computer Interaction},
abstract = {Computer Technology (CT) is now an integral part of our daily lives, influencing various aspects of human activity, particularly those of children. Child Computer Interactions a specialized area within CT, focuses on enhancing children’s physical activities, psychology, education, and communication through diverse computer applications. CCI is a steadily growing field that focuses on children as a prominent and emergent user group. This review article highlights the lack of research regarding the effective use of CCI technologies to promote cognitive development while reducing the risks linked to harmful digital content. The paper systematically examines the influence of CCI technologies on the cognitive development of children aged 0 to 12 years, addressing the research problem of inadequate filtering methods for unsafe video content that children are exposed to in today’s screen-dominated environment. It highlights how technological innovations, particularly in gaming, artificial intelligence, and media applications, are designed to enhance children’s skills while safeguarding their digital environment. A critical aspect of this review is the assessment of methods to filter and mitigate exposure to unsafe video content, a growing concern in today’s screen-dominated environment. The findings reveal that CCI programs significantly enhance children’s knowledge and skills with high accuracy. Moreover, the review underscores the importance of machine ethics in guiding the moral behavior of machines and ensuring the usability and safety of these technologies. This comprehensive analysis provides valuable insights into the role of CCI in fostering cognitive development and protecting children from inappropriate content.}
}
@article{WANG2025101743,
title = {Peeking at low versus high achievers’ problem-solving processes in interactive tasks with multiple items},
journal = {Thinking Skills and Creativity},
volume = {56},
pages = {101743},
year = {2025},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2024.101743},
url = {https://www.sciencedirect.com/science/article/pii/S1871187124002839},
author = {Maohua Wang and Shuai Wang and Yingbin Zhang and Siqi Shen and Shuo Feng},
keywords = {Problem-solving processes, Interactive tasks, Multiple items, Exploration strategies, Navigation strategies},
abstract = {Problem-solving is now emerging as a crucial thinking skill among students. To reflect students’ problem-solving levels, researchers designed a series of interactive tasks, especially tasks with multiple items. Researchers have examined the relationship between achievements and the use of exploration and navigation strategies in such contexts. However, the potential impact of analytical levels (macro versus micro) on problem-solving processes remains unexplored, constraining a comprehensive understanding of their processes. Especially, the difference in how the use of both exploration and navigation strategies evolve between low and high achievers at different analytical levels still needs to be uncovered. As such, our study analyzed log data (i.e., activities and related attributes) of 233 low achievers and 343 high achievers in a task consisting of six successive items at both macro (entire task) and micro (individual item) levels. We harnessed raw log data to generate meaningful insights into students’ problem-solving strategies at both the macro and micro levels, including initial goal-directed, initial non-targeted, and repeated exploration, as well as navigation behavior. The metrics of these behaviors, including behavioral frequencies and patterns, were then examined using independent t-tests and first-order Markov models. Results showed that metrics differentiating achievement levels varied by analytical levels. However, the frequency of initial goal-directed exploration and the pattern of continuous forward behavior appeared to be somewhat context-free, showing differences at both the macro and micro levels. Moreover, problem-solving processes in interactive tasks at the micro level may be linked to differences in item formats, complexity, and nuanced contexts. Our study provides deeper insights into problem-solving processes in more complex contexts, contributing to the development of more targeted tasks, more intelligent assessment and learning systems, and potential support for students’ problem-solving skills.}
}
@article{LI2021711,
title = {Prediction of BLEVE blast loading using CFD and artificial neural network},
journal = {Process Safety and Environmental Protection},
volume = {149},
pages = {711-723},
year = {2021},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2021.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S0957582021001324},
author = {Jingde Li and Qilin Li and Hong Hao and Ling Li},
keywords = {ANN, BLEVE, Blast wave, Peak pressure, CFD, Neural networks},
abstract = {Boiling Liquid Expanding Vapour Explosions (BLEVEs) are extreme explosions driven by nonlinear physical processes associated with explosively expanded vapour and flashed liquid. Blast loading generated from BLEVEs may severely harm structures and people. Prediction of such strong explosions is not currently feasible using simple tools. Physics-based Computational Fluid Dynamics (CFD) methods are commonly utilized to predict the blast loading of BLEVE by going through many empirical formulas that map input variables to the target progressively. The calculation is often time-consuming, and it is therefore impractical to apply these methods to predict explosion loads from BLEVE in normal design analysis. Thinking of the composition of empirical relations in CFD models as a complex and nonlinear function, it is necessary to find an approximation of this function that can be efficiently calculated. The Artificial Neural Network (ANN) is a data-driven computational model that is capable of approximating any functions by learning from training data. Once properly trained, ANN can produce accurate predictions even for unseen inputs. This article presents the development of an ANN model to predict blast loading of BLEVEs in an open environment. A rigorous validation process is presented for the design of ANN structure, and the selected ANN is trained using validated simulation data from CFD models. Extensive evaluation of the network predictive performance is conducted, and it shows that the developed ANN can reproduce the result of CFD models effectively and efficiently, not only on simulation data but also on real experimental data. The prediction of ANN has a percentage error around 6 % and R2 value over 0.99 with the result of CFD simulated data. It speeds up the processing time from hours to seconds and only increases the error from 26.3 %–27.6 %, compared to the CFD simulations of real experimental data. Therefore, the developed ANN model can be potentially applied in the process engineering to generate a large number of reliable data for safety and risk assessment of BLEVEs in a more efficient way.}
}
@article{GAFFLEY2024477,
title = {Survey on the Perceptions of Pregnancy and Parenthood in Trainees: Advances, Obstacles, and Growth Opportunities},
journal = {Journal of Surgical Research},
volume = {295},
pages = {477-486},
year = {2024},
issn = {0022-4804},
doi = {https://doi.org/10.1016/j.jss.2023.10.019},
url = {https://www.sciencedirect.com/science/article/pii/S0022480423005528},
author = {Michaela Gaffley and Sean Hernandez and Katherine M. Riera and Saskia Anzola},
keywords = {Graduate medical education, Parenting, Pregnancy, Residency},
abstract = {Introduction
Despite national policy changes, perspective changes on pregnancy and parenting in training are often lacking. We evaluated current viewpoints regarding pregnancy, parenthood, leave needs, and perceptions of support across trainees at our institution.
Methods
A cross-sectional survey was sent to all residents and fellows at a tertiary care academic center with >700 trainees. Demographic information, opinions on maternity and paternity leave, and opinions on institutional support and career goals were collected. The survey was sent via the Graduate Medical Education Office listserv -- 66 Accreditation Council for Graduate Medical Education (ACGME) programs and 40 non-ACGME programs.
Results
Seven hundred and forty-seven house officers received the survey with a response rate of 21.9% (n = 164). Of respondents, 81% were residents and 99 respondents were female (representing 31% of female trainees at our institution). Thirty-seven point two percent of respondents reported being parents. Twenty-five point three percent of respondents had been pregnant while a trainee with no statistical difference by specialty type (P = 0.0817). Statistically significant difference was noted in having children based on sex with men becoming parents at twice the rate of women (56% vs 26%, P < 0.001). No difference was noted between specialties on perceived support while pregnant and peripartum. Thirty percent of parent respondents reported thinking about leaving medical training after having children given family stressors. Statistical difference in thoughts of leaving medicine overall between females (46%) and males (17.6%; P = 0.0238).
Conclusions
Men and women need support as they navigate becoming parents at a naturally stressful transition period. Females consider leaving medicine at twice the rate of males after becoming parents. Our institution and other ACGME programs need greater transparency and consistent leave practices that reflect changing times.}
}
@article{WHITACRE2019148,
title = {Exploring unfamiliar paths through familiar mathematical territory: Constraints and affordances in a preservice teacher’s reasoning about fraction comparisons},
journal = {The Journal of Mathematical Behavior},
volume = {53},
pages = {148-163},
year = {2019},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2018.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0732312317301608},
author = {Ian Whitacre and Şebnem Atabaş and Kelly Findley},
keywords = {Preservice teachers, Fractions, Environment metaphor, Beliefs, Productive disposition},
abstract = {Preservice elementary teachers (PSTs) have been described as having difficulties with fractions, relying on standard procedures, and experiencing math anxiety. We are interested in productive ways in which PSTs can and do use their prior knowledge when exploring unfamiliar paths through familiar mathematical territory. We conducted interviews with PSTs in which we challenged them with various fraction comparison tasks and encouraged them to develop new strategies. In this paper, we present a case study focused on one PST who made considerable progress in her reasoning about fraction comparisons during such an interview. We use Greeno’s (1991) environment metaphor to conceptualize number sense as situated knowing in a conceptual domain. This perspective helps us account for both cognitive and affective factors. We highlight 4 themes concerning features of the mathematical environment that Jennifer (pseudonym) appeared to inhabit during the interview: (a) the interview context created a safe space that emphasized the interviewer’s interest in Jennifer’s ideas, as opposed to correct answers; (b) Jennifer used her prior knowledge of parts and wholes to ground her arguments meaningfully and as building blocks to invent new strategies; (c) she used her prior knowledge of cross multiplication as an established path that provided reassurance and facilitated her exploration of unfamiliar paths; (d) it was beliefs and affective factors, not deficiencies in knowledge, that constrained Jennifer’s exploration of unfamiliar paths through familiar mathematical territory. We discuss the implications of these findings for research concerning PSTs’ mathematical thinking and learning and for mathematics teacher education.}
}
@incollection{CORICELLI2009427,
title = {Chapter 20 - Reward-based emotions: affective evaluation of outcomes and regret learning},
editor = {Jean-Claude Dreher and Léon Tremblay},
booktitle = {Handbook of Reward and Decision Making},
publisher = {Academic Press},
address = {New York},
pages = {427-439},
year = {2009},
isbn = {978-0-12-374620-7},
doi = {https://doi.org/10.1016/B978-0-12-374620-7.00020-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780123746207000200},
author = {Giorgio Coricelli and Aldo Rustichini},
abstract = {Publisher Summary
The emotions related to experiencing rewards or punishers are not independent from the outcomes that have not occurred. Indeed, it is the counterfactual reasoning between the obtained and unobtained outcomes that determines the quality and intensity of the emotional response. This chapter concerns the behavioral effects and the neural substrates of a class of reward-based emotions, which are emotions elicited by rewards and punishers. It describes how outcome evaluation is influenced by the level of responsibility in the process of choice (agency) and by the available information regarding alternative outcomes. The data reported in the chapter suggests that cognitive context, exemplified by counterfactual thinking exerts a modulatory influence on the orbitofrontal cortex activation to rewards and punishers. The orbitofrontal cortex is also critically involved in learning in environments where the information about the rewards of the alternative foregone actions is available. These processes are addressed in humans, both in the context of normal and altered brain functions.}
}
@article{SIMONE2021103070,
title = {Rome was not built in a day. Resilience and the eternal city: Insights for urban management},
journal = {Cities},
volume = {110},
pages = {103070},
year = {2021},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2020.103070},
url = {https://www.sciencedirect.com/science/article/pii/S0264275120314189},
author = {Cristina Simone and Francesca Iandolo and Irene Fulco and Francesca Loia},
keywords = {Rome, Urban resilience, Urban management, Aspect-based sentiment analysis (ABSA), Collective perception},
abstract = {Resilience has been intensely investigated as the viable quality of individuals, groups, organizations, and systems to respond productively to notable change without engaging in an extended period of regressive behaviour. Recently, there has been growing attention to the relationship between resilience and cities. To contribute to this stimulating debate, this paper first provides the theoretical framework and links the concept of resilience to urban studies. Subsequently, it enlightens, through a systems perspective and the aspect-based sentiment analysis (ABSA) methodology, the possibility to enrich the information variety endowment of urban policymakers, generated by new information units, to foster resilience capabilities in the urban context. Specifically, a large-scale text analysis study was conducted on the city of Rome to understand the sentiments expressed within the text generated online by citizens and visitors. The positive or negative sentiments linked to the hidden problems of the urban context were organized within collective perception-based maps for each of the analysed points of interest (POIs). Since cities represent complex decision-making contexts, this study aimed to outline a methodology and a tool that would help foster resilient thinking in urban policies by enriching the diversity of the information variety endowment of urban decision-makers.}
}
@article{GUAN2022256,
title = {AFE-CNN: 3D Skeleton-based Action Recognition with Action Feature Enhancement},
journal = {Neurocomputing},
volume = {514},
pages = {256-267},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.10.016},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222012784},
author = {Shannan Guan and Haiyan Lu and Linchao Zhu and Gengfa Fang},
keywords = {3D Skeleton, Action Recognition, Feature Enhance, Attention},
abstract = {Existing 3D skeleton-based action recognition approaches reach impressive performance by encoding handcrafted action features to image format and decoding by CNNs. However, such methods are limited in two ways: a) the handcrafted action features are difficult to handle challenging actions, and b) they generally require complex CNN models to improve action recognition accuracy, which usually occur heavy computational burden. To overcome these limitations, we introduce a novel AFE-CNN, which devotes to enhance the features of 3D skeleton-based actions to adapt to challenging actions. We propose feature enhance modules from key joint, bone vector, key frame and temporal perspectives, thus the AFE-CNN is more robust to camera views and body sizes variation, and significantly improve the recognition accuracy on challenging actions. Moreover, our AFE-CNN adopts a light-weight CNN model to decode images with action feature enhanced, which ensures a much lower computational burden than the state-of-the-art methods. We evaluate the AFE-CNN on three benchmark skeleton-based action datasets: NTU RGB + D, NTU RGB + D 120, and UTKinect-Action3D, with extensive experimental results demonstrate our outstanding performance of AFE-CNN.}
}
@article{FLORES201825,
title = {Problem-based science, a constructionist approach to science literacy in middle school},
journal = {International Journal of Child-Computer Interaction},
volume = {16},
pages = {25-30},
year = {2018},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2017.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S2212868917300417},
author = {Christa Flores},
keywords = {Curriculum design, Design science, Learning spaces design, Science literacy, Maker education, Mindsets, Problem-based science, Constructionism, Constructivism, Inclusivity},
abstract = {This paper describes a four-year observation using a model designed and tested in a middle school maker space, called problem-based science (PbS). PbS was used as the primary model for a middle school science curriculum adapted by the tools and mindsets of the maker movement. PbS is learning through inventing and problem solving — while using the latest in fabrication technology, like 3D printers and laser cutters, as well as more traditional making skills, like electronics, robotics, sewing and carpentry. PbS is based on Seymour Papert’s constructionism, set to a science curriculum taught full time in a makerspace or fablab. Bridging ideas in design thinking, maker education, and applied math and science, the term problem-based science was used to describe how learning would look, sound, and feel different in a makerspace, when a focus was on learner-centered curriculum. The design and testing of this curriculum took place as part of the 5th and 6th grade science courses offered at a private (non-public) school in California (USA) the fall of 2012, through the spring of 2016. Through daily formative assessment, as well as exit surveys, the patterns and benefits of learning in a self-directed learning space, designed for constructionism, were observed. This paper shares the highlights of those years. Video taped exit surveys conducted by the author, show that self-direction is both challenging and rewarding, students often felt trusted and respected, even if they did not always feel supported in a manner common in a more teacher directed classroom setting. Daily informal classroom observations revealed that using student driven, open-ended problem solving, rather than a 100% teacher led, step by step lab, lends to a more diverse pool of leadership practice in students and higher engagement in hard problems. Students typically seen as struggling in traditional classrooms, identified as experts and successful learners in this setting. Lastly, using PbS as a model for science literacy allows the youngest of learners to practice mindsets and habits typical of real scientists and inventors, fostering early identify formation in STEM fields.}
}
@article{MOFIDI2020110192,
title = {Intelligent buildings: An overview},
journal = {Energy and Buildings},
volume = {223},
pages = {110192},
year = {2020},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2020.110192},
url = {https://www.sciencedirect.com/science/article/pii/S0378778819338289},
author = {Farhad Mofidi and Hashem Akbari},
keywords = {Intelligent building, Comfort, Energy conservation, Integrated control, Optimization, Productivity, Behavior modeling, Building simulation},
abstract = {The objective of this paper is to review the topics related to the optimized operation of intelligent buildings with respect to occupant comfort and energy consumption. To simultaneously optimize energy costs and indoor environmental quality, intelligent buildings should consider several continuously changing inputs including energy exchange processes across the building, sets of indoor and outdoor environmental parameters, energy prices, occupants’ presence, preferences, and behavior inside the building. Therefore, a well-structured framework supported by computational intelligence and optimization methods, environmental monitoring, and behavior modeling techniques, as well as comfort, productivity, and behavioral studies, are required to make optimal decisions for the indoor environment. In this paper, the main concepts, challenges, the latest studies, findings, and developments related to the six topics of (1) Occupant comfort conditions; (2) Occupant productivity; (3) Building control; (4) Computational optimization; (5) Occupant behavior modeling; (6) Environmental monitoring and analysis, in offices, commercial and residential buildings are reviewed. Moreover, future directions and challenges related to the optimized operation of intelligent buildings are discussed.}
}
@article{LI2025101851,
title = {The effects of school climate on students' creativity:The mediating role of growth mindset and self-efficacy},
journal = {Thinking Skills and Creativity},
pages = {101851},
year = {2025},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2025.101851},
url = {https://www.sciencedirect.com/science/article/pii/S1871187125001002},
author = {Zhenyu Li and Qiong Li},
keywords = {school climate, creativity, growth mindset, self-efficacy},
abstract = {The influence of school climate on students' creativity has garnered significant attention, yet the underlying mechanisms remain underexplored. This study investigates the mediating roles of growth mindset and self-efficacy in the relationship between school climate and creativity among 10- and 15-year-old Chinese students. Data were collected from 7,246 students across 150 schools in Suzhou, China, as part of the Survey on Social and Emotional Skills (SSES). The findings revealed that peer support significantly enhanced students' self-reported creativity, whereas teacher support did not directly influence it. However, teacher support positively affected teachers' and parents' evaluations of students' creativity. Furthermore, growth mindset and self-efficacy served as significant mediators, forming a chain mediation pathway that links school climate to creativity. These results underscore the importance of fostering a supportive educational environment that promotes both psychological growth and creative expression. The study provides valuable insights for educators and policymakers aiming to enhance students' creative capabilities through targeted interventions that improve school climate and develop growth mindset and self-efficacy.}
}
@incollection{HARITASHYA20221,
title = {4.01 - The Development, History and Future of Cryospheric Geomorphology},
editor = {John (Jack) F. Shroder},
booktitle = {Treatise on Geomorphology (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {1-19},
year = {2022},
isbn = {978-0-12-818235-2},
doi = {https://doi.org/10.1016/B978-0-12-818234-5.00181-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128182345001814},
author = {Umesh K. Haritashya and Jon Harbor and Hugh French},
keywords = {Cold regions geomorphology, Cryosphere, Geocryology, Glacial geomorphology, Glaciology, Permafrost, Process geomorphology, Quaternary science},
abstract = {The cryosphere is a broadly defined term associated with areas where water is in solid form. Thus, cryospheric geomorphology, in theory, should include polar and mountain regions, the arctic, ice, sea ice, glaciers, rock glaciers, snow, permafrost, ice shelves and icebergs, karst-glacial interactions, and even planetary cryosphere. No book can completely justify including all these areas/topics. In this treatise on geomorphology, the focus is primarily on conventional and applied glacial geomorphology and periglacial geomorphology with a rich history and promising future. Glacial geomorphology, for example, rose to prominence in debate surrounding the theory of Ice Ages. Detailed descriptions and novel measurements of processes and landforms and now high-end computing facilities led to sophisticated process-form modeling. Current emphases include interactions between glacial and other processes in development of mountain belts, natural hazards, hydrological interplay, and responses to climate change. Periglacial geomorphology begins with Walery von Lozinski and the IGS Spitzbergen excursion, 1910–11, followed by it becoming a descriptive branch of European-dominated climatic geomorphology with a growing emphasis on quantitative studies by the 1960s. More recently, the emergence of geocryology, cold-regions engineering, and sophisticated Quaternary studies is dominating many aspects of basic and applied periglacial geomorphology. The advent of high-resolution satellite and drone images, digital elevation models, and machine learning and large-scale data computational techniques are now leading the discovery process for cryospheric geomorphology.}
}
@article{MILLET2023107707,
title = {Defending humankind: Anthropocentric bias in the appreciation of AI art},
journal = {Computers in Human Behavior},
volume = {143},
pages = {107707},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107707},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223000584},
author = {Kobe Millet and Florian Buehler and Guanzhong Du and Michail D. Kokkoris},
keywords = {Anthropocentrism, Speciesism, Artificial intelligence (AI), Computational creativity, Computer-generated art, Awe},
abstract = {We argue that recent advances of artificial intelligence (AI) in the domain of art (e.g., music, painting) pose a profound ontological threat to anthropocentric worldviews because they challenge one of the last frontiers of the human uniqueness narrative: artistic creativity. Four experiments (N = 1708), including a high-powered preregistered experiment, consistently reveal a pervasive bias against AI-made artworks and shed light on its psychological underpinnings. The same artwork is preferred less when labeled as AI-made (vs. human-made) because it is perceived as less creative and subsequently induces less awe, an emotional response typically associated with the aesthetic appreciation of art. These effects are more pronounced among people with stronger anthropocentric creativity beliefs (i.e., who believe that creativity is a uniquely human characteristic). Systematic depreciation of AI-made art (assignment of lower creative value, suppression of emotional reactions) appears to serve a shaken anthropocentric worldview whereby creativity is exclusively reserved for humans.}
}
@article{CHIZUBEM2025487,
title = {Real-time monitoring using digital platforms for enhanced safety in hydrogen facilities – Current perspectives and future directions},
journal = {International Journal of Hydrogen Energy},
volume = {98},
pages = {487-499},
year = {2025},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2024.12.128},
url = {https://www.sciencedirect.com/science/article/pii/S036031992405331X},
author = {Benson Chizubem and Ajith Subbiah and Obasi Chukwuma Izuchukwu and Kamara Sidikie Musa},
keywords = {Digital tools, Hydrogen safety, Safety design, Risk assessment},
abstract = {Hydrogen, as a versatile and clean energy carrier, holds immense potential for addressing climate change and transitioning towards sustainable energy systems. However, ensuring safety throughout the hydrogen lifecycle remains paramount to widespread adoption. In this context, digital tools have emerged as game-changers, offering innovative solutions to enhance safety practices and mitigate risks associated with hydrogen technologies. This paper provides comprehensive information on the impacts of digital tools on hydrogen safety assessment. Consequently, digital tools have offered a valuable means of monitoring and reporting Health, Safety, and Environmental (HSE) concerns and compliance within hydrogen applications in the operational facility. Through these tools, the presence of hydrogen leaks, fire, and explosion can all be effectively tracked and managed. Also, tools such as computational fluid dynamics (CFD) simulations and machine learning algorithms, virtual reality (VR) environments, and Internet of Things (IoT) sensors for assessing release scenarios, enhanced understanding of hydrogen behaviour in different environments, and real-time monitoring of operational parameters to mitigate potential risks. The tools enable the establishment of predictive maintenance strategies within hydrogen facilities. The outcomes of this study shed light on the significance of digital tools in bolstering hydrogen safety measures and enhancing safety protocols, redefining operational efficiency, and proactively identifying and mitigating potential hazards, thereby facilitating the safety and promoting the development of hydrogen infrastructure and their sustainable deployment of hydrogen as a key enabler of the future energy transition.}
}
@article{CARDENASROBLEDO2019299,
title = {A holistic self-regulated learning model: A proposal and application in ubiquitous-learning},
journal = {Expert Systems with Applications},
volume = {123},
pages = {299-314},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419300089},
author = {Leonor Adriana Cárdenas-Robledo and Alejandro Peña-Ayala},
keywords = {Technology enhanced learning, Ubiquitous–Learning, Self–regulated learning, metacognition, cognitive load},
abstract = {Technology enhanced learning (TEL) represents an expert and intelligent paradigm in which technological affordances are used to facilitate learners' acquisition of domain knowledge (DK), where Ubiquitous–Learning (u–learning) is a TEL instance that recreates situated and immersive settings. However in such settings, learners are stressed by diverse, heterogeneous, and simultaneous stimuli that challenge their cognitive skills, increase the cognitive load, trigger emotional reactions, and bias conduct. Thus this research proposes a smart Sequencing approach that enables TEL systems to lead students to regulate their learning process. The essence of the proposal is a holistic self–regulated learning (SRL) model that encourage students to develop higher–order thinking through the practice of metacognitive skills, motivational factors, and behavioral affairs to become aware of their own learning endeavors. The approach was applied as part of the sequencing module of a u–Learning system, where students follow its suggested cognitive strategies to assist them during the programming of control equipment. Results show that, although experimental subjects had to deal with higher cognitive load, they are expected to reach higher learning achievements than their control peers. The experience reveals how TEL systems are enabled to foster learners to handle their own apprenticeship processes. As a consequence, the smart sequencing functionality of TEL is cognitively enhanced to facilitate students the simultaneal acquisition DK and development of higher–order thinking.}
}
@article{KIRSCHNER2017135,
title = {The myths of the digital native and the multitasker},
journal = {Teaching and Teacher Education},
volume = {67},
pages = {135-142},
year = {2017},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2017.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X16306692},
author = {Paul A. Kirschner and Pedro {De Bruyckere}},
keywords = {Digital native, Multitasking, Homo zappiëns, Educational reform},
abstract = {Current discussions about educational policy and practice are often embedded in a mind-set that considers students who were born in an age of omnipresent digital media to be fundamentally different from previous generations of students. These students have been labelled digital natives and have been ascribed the ability to cognitively process multiple sources of information simultaneously (i.e., they can multitask). As a result of this thinking, they are seen by teachers, educational administrators, politicians/policy makers, and the media to require an educational approach radically different from that of previous generations. This article presents scientific evidence showing that there is no such thing as a digital native who is information-skilled simply because (s)he has never known a world that was not digital. It then proceeds to present evidence that one of the alleged abilities of students in this generation, the ability to multitask, does not exist and that designing education that assumes the presence of this ability hinders rather than helps learning. The article concludes by elaborating on possible implications of this for education/educational policy.}
}
@article{PALANIYAPPAN2023994,
title = {Studying Psychosis Using Natural Language Generation: A Review of Emerging Opportunities},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {8},
number = {10},
pages = {994-1004},
year = {2023},
note = {Natural Language Processing in Psychiatry and Clinical Neuroscience Research},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2023.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S2451902223001040},
author = {Lena Palaniyappan and David Benrimoh and Alban Voppel and Roberta Rocca},
keywords = {Computational psychiatry, Deep learning, Explainable models, Large language models, Neural networks, Neuroimaging},
abstract = {Disrupted language in psychotic disorders, such as schizophrenia, can manifest as false contents and formal deviations, often described as thought disorder. These features play a critical role in the social dysfunction associated with psychosis, but we continue to lack insights regarding how and why these symptoms develop. Natural language generation (NLG) is a field of computer science that focuses on generating human-like language for various applications. The theory that psychosis is related to the evolution of language in humans suggests that NLG systems that are sufficiently evolved to generate human-like language may also exhibit psychosis-like features. In this conceptual review, we propose using NLG systems that are at various stages of development as in silico tools to study linguistic features of psychosis. We argue that a program of in silico experimental research on the network architecture, function, learning rules, and training of NLG systems can help us understand better why thought disorder occurs in patients. This will allow us to gain a better understanding of the relationship between language and psychosis and potentially pave the way for new therapeutic approaches to address this vexing challenge.}
}
@article{MELGAREJO2025121371,
title = {Optimization test function synthesis with generative adversarial networks and adaptive neuro-fuzzy systems},
journal = {Information Sciences},
volume = {686},
pages = {121371},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121371},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524012854},
author = {Miguel Melgarejo and Mariana Medina and Juan Lopez and Angie Rodriguez},
keywords = {Optimization test functions, Generative adversarial networks, Adaptive neuro-fuzzy systems, Fuzzy basis function expansion, Symbolic regression},
abstract = {This paper presents an approach to synthesizing optimization test functions that couples generative adversarial networks and adaptive neuro-fuzzy systems. A generative adversarial network produces optimization landscapes from a database of known optimization test functions, and an adaptive neuro-fuzzy system performs regression on the generated landscapes to provide closed-form expressions. These expressions can be implemented as fuzzy basis function expansions. Eight databases of two-dimensional optimization landscapes reported in the literature are used to train the generative network. Exploratory landscape analysis over the generated samples reveals that the network can lead to new optimization landscapes with features of interest. In addition, fuzzy basis function expansions provide the best approximation results when compared against two symbolic regression frameworks over several selected landscapes. Examples are used to illustrate the ability of these functions to model complex surface artifacts such as plateaus. The proposed approach can be used as a mathematical collaboration tool that couples generative artificial and computational intelligence techniques to formulate high-dimensional optimization test problems from two-dimensional synthesized functions.}
}
@incollection{HALLGRIMSSON20051,
title = {CHAPTER 1 - Variation and Variability: Central Concepts in Biology},
editor = {Benedikt Hallgrímsson and Brian K. Hall},
booktitle = {Variation},
publisher = {Academic Press},
address = {Burlington},
pages = {1-7},
year = {2005},
isbn = {978-0-12-088777-4},
doi = {https://doi.org/10.1016/B978-012088777-4/50003-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012088777450003X},
author = {Benedikt Hallgrímsson and Brian K. Hall},
abstract = {Publisher Summary
Variation is a central topic, both conceptually and historically in evolutionary biology. Phenotypic variation was Darwin's fundamental observation. Indeed, the first two chapters of On the Origin of Species deal explicitly with variation. Variation within and among species has certainly been as central to the thinking of Ernst Mayr (1963) as it was to the thinking of Sewall Wright (1968), two of the fathers of the modern synthesis. However, the study of variability or the propensity to vary, with few exceptions, has remained peripheral to study of the mechanisms of evolutionary change at any level of the biological hierarchy. Although implicit in virtually all research in the biological sciences, whether one is seeking understanding at the genetic, developmental, organismal, species, population, or ecologic/community levels, variation is seldom treated as a subject in and of itself. Variation is an extremely broad topic, and a modern treatment of this subject is not possible without a thematic focus. This chapter introduces this theme through both a hierarchical treatment and integrative approaches that point toward new directions of research.}
}
@article{WINSTON201292,
title = {The next 50years: A personal view},
journal = {Biologically Inspired Cognitive Architectures},
volume = {1},
pages = {92-99},
year = {2012},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2012.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X12000035},
author = {Patrick Henry Winston},
keywords = {Biologically inspired cognitive models, Human intelligence, Evolution of intelligence, Inner language, Story understanding, Directed perception},
abstract = {I review history, starting with Turing’s seminal paper, reaching back ultimately to when our species started to outperform other primates, searching for the questions that will help us develop a computational account of human intelligence. I answer that the right questions are: What’s different between us and the other primates and what’s the same. I answer the what’s different question by saying that we became symbolic in a way that enabled story understanding, directed perception, and easy communication, and other species did not. I argue against Turing’s reasoning-centered suggestions, offering that reasoning is just a special case of story understanding. I answer the what’s the same question by noting that our brains are largely engineered in the same exotic way, with information flowing in all directions at once. By way of example, I illustrate how these answers can influence a research program, describing the Genesis system, a system that works with short summaries of stories, provided in English, together with low-level common-sense rules and higher-level concept patterns, likewise expressed in English. Genesis answers questions, notes abstract concepts such as revenge, tells stories in a listener-aware way, and fills in story gaps using precedents. I conclude by suggesting, optimistically, that a genuine computational theory of human intelligence will emerge in the next 50years if we stick to the right, biologically inspired questions, and work toward biologically informed models.}
}
@incollection{VODOVOTZ20153,
title = {Chapter 1.1 - Interesting Times: The Translational Dilemma and the Need for Translational Systems Biology of Inflammation},
editor = {Yoram Vodovotz and Gary An},
booktitle = {Translational Systems Biology},
publisher = {Academic Press},
address = {Boston},
pages = {3-8},
year = {2015},
isbn = {978-0-12-397884-4},
doi = {https://doi.org/10.1016/B978-0-12-397884-4.00001-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012397884400001X},
author = {Yoram Vodovotz and Gary An},
keywords = {Inflammation, translational research, Translational Dilemma, Translational Systems Biology, computational modeling},
abstract = {Inflammation and critical illness are the final common pathway for many diseases, many of them terminal, and for which there are essentially no medicines. We suggest that this failing is symptomatic of a fragmented continuum of health care and biomedical research, with the primary issue being the inability to translate basic science research into treatments effectively and efficiently, termed the Translational Dilemma. We assert that this present, sad state is due to numerous deficiencies in the way biomedical research is carried out. Accentuating the problem is the fact that the Translational Dilemma is most pronounced with respect to diseases, such as critical illness, that manifest features of so-called complex systems. To address these problems and thereby help alleviate the Translational Dilemma, we have used computational modeling with an explicitly applied focus on generating clinically actionable knowledge. We call this approach Translational Systems Biology. This investigative strategy is predicated on the use of dynamic computational modeling and associated computational methods of data analysis and aggregation to accelerate the Scientific Cycle with an explicit target of generating clinically actionable knowledge.}
}
@article{CARAYON2010657,
title = {Human factors in patient safety as an innovation},
journal = {Applied Ergonomics},
volume = {41},
number = {5},
pages = {657-665},
year = {2010},
note = {Human Factors and Ergonomics in Patient Safety},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2009.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S0003687009001719},
author = {Pascale Carayon},
keywords = {Human factors and ergonomics, Patient safety, Healthcare, Innovation, Adoption, Dissemination, Diffusion, Implementation},
abstract = {The use of Human Factors and Ergonomics (HFE) tools, methods, concepts and theories has been advocated by many experts and organizations to improve patient safety. To facilitate and support the spread of HFE knowledge and skills in healthcare and patient safety, we propose to conceptualize HFE as innovations whose diffusion, dissemination, implementation and sustainability need to be understood and specified. Using Greenhalgh et al. (2004) model of innovation, we identified various factors that can either hinder or facilitate the spread of HFE innovations in healthcare organizations. Barriers include lack of systems thinking, complexity of HFE innovations and lack of understanding about the benefits of HFE innovations. Positive impact of HFE interventions on task performance and the presence of local champions can facilitate the adoption, implementation and sustainability of HFE innovations. This analysis concludes with a series of recommendations for HFE professionals, researchers and educators.}
}
@article{KRAUZLIS2014457,
title = {Attention as an effect not a cause},
journal = {Trends in Cognitive Sciences},
volume = {18},
number = {9},
pages = {457-464},
year = {2014},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2014.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S1364661314001296},
author = {Richard J. Krauzlis and Anil Bollimunta and Fabrice Arcizet and Lupeng Wang},
keywords = {attention, basal ganglia, decision making, learning, perception, superior colliculus},
abstract = {Attention is commonly thought to be important for managing the limited resources available in sensory areas of the neocortex. Here we present an alternative view that attention arises as a byproduct of circuits centered on the basal ganglia involved in value-based decision making. The central idea is that decision making depends on properly estimating the current state of the animal and its environment and that the weighted inputs to the currently prevailing estimate give rise to the filter-like properties of attention. After outlining this new framework, we describe findings from physiological, anatomical, computational, and clinical work that support this point of view. We conclude that the brain mechanisms responsible for attention employ a conserved circuit motif that predates the emergence of the neocortex.}
}
@article{FORTESCUE197967,
title = {Why the ‘language of thought’ is not a language: Some inconsistencies of the computational analogy of thought},
journal = {Journal of Pragmatics},
volume = {3},
number = {1},
pages = {67-80},
year = {1979},
issn = {0378-2166},
doi = {https://doi.org/10.1016/0378-2166(79)90006-7},
url = {https://www.sciencedirect.com/science/article/pii/0378216679900067},
author = {Michael Fortescue}
}
@article{BUCHANAN2019332,
title = {Metal 3D printing in construction: A review of methods, research, applications, opportunities and challenges},
journal = {Engineering Structures},
volume = {180},
pages = {332-348},
year = {2019},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2018.11.045},
url = {https://www.sciencedirect.com/science/article/pii/S0141029618307958},
author = {C. Buchanan and L. Gardner},
keywords = {3D printing, Additive manufacturing, Applications, Concrete, Metal, Polymers, Research, Review, Stainless steel, Structural engineering},
abstract = {3D printing, more formally known as additive manufacturing (AM), has the potential to revolutionise the construction industry, with foreseeable benefits including greater structural efficiency, reduction in material consumption and wastage, streamlining and expedition of the design-build process, enhanced customisation, greater architectural freedom and improved accuracy and safety on-site. Unlike traditional manufacturing methods for construction products, metal 3D printing offers ready opportunities to create non-prismatic sections, internal stiffening, openings, functionally graded elements, variable microstructures and mechanical properties through controlled heating and cooling and thermally-induced prestressing. Additive manufacturing offers many opportunities for the construction sector, but there will also be fresh challenges and demands, such as the need for more digitally savvy engineers, greater use of advanced computational analysis and a new way of thinking for the design and verification of structures, with greater emphasis on inspection and load testing. It is envisaged that AM will complement, rather than replace, conventional production processes, with clear potential for hybrid solutions and structural strengthening and repairs. These opportunities and challenges are explored in this paper as part of a wider review of different methods of metal 3D printing, research and early applications of additive manufacturing in the construction industry. Lessons learnt for metal 3D printing in construction from additive manufacturing using other materials and in other industries are also presented.}
}
@article{WU2025105996,
title = {A multi-sensor interval fusion adaptive regularization data assimilation model for wind direction prediction},
journal = {Journal of Wind Engineering and Industrial Aerodynamics},
volume = {257},
pages = {105996},
year = {2025},
issn = {0167-6105},
doi = {https://doi.org/10.1016/j.jweia.2024.105996},
url = {https://www.sciencedirect.com/science/article/pii/S0167610524003593},
author = {Yuang Wu and Shuo Liu and Jiachen Huang},
keywords = {Data assimilation, Reduced-order modelling, Regularization, Weather forecasting},
abstract = {Real-time forecasting of wind fields is an essential prerequisite for computational fluid predictions of pollutant transport. In the domain of data assimilation for real-time weather forecasting, obtaining high-quality meteorological data measurements poses a challenge that significantly impacts prediction accuracy. Predicting wind direction through data assimilation presents an inverse problem, and low-quality wind direction data resulting from suboptimal sensor placement can lead to ill-posedness when constructing proxy models. Consequently, previous research has extensively investigated the optimal placement of meteorological sensors. However, the data assimilation experiment has thus introduced uncertainties associated with the positions of the sensors. To achieve this goal, this study proposes a adaptive data assimilation model. This model introduces the concept of local convergence intervals on reduced-order response model, and deconstructs ill-posed intervals into well-posed intervals, and obtains a unique solution interval by regularization through the convergence range distance fusing. Finally, the model selects sensors using adaptive local weights, and implements the data assimilation process using inverse Ensemble Kalman Filter. This paper employs data from the Huailai Test Station to design simulated wind direction experiments.The results indicate that the method is capable of overcoming the shortcomings of sensor placement and can enhance the accuracy of prediction.}
}
@article{KECECI2021100386,
title = {A mixed integer programming formulation for Smashed Sums puzzle: Generating and solving problem instances},
journal = {Entertainment Computing},
volume = {36},
pages = {100386},
year = {2021},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2020.100386},
url = {https://www.sciencedirect.com/science/article/pii/S187595212030094X},
author = {Barış Keçeci},
keywords = {Smashed Sums, Sudoku, Mathematical formulation},
abstract = {Playing mind games and puzzles has 2500 years of known history. Puzzles and games constitute a research domain that is attracting the interest of scientists from numerous disciplines such as artificial and computational intelligence, neural networks etc. All types of puzzles and games contain their own logic and mathematics. Able to know the science behind them and modelling the logic that a person uses to solve them would shed light to some decisional concepts. This is particularly true from the perspective of computational intelligence. In this paper a logic-based puzzle game called Smashed Sums is considered. The binary integer linear programming formulation is proposed to use in solving and generating the puzzles. Illustrative examples are given to show the validity of the formulation. Some experimental computations are conducted to analyze the puzzle and its complexity. And several open problems are concluded for the further researches.}
}
@article{LEVIAKANGAS2025102824,
title = {Towards smart, digitalised rural regions and communities – Policies, best practices and case studies},
journal = {Technology in Society},
volume = {81},
pages = {102824},
year = {2025},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2025.102824},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X25000144},
author = {Pekka Leviäkangas and Signe Sønvisen and Diego Casado-Mansilla and Marius Mikalsen and Andrea Cimmino and Anastassios Drosou and Shahid Hussain},
abstract = {Rural communities and regions face specific challenges in terms of thin markets, low population density, and long distances. Also, the demographics of these communities are often skewed towards the elderly, and the socioeconomics is characterized by higher share of low-income populations. While the concept of urban smart communities is quite well established, such as Smart Cities, the concept of smart rural region communities is only beginning to gain scholarly attention. Smart rural communities can be understood as rural areas and communities that build on their existing strengths and assets as well as on developing new opportunities based on the aforementioned. Traditional and new networks and services can be improved by utilizing digital telecommunication technologies, innovations, and better use of data and knowledge to benefit the communities. Investing in both physical and digital connectivity, and building digital environments for innovative services, economic sustainability, jobs, and social capital can be enhanced, thus contributing to active and live rural communities. Consequently, the development of smart rural communities and regions begins to emerge in research. What is becoming evident is that achieving the ambitions of smart rural communities requires not only digital technologies but also innovation of commercial and social services, as well as better digital capabilities and skills to bridge the existing – and in places the widening - divide between rural and urban communities.}
}
@article{ZHANG202340,
title = {MCHA-Net: A multi-end composite higher-order attention network guided with hierarchical supervised signal for high-resolution remote sensing image change detection},
journal = {ISPRS Journal of Photogrammetry and Remote Sensing},
volume = {202},
pages = {40-68},
year = {2023},
issn = {0924-2716},
doi = {https://doi.org/10.1016/j.isprsjprs.2023.05.033},
url = {https://www.sciencedirect.com/science/article/pii/S0924271623001570},
author = {Haiming Zhang and Guorui Ma and Yongxian Zhang and Bin Wang and Heng Li and Lunjun Fan},
keywords = {Change detection, Higher-order attention, Multi-end network, High-resolution remote sensing image, Hierarchical supervision},
abstract = {Change detection (CD) is the main way to detect changes in the Earth’s surface features in a timely and accurate manner and to understand the interactions between humans and nature, CD is also an important scientific tool for supporting decision-making. Many convolution-based methods and Transformer-based methods have gained remarkable success in the field of CD with high-resolution remote sensing images (HRSIs) due to their powerful feature extraction capability and global information modeling capability, respectively. However, the diversity and complexity of HRSIs render CD methods constantly challenging, e.g., off-nadir angle imaging, seasonal turnover, and simultaneous changes in multiple feature types. Common convolution-based or Transformer-based encoding–decoding networks have a single way of data modeling and a low degree of feature fusion, resulting in poor applicability to different remote sensing data, poor recognition of different semantic targets, and limited accuracy. To improve the generalizability and detection accuracy of the network, we developed a composite higher-order attention network with multiple encoding paths named MCHA-Net. MCHA-Net has four encoding backbones, namely, the Siamese-learning path, Residual-learning path, and Transformer-learning path. Different encoding ways are equivalent to different thinking ends, and the integration can make the network have a stronger feature representation capability, forming a local–global-cross domain data modeling approach and making the network have powerful data sensing and mining capability. The decoding end aggregates the semantic information of each layer and integrates them into a unified linearized feature mapping module to achieve full modeling of the separability of information in the target domain. In addition, we propose a new higher-order attention mechanism to perform adaptive feature refinement for each layer of each encoding end, to guide the network in focusing on the targeted region and to guarantee the boundary integrity and internal compactness of the detection results. Moreover, we design a hierarchical network supervision schema that adds supervision signals at different feature abstraction levels to impose differentiated soft constraints on each layer of the network, ensuring high semantic consistency of features across layers and facilitating fast network fitting. Experimental results on three benchmark datasets (S2Looking, SVCD, and SYSU-CD) and one transfer application dataset (Google Dataset) show that MCHA-Net outperforms state-of-the-art methods in both visual interpretation and quantitative evaluation, and exhibits strong generalization and robustness against few-shot learning.}
}
@article{NABORS2003133,
title = {From fractions to proportional reasoning: a cognitive schemes of operation approach},
journal = {The Journal of Mathematical Behavior},
volume = {22},
number = {2},
pages = {133-179},
year = {2003},
note = {Fractions, ratio and proportional reasoning, Part A},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(03)00018-X},
url = {https://www.sciencedirect.com/science/article/pii/S073231230300018X},
author = {Wanda K Nabors},
keywords = {Fractions, Proportional reasoning, Teaching experiment, Cognitive schemes},
abstract = {Four seventh grade students participated in a constructivist teaching experiment in which manipulatives within a computer microworld were used to solve fractional reasoning tasks followed by tasks that involve concepts of rate, ratio and proportion. Through a retrospective analysis of video tapes, their thinking processes were analyzed from the perspective of the types of cognitive schemes of operation used as they engaged in the given problem situations. One result of the study indicates that the modifications of the students’ available schemes of operation when solving the fractional reasoning tasks formed a basis for the cognitive schemes of operation used in their solutions of tasks involving proportionality.}
}
@article{JIAQI2024109752,
title = {Decomposed-coordinated framework with intelligent extremum network for operational reliability analysis of complex system},
journal = {Reliability Engineering & System Safety},
volume = {242},
pages = {109752},
year = {2024},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2023.109752},
url = {https://www.sciencedirect.com/science/article/pii/S095183202300666X},
author = {Liu Jia-Qi and Feng Yun-Wen and Lu Cheng and Pan Wei-Huang},
keywords = {Machine learning method, System operational reliability, Decomposed-coordinated surrogate model, Multi-failure mode, Approximate modeling},
abstract = {The analysis of operational reliability in complex systems, which involve numerous subsystems and multiple disciplines, presents significant computational challenges due to their highly nonlinear, transient nature and the presence of many hyperparameters. Although reliability analysis models have made progress, they are still inadequate for accurately modeling composite functions with multiple sublayers and sub-functions. To improve the performance of modeling composite functions, the decomposed-coordinated intelligent extremum network model (DC-IENM) is proposed in this paper. The present study employs the decomposed-coordinated (DC) strategy as a means to effectively address the coordination relationship among multiple analysis objectives. To assess the efficacy of the proposed approach, two illustrative examples are considered: (1) the approximate and probabilistic analysis of a nonlinear function with multiple responses, and (2) the reliability analysis of civil aircraft brake system temperature. These examples serve to demonstrate the effectiveness of the developed DC-IENM. Furthermore, the modeling and simulation properties are rigorously examined by means of a comparative analysis involving various methodologies. The obtained results unequivocally indicate that the proposed DC-IENM exhibits distinct advantages in terms of both computational efficiency and precision.}
}
@article{MAIR201646,
title = {Briefing: Advanced sensing technologies for structural health monitoring},
journal = {Proceedings of the Institution of Civil Engineers - Forensic Engineering},
volume = {169},
number = {2},
pages = {46-49},
year = {2016},
issn = {2043-9903},
doi = {https://doi.org/10.1680/jfoen.16.00013},
url = {https://www.sciencedirect.com/science/article/pii/S2043990316000119},
author = {Robert J. Mair},
keywords = {buildings, structures & design, diaphragm & in situ walls, field testing & monitoring},
abstract = {The engineering, management, maintenance and upgrading of infrastructure requires fresh thinking to minimise the use of materials, energy and labour while still ensuring resilience. This can only be achieved by a full understanding of the performance of the infrastructure, both during its construction and throughout its design life, through the application of innovative sensor technologies and other emerging technologies. This approach covers the design and commissioning, construction process, exploitation and use and eventual decommissioning. Crucial elements of these emerging technologies are the innovative application of the latest sensor technologies, data management tools, manufacturing processes and supply chain management processes to the construction industry, both during infrastructure construction and throughout its design life. In particular, advances in sensing technologies such as fibre-optic sensors and wireless sensor networks for structural health monitoring have a huge potential to transform the approaches to the design, the construction and the operation of an infrastructure. This briefing article gives some examples of the recent application of these advanced sensing technologies to structural health monitoring for a large number of construction sites and existing infrastructure.}
}
@article{GUO2023100142,
title = {Advances in biorenewables-resource-waste systems and modelling},
journal = {Carbon Capture Science & Technology},
volume = {9},
pages = {100142},
year = {2023},
issn = {2772-6568},
doi = {https://doi.org/10.1016/j.ccst.2023.100142},
url = {https://www.sciencedirect.com/science/article/pii/S2772656823000465},
author = {Miao Guo and Chunfei Wu and Stephen Chapman and Xi Yu and Tom Vinestock and Astley Hastings and Pete Smith and Nilay Shah},
keywords = {Biomass, Biorenewable, Mathematical optimisation, Process design, Modelling advances},
abstract = {The transformation to a resource-circular bio-economy offers a mechanism to mitigate climate change and environmental degradation. As advanced bioeconomy components, biorenewables derived from terrestrial, aquatic biomass and waste resources are expected to play significant roles over the next decades. This study provides an overview of potential biomass resources ranging from higher plant species to phototrophic microbial cluster, and their fundamental photosynthesis processes as well as biogeochemical carbon cycles involved in ecosystems. The review reflects empirical advances in conversion technologies and processes to manufacture value-added biorenewables from biomass and waste resources. The nexus perspective of resource-biorenewable-waste has been analysed to understand their interdependency and wider interaction with environmental resources and ecosystems. We further discussed the systems perspectives of biorenewables to develop fundamental understanding of resource flows and carbon cycles across biorenewable subsystems and highlight their spatial and temporal variability. Our in-depth review suggested the system challenges of biorenewable, which are subject to nonlinearity, variability and complexity. To unlock such system complexity and address the challenges, a whole systems approach is necessary to develop fundamental understanding, design novel biorenewable solutions. Our review reflects recent advances and prospects of computational methods for biorenewable systems modelling. This covers the development and applications of first principle models, process design, quantitative evaluation of sustainability and ecosystem services and mathematical optimisation to improve design, operation and planning of processes and develop emerging biorenewable systems. Coupling these advanced computational methods, a whole systems approach enables a multi-scale modelling framework to inherently link the processes and subsystems involved in biomass ecosystems and biorenewable manufacturing. Reviewing modelling advances, our study provides insights into the emerging opportunities in biorenewable research and highlights the frontier research directions, which have the potential to impact biorenewable sector sustainability.}
}
@article{SCHACTER2012677,
title = {The Future of Memory: Remembering, Imagining, and the Brain},
journal = {Neuron},
volume = {76},
number = {4},
pages = {677-694},
year = {2012},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2012.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0896627312009919},
author = {Daniel L. Schacter and Donna Rose Addis and Demis Hassabis and Victoria C. Martin and R. Nathan Spreng and Karl K. Szpunar},
abstract = {During the past few years, there has been a dramatic increase in research examining the role of memory in imagination and future thinking. This work has revealed striking similarities between remembering the past and imagining or simulating the future, including the finding that a common brain network underlies both memory and imagination. Here, we discuss a number of key points that have emerged during recent years, focusing in particular on the importance of distinguishing between temporal and nontemporal factors in analyses of memory and imagination, the nature of differences between remembering the past and imagining the future, the identification of component processes that comprise the default network supporting memory-based simulations, and the finding that this network can couple flexibly with other networks to support complex goal-directed simulations. This growing area of research has broadened our conception of memory by highlighting the many ways in which memory supports adaptive functioning.}
}
@article{LI20151,
title = {The association between alexithymia as assessed by the 20-item Toronto Alexithymia Scale and depression: A meta-analysis},
journal = {Psychiatry Research},
volume = {227},
number = {1},
pages = {1-9},
year = {2015},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2015.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S0165178115000761},
author = {Shuwen Li and Bin Zhang and Yufang Guo and Jingping Zhang},
keywords = {Alexithymia, Depression, Meta-analysis, Questionnaire},
abstract = {Patients with depression exhibit high rates of alexithymia, representing a major public health concern. We sought to examine relationships between depression severity and alexithymia as assessed by the Toronto Alexithymia Scale (TAS-20) and the TAS-20 subscales of difficulty identifying feelings (DIF), difficulty describing feelings (DDF), and externally oriented thinking (EOT). Potentially relevant studies were obtained independently by two reviewers. Chi-square statistics based on the Q-test and I2 index assessed statistical heterogeneity between studies. Subgroup analyses were mainly used to explore sources of heterogeneity. Begg׳s test and Duval and Tweedie’ trim and fill were used to assess potential publication bias. Altogether, 3572 subjects from 20 study groups across 19 studies were included. Medium relationships were observed between depression and TAS-total score (TAS-TS), DIF, and DDF. There was also a weak relationship between EOT and depression. Subgroup analyses showed a stronger correlation between TAS-TS and depression assessed by self-reported tools than that assessed by the Hamilton Rating Scale for Depression. The heterogeneity significantly decreased only in the subgroup analysis by depression tool. We conclude that alexithymia, as assessed by the TAS-20 and its subscales DIF and DDF, is closely related to depression. These relationships were affected by depression measurement tools.}
}
@article{BARAK20171,
title = {Recurrent neural networks as versatile tools of neuroscience research},
journal = {Current Opinion in Neurobiology},
volume = {46},
pages = {1-6},
year = {2017},
note = {Computational Neuroscience},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2017.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S0959438817300429},
author = {Omri Barak},
abstract = {Recurrent neural networks (RNNs) are a class of computational models that are often used as a tool to explain neurobiological phenomena, considering anatomical, electrophysiological and computational constraints. RNNs can either be designed to implement a certain dynamical principle, or they can be trained by input–output examples. Recently, there has been large progress in utilizing trained RNNs both for computational tasks, and as explanations of neural phenomena. I will review how combining trained RNNs with reverse engineering can provide an alternative framework for modeling in neuroscience, potentially serving as a powerful hypothesis generation tool. Despite the recent progress and potential benefits, there are many fundamental gaps towards a theory of these networks. I will discuss these challenges and possible methods to attack them.}
}
@article{LI2022107258,
title = {Transformer helps identify kiwifruit diseases in complex natural environments},
journal = {Computers and Electronics in Agriculture},
volume = {200},
pages = {107258},
year = {2022},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2022.107258},
url = {https://www.sciencedirect.com/science/article/pii/S0168169922005713},
author = {Xiaopeng Li and Xiaoyu Chen and Jialin Yang and Shuqin Li},
keywords = {Identify kiwifruit diseases, Deep learning, Vision transformer, Complex environments, Convolutional neural network},
abstract = {The complex background of disease images and the small contrast between the disease area and the background easily confuse, seriously affecting the robustness and accuracy of kiwifruit disease identification models. To address the above problems, this paper proposes a disease identification model based on Vision Transformer and Convolutional Neural Network, ConvViT(Convolutional Neural Network and Vision Transformer), to identify diseases by extracting effective features of kiwifruit disease spots. The proposed ConvViT includes convolutional structure and Transformer structure: The convolutional structure is used to extract the global features of the image, and the Transformer structure is used to obtain the local features of the disease area to help the CNN see better. Meanwhile, the paper designs different models according to the number of parameters and FLOPs (floating-point operations) to improve the model's scalability. The model variants of different sizes are designed to be lightweight to run on devices with different resource constraints. We achieved 98.78% identification accuracy on the self-built kiwifruit disease dataset, with up to 4.53% improvement in identification accuracy compared to the same level of Resnet, ViT, and ResMLP, and more than 10% reduction in the number of parameters and FLOPs. Experimental results on the PlantVillage dataset and the AI Challenger 2018 also show that ConvViT has good generalizability, indicating that the proposed model can solve kiwifruit disease identification problems in complex environments and be valuable a backbone network for other identification tasks with practical applications.}
}
@article{SOBEL20101060,
title = {Interactions between causal models, theories, and social cognitive development},
journal = {Neural Networks},
volume = {23},
number = {8},
pages = {1060-1071},
year = {2010},
note = {Social Cognition: From Babies to Robots},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2010.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608010001176},
author = {David M. Sobel and David W. Buchanan and Jesse Butterfield and Odest Chadwicke Jenkins},
keywords = {Causal models, Pretense, Learning from testimony, Markov random fields, Causal generalization, Rational Model of Categorization},
abstract = {We propose a model of social cognitive development based not on a single modeling framework or the hypothesis that a single model accounts for children’s developing social cognition. Rather, we advocate a Causal Model approach (cf. Waldmann, 1996), in which models of social cognitive development take the same position as theories of social cognitive development, in that they generate novel empirical hypotheses. We describe this approach and present three examples across various aspects of social cognitive development. Our first example focuses on children’s understanding of pretense and involves only considering assumptions made by a computational framework. The second example focuses on children’s learning from “testimony”. It uses a modeling framework based on Markov random fields as a computational description of a set of empirical phenomena, and then tests a prediction of that description. The third example considers infants’ generalization of action learned from imitation. Here, we use a modified version of the Rational Model of Categorization to explain children’s inferences. Taken together, these examples suggest that research in social cognitive development can be assisted by considering how computational modeling can lead researchers towards testing novel hypotheses.}
}
@article{TUYSUZ2024107221,
title = {A novel decomposed Z-fuzzy TOPSIS method with functional and dysfunctional judgments: An application to transfer center location selection},
journal = {Engineering Applications of Artificial Intelligence},
volume = {127},
pages = {107221},
year = {2024},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.107221},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623014057},
author = {Nurdan Tüysüz and Cengiz Kahraman},
keywords = {Decomposed fuzzy sets, Fuzzy MCDM, Reliability, TOPSIS, Z-Fuzzy numbers},
abstract = {Decomposed fuzzy sets (DFSs) are one of the latest extensions of intuitionistic fuzzy sets which are introduced to express vague and imprecise information to be used in multi-criteria decision-making. DFSs represent the human thinking structure in a multidirectional way, and they enable it through functional and dysfunctional judgments. However, DFSs cannot completely represent the entire human mindset as they are incapable of capturing reliability information, as it is in the other extensions, and this inability may cause wrong decisions to be given. To handle this problem, decomposed Z-fuzzy numbers, which are the integrated DFSs with reliability information provided by Z-fuzzy numbers, are introduced to model functional and dysfunctional judgments for taking the consistency of decision makers into account. Collecting judgments with both their fuzzy restrictions and fuzzy reliabilities from decision makers based on functional and dysfunctional questions provide more consistent and reliable judgments in the practice. Subsequently, a new decomposed Z-fuzzy linguistic scale and defuzzification formula are introduced to reach a final solution. Then, decomposed Z-fuzzy TOPSIS method is developed. Finally, we analyze the effect of the reliability parameter on the given decisions and present a comparative analysis with crisp TOPSIS method by an application of transfer center location selection for a private cargo company in Marmara Region of Turkey.}
}
@article{ZHU20207240,
title = {A New Distribution-Free Concept for Representing, Comparing, and Propagating Uncertainty in Dynamical Systems with Kernel Probabilistic Programming⁎⁎This project has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 798321, the German Federal Ministry for Economic Affairs and Energy (BMWi) via eco4wind (0324125B) and DyConPV (0324166B), and by DFG via Research Unit FOR 2401.},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {7240-7247},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.557},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320308569},
author = {Jia-Jie Zhu and Krikamol Muandet and Moritz Diehl and Bernhard Schölkopf},
keywords = {Uncertainty Quantification, Machine Learning, Kernel Methods, Nonparametric Methods, Stochastic System Identification, Robust Control, Randomized Algorithms},
abstract = {This work presents the concept of kernel mean embedding and kernel probabilistic programming in the context of stochastic systems. We propose formulations to represent, compare, and propagate uncertainties for fairly general stochastic dynamics in a distribution-free manner. The new tools enjoy sound theory rooted in functional analysis and wide applicability as demonstrated in distinct numerical examples. The implication of this new concept is a new mode of thinking about the statistical nature of uncertainty in dynamical systems.}
}
@article{PRIORELLI2024e39129,
title = {Slow but flexible or fast but rigid? Discrete and continuous processes compared},
journal = {Heliyon},
volume = {10},
number = {20},
pages = {e39129},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e39129},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024151602},
author = {Matteo Priorelli and Ivilin Peev Stoianov},
abstract = {A tradeoff exists when dealing with complex tasks composed of multiple steps. High-level cognitive processes can find the best sequence of actions to achieve a goal in uncertain environments, but they are slow and require significant computational demand. In contrast, lower-level processing allows reacting to environmental stimuli rapidly, but with limited capacity to determine optimal actions or to replan when expectations are not met. Through reiteration of the same task, biological organisms find the optimal tradeoff: from action primitives, composite trajectories gradually emerge by creating task-specific neural structures. The two frameworks of active inference – a recent brain paradigm that views action and perception as subject to the same free energy minimization imperative – well capture high-level and low-level processes of human behavior, but how task specialization occurs in these terms is still unclear. In this study, we compare two strategies on a dynamic pick-and-place task: a hybrid (discrete-continuous) model with planning capabilities and a continuous-only model with fixed transitions. Both models rely on a hierarchical (intrinsic and extrinsic) structure, well suited for defining reaching and grasping movements, respectively. Our results show that continuous-only models perform better and with minimal resource expenditure but at the cost of less flexibility. Finally, we propose how discrete actions might lead to continuous attractors and compare the two frameworks with different motor learning phases, laying the foundations for further studies on bio-inspired task adaptation.}
}
@article{TILSTRA201366,
title = {Cognitive processes of middle grade readers when reading expository text with an assigned goal},
journal = {Learning and Individual Differences},
volume = {28},
pages = {66-74},
year = {2013},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2013.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S1041608013001271},
author = {Janet Tilstra and Kristen L. McMaster},
keywords = {Reading, Comprehension-building, Struggling readers, Think alouds, Cognitive processes, Expository text},
abstract = {The purpose of this study was to examine 5th-grade readers' cognitive processes during reading when assigned to read for a specific goal as compared to reading for general comprehension. Equal groups of good and struggling readers (N=40) read expository texts and thought aloud while reading. In addition, the readers completed a text retell to examine the impact of an assigned goal on comprehension. During reading in the specific goal condition, both groups of readers used more study statements (monitoring, repetitions, and paraphrases) and fewer inferences (elaborative, predictive, and text-based) when thinking aloud compared with general comprehension. No reliable condition differences were noted in the amount or type of information included in retells. Implications for developing readers' comprehension-building processes when assigned a goal for reading are discussed.}
}
@article{DELGIUDICE201644,
title = {The evolutionary future of psychopathology},
journal = {Current Opinion in Psychology},
volume = {7},
pages = {44-50},
year = {2016},
note = {Evolutionary psychology},
issn = {2352-250X},
doi = {https://doi.org/10.1016/j.copsyc.2015.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S2352250X15001888},
author = {Marco {Del Giudice}},
abstract = {Evolutionary approaches to psychopathology have made considerable progress over the last years. In this paper, I review recent advances in the field focusing on three core themes: the role of trade-offs and conflicts in the origins mental disorders, the evolution of developmental mechanisms, and the emergence of alternative classification systems based on life history theory. I situate these advances in the context of current research in psychopathology, and highlight their connections with other innovative approaches such as developmental psychopathology and computational psychiatry. In total, I argue that evolutionary psychopathology offers an integrative framework for the study of mental disorders, and allows complementary approaches to connect and cross-fertilize.}
}
@article{TANAKA201664,
title = {Modeling the motor cortex: Optimality, recurrent neural networks, and spatial dynamics},
journal = {Neuroscience Research},
volume = {104},
pages = {64-71},
year = {2016},
note = {Body representation in the brain},
issn = {0168-0102},
doi = {https://doi.org/10.1016/j.neures.2015.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0168010215002631},
author = {Hirokazu Tanaka},
keywords = {Body representation, Motor control, Computational modeling, Optimization, Neural computation},
abstract = {Specialization of motor function in the frontal lobe was first discovered in the seminal experiments by Fritsch and Hitzig and subsequently by Ferrier in the 19th century. It is, however, ironical that the functional and computational role of the motor cortex still remains unresolved. A computational understanding of the motor cortex equals to understanding what movement variables the motor neurons represent (movement representation problem) and how such movement variables are computed through the interaction with anatomically connected areas (neural computation problem). Electrophysiological experiments in the 20th century demonstrated that the neural activities in motor cortex correlated with a number of motor-related and cognitive variables, thereby igniting the controversy over movement representations in motor cortex. Despite substantial experimental efforts, the overwhelming complexity found in neural activities has impeded our understanding of how movements are represented in the motor cortex. Recent progresses in computational modeling have rekindled this controversy in the 21st century. Here, I review the recent developments in computational models of the motor cortex, with a focus on optimality models, recurrent neural network models and spatial dynamics models. Although individual models provide consistent pictures within their domains, our current understanding about functions of the motor cortex is still fragmented.}
}
@article{MORAVEC2025100691,
title = {Environmental footprint of GenAI – Changing technological future or planet climate?},
journal = {Journal of Innovation & Knowledge},
volume = {10},
number = {3},
pages = {100691},
year = {2025},
issn = {2444-569X},
doi = {https://doi.org/10.1016/j.jik.2025.100691},
url = {https://www.sciencedirect.com/science/article/pii/S2444569X25000411},
author = {Vaclav Moravec and Beata Gavurova and Viliam Kovac},
keywords = {Generative artificial intelligence, ChatGPT, DeepSeek, Artificial intelligence literacy, Climate change, Data centre, Czechia},
abstract = {The beginnings of generative artificial intelligence (GenAI), led by Chat Generative Pre-Trained Transformer (ChatGPT), not only change the behaviour of digital media ecosystem users but also increase the energy consumption of enterprises working with GenAI, which presents them with a fundamental challenge in the era of climate change. This study aims to examine the relationships between the selected aspects of the use of GenAI tools and the environmental perception and behaviour of their users to understand the population's current environmental attitudes towards environmental risks and environmental sustainability. The survey was conducted in October 2024 on a sample of 1,268 respondents of the Czech Republic population. To process the data set, a logistic regression analysis, chi-squared test, Akaike information criterion, and Bayesian information criterion are employed. The results show that the more often people use GenAI tools, the more distant they consider the effects of climate change in time. The low frequency of use of ChatGPT may influence a higher willingness to change popular GenAI tools that are not maintained by environmentally friendly data centres. The frequency of ChatGPT use influences individuals’ perception of the importance of climate-change solving. The more frequently the respondents use artificial intelligence (AI) systems, they less perceive climate change as important. The low frequency of ChatGPT usage is associated with lower willingness to change email provider, transfer own data, leave social networks, stop using a favourite streaming platform and stop using a favourite GenAI platform. The respondents’ attitudes show a visible behavioural change. Internal personal motivation and self-confidence in learning, interest in career and self-confidence when using AI, the behavioural aspects, and the cognitive aspects are altered considerably. Based on the outcomes of the population survey, the study concludes that the issue of environmental friendliness of AI tools should become part of AI literacy that could strengthen population's willingness to use more energy-efficient GenAI platforms. The listed challenges are important in the perspective of the latest technological development, as shown by the discussion on the energy and computational demands of the GenAI platform DeepSeek, which is also discussed in the study.}
}
@article{CHA2011990,
title = {Measuring achievement of ICT competency for students in Korea},
journal = {Computers & Education},
volume = {56},
number = {4},
pages = {990-1002},
year = {2011},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2010.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0360131510003246},
author = {Seung Eun Cha and Soo Jin Jun and Dai Yong Kwon and Han Sung Kim and Seung Bum Kim and Ja Mee Kim and Young Ae Kim and Sun Gwan Han and Soon Sik Seo and Woo Cheon Jun and Hyun Cheol Kim and Won Gyu Lee},
keywords = {Elementary education, Pedagogical issues, Teaching/learning strategies},
abstract = {In the current information society, the need for securing human resources acquired with ICT competency is becoming a very important issue. In USA, England, Japan, India and Israel improving students’ ICT competency has become a pedagogical issue. Accordingly, education on ICT competency is changing in many countries emphasizing the basis of computer science. The Korean government revised the ICT curriculum of 2001 focused on the basic concepts and principles of computer science as educational policy in 2005. However, it is still difficult to determine a student’s ICT competency level and the outcome of ICT curriculum based on changed direction. Thereupon, this study has developed test tool for measuring the level of Korean elementary school students’ ICT competency based on computer science. In this study, ‘Content’ and ‘Information processing’ are established as the two axes of the test frame standard through literature research, consideration and discussion. The validity and reliability of questions are verified though the preliminary test and the main test tool has completed through question revisions considering the distribution of answers. About 40,000 students, roughly 1% of the total elementary school students, are selected for the main test. There were several findings made in this study. Korea’s elementary school students have a weakness in ‘algorithm and modeling’. Information processing stage has been found to vary by grade. A modified ‘Angoff method’ is used to confirm the spread of the ICT competency levels of the target students. From the results, the cutoff score employed to divide the subjects into three levels, excellent, average and below average, the ratio of excellent levels decreases and the ratio of below average increases in higher grades. To solve these problems, we need to emphasize algorithmic thinking oriented more principal of computer science in ICT curriculum. For more effective ICT elementary education, teaching and learning strategies appropriate for young children to teach computer science should be introduced.}
}
@incollection{HARSTON199029,
title = {3 - THE NEUROLOGICAL BASIS FOR NEURAL COMPUTATIONS},
editor = {Alianna J. Maren and Craig T. Harston and Robert M. Pap},
booktitle = {Handbook of Neural Computing Applications},
publisher = {Academic Press},
pages = {29-44},
year = {1990},
isbn = {978-0-12-546090-3},
doi = {https://doi.org/10.1016/B978-0-12-546090-3.50007-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780125460903500073},
author = {Craig T. Harston},
abstract = {Publisher Summary
This chapter focuses on using the human nervous system as a model for computer simulations. The brain, for example, can be better understood with computer simulations. Applying ideas from the human nervous system can solve difficult problems. There are several major design principles that can be found underlying the structural organization of different areas of the brain and that may offer long-term potential as models for design of artificial neural systems. The three major design principles are (1) layers of processing elements, (2) columns of processing elements, and (3) specialization of neural tissue into both specific and nonspecific systems. Several dynamic processes that occur in biological neural systems are integrally linked to the structures of computer simulations. These dynamics form the basis from which the higher properties of the system emerge. These structurally linked dynamic processes include distributed representation of information, temporal encoding of information, role of inhibition, and feedforward and feedback processing loops.}
}
@incollection{ADAMS2024593,
title = {Nature's novel materials: A review of quantum biology},
editor = {Tapash Chakraborty},
booktitle = {Encyclopedia of Condensed Matter Physics (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {593-604},
year = {2024},
isbn = {978-0-323-91408-6},
doi = {https://doi.org/10.1016/B978-0-323-90800-9.00268-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323908009002687},
author = {Betony Adams and Francesco Petruccione},
keywords = {Avian migration, Chirality, Coherent energy and charge transfer, Consciousness, DNA, Entanglement, Enzymes, Microtubules, Mitochondria, Nontrivial quantum effects, Photosynthesis, Posner molecules, Quantum biology, Quantum tunnelling, Reactive oxygen species, Receptors, Spin chemistry},
abstract = {Quantum biology is, most simply, the investigation of quantum effects in biological systems. While all matter is fundamentally quantum, quantum biology focuses on those quantum effects that result in properties of biological scale and relevance. It has long been argued that quantum effects are incompatible with biological systems, due to the likelihood of decoherence. But nature is a surprising and enterprising engineer. In this sense, quantum biology is the study of nature's materials and how they might sustain and enhance quantum effects, even in hot and wet environments. Quantum mechanics was developed in response to seemingly anomalous interactions between light and matter. It seems fitting that a great deal of the progress made in quantum biology in the last few decades has been on the topic of photosynthesis: the interaction of light with living matter. The interaction of electromagnetic radiation with matter is also at the heart of many other topics of quantum biology. This review is a broad overview of the current state of this research. While the underlying quantum effects are often similar—energy and charge transfer, tunnelling, spin dynamics—the biological contexts are varied and continue to grow. These contexts include enzyme catalysis, DNA mutation, receptor binding, photosynthesis, microtubule and mitochondrial function, magnetoreception, regulation of the production of reactive oxygen species, calcium ion storage and release and, potentially, consciousness. While there remain a number of reservations regarding quantum biology, progress made in this regard might further our understanding of both quantum and biological systems. With respect to the latter, advances in understanding biology might also contribute to a better understanding of physiology and the development of new therapeutics. But quantum biology might equally advance our understanding of quantum mechanics, by illustrating what quantum effects could look like in the novel materials out of which living systems are built.}
}
@article{TARI2023119635,
title = {Expansion-based Hill-climbing},
journal = {Information Sciences},
volume = {649},
pages = {119635},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.119635},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523012203},
author = {Sara Tari and Matthieu Basseur and Adrien Goëffon},
keywords = {Maximum expansion, Hill-climbing algorithm, Local search, Fitness landscapes, Landscape-aware heuristics, Combinatorial optimization},
abstract = {This paper investigates the influence of adaptive walks heuristics within local searches, by studying to what extent a wiser choice among improving neighbors influences the expected quality of the attained local optima. To this aim, we specifically focus on hill-climbers and introduce the maximum expansion pivoting rule, which selects the improving neighbor having the highest number of improving neighbors. Experiments show that having one step ahead of information allows for wiser neighbor choices, leading to better local optima. As the best improvement climber, a maximum expansion climber selects a particular search trajectory among all possible first improvement trajectories, however, it significantly increases the expected quality of the trajectory. On the other hand, the computational overhead makes this heuristic less valuable when included in an iterated search process, where repeating fast random hill-climbings from random starting points still allows a better exploration of the search space. This paper, therefore, extends previous studies on the relative efficiency of hill-climbing pivoting rules, by focusing on the original maximum expansion selection criterion. Results suggest that the achievement of good local optima combined with the use of approximation techniques and problem specificities could lead to the design of effective advanced metaheuristics that exploit the maximum expansion principle.}
}
@article{ZHAO2021270,
title = {Learnable Heterogeneous Convolution: Learning both topology and strength},
journal = {Neural Networks},
volume = {141},
pages = {270-280},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.03.038},
url = {https://www.sciencedirect.com/science/article/pii/S089360802100126X},
author = {Rongzhen Zhao and Zhenzhi Wu and Qikun Zhang},
keywords = {Convolution neural network, Efficiency & performance, Learning topology & strength, Fine-grained but structural, Hardware acceleration},
abstract = {Existing convolution techniques in artificial neural networks suffer from huge computation complexity, while the biological neural network works in a much more powerful yet efficient way. Inspired by the biological plasticity of dendritic topology and synaptic strength, our method, Learnable Heterogeneous Convolution, realizes joint learning of kernel shape and weights, which unifies existing handcrafted convolution techniques in a data-driven way. A model based on our method can converge with structural sparse weights and then be accelerated by devices of high parallelism. In the experiments, our method either reduces VGG16/19 and ResNet34/50 computation by nearly 5× on CIFAR10 and 2× on ImageNet without harming the performance, where the weights are compressed by 10× and 4× respectively; or improves the accuracy by up to 1.0% on CIFAR10 and 0.5% on ImageNet with slightly higher efficiency. The code will be available on www.github.com/Genera1Z/LearnableHeterogeneousConvolution.}
}
@article{SPELDA2020102531,
title = {The future of human-artificial intelligence nexus and its environmental costs},
journal = {Futures},
volume = {117},
pages = {102531},
year = {2020},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2020.102531},
url = {https://www.sciencedirect.com/science/article/pii/S0016328720300215},
author = {Petr Spelda and Vit Stritecky},
keywords = {Machine learning, Artificial intelligence, Inductive generalisations, Anthropocene, Climate change},
abstract = {The environmental costs and energy constraints have become emerging issues for the future development of Machine Learning (ML) and Artificial Intelligence (AI). So far, the discussion on environmental impacts of ML/AI lacks a perspective reaching beyond quantitative measurements of the energy-related research costs. Building on the foundations laid down by Schwartz et al. (2019) in the GreenAI initiative, our argument considers two interlinked phenomena, the gratuitous generalisation capability and the future where ML/AI performs the majority of quantifiable inductive inferences. The gratuitous generalisation capability refers to a discrepancy between the cognitive demands of a task to be accomplished and the performance (accuracy) of a used ML/AI model. If the latter exceeds the former because the model was optimised to achieve the best possible accuracy, it becomes inefficient and its operation harmful to the environment. The future dominated by the non-anthropic induction describes a use of ML/AI so all-pervasive that most of the inductive inferences become furnished by ML/AI generalisations. The paper argues that the present debate deserves an expansion connecting the environmental costs of research and ineffective ML/AI uses (the issue of gratuitous generalisation capability) with the (near) future marked by the all-pervasive Human-Artificial Intelligence Nexus.}
}
@article{WYLIE2025107472,
title = {15+ years of joint parallel application performance analysis/tools training with Scalasca/Score-P and Paraver/Extrae toolsets},
journal = {Future Generation Computer Systems},
volume = {162},
pages = {107472},
year = {2025},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.07.050},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24004187},
author = {Brian J.N. Wylie and Judit Giménez and Christian Feld and Markus Geimer and Germán Llort and Sandra Mendez and Estanislao Mercadal and Anke Visser and Marta García-Gasulla},
keywords = {Hybrid parallel programming, MPI message-passing, OpenMP multithreading, OpenACC device offload acceleration, HPC application execution performance measurement & analysis, Performance assessment & optimisation methodology & tools, Hands-on training & coaching},
abstract = {The diverse landscape of distributed heterogeneous computer systems currently available and being created to address computational challenges with the highest performance requirements presents daunting complexity for application developers. They must effectively decompose and distribute their application functionality and data, efficiently orchestrating the associated communication and synchronisation, on multi/manycore CPU processors with multiple attached acceleration devices structured within compute nodes with interconnection networks of various topologies. Sophisticated compilers, runtime systems and libraries are (loosely) matched with debugging, performance measurement and analysis tools, with proprietary versions by integrators/vendors provided exclusively for their systems complemented by portable (primarily) open-source equivalents developed and supported by the international research community over many years. The Scalasca and Paraver toolsets are two widely employed examples of the latter, installed on personal notebook computers through to the largest leadership HPC systems. Over more than fifteen years their developers have worked closely together in numerous collaborative projects culminating in the creation of a universal parallel performance assessment and optimisation methodology focused on application execution efficiency and scalability, and the associated training and coaching of application developers (often in teams) in its productive use, reviewed in this article with lessons learnt therefrom.}
}
@article{HEISS2023107908,
title = {Social media information literacy: Conceptualization and associations with information overload, news avoidance and conspiracy mentality},
journal = {Computers in Human Behavior},
volume = {148},
pages = {107908},
year = {2023},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.107908},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223002595},
author = {Raffael Heiss and Andreas Nanz and Jörg Matthes},
keywords = {Social media, Media literacy, Information literacy, Information overload, News avoidance, Conspiracy mentality},
abstract = {In this study, we present a novel scale for measuring social media information literacy (SMIL) that encompasses six sub-dimensions: navigation, curation, appraisal, comprehension, creation, and interaction. We also examine antecedents of SMIL, its association with information overload, and possible indirect consequences such as news avoidance and conspiracy thinking. Relying on a two-wave panel dataset (n = 901), we first used factor analysis to test the proposed measurement. The results showed that the six dimensions were empirically distinct and loaded on a higher order SMIL factor. In a second step, we explored antecedents and outcomes of SMIL and its sub-dimensions. We found that not age, but education and frequency of social media use were positively associated with gains in SMIL. Furthermore, SMIL was associated with a decrease in information overload. Information overload, in turn, was associated with a decrease in news avoidance and an increase in conspiracy mentality. Taken together, our results lend support that SMIL may support positive civic outcomes by its potential role in lowering information overload. Helping citizens to acquire SMIL may be one valuable measure to foster democratic resilience.}
}
@article{SUN2020104036,
title = {R4 Det: Refined single-stage detector with feature recursion and refinement for rotating object detection in aerial images},
journal = {Image and Vision Computing},
volume = {103},
pages = {104036},
year = {2020},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2020.104036},
url = {https://www.sciencedirect.com/science/article/pii/S0262885620301682},
author = {Peng Sun and Yongbin Zheng and Zongtan Zhou and Wanying Xu and Qiang Ren},
keywords = {Rotating object detection in aerial images and videos, Single-stage detector, A novel encoder-decoder architecture, Recursive feature pyramid, Recursive feature contact, RetinaNet-based rotation detection, Feature refinement},
abstract = {The detection of objects with multi-orientations and multi-scales in aerial images is receiving increasing attention because of numerous useful applications in computer vision, image understanding, satellite remote sensing and surveillance. However, such detection can be exceedingly challenging because of a birds eye view, multi-scale rotating objects with large aspect ratios, dense distributions and extremely imbalanced categories. Despite the considerable progress that has been made, detection performance falls considerably below that required for real-world applications. In this paper, we propose an accurate and fast end-to-end detector to address the aforementioned challenges. Our contributions are threefold. First, inspired by the looking and thinking twice mechanism, recursive neural networks and the DetectoRS detector, we propose a novel encoder-decoder based architecture by introducing the recursive feature pyramid into a single-stage object detection framework. The improved backbone network can generate increasingly powerful multi-scale representations for classification and regression. Second, we propose a refined single-stage detector with feature recursion and refinement for rotating objects. Third, we use instance balance to improve focal loss, thereby optimizing the loss in the correct direction. Extensive experiments on two challenging aerial image object detection public datasets, DOTA and HRSC2016, show that the proposed R4Det detector achieves the state-of-the-art accuracy while running very fast. Moreover, further experiments show that our detector is more robust to adversarial image patch attacks than the previous state-of-art detector.}
}
@article{NIXON2024100992,
title = {Using machine learning to predict investors’ switching behaviour},
journal = {Journal of Behavioral and Experimental Finance},
volume = {44},
pages = {100992},
year = {2024},
issn = {2214-6350},
doi = {https://doi.org/10.1016/j.jbef.2024.100992},
url = {https://www.sciencedirect.com/science/article/pii/S2214635024001072},
author = {Paul Nixon and Evan Gilbert},
keywords = {Supervised machine learning, Random forest, Risk behaviour, Risk perception},
abstract = {Individual investors’ decisions to switch investments very often lead to significantly lower investment returns so having an effective predictive model of these switches would be of value to clients, advisors and investment managers. A random forest algorithm was applied to a new dataset of over 20 million observations relating to 95,685 clients on Momentum Investments’ platform between 2018 and 2024. It identified a combination of investor characteristics (number of holdings, past switching behaviour, total assets) and external features (past returns, macroeconomic variables) as the key features of investor switch behaviour. This model exceeds commercially accepted standards in respect of the AUC and Gini metrics showcasing the model’s strength in its ranking capability. It can thus provide a useful basis for client segmentation and engagement by financial advisors.}
}
@article{REGIER201440,
title = {Task complexity and response certainty in discrete choice experiments: An application to drug treatments for juvenile idiopathic arthritis},
journal = {Journal of Behavioral and Experimental Economics},
volume = {50},
pages = {40-49},
year = {2014},
issn = {2214-8043},
doi = {https://doi.org/10.1016/j.socec.2014.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S221480431400038X},
author = {Dean A. Regier and Verity Watson and Heather Burnett and Wendy J. Ungar},
keywords = {Discrete choice experiment, Stated response certainty, Random heterogeneity, Sampling uncertainty, Dual-process thinking},
abstract = {Responding to a stated preference discrete choice experiment (DCE) is a complex task for respondents to undertake. Task complexity can induce response error, thereby decreasing the statistical precision of the econometric model. This study explores the link between task complexity and statistical precision as moderated by the level of thoughtful deliberation respondents exert when completing choice tasks. To do this, we make novel use of subjects’ certainty of response to DCE tasks as a measure of their deliberation. The distinction between intuitive and deliberate thought (System 1 and System 2, respectively) motivates how task complexity will differentially affect System 1 and System 2 respondents. The principle of utility balance in experimental design theory is used to understand how greater deliberation will increase task complexity, but will also improve statistical precision if respondents are engaging in System 2 processing. Our analyses find that increases in choice task utility balance decreases response certainty, and re-weighting the regression to favor respondents who are more uncertain of their choices increases the statistical precision of the econometric model.}
}
@article{MCNAUGHTON2024102653,
title = {The homogenous hippocampus: How hippocampal cells process available and potential goals},
journal = {Progress in Neurobiology},
volume = {240},
pages = {102653},
year = {2024},
issn = {0301-0082},
doi = {https://doi.org/10.1016/j.pneurobio.2024.102653},
url = {https://www.sciencedirect.com/science/article/pii/S0301008224000893},
author = {Neil McNaughton and David Bannerman},
keywords = {Hippocampus, Cell fields, Iteration, Goals, Space, Anxiety, Memory, Eye blink conditioning, Vinogradova},
abstract = {We present here a view of the firing patterns of hippocampal cells that is contrary, both functionally and anatomically, to conventional wisdom. We argue that the hippocampus responds to efference copies of goals encoded elsewhere; and that it uses these to detect and resolve conflict or interference between goals in general. While goals can involve space, hippocampal cells do not encode spatial (or other special types of) memory, as such. We also argue that the transverse circuits of the hippocampus operate in an essentially homogeneous way along its length. The apparently different functions of different parts (e.g. memory retrieval versus anxiety) result from the different (situational/motivational) inputs on which those parts perform the same fundamental computational operations. On this view, the key role of the hippocampus is the iterative adjustment, via Papez-like circuits, of synaptic weights in cell assemblies elsewhere.}
}
@article{VALENAS2017102,
title = {Prediction of pre-exam state anxiety from ruminative disposition: The mediating role of impaired attentional disengagement from negative information},
journal = {Behaviour Research and Therapy},
volume = {91},
pages = {102-110},
year = {2017},
issn = {0005-7967},
doi = {https://doi.org/10.1016/j.brat.2017.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S0005796717300232},
author = {Sergiu P. Vălenaş and Aurora Szentágotai-Tătar and Ben Grafton and Lies Notebaert and Andrei C. Miu and Colin MacLeod},
keywords = {Anxiety, Rumination, Attentional biases},
abstract = {Rumination is a maladaptive form of repetitive thinking that enhances stress responses, and heightened disposition to engage in rumination may contribute to the onset and persistence of stress-related symptoms. However, the cognitive mechanisms through which ruminative disposition influences stress reactivity are not yet fully understood. This study investigated the hypothesis that the impact of ruminative disposition on stress reactivity is carried by an attentional bias reflecting impaired attentional disengagement from negative information. We examined the capacity of a measure of ruminative disposition to predict both attentional biases to negative exam-related information, and state anxiety, in students approaching a mid-term exam. As expected, ruminative disposition predicted state anxiety, over and above the level predicted by trait anxiety. Ruminative disposition also predicted biased attentional disengagement from, but not biased attentional engagement with, negative information. Importantly, biased attentional disengagement from negative information mediated the relation between ruminative disposition and state anxiety. These findings confirm that dispositional rumination is associated with difficulty disengaging attention from negative information, and suggest that this attentional bias may be one of the mechanisms through which ruminative disposition influences stress reactivity.}
}
@incollection{GULATI1991173,
title = {Neurocomputing Formalisms for Computational Learning and Machine Intelligence},
editor = {Marshall C. Yovits},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {33},
pages = {173-245},
year = {1991},
issn = {0065-2458},
doi = {https://doi.org/10.1016/S0065-2458(08)60167-9},
url = {https://www.sciencedirect.com/science/article/pii/S0065245808601679},
author = {S. Gulati and J. Barhen and S.S. Iyengar},
abstract = {Publisher Summary
The chapter discusses the capabilities of neural-network learning that is central to the deeper question of its feasibility to artificial intelligence. It focuses on machine learning in the context of neural networks from the standpoints of computational complexity and algorithms information theory, and on the emerging area of learning theory in the context of dynamic systems. Engineered intelligent systems behave with remarkable rigidity when compared with, their biological counterparts, especially in their ability to recognize objects or speech, to manipulate and adapt in an unstructured environment, and to learn from past experience. A major reason for this limited technical success in emulating some of the more fundamental aspects of human intelligence lies in the differences between the organization and structuring of knowledge, and the dynamics of biological neuronal circuitry and its emulation using the symbolic-processing paradigm. The chapter rederives a theoretical framework for neural learning of nonlinear mappings, wherein both the topology of the network and synaptic interconnection strengths are evolved adaptively. The proposed methodology exploits a new class of mathematical constructs, terminal attractors, which provide unique information-processing capabilities to artificial neural systems. Terminal attractor representations are used not only to ensure infinite local stability of the encoded information, but also to provide a qualitative as well as quantitative change in the nature of the learning process. The chapter also draws from mathematical constructs in sensitivity theory for nonlinear systems to illustrate the notion of forward and adjoint-operators. The formalism exploits the concept of adjoint-operators to enable a fast global computation of the network's response to perturbations in all system parameters. This formalism eliminates the heuristic overtones of the proposed framework.}
}
@article{SHAIKHOUNI2012392,
title = {Computers and Neurosurgery},
journal = {World Neurosurgery},
volume = {78},
number = {5},
pages = {392-398},
year = {2012},
issn = {1878-8750},
doi = {https://doi.org/10.1016/j.wneu.2012.08.020},
url = {https://www.sciencedirect.com/science/article/pii/S1878875012010169},
author = {Ammar Shaikhouni and J. Bradley Elder},
keywords = {Computers, Neuroimaging, Neurosurgery, Technology},
abstract = {At the turn of the twentieth century, the only computational device used in neurosurgical procedures was the brain of the surgeon. Today, most neurosurgical procedures rely at least in part on the use of a computer to help perform surgeries accurately and safely. The techniques that revolutionized neurosurgery were mostly developed after the 1950s. Just before that era, the transistor was invented in the late 1940s, and the integrated circuit was invented in the late 1950s. During this time, the first automated, programmable computational machines were introduced. The rapid progress in the field of neurosurgery not only occurred hand in hand with the development of modern computers, but one also can state that modern neurosurgery would not exist without computers. The focus of this article is the impact modern computers have had on the practice of neurosurgery. Neuroimaging, neuronavigation, and neuromodulation are examples of tools in the armamentarium of the modern neurosurgeon that owe each step in their evolution to progress made in computer technology. Advances in computer technology central to innovations in these fields are highlighted, with particular attention to neuroimaging. Developments over the last 10 years in areas of sensors and robotics that promise to transform the practice of neurosurgery further are discussed. Potential impacts of advances in computers related to neurosurgery in developing countries and underserved regions are also discussed. As this article illustrates, the computer, with its underlying and related technologies, is central to advances in neurosurgery over the last half century.}
}
@article{VANDENENDE2022107201,
title = {A review of mathematical modeling of addiction regarding both (neuro-) psychological processes and the social contagion perspectives},
journal = {Addictive Behaviors},
volume = {127},
pages = {107201},
year = {2022},
issn = {0306-4603},
doi = {https://doi.org/10.1016/j.addbeh.2021.107201},
url = {https://www.sciencedirect.com/science/article/pii/S0306460321003865},
author = {Maarten W.J. {van den Ende} and Sacha Epskamp and Michael H. Lees and Han L.J. {van der Maas} and Reinout W. Wiers and Peter M.A. Sloot},
keywords = {Computational modeling, Addiction, Dynamical systems, Agent-based modeling, Formal theories, Review},
abstract = {Addiction is a complex biopsychosocial phenomenon, impacted by biological predispositions, psychological processes, and the social environment. Using mathematical and computational models that allow for surrogative reasoning may be a promising avenue for gaining a deeper understanding of this complex behavior. This paper reviews and classifies a selection of formal models of addiction focusing on the intra- and inter-individual dynamics, i.e., (neuro) psychological models and social models. We find that these modeling approaches to addiction are too disjoint and argue that in order to unravel the complexities of biopsychosocial processes of addiction, models should integrate intra- and inter-individual factors.}
}
@article{BENITEZANDRADES2022,
title = {Traditional Machine Learning Models and Bidirectional Encoder Representations From Transformer (BERT)–Based Automatic Classification of Tweets About Eating Disorders: Algorithm Development and Validation Study},
journal = {JMIR Medical Informatics},
volume = {10},
number = {2},
year = {2022},
issn = {2291-9694},
doi = {https://doi.org/10.2196/34492},
url = {https://www.sciencedirect.com/science/article/pii/S2291969422001004},
author = {José Alberto Benítez-Andrades and José-Manuel Alija-Pérez and Maria-Esther Vidal and Rafael Pastor-Vargas and María Teresa García-Ordás},
keywords = {natural language processing, NLP, social media, data, bidirectional encoder representations from transformer, BERT, deep learning, machine learning, eating disorder, mental health, model, classification, Twitter, nutrition, diet, weight, disorder, performance},
abstract = {Background
Eating disorders affect an increasing number of people. Social networks provide information that can help.
Objective
We aimed to find machine learning models capable of efficiently categorizing tweets about eating disorders domain.
Methods
We collected tweets related to eating disorders, for 3 consecutive months. After preprocessing, a subset of 2000 tweets was labeled: (1) messages written by people suffering from eating disorders or not, (2) messages promoting suffering from eating disorders or not, (3) informative messages or not, and (4) scientific or nonscientific messages. Traditional machine learning and deep learning models were used to classify tweets. We evaluated accuracy, F1 score, and computational time for each model.
Results
A total of 1,058,957 tweets related to eating disorders were collected. were obtained in the 4 categorizations, with The bidirectional encoder representations from transformer–based models had the best score among the machine learning and deep learning techniques applied to the 4 categorization tasks (F1 scores 71.1%-86.4%).
Conclusions
Bidirectional encoder representations from transformer–based models have better performance, although their computational cost is significantly higher than those of traditional techniques, in classifying eating disorder–related tweets.}
}
@incollection{KERN20241,
title = {Chapter 1 - Introduction and overview},
editor = {Eugene Barton Kern and Oren Friedman},
booktitle = {Empty Nose Syndrome},
publisher = {Elsevier},
pages = {1-31},
year = {2024},
isbn = {978-0-443-10715-3},
doi = {https://doi.org/10.1016/B978-0-443-10715-3.00001-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443107153000019},
author = {Eugene Barton Kern and Oren Friedman},
keywords = {Empty nose syndrome (ENS), American Rhinologic Society (ARS), secondary atrophic rhinitis, “approximate temporary truth,” on-urgical urbinate eductive djunctive rocedure (n-sTRAP), evidence-based medicine (EBM), randomized controlled trials (RCTs), total inferior turbinectomy, nasal crime, conservative submucosal inferior turbinoplasty, authorized research, clustered regularly interspaced short palindromic repeats (CRISPER), “genetic engineering,” solidated tandards f eporting rials (CONSORT statement), propensity score matching (PSM), clinical practice guidelines (CPGs), conflicts of interest (COI), computational fluid dynamics (CFD), medical journals, peer reviewers, nasal cripple, nasal physiology, “organ of the nose,” primary functions of the nose, nasal cycle, and the autonomic nervous system},
abstract = {This chapter presents the introduction and overview of the empty nose syndrome (ENS), highlighting subjects we will consider in detail with corresponding citations from the literature (included in the broad bibliography of 376 references), besides providing complete commentary regarding the fundamental features of ENS along with a review of pertinent nasal physiology, a comprehensive differential diagnosis and an analysis of the various medical, surgical, and psychological treatment options for these desperately distraught patients, some of whom have committed suicide because of their horrific torment. ENS was initially recognized and formally presented to the profession by the Mayo Clinic team in 1994. With over a quarter of a century experience treating hundreds of patients devastated by ENS, we reexamined the existing thinking concerning the etiology, differential diagnosis, diagnosis, treatment, and ultimately preventing this crippling disorder.}
}
@article{TSAI2025105305,
title = {Effects of integrating self-regulation scaffolding supported by chatbot and online collaborative reflection on students’ learning in an artificial intelligence course},
journal = {Computers & Education},
volume = {232},
pages = {105305},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2025.105305},
url = {https://www.sciencedirect.com/science/article/pii/S0360131525000739},
author = {Chia-Wen Tsai and Lynne Lee and Michael {Yu-Ching Lin} and Yih-Ping Cheng and Chih-Hsien Lin and Meng-Chuan Tsai},
keywords = {Cooperative/collaborative learning, Distance education and online learning, Pedagogical issues, Teaching/learning strategies, 21st century abilities},
abstract = {Artificial intelligence (AI) includes complex concepts and could be difficult for non-computer or information students, and they may experience difficulties in an AI course. In order to develop students' AI skills, regulate their academic stress, and reduce student loneliness, the researchers in this study integrated self-regulation scaffolding (supported by a chatbot designed in this study) with online collaborative reflection, and investigated their effects on students' learning. The researchers conducted a quasi-experiment to explore the effects of self-regulation scaffolding and online collaborative reflection. The participants in this experiment were 116 undergraduates from three classes (groups) of a non-computer department taking a compulsory course titled ‘Introduction to Artificial Intelligence’. The experimental groups in this study included the first class (G1) simultaneously receiving the interventions of self-regulation scaffolding and online collaborative reflection, as well as the second class (G2) only receiving the intervention of self-regulation scaffolding. The last class (G3), that received a traditional teaching method (non-self-regulation scaffolding and non-online collaborative reflection) in an online AI course, served as the control group. According to the statistical analysis in this study, the self-regulation scaffolding approach in G2 significantly promoted participants' AI skills and fostered their ability to regulate academic stress compared to the control group. However, G1 students who received concurrent online collaborative reflection did not demonstrate the expected effects of enhancing their learning compared with those (G2) who did not receive it. This study represents an early attempt to implement self-regulation scaffolding and online collaborative reflection integrated with chatbot in an online AI course. The results of this study could serve as a reference for online course designers, educators, and scholars planning to adopt these teaching methods, especially for those focusing on implementing online AI education and educational technologies (such as chatBot).}
}
@article{ALBERT2013353,
title = {Extending SysML for Engineering Designers by Integration of the Contact & Channel – Approach (C&C2-A) for Function-Based Modeling of Technical Systems},
journal = {Procedia Computer Science},
volume = {16},
pages = {353-362},
year = {2013},
note = {2013 Conference on Systems Engineering Research},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.01.037},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913000380},
author = {Albers Albert and Zingel Christian},
keywords = {Function-based modeling, state-based structure modeling, SysML, Contact & Channel - Approach, C&C-A, Engineering design},
abstract = {Model-Based Systems Engineering (MBSE) is advancing rapidly in the domains of software and embedded systems. In contrast, mechanical engineers still have trouble in application of MBSE. SysML has established as the leading modeling language for multidisciplinary systems, but some limitations still hinder mechanical engineers from its application. The provided behavioral and structural diagrams seem to not being sufficiently capable to represent all relevant information regarding mechanical systems. This paper presents an extending profile which aims to overcome some of those limitations. The profile is based on the Contact & Channel – Approach (C&C2-A), which is well-proven in function-based modeling of technical systems comprising function-relevant structural properties. The goals of the C&C2-A are to retain a maximal solution space, to overcome component- afflicted thinking and to provide an adequate modeling approach for mechanical relevant information. A prototypic tool implementation of the extending SysML-profile, complemented by some automatisms in form of a plugin, is evaluated at the example of a small gearbox.}
}
@article{CARVALHAES2021102165,
title = {An overview & synthesis of disaster resilience indices from a complexity perspective},
journal = {International Journal of Disaster Risk Reduction},
volume = {57},
pages = {102165},
year = {2021},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2021.102165},
url = {https://www.sciencedirect.com/science/article/pii/S221242092100131X},
author = {Thomaz M. Carvalhaes and Mikhail V. Chester and Agami T. Reddy and Braden R. Allenby},
keywords = {Complex adaptive systems, Resilience, Indicators, Disaster index, Urban systems, Socio-ecological systems},
abstract = {Identifying Disaster resilience indices (DRI) for cities and communities remains a common approach for assessing their structural ability and inherent capacity to cope with, recover from, and adapt to disasters. Particularly popular are composite DRI methodologies that are quantitative, top-down, and geographically mappable. DRI have become more comprehensive as the complexity of urban systems is increasingly acknowledged. However, DRI remain criticized as static, reductive, and inadequate when viewed under a complexity paradigm, which views urban systems as Complex Adaptive Systems (CAS), where observed properties (like resilience) emerge from many interactions among heterogenous agents in a network. Literature reviews have covered the state and trends for DRI development. Our objective is to synthesize literature at the nexus of these reviews, CAS, and Socio-ecological Systems (SES) to determine the extent to which commonly adopted indicators relate to widely accepted tenets of CAS. Findings show that DRI indicators usually relate more closely to temporal snapshots of vulnerability, and alternative framings of current indicators along with interdisciplinary approaches could better capture CAS aspects of urban resilience. Research and development should strive to develop DRI based on underlying principles of CAS and SES, and consider adapting top-down quantitative approaches with thick data, network models, and mixed-method triangulations. Explicitly associating complexity theory with DRI can (i) help researchers in socio-technical and socio-ecological domains develop improved resilience indicators and assessment methods that are clearly differentiated from vulnerability metrics, and (ii) guide policy and decision-makers, amid future uncertainty, to better identify, implement and track capacity-enhancing measures.}
}
@article{SCHORR2003465,
title = {Motion, speed, and other ideas that “should be put in books”},
journal = {The Journal of Mathematical Behavior},
volume = {22},
number = {4},
pages = {465-477},
year = {2003},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2003.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0732312303000531},
author = {Roberta Y. Schorr},
keywords = {Motion, Speed, Velocity, Graphs, Graphical representations},
abstract = {This paper focuses on a portion of a research project involving a group of inner-city middle school students who used SimCalc simulation software over the course of an entire school year to investigate ideas relating to graphical representations of motion and speed. The classroom environment was one in which students openly defended and justified their thinking as they actively explored and solved rich mathematical problems. The activities, generally speaking, involved functions that were intended to tap students’ real world intuitions as well as prior mathematical skills and understandings about speed, motion, and other graphical representations that underlie the mathematics of motion. Results indicate that these students did build ideas related to those concepts. This paper will provide documentation of the ways in which these students interpreted graphical representations involving linear and quadratic functions that are associated with constant and linearly changing velocities, respectively.}
}
@article{SALIS202320,
title = {An Edge-Cloud based Reference Architecture to support cognitive solutions in Process Industry},
journal = {Procedia Computer Science},
volume = {217},
pages = {20-30},
year = {2023},
note = {4th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.198},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922022761},
author = {Antonio Salis and Angelo Marguglio and Gabriele {De Luca} and Silvia Razzetti and Walter Quadrini and Sergio Gusmeroli},
keywords = {Industry 4.0, process industry, smart manufacturing, reference architecture, cloud, edge computing, cognitive computing, artificial intelligence, big data analytics},
abstract = {Process Industry is one of the leading sectors of the world economy, characterized however by intense environmental impact, and very high-energy consumption. Despite a traditional low innovation pace in PI, in the recent years a strong push at worldwide level towards the dual objective of improving the efficiency of plants and the quality of products, significantly reducing the consumption of electricity and CO2 emissions has taken momentum. Digital Technologies (namely Smart Embedded Systems, IoT, Data, AI and Edge-to-Cloud Technologies) are enabling drivers for a Twin Digital-Green Transition, as well as foundations for human centric, safe, comfortable and inclusive workplaces. Currently, digital sensors in plants produce a large amount of data, which in most cases constitutes just a potential and not a real value for Process Industry, often locked-in in close proprietary systems and seldomly exploited. Digital technologies, with process modelling-simulation via digital twins, can build a bridge between the physical and the virtual worlds, bringing innovation with great efficiency and drastic reduction of waste. In accordance with the guidelines of Industrie 4.0 this work proposes a modular and scalable Reference Architecture, based on open source software, which can be implemented both in brownfield and greenfield scenarios. The ability to distribute processing between the edge, where the data have been created, and the cloud, where the greatest computational resources are available, facilitates the development of integrated digital solutions with cognitive capabilities. The reference architecture is being validated in the three pilot plants, paving the way to the development of integrated planning solutions, with scheduling and control of the plants, optimizing the efficiency and reliability of the supply chain, and balancing energy efficiency.}
}
@article{GUERRERO2023107817,
title = {Forming We-intentions under breakdown situations in human-robot interactions},
journal = {Computer Methods and Programs in Biomedicine},
volume = {242},
pages = {107817},
year = {2023},
issn = {0169-2607},
doi = {https://doi.org/10.1016/j.cmpb.2023.107817},
url = {https://www.sciencedirect.com/science/article/pii/S0169260723004832},
author = {Esteban Guerrero and Maitreyee Tewari and Panu Kalmi and Helena Lindgren},
keywords = {We-intentions, Breakdown situations, Conflict of intentions, Repairing conflicts, Human-robot interaction, Answer set programming, Logic programming, Shared intentions, Social robots, Healthcare scenarios},
abstract = {Background and Objective: When agents (e.g. a person and a social robot) perform a joint activity to achieve a joint goal, they require sharing a relevant group intention, which has been defined as a We-intention. In forming We-intentions, breakdown situations due to conflicts between internal and “external” intentions are unavoidable, particularly in healthcare scenarios. To study such We-intention formation and “reparation” of conflicts, this paper has a two-fold objective: introduce a general computational mechanism allowing We-intention formation and reparation in interactions between a social robot and a person; and exemplify how the formal framework can be applied to facilitate interaction between a person and a social robot for healthcare scenarios. Method: The formal computational framework for managing We-intentions was defined in terms of Answer set programming and a Belief-Desire-Intention control loop. We exemplify the formal framework based on earlier theory-based user studies consisting of human-robot dialogue scenarios conducted in a Wizard of Oz setup, video-recorded and evaluated with 20 participants. Data was collected through semi-structured interviews, which were analyzed qualitatively using thematic analysis. N=20 participants (women n=12, men=8, age range 23-72) were part of the study. Two age groups were established for the analysis: younger participants (ages 23-40) and older participants (ages 41-72). Results: We proved four theoretical propositions, which are well-desired characteristics of any rational social robot. In our study, most participants suggested that people were the cause of breakdown situations. Over half of the young participants perceived the social robot's avoidant behavior in the scenarios. Conclusions: This work covered in depth the challenge of aligning the intentions of two agents (for example, in a person-robot interaction) when they try to achieve a joint goal. Our framework provides a novel formalization of the We-intentions theory from social science. The framework is supported by formal properties proving that our computational mechanism generates consistent potential plans. At the same time, the agent can handle incomplete and inconsistent intentions shared by another agent (for example, a person). Finally, our qualitative results suggested that this approach could provide an acceptable level of action/intention agreement generation and reparation from a person-centric perspective.}
}
@article{ESSAKALI2024123910,
title = {Advanced predictive maintenance and fault diagnosis strategy for enhanced HVAC efficiency in buildings},
journal = {Applied Thermal Engineering},
volume = {254},
pages = {123910},
year = {2024},
issn = {1359-4311},
doi = {https://doi.org/10.1016/j.applthermaleng.2024.123910},
url = {https://www.sciencedirect.com/science/article/pii/S1359431124015783},
author = {Niima Es-sakali and Zineb Zoubir and Samir {Idrissi Kaitouni} and Mohamed Oualid Mghazli and Moha Cherkaoui and Jens Pfafferott},
keywords = {Variable refrigerant flow, Predictive maintenance, Fault detection and diagnosis, Refrigerant leakage, Noise reduction models, Building Environment},
abstract = {Variable refrigerant flow (VRF) systems are constantly prone to failures during their lifespan, causing breakdowns, high energy bills, and indoor discomfort. In addition to correctly identifying these defects, fault detection, and diagnostic studies should be able to anticipate and predict the anomalies before they occur for efficient maintenance. Therefore, this study introduces an efficient self-learning predictive maintenance system, CACMMS (Cloud Air Conditioning Monitoring & Management System), designed to anticipate refrigerant leaks in VRF systems. Unlike previous efforts, this system leverages advanced fault detection and diagnosis strategies in a real existing building to enhance prediction accuracy. The study employed three noise filtering models (Kalman filter, moving average, S-G smoothing) in the preprocessing phase. Ten features were selected for assessment, and four machine learning models (decision tree, random forest, K-nearest neighbor, support vector machine) were compared. The accuracy, precision, sensitivity, computation time as well and confusion matrix were used as performance indicators and metrics to evaluate and choose the best performant model. Results indicated that decision tree and random forest models achieved over 95 % accuracy with execution times between 0.70 s and 3.32 s, outperforming K-nearest neighbor and support vector machine models. These findings highlight the system’s potential to reduce downtime and energy costs through effective predictive maintenance.}
}
@article{KASALICA20212157,
title = {APE in the Wild: Automated Exploration of Proteomics Workflows in the bio.tools Registry},
journal = {Journal of Proteome Research},
volume = {20},
number = {4},
pages = {2157-2165},
year = {2021},
issn = {1535-3907},
doi = {https://doi.org/10.1021/acs.jproteome.0c00983},
url = {https://www.sciencedirect.com/science/article/pii/S1535390721002031},
author = {Vedran Kasalica and Veit Schwämmle and Magnus Palmblad and Jon Ison and Anna-Lena Lamprecht},
keywords = {proteomics, scientific workflows, computational pipelines, workflow exploration, automated workflow composition, semantic tool annotation},
abstract = {The bio.tools registry is a main catalogue of computational tools in the life sciences. More than 17 000 tools have been registered by the international bioinformatics community. The bio.tools metadata schema includes semantic annotations of tool functions, that is, formal descriptions of tools’ data types, formats, and operations with terms from the EDAM bioinformatics ontology. Such annotations enable the automated composition of tools into multistep pipelines or workflows. In this Technical Note, we revisit a previous case study on the automated composition of proteomics workflows. We use the same four workflow scenarios but instead of using a small set of tools with carefully handcrafted annotations, we explore workflows directly on bio.tools. We use the Automated Pipeline Explorer (APE), a reimplementation and extension of the workflow composition method previously used. Moving “into the wild” opens up an unprecedented wealth of tools and a huge number of alternative workflows. Automated composition tools can be used to explore this space of possibilities systematically. Inevitably, the mixed quality of semantic annotations in bio.tools leads to unintended or erroneous tool combinations. However, our results also show that additional control mechanisms (tool filters, configuration options, and workflow constraints) can effectively guide the exploration toward smaller sets of more meaningful workflows.
}
}
@article{CALDERON201860,
title = {Sunrise Hotels: An integrated managerial accounting teaching case},
journal = {Journal of Accounting Education},
volume = {44},
pages = {60-72},
year = {2018},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2018.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0748575117302439},
author = {Thomas Calderon and James W. Hesford and Nicolas Mangin and Mina Pizzini},
keywords = {Managerial accounting, Integrated learning, Case study, Teaching case},
abstract = {“Sunrise Hotels” consists of six, linked cases developed from a field study of a large hotel chain in North America. The cases are short, so they can be distributed and solved in less than a full class period, after a short lecture by the instructor. Students often see managerial topics as an unrelated collection of tools rather than as a coherent, integrated framework for decision-making and management control. Questions included with each short case guide students, and the integration developed across six cases in a single setting should help students view managerial accounting topics as inter-related tools for decision making and control.}
}
@article{NABONI2017110,
title = {Thermal Comfort-CFD maps for Architectural Interior Design},
journal = {Procedia Engineering},
volume = {180},
pages = {110-117},
year = {2017},
note = {International High-Performance Built Environment Conference – A Sustainable Built Environment Conference 2016 Series (SBE16), iHBE 2016},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.04.170},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817316776},
author = {Emanuele Naboni and Daniel Sang-Hoon Lee and Kristian Fabbri},
keywords = {Building Retrofit, Thermal Comfort Map, Computational Fluid Dynamic, Mean Radiant Temperature, Architectural Design Process},
abstract = {Within the context of nearly Zero-Energy Buildings, it is debated that the energy-centred notion of design, proposed by regulatory frames, needs to be combined with a further focus toward users’ comfort and delight. Accordingly, the underlying theory of the research is that designers should take responsibility for understanding the heat flows through the building parts and its spaces. A design, which is sensible to the micro-thermal conditions coexisting in space, allows the inhabitants to control the building to their needs and desires: for instance, maximising the benefits of heat gain from the sun moving a series of internal partitions so as to avoid the danger of over-heating. It is thus necessary that existing simulation software tools are tested to the purpose of modelling and visualising the indoor thermal environment complexity. The research discusses how thermal comfort maps, which are prepared with the use of Computational Fluid Dynamic simulation method, could integrate energy simulation outputs to uphold qualitative architectural design decisions. Mean radiant temperature maps were thus used to design the retrofit of a small educational building in Copenhagen. The thermal opportunities of movable interior partitions (operated by the users) could be estimated, providing a new layer of information to the designer. The applicability of the thermal maps within an architectural design process is discussed adopting standard energy simulation comfort outputs as a reference. The capabilities and the limitations of the method are appraised.}
}
@incollection{KAMEDA2015441,
title = {Evolutionary Group Dynamics},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {441-447},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.81040-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868810405},
author = {Tatsuya Kameda and Mark {Van Vugt} and Robert Scott Tindale},
keywords = {Adaptation, Coordination, Evolution, Evolutionary psychology, Group cohesion, Group decision making, Group living, Human sociality, Intergroup relations, Psychological processes, Social brain, Social exchange, Social psychology, Status},
abstract = {Evolutionary psychology adds many insights to the literature on group dynamics and small-group processes. First, groups are a fundamental aspect of human evolution, suggesting that humans have evolved a range of adaptations to deal with specific threats and opportunities afforded by living in groups. Second, an evolutionary perspective integrates knowledge from numerous behavioral science disciplines such as psychology, evolutionary biology, primatology, biological anthropology, social neuroscience, and behavioral economics that are all concerned with group dynamics. Third, an evolutionary analysis produces many novel hypotheses about different aspects of our group psychology. We show the generativity of an evolutionary psychological approach through discussing examples of research applying evolutionary thinking to understanding key adaptive group tasks such as coordination, social exchange, status, group cohesion, collective decision making, and intergroup relations.}
}
@incollection{BARTHEYE2025373,
title = {16 - Self-visualization for the human–machine mind–body problem},
editor = {William Lawless and Ranjeev Mittu and Donald Sofge and Hesham Fouad},
booktitle = {Interdependent Human-Machine Teams},
publisher = {Academic Press},
pages = {373-402},
year = {2025},
isbn = {978-0-443-29246-0},
doi = {https://doi.org/10.1016/B978-0-443-29246-0.00013-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443292460000134},
author = {Olivier Bartheye and Laurent Chaudron},
keywords = {Consistency, Descartes' states, Human–machine, Local monism, Maximum entropy production, Meta-consciousness, Mind/body problem, Structural entropy production, Trialism},
abstract = {Recent machine-learning results and consciousness, while impressive, have nothing in common while rich human/machine interactions require an interpretation of sense-making. From their experience in symbolic AI, the authors attempt to address this global issue from an old philosophical question, the so-called mind-body problem, according to three distinct points of view: the logical one from proof theory; the informational one according to computer science; and, finally, the third: the physical one (including quantum physics and relativity), to study this mysterious missing link, and its corresponding aporia. The intention is to propose a formal mathematical framework to unify these three points of view by giving absolute priority to virtual representations, that is, abstract representations free from “real” considerations. In effect, our original motivation concerns the necessity to qualify precisely what a decision means to be automatized, a quite old question proposed from the early days of Artificial Intelligence at the Dartmouth summer research project in 1956. The artificial feature justifies a start from natural models based on experimentation, but relaxing them as much as possible to favor axioms from abstract models disconnected from reality. Unsurprisingly, all unsolvable mathematical, logical, physical, and metaphysical issues reappear, notably the dualism versus monism debate, to be reinterpreted on neutral grounds. Starting from our assumption in previous works, where a decision is a process able to fill a causal break, our proposal orients the representation of a decision as a collapse of the wave function as a measurement process acting on twisted consciousness, a pulsating form of consciousness. Applied mathematically to the human–machine problem, which is our goal, we expect to find an energy-entropy tradeoff: Well-structured interactions require less energy, operating at a lower cost characterized by low entropy; as a result, the interacting human–machine team is able to apply its maximum available energy to its performance, characterized by greater entropy, reaching a maximum.}
}
@incollection{MARINESCU2017113,
title = {Chapter 4 - Computer Clouds},
editor = {Dan C. Marinescu},
booktitle = {Complex Systems and Clouds},
publisher = {Elsevier},
address = {Boston},
pages = {113-145},
year = {2017},
series = {Computer Science Reviews and Trends},
isbn = {978-0-12-804041-6},
doi = {https://doi.org/10.1016/B978-0-12-804041-6.00004-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128040416000049},
author = {Dan C. Marinescu},
keywords = {Cloud delivery models, Hierarchical organization, Warehouse-scale computers, Over-provisioning, Cloud energy consumption, Cloud resource management, Cloud federations, Combinatorial auctions},
abstract = {Computer clouds have altered our thinking about computing and in this chapter, we first provide a down-to-earth view of the new paradigm and present the cloud delivery models. Next we discuss the hierarchical organization of the cloud infrastructure, consisting of multiple warehouse-scale computers. Cloud elasticity, the effects of over-provisioning on costs and energy consumption, and existing cloud resource management (CRM) policies and mechanisms for implementing these policies are analyzed. Alternative CRMs based on market mechanisms, such as auctions and server coalitions are then introduced. Combinatorial auctions allow access to packages of resources for applications with a complex workflow.}
}
@article{CHU2023106290,
title = {A data-driven meta-learning recommendation model for multi-mode resource constrained project scheduling problem},
journal = {Computers & Operations Research},
volume = {157},
pages = {106290},
year = {2023},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2023.106290},
url = {https://www.sciencedirect.com/science/article/pii/S0305054823001545},
author = {Xianghua Chu and Shuxiang Li and Fei Gao and Can Cui and Forest Pfeiffer and Jianshuang Cui},
keywords = {Data-driven decision making, Meta-learning, Feature extraction, Meta-heuristics},
abstract = {Meta-heuristics widely proposed in addressing multi-mode resource constrained project scheduling problem (MRCPSP) are problem-dependent. This paper first proposes an adaptive data-driven meta-learning Meta-heuristic Recommendation Model (MRM) to solve MRCPSP intelligently and efficiently. By learning the association between problem meta-features and algorithm performance, MRM can identify the most appropriate algorithm for different MRCPSPs. Multiclass Support Vector Machine (MCSVM) are integrated to train the classifiers for predicting the performance of the candidate meta-heuristics. To validate the proposed MRM, the performance is evaluated and compared in terms of accuracy, precision, sensitivity, and comprehensive evaluation index. In the experiments of 4 scenarios with 2 strategies, the average optimization and prediction accuracies are higher than 90% without increase in computational complexity. Comprehensive experiments and numerical results demonstrate the outperforming performance of the proposed MRM across various MRCPSP.}
}
@article{LAPIDUS202183,
title = {The road less traveled in protein folding: evidence for multiple pathways},
journal = {Current Opinion in Structural Biology},
volume = {66},
pages = {83-88},
year = {2021},
note = {Centrosomal Organization and Assemblies ● Folding and Binding},
issn = {0959-440X},
doi = {https://doi.org/10.1016/j.sbi.2020.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0959440X20301809},
author = {Lisa J Lapidus},
abstract = {Free Energy Landscape theory of Protein Folding, introduced over 20 years ago, implies that a protein has many paths to the folded conformation with the lowest free energy. Despite the knowledge in principle, it has been remarkably hard to detect such pathways. The lack of such observations is primarily due to the fact that no one experimental technique can detect many parts of the protein simultaneously with the time resolution necessary to see such differences in paths. However, recent technical developments and employment of multiple experimental probes and folding prompts have illuminated multiple folding pathways in a number of proteins that had all previously been described with a single path.}
}
@incollection{LINSTER2020650,
title = {3.33 - Modeling of Olfactory Processing☆},
editor = {Bernd Fritzsch},
booktitle = {The Senses: A Comprehensive Reference (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {650-660},
year = {2020},
isbn = {978-0-12-805409-3},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.24153-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093245241539},
author = {Christiane Linster and Thomas A. Cleland},
keywords = {Bulbar neurons, Filtering and contrast enhancement, Odorant molecules, Odorant receptors, Olfactory bulb, Pattern of activation},
abstract = {Synopsis
We here review how computational models of olfactory system have guided our understanding of olfactory processing. We discuss how the neural mechanisms underlying functionalities such as contrast enhancement, filtering and associative memory processes have been elucidated by computational models at different levels of detail. We provide an overall view of current theories of olfactory processing.}
}
@article{ZHAO2023100521,
title = {Toward parallel intelligence: An interdisciplinary solution for complex systems},
journal = {The Innovation},
volume = {4},
number = {6},
pages = {100521},
year = {2023},
issn = {2666-6758},
doi = {https://doi.org/10.1016/j.xinn.2023.100521},
url = {https://www.sciencedirect.com/science/article/pii/S2666675823001492},
author = {Yong Zhao and Zhengqiu Zhu and Bin Chen and Sihang Qiu and Jincai Huang and Xin Lu and Weiyi Yang and Chuan Ai and Kuihua Huang and Cheng He and Yucheng Jin and Zhong Liu and Fei-Yue Wang},
abstract = {The growing complexity of real-world systems necessitates interdisciplinary solutions to confront myriad challenges in modeling, analysis, management, and control. To meet these demands, the parallel systems method rooted in the artificial systems, computational experiments, and parallel execution (ACP) approach has been developed. The method cultivates a cycle termed parallel intelligence, which iteratively creates data, acquires knowledge, and refines the actual system. Over the past two decades, the parallel systems method has continuously woven advanced knowledge and technologies from various disciplines, offering versatile interdisciplinary solutions for complex systems across diverse fields. This review explores the origins and fundamental concepts of the parallel systems method, showcasing its accomplishments as a diverse array of parallel technologies and applications while also prognosticating potential challenges. We posit that this method will considerably augment sustainable development while enhancing interdisciplinary communication and cooperation.}
}
@article{SHAPIRO2007807,
title = {Bacteria are small but not stupid: cognition, natural genetic engineering and socio-bacteriology},
journal = {Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences},
volume = {38},
number = {4},
pages = {807-819},
year = {2007},
issn = {1369-8486},
doi = {https://doi.org/10.1016/j.shpsc.2007.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S1369848607000544},
author = {J.A. Shapiro},
keywords = {Computation, Sensing, Regulation, Cybernetic, Evolution},
abstract = {Forty years’ experience as a bacterial geneticist has taught me that bacteria possess many cognitive, computational and evolutionary capabilities unimaginable in the first six decades of the twentieth century. Analysis of cellular processes such as metabolism, regulation of protein synthesis, and DNA repair established that bacteria continually monitor their external and internal environments and compute functional outputs based on information provided by their sensory apparatus. Studies of genetic recombination, lysogeny, antibiotic resistance and my own work on transposable elements revealed multiple widespread bacterial systems for mobilizing and engineering DNA molecules. Examination of colony development and organization led me to appreciate how extensive multicellular collaboration is among the majority of bacterial species. Contemporary research in many laboratories on cell–cell signaling, symbiosis and pathogenesis show that bacteria utilise sophisticated mechanisms for intercellular communication and even have the ability to commandeer the basic cell biology of ‘higher’ plants and animals to meet their own needs. This remarkable series of observations requires us to revise basic ideas about biological information processing and recognise that even the smallest cells are sentient beings.}
}
@article{CLAPIN2002231,
title = {Content and cognitive science},
journal = {Language & Communication},
volume = {22},
number = {3},
pages = {231-242},
year = {2002},
issn = {0271-5309},
doi = {https://doi.org/10.1016/S0271-5309(02)00004-6},
url = {https://www.sciencedirect.com/science/article/pii/S0271530902000046},
author = {Hugh Clapin},
keywords = {Symbols, Classical, Cognitive science, Tacit representation, Architectural content, Computation},
abstract = {The computer model of the mind has informed and guided debate in the cognitive sciences for over 40 years, and gives pride of place to symbols. In this paper I investigate the nature of computational symbols and show that even in the parade cases of symbolic computation, symbols are not doing all the semantic and computational work. This analysis has important consequences for the scope of cognitive science, particularly with regard to what constitutes its domain: cognition.}
}
@article{DIPAOLA2014212,
title = {Using a Contextual Focus Model for an Automatic Creativity Algorithm to Generate Art Work},
journal = {Procedia Computer Science},
volume = {41},
pages = {212-219},
year = {2014},
note = {5th Annual International Conference on Biologically Inspired Cognitive Architectures, 2014 BICA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.11.105},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914015506},
author = {Steve DiPaola},
keywords = {Evolutionary Systems, Genetic Programming, Contextual Focus, Creativity, Computational Modelling},
abstract = {We sought to implement and determine whether incorporating cognitive based contextual focus into a genetic programming fitness function would play a crucial role in enabling the computer system to generate art that humans find “creative” (i.e. possessing qualities of novelty and aesthetic value typically ascribed to the output of a creative artistic process). We implemented contextual focus in the evolutionary art algorithm by giving the program the capacity to vary its level of fluidity and functional triggered dynamic control over different phases of the creative process. The domain of portrait painting was chosen because it requires both focused attention (analytical thought) to accomplish the primary goal of creating portrait sitter resemblance as well as defocused attention (associative thought) to creativity deviate from resemblance i.e., to meet the broad and often conflicting criteria of aesthetic art. Since judging creative art is subjective, rather than use quantitative analysis, a representative subset of the automatically produced art-work from this system was selected and submitted to many peer reviewed and commissioned art shows, thereby allowing it to be judged positively or negatively as creative by human art curators, reviewers and the art gallery going public.}
}