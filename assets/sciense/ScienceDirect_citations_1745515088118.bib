@article{WILLS2019104027,
title = {Reflexivity, coding and quantum biology},
journal = {Biosystems},
volume = {185},
pages = {104027},
year = {2019},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2019.104027},
url = {https://www.sciencedirect.com/science/article/pii/S0303264719302394},
author = {Peter R Wills},
keywords = {Genetic coding, Reflexivity, Quantum biology, Information, Autocatalysis, Aminoacyl-tRRNA synthetase (aaRS)},
abstract = {Biological systems are fundamentally computational in that they process information in an apparently purposeful fashion rather than just transferring bits of it in a purely syntactical manner. Biological information, such has genetic information stored in DNA sequences, has semantic content. It carries meaning that is defined by the molecular context of its cellular environment. Information processing in biological systems displays an inherent reflexivity, a tendency for the computational information-processing to be “about” the behaviour of the molecules that participate in the computational process. This is most evident in the operation of the genetic code, where the specificity of the reactions catalysed by the aminoacyl-tRNA synthetase (aaRS) enzymes is required to be self-sustaining. A cell’s suite of aaRS enzymes completes a reflexively autocatalytic set of molecular components capable of making themselves through the operation of the code. This set requires the existence of a body of reflexive information to be stored in an organism’s genome. The genetic code is a reflexively self-organised mapping of the chemical properties of amino acid sidechains onto codon “tokens”. It is a highly evolved symbolic system of chemical self-description. Although molecular biological coding is generally portrayed in terms of classical bit-transfer events, various biochemical events explicitly require quantum coherence for their occurrence. Whether the implicit transfer of quantum information, qbits, is indicative of wide-ranging quantum computation in living systems is currently the subject of extensive investigation and speculation in the field of Quantum Biology.}
}
@article{SHU2025121176,
title = {Altered brain network dynamics during rumination in remitted depression},
journal = {NeuroImage},
volume = {310},
pages = {121176},
year = {2025},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2025.121176},
url = {https://www.sciencedirect.com/science/article/pii/S1053811925001788},
author = {Su Shu and Wenwen Ou and Mohan Ma and Hairuo He and Qianqian Zhang and Mei Huang and Wentao Chen and Aoqian Deng and Kangning Li and Zhenman Xi and Fanyu Meng and Hui Liang and Sirui Gao and Yilin Peng and Mei Liao and Li Zhang and Mi Wang and Jin Liu and Bangshan Liu and Yumeng Ju and Yan Zhang},
keywords = {Rumination, Energy landscape, Brain state, Large-scale network, Depression},
abstract = {Rumination is a known risk factor for depression relapse. Understanding its neurobiological mechanisms during depression remission can inform strategies to prevent relapse, yet the temporal dynamics of brain networks during rumination in remitted depression remain unclear. Here, we collected rumination induction fMRI data from 42 patients with remitted depression and 41 healthy controls (HCs). Using an energy landscape approach, we investigated the temporal dynamics of brain networks during rumination. The appearance frequency (AF) and transition frequency (TF) metrics were defined to quantify the dynamic properties of brain states. Patients during remission showed higher levels of rumination than HCs. Both groups exhibited four brain states during rumination, which consisted of complementary network group activation (states 1 and 2, states 3 and 4). In patients, the AFs of and reciprocal TFs between states 1 and 2 during rumination were significantly increased, while AFs of states 3 and 4 and reciprocal TFs involving states 1–3, 1–4, 2–3, and 2–4 were decreased, both when compared to HCs and relative to patients themselves during distraction. Moreover, we found that for patients, the AF of state 1 was negatively correlated with rumination levels and marginally positively associated with attention, while the AF of state 2 was negatively associated with performance on attention tasks. Our study revealed altered dynamic characteristics of brain states composed of network groups during rumination in remitted depression. Additionally, the findings suggest that heightened self-focus linked to rumination may impair the brain's ability to efficiently allocate attentional resources.}
}
@article{TANG2019101065,
title = {Addressing cascading effects of earthquakes in urban areas from network perspective to improve disaster mitigation},
journal = {International Journal of Disaster Risk Reduction},
volume = {35},
pages = {101065},
year = {2019},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2019.101065},
url = {https://www.sciencedirect.com/science/article/pii/S2212420918308720},
author = {Pan Tang and Qi Xia and Yueyao Wang},
keywords = {Earthquakes, Urban areas, Cascading effects, Disaster mitigation, Disaster chains, Social network analysis},
abstract = {Given the rising size and complexity of urban areas, the city governments are faced to the challenges of cascading effects triggered by devastating earthquakes, in which the disastrous consequences are amplified significantly by combined effects of the occurred secondary events with interrelationships on the elements at risks. As a low-probability and high impact natural disaster, the escalation of secondary events are guided by the vulnerability paths, as well as their interconnections should be considered from system perspectives during the preparedness and mitigation process. This research aims to develop, model and analyze cascading effects scenario of earthquakes in urban areas for supporting decision making in disaster risk reduction. A framework for addressing cascading effects of earthquakes in urban area is presented. The procedure for developing cascading effects scenario of such highly complex and uncertain disasters by identifying the triggered disaster chains is introduced. A directed network was built to model and visualize the secondary events with interrelationships involving in the cascading effects scenario. In particular, a range of network metrics are developed to examine the relational patterns of hazardous events based on Social Network Analysis. Together with, how to design disaster mitigation strategies according to network analysis results is introduced, such as disaster chains with priorities to be blocked, hazardous events to be mitigated firstly, and essential collaborative relationships among the responsible organizations. Furthermore, a case study in an urban area in Shenzhen City, China was conducted to highlight the application of the proposed framework. This research presents an innovative approach to address cascading effects in urban areas of earthquakes by developing the triggered worst case scenario, as well as understanding secondary events with interrelationships using network analysis method for providing insights to design disaster mitigation strategies from system thinking perspectives.}
}
@article{CHEN2019398,
title = {Form-finding with robotics: Rapid and flexible fabrication of glass fiber reinforced concrete panels using thermoformed molds},
journal = {Journal of Computational Design and Engineering},
volume = {6},
number = {3},
pages = {398-403},
year = {2019},
issn = {2288-4300},
doi = {https://doi.org/10.1016/j.jcde.2018.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S2288430018300642},
author = {Yutong Chen and Jing Lin Koh and Xia Tian and Yi Qian Goh and Stylianos Dritsas},
keywords = {Form-finding, Digital fabrication, Parametric design},
abstract = {We present a process that revisits form-finding within digital media, namely parametric design and robotic fabrication. It is inspired by classical architectural and engineering experiments producing minimal surfaces and tensile structures by physical simulation of materials and natural forces. Fabrication is based on thermoforming, where thin sheets of amorphous PET are heat-treated and while in malleable state, where the material behaves like stretchable membrane, an industrial robot imprints a shape and sheets are rapidly cooled down assuming their final form. Key aspects of the approach include: (a) Speed: as each sheet is formed within seconds; (b) Flexibility, as a wide-range of shapes are produced without fabrication of unique dies; and (c) Resilience, as unlike traditional form-finding processes where the derived forms are ephemeral, the objects produced here are robust, they may be used directly or employed in subsequent fabrication processes. The produced sheets are used here as molds for glass-reinforced concrete casting offering excellent surface quality and the ability to create geometry unlike any conventional fabrication techniques. We present the design and development of the process and a proof-of-concept artwork produced.}
}
@article{ZHANG2022846,
title = {A novel resilience modeling method for community system considering natural gas leakage evolution},
journal = {Process Safety and Environmental Protection},
volume = {168},
pages = {846-857},
year = {2022},
issn = {0957-5820},
doi = {https://doi.org/10.1016/j.psep.2022.10.030},
url = {https://www.sciencedirect.com/science/article/pii/S0957582022008953},
author = {Xinqi Zhang and Guoming Chen and Dongdong Yang and Rui He and Jingyu Zhu and Shengyu Jiang and Jiawei Huang},
keywords = {Resilience modeling, Natural gas pipeline leakage, Community system, Computational Fluid Dynamics (CFD), Dynamic Bayesian network (DBN)},
abstract = {With rising natural gas demand, the issue of emergency management of gas leaks in communities is becoming more prominent. Resilience engineering is a successful approach to improving the system's ability in dealing with emergencies. In contrast to natural disasters such as floods and tornadoes, the consequences of a gas leak are closely related to the accident's evolutionary path. However, few reported works have taken the evolution of disasters into account in the assessment of resilience. Conventional methods fail to quantify the public safety performance of a disturbance caused by a natural gas leak. This work presents a novel dynamic approach to assessing resilience that incorporates the evolution of an incident and its interaction with emergency measures. A network structuring model of accident evolution is developed by the Functional Resonance Analysis Method (FRAM) to analyze the potential accident propagation. The explosion consequence of gas leakage escalation is simulated based on Computational Fluid Dynamics (CFD), and the personnel injury criterion is adopted to quantify the temporal and spatial variation characteristics of the system degradation. Then, the dynamic Bayesian network (DBN) is then used to track the interactions between accidents and emergency measures. Taking a real accident case (Shiyan underground gas explosion) as an example, the case study indicates that the proposed method can identify and prioritize emergency measures, as well as provide strong support for decision-making and arrangements in emergency management.}
}
@article{CHONG2024103352,
title = {Integrable approximations of dispersive shock waves of the granular chain},
journal = {Wave Motion},
volume = {130},
pages = {103352},
year = {2024},
issn = {0165-2125},
doi = {https://doi.org/10.1016/j.wavemoti.2024.103352},
url = {https://www.sciencedirect.com/science/article/pii/S0165212524000829},
author = {Christopher Chong and Ari Geisler and Panayotis G. Kevrekidis and Gino Biondini},
abstract = {In the present work we revisit the shock wave dynamics in a granular chain with precompression. By approximating the model by an α-Fermi–Pasta–Ulam–Tsingou chain, we leverage the connection of the latter in the strain variable formulation to two separate integrable models, one continuum, namely the KdV equation, and one discrete, namely the Toda lattice. We bring to bear the Whitham modulation theory analysis of such integrable systems and the analytical approximation of their dispersive shock waves in order to provide, through the lens of the reductive connection to the granular crystal, an approximation to the shock wave of the granular problem. A detailed numerical comparison of the original granular chain and its approximate integrable-system-based dispersive shocks proves very favorable in a wide parametric range. The gradual deviations between (approximate) theory and numerical computation, as amplitude parameters of the solution increase are quantified and discussed.}
}
@article{HURST2024105918,
title = {Continuous and discrete proportion elicit different cognitive strategies},
journal = {Cognition},
volume = {252},
pages = {105918},
year = {2024},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2024.105918},
url = {https://www.sciencedirect.com/science/article/pii/S001002772400204X},
author = {Michelle A. Hurst and Steven T. Piantadosi},
keywords = {Proportion, Strategy, Bayesian analysis, Model comparison},
abstract = {Despite proportional information being ubiquitous, there is not a standard account of proportional reasoning. Part of the difficulty is that there are several apparent contradictions: in some contexts, proportion is easy and privileged, while in others it is difficult and ignored. One possibility is that although we see similarities across tasks requiring proportional reasoning, people approach them with different strategies. We test this hypothesis by implementing strategies computationally and quantitatively comparing them with Bayesian tools, using data from continuous (e.g., pie chart) and discrete (e.g., dots) stimuli and preschoolers, 2nd and 5th graders, and adults. Overall, people's comparisons of highly regular and continuous proportion are better fit by proportion strategy models, but comparisons of discrete proportion are better fit by a numerator comparison model. These systematic differences in strategies suggest that there is not a single, simple explanation for behavior in terms of success or failure, but rather a variety of possible strategies that may be chosen in different contexts.}
}
@article{HOFFMAN202472,
title = {AI’s impact on war’s enduring nature},
journal = {Orbis},
volume = {68},
number = {1},
pages = {72-91},
year = {2024},
issn = {0030-4387},
doi = {https://doi.org/10.1016/j.orbis.2023.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0030438723000583},
author = {Frank Hoffman and Axel D'Amelio},
abstract = {This article reassesses the impact of Artificial Intelligence on war and revisits an article published in 2018 by one of the authors in Orbis. Despite the remarkable progress in generative AI, the authors contend that war’s essential nature will be impacted to a degree but will not be substantially altered.}
}
@article{PINCETL2012S32,
title = {Nature, urban development and sustainability – What new elements are needed for a more comprehensive understanding?},
journal = {Cities},
volume = {29},
pages = {S32-S37},
year = {2012},
note = {Current Research on Cities},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2012.06.009},
url = {https://www.sciencedirect.com/science/article/pii/S0264275112001059},
author = {Stephanie Pincetl},
keywords = {Urban metabolism, Sustainability, Political ecology, Urban ecosystem services},
abstract = {With the rise of interest in urban sustainability, the question of nature is front and center. This review suggests bridging between three distinct research paths concerned with urban areas and nature: urban ecosystem services, urban metabolism and urban political ecology to forge new thinking to transition from the sanitary city of the twentieth century to the sustainable city of the twenty-first. Cities are anthropogenic creations, sourcing their materials from nearby and far-off places, transforming those materials into products, goods and the physical infrastructure of cities. Tracking that flow of nature into the built environment, and the other flows such as water, needs to be accounted for as part of nature in the city. Cities – having entirely transformed the place they are located through building – have a unique nature, a nature planted by people, and made up of plants and animals that are often different than what had existed in the first place. The services of this new assemblage of species in the city, need to be studied critically. But ultimately, cities are the product of human volition, driven by economics, culture, politics and history. Understanding those drivers – the political ecology of place – provides an interpretive framework for reconsidering the nature of cities and its place in moving from a modernist sanitary city to a gray/green sustainable city.}
}
@article{MERIVAARA2021480,
title = {Preservation of biomaterials and cells by freeze-drying: Change of paradigm},
journal = {Journal of Controlled Release},
volume = {336},
pages = {480-498},
year = {2021},
issn = {0168-3659},
doi = {https://doi.org/10.1016/j.jconrel.2021.06.042},
url = {https://www.sciencedirect.com/science/article/pii/S0168365921003400},
author = {Arto Merivaara and Jacopo Zini and Elle Koivunotko and Sami Valkonen and Ossi Korhonen and Francisco M. Fernandes and Marjo Yliperttula},
keywords = {Freeze-drying, Cells, Extracellular vesicles, Process analytical technology, Quality-by-design, Biophotonics},
abstract = {Freeze-drying is the most widespread method to preserve protein drugs and vaccines in a dry form facilitating their storage and transportation without the laborious and expensive cold chain. Extending this method for the preservation of natural biomaterials and cells in a dry form would provide similar benefits, but most results in the domain are still below expectations. In this review, rather than consider freeze-drying as a traditional black box we “break it” through a detailed process thinking approach. We discuss freeze-drying from process thinking aspects, introduce the chemical, physical, and mechanical environments important in this process, and present advanced biophotonic process analytical technology. In the end, we review the state of the art in the freeze-drying of the biomaterials, extracellular vesicles, and cells. We suggest that the rational design of the experiment and implementation of advanced biophotonic tools are required to successfully preserve the natural biomaterials and cells by freeze-drying. We discuss this change of paradigm with existing literature and elaborate on our perspective based on our new unpublished results.}
}
@article{ZHENG2018266,
title = {When algorithms meet journalism: The user perception to automated news in a cross-cultural context},
journal = {Computers in Human Behavior},
volume = {86},
pages = {266-275},
year = {2018},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2018.04.046},
url = {https://www.sciencedirect.com/science/article/pii/S0747563218302103},
author = {Yue Zheng and Bu Zhong and Fan Yang},
keywords = {Automated news, Algorithm, Computational journalism, Robot journalism, News user, Cultural difference},
abstract = {Automated journalism – the use of algorithms in writing news reports – underscores the new direction of media transformation in the 21st century as it may reshape how the news is produced and consumed. Such writing algorithms have been increasingly adopted in U.S. and Chinese newsroom, but how well they are accepted by news users deserves more research. A comparative study was thus conducted to examine how U.S. and Chinese news users perceive the quality of algorithm-generated news reports, how much they like and trust such reports. Results show that U.S. and Chinese users demonstrated more shared, rather than different, perceptions to automated news. The users did not perceive automated content in a linear way, but viewed them by considering the interaction of the authors (i.e., journalists or algorithms), the media outlets (i.e., traditional or online media) and cultural background (i.e., U.S. or Chinese users).}
}
@article{WEST2023300,
title = {Agent-based methods facilitate integrative science in cancer},
journal = {Trends in Cell Biology},
volume = {33},
number = {4},
pages = {300-311},
year = {2023},
issn = {0962-8924},
doi = {https://doi.org/10.1016/j.tcb.2022.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0962892422002409},
author = {Jeffrey West and Mark Robertson-Tessi and Alexander R.A. Anderson},
keywords = {agent-based mathematical models, integrative science, tissue homeostasis, cancer metabolism, immune–tumor interactions},
abstract = {In this opinion, we highlight agent-based modeling as a key tool for exploration of cell–cell and cell–environment interactions that drive cancer progression, therapeutic resistance, and metastasis. These biological phenomena are particularly suited to be captured at the cell-scale resolution possible only within agent-based or individual-based mathematical models. These modeling approaches complement experimental work (in vitro and in vivo systems) through parameterization and data extrapolation but also feed forward to drive new experiments that test model-generated predictions.}
}
@article{FESSAKIS201387,
title = {Problem solving by 5–6 years old kindergarten children in a computer programming environment: A case study},
journal = {Computers & Education},
volume = {63},
pages = {87-97},
year = {2013},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2012.11.016},
url = {https://www.sciencedirect.com/science/article/pii/S0360131512002813},
author = {G. Fessakis and E. Gouli and E. Mavroudi},
keywords = {Programming and programming languages, Kindergarten, Improving classroom teaching, Teaching/learning strategies},
abstract = {Computer programming is considered an important competence for the development of higher-order thinking in addition to algorithmic problem solving skills. Its horizontal integration throughout all educational levels is considered worthwhile and attracts the attention of researchers. Towards this direction, an exploratory case study is presented concerning dimensions of problem solving using computer programming by 5–6 years old kindergarten children. After a short introductory experiential game the children were involved in solving a series of analogous computer programming problems, using a Logo-based environment on an Interactive White Board. The intervention was designed as a part of the structured learning activities of the kindergarten which are teacher-guided and are conducted in a whole-class social mode. The observation of the video recording of the intervention along with the analysis of teacher's interview and the researcher's notes allow for a realistic evaluation of the feasibility, the appropriateness and the learning value of integrating computer programming in such a context. The research evidence supports the view that children enjoyed the engaging learning activities and had opportunities to develop mathematical concepts, problem solving and social skills. Interesting results about children learning, difficulties, interactions, problem solving strategies and the teacher's role are reported. The study also provides proposals for the design of future research.}
}
@article{PENG2024100093,
title = {Memristor-based spiking neural networks: cooperative development of neural network architecture/algorithms and memristors},
journal = {Chip},
volume = {3},
number = {2},
pages = {100093},
year = {2024},
issn = {2709-4723},
doi = {https://doi.org/10.1016/j.chip.2024.100093},
url = {https://www.sciencedirect.com/science/article/pii/S270947232400011X},
author = {Huihui Peng and Lin Gan and Xin Guo},
keywords = {Spike neural networks, Hardware, Memristor, Algorithm, Cooperative development},
abstract = {Inspired by the structure and principles of the human brain, spike neural networks (SNNs) appear as the latest generation of artificial neural networks, attracting significant and universal attention due to their remarkable low-energy transmission by pulse and powerful capability for large-scale parallel computation. Current research on artificial neural networks gradually change from software simulation into hardware implementation. However, such a process is fraught with challenges. In particular, memristors are highly anticipated hardware candidates owing to their fast-programming speed, low power consumption, and compatibility with the complementary metal–oxide semiconductor (CMOS) technology. In this review, we start from the basic principles of SNNs, and then introduced memristor-based technologies for hardware implementation of SNNs, and further discuss the feasibility of integrating customized algorithm optimization to promote efficient and energy-saving SNN hardware systems. Finally, based on the existing memristor technology, we summarize the current problems and challenges in this field.}
}
@article{HEIRDSFIELD2004443,
title = {Factors affecting the process of proficient mental addition and subtraction: case studies of flexible and inflexible computers},
journal = {The Journal of Mathematical Behavior},
volume = {23},
number = {4},
pages = {443-463},
year = {2004},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2004.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0732312304000495},
author = {Ann M. Heirdsfield and Tom J. Cooper},
keywords = {Mental computation, Addition, Subtraction, Accuracy},
abstract = {The relationship between mental computation and number sense is complex: mental computation can facilitate number sense when students are encouraged to be flexible, but flexibility and number sense is neither sufficient nor necessary for accuracy in mental computation. It is possible for familiarity with a strategy to compensate for a lack of number sense and inefficient processes. This study reports on six case studies exploring Year 3 students’ procedures for and understanding of mental addition and subtraction, and understanding of number sense and other cognitive, metacognitive, and affective factors associated with mental computation. The case studies indicate that the mental computation process is composed of four stages in which cognitive, metacognitive and affective factors operate differently for flexible and inflexible computers. The authors propose a model in which the differences between computer types are seen in terms of the application of different knowledges in number facts, numeration, effect of operation on number, and beliefs and metacognition on strategy choice and strategy implementation.}
}
@article{DRAGO2011361,
title = {Cyclic alternating pattern in sleep and its relationship to creativity},
journal = {Sleep Medicine},
volume = {12},
number = {4},
pages = {361-366},
year = {2011},
issn = {1389-9457},
doi = {https://doi.org/10.1016/j.sleep.2010.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S1389945711000578},
author = {Valeria Drago and Paul S. Foster and Kenneth M. Heilman and Debora Aricò and John Williamson and Pasquale Montagna and Raffaele Ferri},
keywords = {Sleep, Creativity, Cyclic alternating pattern, Torrance, Frontal lobe functions, Arousal},
abstract = {Background/objectives
Sleep has been shown to enhance creativity, but the reason for this enhancement is not entirely known. There are several different physiologic states associated with sleep. In addition to rapid (REM) and non-rapid eye movement (NREM) sleep, NREM sleep can be broken down into Stages (1–4) that are characterized by the degree of EEG slow-wave activity. In addition, during NREM sleep the cyclic alternating pattern (CAPs) of EEG activity has been described which can also be divided into three subtypes (A1–A3) according to the frequency of the EEG waves. Differences in CAP subtype ratios have been previously linked to cognitive performances. The purpose of this study was to asses the relationship between CAP activity during sleep and creativity.
Methods
The participants were eight healthy young adults (four women) who underwent three consecutive nights of polysomnographic recording and took the Abbreviated Torrance Test for Adults (ATTA) on the second and third mornings after the recordings.
Results
There were positive correlations between Stage 1 of NREM sleep and some measures of creativity such as fluency (R=.797; p=.029) and flexibility (R=.43; p=.002), between Stage 4 of NREM sleep and originality (R=.779; p=.034) and a global measure of figural creativity (R=.758; p=.040). There was also a negative correlation between REM sleep and originality (R=−.827; p=.042). During NREM sleep the CAP rate, which in young people reflects primarily the A1 subtype, also correlated with originality (R=.765; p=.038).
Conclusions
NREM sleep is associated with low levels of cortical arousal, and low cortical arousal may enhance the ability of people to access to the remote associations that are critical for creative innovations. In addition, A1 CAP subtypes reflect frontal activity, and the frontal lobes are important for divergent thinking, also a critical aspect of creativity.}
}
@incollection{CHIB20013569,
title = {Chapter 57 - Markov Chain Monte Carlo Methods: Computation and Inference},
editor = {James J. Heckman and Edward Leamer},
series = {Handbook of Econometrics},
publisher = {Elsevier},
volume = {5},
pages = {3569-3649},
year = {2001},
issn = {1573-4412},
doi = {https://doi.org/10.1016/S1573-4412(01)05010-3},
url = {https://www.sciencedirect.com/science/article/pii/S1573441201050103},
author = {Siddhartha Chib},
keywords = {:, Cl, C4},
abstract = {This chapter reviews the recent developments in Markov chain Monte Carlo simulation methods. These methods, which are concerned with the simulation of high dimensional probability distributions, have gained enormous prominence and revolutionized Bayesian statistics. The chapter, provides background on the relevant Markov chain theory and provides detailed information on the theory and practice of Markov chain sampling based on the Metropolis–Hastings and Gibbs sampling algorithms. Convergence diagnostics and strategies for implementation are also discussed. A number of examples drawn from Bayesian statistics are used to illustrate the ideas. The chapter also covers in detail the application of MCMC methods to the problems of prediction and model choice.}
}
@article{CERKA2015376,
title = {Liability for damages caused by artificial intelligence},
journal = {Computer Law & Security Review},
volume = {31},
number = {3},
pages = {376-389},
year = {2015},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2015.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S026736491500062X},
author = {Paulius Čerka and Jurgita Grigienė and Gintarė Sirbikytė},
keywords = {Artificial intelligence, Liability for damages, Legal regulation, AI-as-Tool, Risks by AI, Respondeat (respondent) superior, Vicarious liability, Strict liability},
abstract = {The emerging discipline of Artificial Intelligence (AI) has changed attitudes towards the intellect, which was long considered to be a feature exclusively belonging to biological beings, i.e. homo sapiens. In 1956, when the concept of Artificial Intelligence emerged, discussions began about whether the intellect may be more than an inherent feature of a biological being, i.e. whether it can be artificially created. AI can be defined on the basis of the factor of a thinking human being and in terms of a rational behavior: (i) systems that think and act like a human being; (ii) systems that think and act rationally. These factors demonstrate that AI is different from conventional computer algorithms. These are systems that are able to train themselves (store their personal experience). This unique feature enables AI to act differently in the same situations, depending on the actions previously performed. The ability to accumulate experience and learn from it, as well as the ability to act independently and make individual decisions, creates preconditions for damage. Factors leading to the occurrence of damage identified in the article confirm that the operation of AI is based on the pursuit of goals. This means that with its actions AI may cause damage for one reason or another; and thus issues of compensation will have to be addressed in accordance with the existing legal provisions. The main issue is that neither national nor international law recognizes AI as a subject of law, which means that AI cannot be held personally liable for the damage it causes. In view of the foregoing, a question naturally arises: who is responsible for the damage caused by the actions of Artificial Intelligence? In the absence of direct legal regulation of AI, we can apply article 12 of United Nations Convention on the Use of Electronic Communications in International Contracts, which states that a person (whether a natural person or a legal entity) on whose behalf a computer was programmed should ultimately be responsible for any message generated by the machine. Such an interpretation complies with a general rule that the principal of a tool is responsible for the results obtained by the use of that tool since the tool has no independent volition of its own. So the concept of AI-as-Tool arises in the context of AI liability issues, which means that in some cases vicarious and strict liability is applicable for AI actions.}
}
@article{CLINDANIEL2024105890,
title = {Digital formation processes: A high-frequency, large-scale investigation},
journal = {Journal of Archaeological Science},
volume = {161},
pages = {105890},
year = {2024},
issn = {0305-4403},
doi = {https://doi.org/10.1016/j.jas.2023.105890},
url = {https://www.sciencedirect.com/science/article/pii/S030544032300170X},
author = {Jon Clindaniel and Matthew Magnani},
keywords = {Formation processes, Palimpsests, Big data, Digital materiality, Craigslist, Free stuff, Social stratification},
abstract = {Large sources of digital trace data (i.e. “Big Data”) have become increasingly important in the study of material culture. However, akin to the offline material culture traditionally studied by archaeologists, digital trace data is rarely a passive reflection of human behavior – it is a complex palimpsest produced through a variety of erasure and accretion formation processes. To better understand how digital trace palimpsests are formed and how digital formation processes influence and inform our ability to interpret the offline material processes they index, we introduce a computational method – high-frequency archaeological survey – which allows us to observe digital formation processes at a high temporal resolution, as well as a large spatial scale. Using this method every hour for one month, we surveyed posts from across the United States in Craigslist's “Free Stuff” category (popularly called “Curb Alert”), a user-generated source of big digital trace data, indexing material things that have been placed on users' curbs for removal by scavengers or trash collectors. For each post, we observed its time-to-erasure and any edits that were made during the study period – finding that the posts that survive represent a biased sample of those that were posted over the course of the month, conditioned by how recently and on what day the post is posted, the material characteristics of things that are posted about, as well as regional variation. Far from only being evidence of biased end-of-month data, however, we show that further analysis of identified digital formation processes can be an important object of study in its own right – in this case, shedding new light on social scientific questions linking the exchange of “free stuff” with the process of social stratification and urban inequality in the United States. Overall, our findings suggest the importance of accounting for and explicitly analyzing digital formation processes in studies that utilize digital trace data.}
}
@article{ARCK201954,
title = {When 3 Rs meet a forth R: Replacement, reduction and refinement of animals in research on reproduction},
journal = {Journal of Reproductive Immunology},
volume = {132},
pages = {54-59},
year = {2019},
issn = {0165-0378},
doi = {https://doi.org/10.1016/j.jri.2019.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0165037819300385},
author = {Petra Clara Arck},
keywords = {Reproduction, Mouse models, 3R principle, Immunology},
abstract = {Research endeavors aiming to understand the maternal immune adaptation to pregnancy significantly rely on the use of animal models, such as mice and rats. These models have provided important insights into the pathophysiology of a number of pregnancy disorders in humans. However, the use of animal models in scientific research is a vividly debated and emotive topic. The 3R principles – replacement, reduction and refinement of research animals – have been propagated a few decades ago. The present review advocates a forward-thinking consciousness to address the 3R principles in research projects in the field of reproductive biology and immunology. Specific measures and alternative methods are being proposed to replace research animals by using e.g. tissue engineering approaches, biobank-derived tissue, ‘placenta-on-a-chip’ devices or in silico methods. The latter may involve data queries from repositories now available to provide single cell sequencing information on reproductive tissues. Reduction of research animals by gestational imaging and a wealth of suggestions for refinement are proposed. Taken together, the measures and guidelines introduced in this review are expected to spark a reconsideration of experimental designs in the area of reproductive biology and immunology in order to implement 3R principle where applicable.}
}
@article{ALOIMONOS201542,
title = {The Cognitive Dialogue: A new model for vision implementing common sense reasoning},
journal = {Image and Vision Computing},
volume = {34},
pages = {42-44},
year = {2015},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2014.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S0262885614001656},
author = {Yiannis Aloimonos and Cornelia Fermüller},
keywords = {Vision and language, Integration of perception, action, and cognition, Cognition modulating image processing},
abstract = {We propose a new model for vision, where vision is part of an intelligent system that reasons. To achieve this we need to integrate perceptual processing with computational reasoning and linguistics. In this paper we present the basics of this formalism.}
}
@article{RONI2022100796,
title = {Integrated water-power system resiliency quantification, challenge and opportunity},
journal = {Energy Strategy Reviews},
volume = {39},
pages = {100796},
year = {2022},
issn = {2211-467X},
doi = {https://doi.org/10.1016/j.esr.2021.100796},
url = {https://www.sciencedirect.com/science/article/pii/S2211467X21001796},
author = {Mohammad S. Roni and Thomas Mosier and Tzvi D. Feinberg and Timothy McJunkin and Ange-Lionel Toba and Liam D. Boire and Luis Rodriguez-Garcia and Majid Majidi and Masood Parvania},
keywords = {Resiliency, Irrigation, Integrated water-power system, Optimization},
abstract = {Resiliency has been studied in the power and water systems separately. Often the resiliency study is not so comprehensive as to understand interdependent, integrated water and power systems. This research outlines the relevant factors necessary to understand and advance quantification of such integrated systems. It also presents a review of integrated water-power systems resiliency. Based on literature survey and identification of challenges, the authors present quantification and computational steps needed to understand integrated water-power systems resiliency. A conceptual framework is proposed to quantify integrated water-power system resiliency. Finally, the authors presented an opportunity for improved water and power system resilience.}
}
@article{DEMURO20241,
title = {Artificial intelligence and the ethnographic encounter: Transhuman language ontologies, or what it means “to write like a human, think like a machine”},
journal = {Language & Communication},
volume = {96},
pages = {1-12},
year = {2024},
issn = {0271-5309},
doi = {https://doi.org/10.1016/j.langcom.2024.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0271530924000119},
author = {Eugenia Demuro and Laura Gurney},
keywords = {Artificial intelligence, Language ontologies, Ethnographic encounter, Transhumanism, Posthumanism},
abstract = {In this paper, we employ the language ontologies framework to artificial intelligence (specifically, OpenAI's ChatGPT) to investigate the ‘ethnographic encounter’ between human and non-human language users. Our focus is on the exchange and interplay between human language users and non-human artificial language generators in the production of written text. We analyse how such programs transform our understanding of what language is or might be; their practices to create language are unfamiliar, and yet they make sense to human interlocutors. Drawing from, and building on, the language ontologies framework, we discuss the practices involved in such encounters and suggest the need for an updated ‘toolkit’ in our understanding of language to account for transhuman interactions.}
}
@incollection{REYESGARCIA2025151,
title = {Chapter 7 - Decoding imagined speech for EEG-based BCI},
editor = {Ayman S. El-Baz and Jasjit S. Suri},
booktitle = {Brain-Computer Interfaces},
publisher = {Academic Press},
pages = {151-175},
year = {2025},
series = {Advances in Neural Engineering},
isbn = {978-0-323-95439-6},
doi = {https://doi.org/10.1016/B978-0-323-95439-6.00004-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780323954396000041},
author = {Carlos A. Reyes-García and Alejandro A. Torres-García and Tonatiuh Hernández-del-Toro and Jesús S. García-Salinas and Luis Villaseñor-Pineda},
keywords = {Brain–computer interfaces (BCI), Classification, Electroencephalograms (EEG), Imagined speech, Silent speech interfaces (SSI)},
abstract = {Brain–computer interfaces (BCIs) are systems that transform the brain's electrical activity into commands to control a device. To create a BCI, it is necessary to establish the relationship between a certain stimulus, internal or external, and the brain activity it provokes. A common approach in BCIs is motor imagery, which involves imagining limb movement. Unfortunately, this approach allows few commands. As an alternative, this chapter presents another approach, an internal language-related stimulus known as imagined speech, which is the action of imagining the diction of a word without emitting any sound or articulating any movement. This neuroparadigm is more intuitive, less subjective, and ambiguous, which are very relevant advantages; however, the cost to properly process the brain signal is not trivial. This chapter describes the main components of an EEG-based imagined speech BCI, along with key works, emerging trends, and challenges in this research area. Regarding the challenges, we present four of them in the pursuit of decoding imagined speech. The first challenge involves accurately recognizing isolated words. The second one is the automatic selection of a subset of EEG channels aiming to reduce computational cost and provide evidence of promising locations for studying imagined speech. The third challenge introduces an innovative approach to addressing scenarios where a new word needs to be added to the vocabulary after the computational model has been trained. Lastly, the fourth challenge concerns the online recognition of words from continuous EEG signals. Despite advances in the area, there is still much work to be done. Important initial steps have been taken in terms of the application of novel techniques for preprocessing, artifact removal, feature extraction, and classification which are the stages to be taken to process the collected signal. Additionally, the community has shared datasets and organized evaluation forums to accelerate the search for solutions.}
}
@article{SETO2022102891,
title = {Connected in health: Place-to-place commuting networks and COVID-19 spillovers},
journal = {Health & Place},
volume = {77},
pages = {102891},
year = {2022},
issn = {1353-8292},
doi = {https://doi.org/10.1016/j.healthplace.2022.102891},
url = {https://www.sciencedirect.com/science/article/pii/S1353829222001526},
author = {Christopher H. Seto and Corina Graif and Aria Khademi and Vasant G. Honavar and Claire E. Kelling},
keywords = {Commuting networks, COVID-19, Fixed-effects, Spatial models, Computational statistics, Mobility data},
abstract = {Biweekly county COVID-19 data were linked with Longitudinal Employer-Household Dynamics data to analyze population risk exposures enabled by pre-pandemic, country-wide commuter networks. Results from fixed-effects, spatial, and computational statistical approaches showed that commuting network exposure to COVID-19 predicted an area's COVID-19 cases and deaths, indicating spillovers. Commuting spillovers between counties were independent from geographic contiguity, pandemic-time mobility, or social media ties. Results suggest that commuting connections form enduring social linkages with effects on health that can withstand mobility disruptions. Findings contribute to a growing relational view of health and place, with implications for neighborhood effects research and place-based policies.}
}
@article{GTOTH202138,
title = {Revascularization decisions in patients with chronic coronary syndromes: Results of the second International Survey on Interventional Strategy (ISIS-2)},
journal = {International Journal of Cardiology},
volume = {336},
pages = {38-44},
year = {2021},
issn = {0167-5273},
doi = {https://doi.org/10.1016/j.ijcard.2021.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167527321008172},
author = {Gabor {G. Toth} and Nils P. Johnson and William Wijns and Balint Toth and Alexandru Achim and Stephane Fournier and Emanuele Barbato},
keywords = {Chronic coronary syndrome, Coronary ischemia, Coronary revascularization},
abstract = {Background
In chronic coronary syndromes, guidelines mandate invasive functional guidance of revascularization whenever non-invasive proof of ischemia is missing. ISIS-2 survey aimed to evaluate how the adoption of guideline recommendation on ischemia-guided revascularization has evolved over the last 5–7 years.
Methods
In ISIS-2 participants assessed five complete angiograms, presenting only intermediate stenoses without information on non-invasive pre-testing. Fractional flow reserve was known for each stenosis, but remained undisclosed. Participants could determine stenosis significance either by angiography or by requesting an adjunctive invasive diagnostic method (intravascular imaging or functional tests). Primary endpoint was the rate of requesting adjunctive functional assessment. Secondary endpoints were the rate of concordance between angiography-based decisions and know functional severity. ISIS-2 utilized the same web-based platform as ISIS-1 in 2013. (NCT04001452).
Results
334 participants performed 2059 lesion evaluations: 1202 (59%) decisions were based solely on angiography without expressed need for further evaluation. These decisions were discordant with known functional significance in 39%, mainly with potential of overtreatment. Participants requested invasive functional assessment in 643 (31%) and intravascular imaging in 214 (10%) cases. Compared to ISIS-1 the rate of purely angiography-based decisions has decreased (59% vs 66%; p < 0.001), while invasive functional tests were more frequently requested (31% vs 25%; p < 0.001).
Conclusions
ISIS-2 suggests an evolving pattern in the intention to integrate invasive coronary physiology into the revascularization decisions. However, the disconnect between recommendations and current thinking is still dominant.}
}
@article{HOLZER20163,
title = {Design exploration supported by digital tool ecologies},
journal = {Automation in Construction},
volume = {72},
pages = {3-8},
year = {2016},
note = {Computational and generative design for digital fabrication: Computer-Aided Architectural Design Research in Asia (CAADRIA)},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2016.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0926580516301467},
author = {Dominik Holzer},
keywords = {Parametric design, Environmental performance optimization, Interoperability, multidisciplinary design, Convergence, Optioneering, Multi-criteria},
abstract = {Designers take advantage of tool ecologies in order to find the most purposeful way of connecting often distinct processes that inform morphological design and associated building performance feedback. The ability to set up logical connections of design parameters across different digital applications becomes ever more relevant in a time where the proliferation of computational tools has led to a fundamental transformation in architectural education. Morphological exploration and form-finding get increasingly enriched by environmental performance feedback. This paper points out a major step forward in software interoperability and the alignment of digital design applications, allowing users to engage with morphological form-finding enriched by real-time physical building performance feedback. The key innovation presented here relates to tools available to designers who neither possess in-depth programming skills, nor need to rely on custom-developed scripts in order to advance their concepts. A recent architectural design studio serves as a testbed to interrogate the level of convergence among tools for morphological design and performance optimization.}
}
@incollection{SEN201693,
title = {5 - Conclusions},
editor = {Syamal K. Sen and Ravi P. Agarwal},
booktitle = {Zero},
publisher = {Academic Press},
pages = {93-142},
year = {2016},
isbn = {978-0-08-100774-7},
doi = {https://doi.org/10.1016/B978-0-08-100774-7.00005-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780081007747000053},
author = {Syamal K. Sen and Ravi P. Agarwal},
keywords = {Black holes, computational zero versus absolute zero, dark matter, Epoch, error in error-free computation, existence of year zero, fast computation, fundamental particle, glory of zero, intense concentration, irrational number without zero, numerical zero consciousness, place-value system, Pythagoras theorem, quantum universe, quantum zero, storage capacity and computational power, Vedic-Hindu-Buddhist legacy, zero in natural mathematics, zero space},
abstract = {We have stressed, among others, on the globally accepted Indian zero along with its place-value system. Pointed out are (i) the impossibility of a zero-free irrational number, (ii) the requirement of a minimum two symbols to represent any information, (iii) the reason for the survival of radix-10 number system, (iv) the distinction of a numerical zero and the absolute zero and the consequent eternal nonremovable error, (v) existence/nonexistence of a building block of matter beyond a fundamental particle, (vi) the importance of intense concentration for innovation, (vii) the nonexactness of zero in physics, (viii) our continuing scientific quest for the origin of the universe, (ix) the existence of pure consciousness eternally everywhere as well as the distinction between artificial and natural consciousness, (x) the pervasiveness of computational mathematics/science in all sciences, engineering, and technologies, (xi) the distinction between living and nonliving computers in terms of storage and computational power, (xii) the extraordinary living computers among human beings, and (xiii) the solution of Y2K problem.}
}
@article{HUDLICKA201498,
title = {Affective BICA: Challenges and open questions},
journal = {Biologically Inspired Cognitive Architectures},
volume = {7},
pages = {98-125},
year = {2014},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2013.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X13000947},
author = {Eva Hudlicka},
keywords = {Emotion modeling, Emotion theories, Cognitive–affective architectures},
abstract = {In spite of the progress in emotion research over the past 20years, emotions remain an elusive phenomenon. While some underlying circuitry has been identified for some aspects of affective processing (e.g., amygdala-mediated processing of threatening stimuli, the role of orbitofrontal cortex in emotion regulation), much remains unknown about the mechanisms of emotions. Computational models of cognitive and affective processes provide a unique and powerful means of refining psychological theories, and can help elucidate the mechanisms that mediate affective phenomena. This paper outlines a number of open questions and challenges associated with developing computational models of emotion, and with their integration within biologically-inspired cognitive architectures. These include the following: the extent to which mechanisms in biological affective agents should be simulated or emulated in affective BICAs; importance of more precise, design-based terminology; identification of fundamental affective processes, and the computational tasks necessary for their implementation; improved understanding of affective dynamics and development of more accurate models of these phenomena; and understanding the alternative means of integrating emotions within agent architectures. The challenges associated with data availability and model validation are also discussed.}
}
@article{PASMAN201880,
title = {How can we improve process hazard identification? What can accident investigation methods contribute and what other recent developments? A brief historical survey and a sketch of how to advance},
journal = {Journal of Loss Prevention in the Process Industries},
volume = {55},
pages = {80-106},
year = {2018},
issn = {0950-4230},
doi = {https://doi.org/10.1016/j.jlp.2018.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S0950423018300329},
author = {Hans J. Pasman and William J. Rogers and M. Sam Mannan},
keywords = {Accident-incident investigation, Hazard identification, Causation, System approach},
abstract = {Risk assessment is essential for various purposes such as facility siting, safeguarding, and licensing. Hazard identification (HAZID), which suffers greatly from incompleteness, is still the weakest link in risk assessment. Of course, this recognition is not new and many efforts have been spent to improve the situation, of which some have been rather successful. To find out what can go wrong, creative divergent thinking is required. Hazard identification should result in scenario definition. In that respect, applying the present tools as HAZOP and FMEA there is still a great emphasis on the material and equipment aspects. In contrast, underlying management and leadership failure in its many forms reflecting in organizational and human failure, due to complexity, attracts much less attention. Unlike in HAZID, in accident investigation the occurrence of an event with nasty consequences is no doubt a fact, so there must be one or more causes and the traces will lead to them. Over the years, methods for accident and incident investigation have gone through a significant evolution. From the early-on simplistic domino stone model and the human operator always at fault, via models of latent failure due to failing management involvement and via extensive root cause analysis (RCA) to a system approach. Hence, in accident investigation, management failure appearing in the many possible forms of human and organizational factors, obtained already 30 years ago with the RCA technique much attention, while it nowadays culminates in the socio-technical system approach. So, the question arises whether for improved HAZID we can learn from the accident investigation experience. In addition, safer design and advances from static risk assessment towards more accurate predictive operational dynamic risk assessment and management, will also be enabled by possibilities offered by big data and analytics. Digitization, automation and simulation, hence computerization, will be of great help in improving the identification of hazards and tracing the corresponding scenarios. The paper reviews the developmental history of both accident investigation and hazard identification methodology; incidentally it will identify commonality and differences. On the basis of the comparison and of recent advances in computerization, the paper will investigate to what extent beneficial modifications and additions can be made to obtain a higher degree of completeness in HAZID.}
}
@article{ABDULLAH2024100212,
title = {Recent development of combined heat transfer performance for engine systems: A comprehensive review},
journal = {Results in Surfaces and Interfaces},
volume = {15},
pages = {100212},
year = {2024},
issn = {2666-8459},
doi = {https://doi.org/10.1016/j.rsurfi.2024.100212},
url = {https://www.sciencedirect.com/science/article/pii/S2666845924000321},
author = {Md. Abdullah and Mohammad {Zoynal Abedin}},
keywords = {Combined heat transfer, Performance, Engine system, Enhancement},
abstract = {Heat transfer regulation between engine components has a direct impact on engine efficiency and performance. Improving engine system efficiency, reducing emissions, and prolonging component life all depend on efficient heat management. This review looks at recent advancements in integrated heat transfer optimization to boost efficiency, reduce emissions, and improve engine system performance. This effort also investigates producing intricate heat transfer components with improved geometry using additive manufacturing techniques. With additive printing, designers may more easily construct complex structures and optimize heat transfer surfaces for better performance. The development of nanofluids and nanocoatings now allows for improving heat transfer qualities. Nanotechnology advancements have made this possible, and nanostructured materials' enhanced surface properties as well as thermal conductivity contribute to better heat dissipation in engine systems. The modern automotive and aerospace sectors have high standards, which are met through novel designs, materials, computational tools, and integrated cooling systems. Additionally, these improvements pave the way for more efficient and environmentally friendly engine operation. Because of the integration of computational technologies like numerical modeling and CFD, engineers can now see complex heat flow patterns and construct more efficient cooling solutions. Additive manufacturing has changed component manufacturing by enabling sophisticated designs that maximize heat transfer surfaces.}
}
@article{XIONG2023105811,
title = {Neural vortex method: From finite Lagrangian particles to infinite dimensional Eulerian dynamics},
journal = {Computers & Fluids},
volume = {258},
pages = {105811},
year = {2023},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2023.105811},
url = {https://www.sciencedirect.com/science/article/pii/S0045793023000361},
author = {Shiying Xiong and Xingzhe He and Yunjin Tong and Yitong Deng and Bo Zhu},
keywords = {Vortex method, Neural network, Lagrangian dynamics, Eulerian dynamics},
abstract = {In fluid analysis, there has been a long-standing problem: lacking a rigorous mathematical tool to map from a continuous flow field to finite discrete particles, hurdling the Lagrangian particles from inheriting the high resolution of a large-scale Eulerian solver. To tackle this challenge, we propose a novel learning-based framework, the neural vortex method (NVM). NVM builds a neural-network description of the Lagrangian vortex structures and their interaction dynamics to reconstruct the high-resolution Eulerian flow field in a physically-precise manner. The key components of our infrastructure consist of two networks: a vortex detection network to identify the Lagrangian vortices from a grid-based velocity field and a vortex dynamics network to learn the underlying governing interactions of these finite structures. By embedding these two networks with a vorticity-to-velocity Poisson solver and training its parameters using the fluid data obtained from grid-based numerical simulation, we can predict the accurate fluid dynamics on a precision level that was infeasible for all the previous conventional vortex methods. We demonstrate the efficacy of our method in generating highly accurate prediction results with low computational cost by predicting the evolution of the leapfrogging vortex rings system, the turbulence system, and the systems governed by Navier–Stokes (NS) equations with different external forces. We compare the prediction results made by NVM and the Lagrangian vortex method (LVM) for solving the NS equation in the periodic box and find that the relative error of the predicted velocity using NVM is more than 10 times lower than that of the LVM. Moreover, our method only requires data collected from a very short training window, more than 100 times smaller than the prediction period, which potentially facilitates data acquisition in real systems.}
}
@article{GALDO2022101508,
title = {The quest for simplicity in human learning: Identifying the constraints on attention},
journal = {Cognitive Psychology},
volume = {138},
pages = {101508},
year = {2022},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2022.101508},
url = {https://www.sciencedirect.com/science/article/pii/S0010028522000445},
author = {Matthew Galdo and Emily R. Weichart and Vladimir M. Sloutsky and Brandon M. Turner},
keywords = {Attention, Category learning, Eye-tracking, Cognitive bias, Capacity, Model comparison},
abstract = {For better or worse, humans live a resource-constrained existence; only a fraction of physical sensations ever reach conscious awareness, and we store a shockingly small subset of these experiences in memory for later use. Here, we examined the effects of attention constraints on learning. Among models that frame selective attention as an optimization problem, attention orients toward information that will reduce errors. Using this framing as a basis, we developed a suite of models with a range of constraints on the attention available during each learning event. We fit these models to both choice and eye-fixation data from four benchmark category-learning data sets, and choice data from another dynamic categorization data set. We found consistent evidence for computations we refer to as “simplicity”, where attention is deployed to as few dimensions of information as possible during learning, and “competition”, where dimensions compete for selective attention via lateral inhibition.}
}
@article{FIELDS2023104927,
title = {Regulative development as a model for origin of life and artificial life studies},
journal = {Biosystems},
volume = {229},
pages = {104927},
year = {2023},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2023.104927},
url = {https://www.sciencedirect.com/science/article/pii/S0303264723001028},
author = {Chris Fields and Michael Levin},
keywords = {Free energy principle, Kinematic replication, Learning, Multicellularity, Multiscale competency architecture, Target morphology},
abstract = {Using the formal framework of the Free Energy Principle, we show how generic thermodynamic requirements on bidirectional information exchange between a system and its environment can generate complexity. This leads to the emergence of hierarchical computational architectures in systems that operate sufficiently far from thermal equilibrium. In this setting, the environment of any system increases its ability to predict system behavior by “engineering” the system towards increased morphological complexity and hence larger-scale, more macroscopic behaviors. When seen in this light, regulative development becomes an environmentally-driven process in which “parts” are assembled to produce a system with predictable behavior. We suggest on this basis that life is thermodynamically favorable and that, when designing artificial living systems, human engineers are acting like a generic “environment”.}
}
@article{KEARNS2025256,
title = {Biomimetic Digital Twins and Multiomics: Applications to Rheumatoid Arthritis and the Potential Reclassification of Variants of Unknown Clinical Significance},
journal = {The Journal of Molecular Diagnostics},
volume = {27},
number = {4},
pages = {256-269},
year = {2025},
issn = {1525-1578},
doi = {https://doi.org/10.1016/j.jmoldx.2024.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S1525157825000376},
author = {William G. Kearns and Joe Glick and Lawrence Baisch and Andrew Benner and Dalton Brough and Luke Du and Chandra Germain and Laura Kearns and Georgios Stamoulis},
abstract = {The National Academies of Sciences, Engineering, and Medicine issued a report on December 15, 2023, “Foundational Research Gaps and Future Directions for Digital Twins.” This described the importance of using biomimetic digital twins and multiomics in research. These were incorporated in the current analysis of patients with rheumatoid arthritis (RA). Exome sequencing, genotype-phenotype ranking, and biomimetic digital twin analysis were used to identify five pathogenic and one likely pathogenic DNA variants in patient samples analyzed, which were absent from controls. The variants identified in these genes, P2RX7, HTRA2, PTPN22, FLG, CD46, and EIF4G1, play a role in the development of RA. Additionally, 3172 variants of unknown clinical significance (VUSs) were identified in patient samples, which were absent from controls. All VUSs appeared to be associated with RA. Hidden or dark data were identified from six genes. These genes, often found in patient samples, included HIF1A, HLA-DOA, PTGER3, HIPK3, TGFBR3, and HIF1A-AS3. VUSs identified in genes HIF1A, HLA-DOA, PTGER3, and HIPK3 were directly related to the pathogenesis of RA, whereas VUSs identified in genes TGFBR3 and HIF1A-AS3 were indirectly related. The current results suggest that biomimetic digital twins and multiomics can provide further insight into the development of RA. This may also potentially help with the process of reclassifying VUSs. The reclassification of VUSs will play a critical role in complex molecular diagnostics and drug development.}
}
@article{VANBAVEL2015167,
title = {The neuroscience of moral cognition: from dual processes to dynamic systems},
journal = {Current Opinion in Psychology},
volume = {6},
pages = {167-172},
year = {2015},
note = {Morality and ethics},
issn = {2352-250X},
doi = {https://doi.org/10.1016/j.copsyc.2015.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S2352250X15002031},
author = {Jay J {Van Bavel} and Oriel FeldmanHall and Peter Mende-Siedlecki},
abstract = {Prominent theories of morality have integrated philosophy with psychology and biology. Although this approach has been highly generative, we argue that it does not fully capture the rich and dynamic nature of moral cognition. We review research from the dual-process tradition, in which moral intuitions are automatically elicited and reasoning is subsequently deployed to correct these initial intuitions. We then describe how the computations underlying moral cognition are diverse and widely distributed throughout the brain. Finally, we illustrate how social context modulates these computations, recruiting different systems for real (vs. hypothetical) moral judgments, examining the dynamic process by which moral judgments are updated. In sum, we advocate for a shift from dual-process to dynamic system models of moral cognition.}
}
@article{LI2000233,
title = {Can younger students succeed where older students fail? An examination of third graders' solutions of a division-with-remainder (DWR) problem},
journal = {The Journal of Mathematical Behavior},
volume = {19},
number = {2},
pages = {233-246},
year = {2000},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(00)00046-8},
url = {https://www.sciencedirect.com/science/article/pii/S0732312300000468},
author = {Yeping Li and Edward A. Silver},
keywords = {Problem solving, Mathematics education, Sense making, Division, Elementary school students, Mathematics cognition},
abstract = {In this study, 14 third graders, who had not yet been taught the division algorithm (DA), solved a division-with-remainder (DWR) problem, and their solution processes were examined. Students also solved a set of eight numerical computation tasks involving addition, subtraction, multiplication, and division. In contrast to their poor performance on the division computation tasks and in contrast to the findings in previous studies of DWR problem solving by middle-school students and to the third graders in this study were quite successful in solving the DWR problem. Although the students lacked knowledge of formal procedures for division computation, they successfully used non-division solution strategies that were closely tied to problem context in order to solve the problem. In general, the results indicate that student performance in solving this complex problem was enhanced by their engagement in sense making as they solved the DWR problem. The highly context-embedded approaches used by these students also allowed them to avoid the difficulties in treating the “remainder,” as have been reported in research involving older students.}
}
@article{HONG2025134554,
title = {Energy-saving optimal control of secondary district cooling system based on tribal intelligent evolution optimization algorithm},
journal = {Energy},
volume = {316},
pages = {134554},
year = {2025},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2025.134554},
url = {https://www.sciencedirect.com/science/article/pii/S0360544225001963},
author = {Xiaoxi Hong and Ye Yao and Kui Wang and Jianzhong Yang and Qimei Liu},
keywords = {Secondary district cooling system, Energy-saving optimization control, Energy consumption calculation model, Tribal intelligent evolution optimization algorithm},
abstract = {With the significant increase in energy consumption for large central air conditioning systems, optimal control of district cooling systems is crucial for energy conservation and CO2 emission reduction. This study focuses on the energy-saving optimal control of secondary district cooling systems (SDCSs). Firstly, energy models for SDCSs utilizing distribution manifolds or plate heat exchangers are presented, with the goal of establishing global optimal control models for energy conservation. Next, a novel metaheuristic algorithm called Tribal Intelligent Evolution Optimization (TIEO) is proposed, which innovatively introduces human intelligent behavioral characteristics into the tribal evolution process. The TIEO and seven other optimization algorithms have been tested for optimizing the SDCSs. The test results demonstrated that the TIEO surpassed other algorithms in terms of optimization effectiveness, stability, and computational efficiency. Additionally, this study has conducted engineering validation of the TIEO algorithm on the SDCSs with distribution manifolds or plate heat exchangers. Compared to the traditional control strategies, the TIEO algorithm improved the energy efficiency ratio of the system by 13.56 % and 11.40 %, respectively. Therefore, the TIEO algorithm has the potential to serve as an optimization tool for contributing to energy conservation and promoting the sustainable development of large-scale district cooling systems.}
}
@article{BOETTKE202344,
title = {On the feasibility of technosocialism},
journal = {Journal of Economic Behavior & Organization},
volume = {205},
pages = {44-54},
year = {2023},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2022.10.046},
url = {https://www.sciencedirect.com/science/article/pii/S0167268122004048},
author = {Peter J. Boettke and Rosolino A. Candela},
keywords = {Economic calculation, F.A. Hayek, Ludwig von Mises, Technosocialism},
abstract = {Technological advances associated with computing power and the prospect of artificial intelligence have renewed interest on the economic feasibility of socialism. The question of such feasibility turns on whether the problem of economic calculation has fundamentally changed. In spite of the prospect of what King and Petty (2021) refer to as “technosocialism,” we argue that technological advances in computation cannot replace the competitive discovery process that takes place within the context of the market. We do so by situating the case for technosocialism in the context of the socialist calculation debate. Understood in these terms, technosocialism represents a restatement of the case for market socialism, which incorrectly framed the “solution” to economic calculation under socialism as one of computing data, rather than the discovery of context-specific knowledge that only emerges through the exchange of property rights. Therefore, the arguments put forth by Ludwig von Mises and F.A. Hayek, and later Israel Kirzner and Don Lavoie, regarding the impossibility of economic calculation under socialism remains just as relevant today.}
}
@article{SUN2012227,
title = {Memory systems within a cognitive architecture},
journal = {New Ideas in Psychology},
volume = {30},
number = {2},
pages = {227-240},
year = {2012},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2011.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X11000729},
author = {Ron Sun},
keywords = {Memory, Cognitive architecture, CLARION},
abstract = {This article addresses the division of memory systems in relation to an overall cognitive architecture. As understanding the architecture is essential to understanding the mind, developing computational cognitive architectures is an important enterprise in computational psychology (computational cognitive modeling). The article proposes a set of hypotheses concerning memory systems from the standpoint of a cognitive architecture, in particular, the four-way division of memory (including explicit and implicit procedural memory and explicit and implicit declarative memory). It then discusses in detail how these hypotheses may be validated through examining qualitatively the literature on memory. A quick review follows of computational simulations of a variety of quantitative data (which are not limited to narrowly conceived “memory tasks”). Results of accounting for both qualitative and quantitative data point to the promise of this approach.}
}
@incollection{CAMERER2014479,
title = {Chapter 25 - The Neural Basis of Strategic Choice},
editor = {Paul W. Glimcher and Ernst Fehr},
booktitle = {Neuroeconomics (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {479-492},
year = {2014},
isbn = {978-0-12-416008-8},
doi = {https://doi.org/10.1016/B978-0-12-416008-8.00025-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780124160088000255},
author = {Colin F. Camerer and Todd A. Hare},
keywords = {Game theory, Learning, Strategic choice},
abstract = {In this chapter, we present a set of concepts and tools for defining and examining strategic choice that are drawn from behavioral economics and discuss how they can be applied to and tested with neuroscience techniques. The standard language for studying strategic choice in economics is game theory. Game theory provides concrete mathematical formulas for linking strategic actions to rewarding payoffs. After outlining the four components necessary to make predictions about strategic social behavior, we present recent evidence that the computations predicted by game theory in specific strategic choice contexts are reflected in the brain. In addition, we discuss links between strategic decision making and the psychological concept of theory of mind. We conclude by suggesting that developing mathematical models of social and strategic actions may aid in the understanding of how the brain implements typical choice behavior as well as categorizing dysfunctions that lead to aberrant behavior in psychiatric disorders.}
}
@incollection{SANCHEZ1995865,
title = {Interfaces for learning},
editor = {Yuichiro Anzai and Katsuhiko Ogawa and Hirohiko Mori},
series = {Advances in Human Factors/Ergonomics},
publisher = {Elsevier},
volume = {20},
pages = {865-870},
year = {1995},
booktitle = {Symbiosis of Human and Artifact},
issn = {0921-2647},
doi = {https://doi.org/10.1016/S0921-2647(06)80136-X},
url = {https://www.sciencedirect.com/science/article/pii/S092126470680136X},
author = {J. Sanchez and M. Lumbreras},
abstract = {The current literature emphasizes critical aspects of learning and cognition involved in human-computer interaction. We present a conceptualization for designing interfaces for learning and thinking through the use of modern ideas for building educational software. We address the construction of Hyperstories as a metaphor for enhancing thought and reasoning skills. The advantages of using multimedia for building this type of software, as well as the complexities involved are analyzed and discussed.}
}
@article{NAKHLE2024100411,
title = {Shrinking the giants: Paving the way for TinyAI},
journal = {Device},
volume = {2},
number = {8},
pages = {100411},
year = {2024},
issn = {2666-9986},
doi = {https://doi.org/10.1016/j.device.2024.100411},
url = {https://www.sciencedirect.com/science/article/pii/S2666998624002473},
author = {Farid Nakhle},
keywords = {accelerated models, compressed models, miniaturized intelligence, tiny artificial intelligence, tiny machine learning},
abstract = {Summary
In the current era of technological advancement, the quest for more efficient and accessible artificial intelligence (AI) is driving the investigation of the predictive potential of small architecture-based, compressed, and accelerated AI models (TinyAI) and the benefits of running those on small-scale digital edge computing devices. This perspective delves into the expanding world of TinyAI, envisioning a future in which powerful machine intelligence can be encapsulated within pocket-sized devices, and discusses the technological challenges and opportunities associated with it. In addition, some of the myriad applications and benefits that can arise from their deployment will be discussed.}
}
@article{JETTER201445,
title = {Fuzzy Cognitive Maps for futures studies—A methodological assessment of concepts and methods},
journal = {Futures},
volume = {61},
pages = {45-57},
year = {2014},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2014.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0016328714000809},
author = {Antonie J. Jetter and Kasper Kok},
keywords = {Fuzzy Cognitive Maps, Future studies, Scenarios, Mental model, System thinking},
abstract = {Fuzzy Cognitive Map (FCM) modelling is highly suitable for the demands of future studies: it uses a mix of qualitative and quantitative approaches, it enables the inclusion of multiple and diverse sources to overcome the limitations of expert opinions, it considers multivariate interactions that lead to nonlinearities, and it aims to make implicit assumptions (or mental models) explicit. Despite these properties, the field of future studies is slow to adopt FCM and to apply the increasingly solid theoretical foundations and rigorous practices for FCM applications that are evolving in other fields. This paper therefore discusses theoretical and practical aspects of constructing and applying FCMs within the context of future studies: based on an extensive literature review and the authors’ experience with FCM projects, it provides an introduction of fundamental concepts of FCM modelling, a step-wise description and discussion of practical methods and their pitfalls, and an overview over future research directions for FCM in future studies.}
}
@article{MATTILA2025237,
title = {Closing the Gap: Advancing service management in the hospitality and tourism industry amidst the AI revolution},
journal = {Journal of Hospitality and Tourism Management},
volume = {62},
pages = {237-245},
year = {2025},
issn = {1447-6770},
doi = {https://doi.org/10.1016/j.jhtm.2025.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S1447677025000178},
author = {Anna S. Mattila and Laurie Wu and Peihao Wang},
keywords = {Artificial intelligence, Service management, Gaps model, Service automation, Service experience, Service design},
abstract = {The Gaps Model of service quality has long been the keystone of the service management literature. This classic theoretical model offers valuable guidance for understanding the opportunities and challenges posed by the increasing adoption of artificial intelligence (AI) in service management. Drawing on recent research and practical insights, this research examines the impact of AI on each one of the service quality gaps identified in the Gaps Model. In addition, we discuss the dual potential of AI to either bridge or widen these gaps, cautioning for digitally responsible implementations of AI. Towards the end of the paper, we discuss future research directions to advance service management with AI integrations in the hospitality and tourism industry.}
}
@incollection{YU2018177,
title = {Chapter 11 - Geospatial Data Discovery, Management, and Analysis at National Aeronautics and Space Administration (NASA)},
editor = {Feras A. Batarseh and Ruixin Yang},
booktitle = {Federal Data Science},
publisher = {Academic Press},
pages = {177-191},
year = {2018},
isbn = {978-0-12-812443-7},
doi = {https://doi.org/10.1016/B978-0-12-812443-7.00011-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128124437000119},
author = {Manzhu Yu and Min Sun},
keywords = {Big data, Big data management, Climate, Data discovery, Large-scale scientific simulation, Natural phenomena, Spatiotemporal},
abstract = {The 21st century is experiencing simultaneous changes in global population, urbanization, and climate. These changes, along with the rapid growth of geospatial data and increasing popularity of data discovery, access, and analytics techniques, lead to the promising future of innovation and achievements in geospatial and spatiotemporal thinking, computing, and application. However, big geospatial data bring forth challenges that require the cutting-edge science and technology to address. In this chapter, we highlight some of the characteristics and challenges in geospatial and spatiotemporal data discovery, management, processing, and analytics, and provide solutions based on recent research achievements as case studies. These study cases provide concrete examples of challenges faced when handling geospatial and spatiotemporal big data and the potential solutions in high level of accuracy, interoperability, and scalability.}
}
@article{YANG2023103712,
title = {Study of the water entry and exit problems by coupling the APR and PST within SPH},
journal = {Applied Ocean Research},
volume = {139},
pages = {103712},
year = {2023},
issn = {0141-1187},
doi = {https://doi.org/10.1016/j.apor.2023.103712},
url = {https://www.sciencedirect.com/science/article/pii/S0141118723002535},
author = {Xi Yang and Song Feng and Jinxin Wu and Guiyong Zhang and Guangqi Liang and Zhifan Zhang},
keywords = {Water entry, Water exit, Particle shifting technique, Adaptive particle refinement, Coupled scheme},
abstract = {In this manuscript, the adaptive particle refinement (APR) and particle shifting technique (PST) are coupled and applied to investigate the entire process of water entry and exit. The PST implementation for various types of particles is quantitatively discussed in detail in the coupled APR-PST approach. A comprehensive analysis of different APR-PST coupled schemes is conducted with crucial variables compared and analyzed for the first time. The stability, accuracy and robustness of the developed numerical model are verified by three benchmark tests: 2D wedge water entry, 2D cylinder water exit and 2D cylinder water entry and exit. The obtained results show that the current model can ensure the overall computational precision in the situation of local refinement, which indicates it is a viable way to solve the problems of water entry and exit.}
}
@article{TRESADERN20171478,
title = {Industrial medicinal chemistry insights: neuroscience hit generation at Janssen},
journal = {Drug Discovery Today},
volume = {22},
number = {10},
pages = {1478-1488},
year = {2017},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2017.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S1359644617301216},
author = {Gary Tresadern and Frederik J.R. Rombouts and Daniel Oehlrich and Gregor Macdonald and Andres A. Trabanco},
abstract = {The role of medicinal chemistry has changed over the past 10 years. Chemistry had become one step in a process; funneling the output of high-throughput screening (HTS) on to the next stage. The goal to identify the ideal clinical compound remains, but the means to achieve this have changed. Modern medicinal chemistry is responsible for integrating innovation throughout early drug discovery, including new screening paradigms, computational approaches, novel synthetic chemistry, gene-family screening, investigating routes of delivery, and so on. In this Foundation Review, we show how a successful medicinal chemistry team has a broad impact and requires multidisciplinary expertise in these areas.}
}
@article{ANDERSON2015283,
title = {Sparse factors for the positive and negative syndrome scale: Which symptoms and stage of illness?},
journal = {Psychiatry Research},
volume = {225},
number = {3},
pages = {283-290},
year = {2015},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2014.12.025},
url = {https://www.sciencedirect.com/science/article/pii/S016517811401018X},
author = {Ariana Anderson and Marsha Wilcox and Adam Savitz and Hearee Chung and Qingqin Li and Giacomo Salvadore and Dai Wang and Isaac Nuamah and Steven P. Riese and Robert M. Bilder},
keywords = {PANSS, Confirmatory factor analysis, Exploratory factor analysis, Schizophrenia, RDoC, Dimensional Measures},
abstract = {The Positive and Negative Syndrome Scale (PANSS) is frequently described with five latent factors, yet published factor models consistently fail to replicate across samples and related disorders. We hypothesize that (1) a subset of the PANSS, instead of the entire PANSS scale, would produce the most replicable five-factor models across samples, and that (2) the PANSS factor structure may be different depending on the treatment phase, influenced by the responsiveness of the positive symptoms to treatment. Using exploratory factor analysis, confirmatory factor analysis and cross validation on baseline and post-treatment observations from 3647 schizophrenia patients, we show that five-factor models fit best across samples when substantial subsets of the PANSS items are removed. The optimal model at baseline (five factors) omits 12 items: Motor Retardation, Grandiosity, Somatic Concern, Lack of Judgment and Insight, Difficulty in Abstract Thinking, Mannerisms and Posturing, Disturbance of Volition, Preoccupation, Disorientation, Excitement, Guilt Feelings and Depression. The PANSS factor models fit differently before and after patients have been treated. Patients with larger treatment response in positive symptoms have larger variations in factor structure across treatment stage than the less responsive patients. Negative symptom scores better predict the positive symptoms scores after treatment than before treatment. We conclude that sparse factor models replicate better on new samples, and the underlying disease structure of Schizophrenia changes upon treatment.}
}
@incollection{HU2025629,
title = {Chapter 28 - The lung exposome: Accelerating precision respiratory health},
editor = {Kent E. Pinkerton and Richard Harding and Elizabeth Georgian},
booktitle = {The Lung (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
pages = {629-645},
year = {2025},
isbn = {978-0-323-91824-4},
doi = {https://doi.org/10.1016/B978-0-323-91824-4.00017-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323918244000174},
author = {Xin Hu},
keywords = {Environmental exposure, Exposome, High-resolution mass spectrometry, Mixed chemical exposure, Multi-omics, Respiratory health, Systems biology},
abstract = {The exposome concept, first proposed as a complement to the genome for understanding non-genetic causes of disease, has rapidly undergone significant evolution to recognize the totality of environmental exposures experienced across an individual's lifespan and to embrace the complexity of biological responses to those exposures. Over almost twodecades of work, the landscape of lung exposome research has expanded to include extensive characterization of external and internal exposure levels, molecular responses, and health parameters. The exposome approach has identified new environmental factors and profiles that impact the variation of lung function trajectories and the development of chronic respiratory diseases over the life course. Aligned with the goal of precision medicine and leveraging systems biology, exposome research directly addresses the heterogeneity in disease origins and management. Recent advancements in omics technologies, particularly omics-scale chemical quantification using high-resolution mass spectrometry, have revolutionized exposure assessment and biological discovery. These analytic platforms, now widely available for large population studies, have propelled advanced statistical models and computational pipelines, which together contribute to an affordable, automatable, and standardizable framework that can be integrated into health care and respiratory epidemiology for studies with greater precision, breadth, and depth.}
}
@incollection{MANIKTALA2008247,
title = {Chapter 12 - Discussion Forums, Datasheets, and Other Real-World Issues},
editor = {Sanjaya Maniktala},
booktitle = {Troubleshooting Switching Power Converters},
publisher = {Newnes},
address = {Burlington},
pages = {247-289},
year = {2008},
isbn = {978-0-7506-8421-7},
doi = {https://doi.org/10.1016/B978-075068421-7.50014-X},
url = {https://www.sciencedirect.com/science/article/pii/B978075068421750014X},
author = {Sanjaya Maniktala},
abstract = {Publisher Summary
This chapter explains a few concepts such as thinking is the key, one needs to cross check everything, and product liability concerns. The chapter describes that the company's online tools can be used to discover design problems and correct them as long as thinking is applied as well. But what about “errors” in the online tools themselves? The chapter deals in and highlights that if the thinking process is done assiduously, sometimes one might arrive at the opposite conclusion that one initially foresaw. Anyone can even suddenly realize that he/she can be a part of the very problem that they are trying to fix; it could be in his/her own backyard. The chapter thinks about the customers and highlights that the very idea of a company starting a forum such as this one is essentially brilliant and thoroughly laudable. It also imparts a perception of transparency to their operations from the get-go. One should not outrightly believe anything and put in front of everyone, even if it is on semiglossy paper or in high-definition video or Flash HTML format. As engineers, one is required to put pen to paper, and at least do a sanity check.}
}
@incollection{TURKLE20017035,
title = {Human–Computer Interface},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {7035-7038},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/04333-3},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767043333},
author = {S. Turkle},
abstract = {The computer/human interface refers to the modalities through which people interact with computational technologies. Looked at over the last half century, the trend has been from a style of interaction in which the computer is approached as a mechanism to a style of interaction in which the computer is approached as a behaving and aware organism. This first trend has been related to another, from people acting directly on the machine to acting indirectly on its increasingly elaborate self presentation in which many layers of programming stand between underlying processes and what is presented to the user. This movement is more than technical. Computational technologies have served as objects to think with, that is, as carrier objects for ideas. Through their changing interfaces, the computer has moved from being the carrier object of a culture of calculation to that of a culture of simulation. The culture of simulation promotes a way of understanding in which users are encouraged to take computers ‘at interface value.’}
}
@article{WAN2025122214,
title = {Hesitant multiplicative best and worst method for multi-criteria group decision making},
journal = {Information Sciences},
pages = {122214},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2025.122214},
url = {https://www.sciencedirect.com/science/article/pii/S0020025525003469},
author = {Shu-Ping Wan and Xi-Nuo Chen and Jiu-Ying Dong and Yu Gao},
keywords = {Hesitant multiplicative preference relation, Multi-criteria decision-making, Best and worst method, Analytic hierarchy process},
abstract = {Best-worst method (BWM) has been extended in various uncertain scenarios owing to fewer comparisons and better reliability. This article utilizes hesitant multiplicative (HM) sets (HMSs) to express reference comparisons (RCs) and develops a novel HM BWM (HMBWM). We first define the multiplicative consistency for HM preference relation (HMPR). A fast and effective approach is designed to derive the priority weights (PWs) from an HMPR. To extend BW into HMPR, the score value of each criterion is computed to identify the best and worst criteria. Then, the PWs are acquired through constructing a 0–1 mixed goal programming model based on the HM RCs (HMRCs). The consistency ratio is given to judge the multiplicative consistency of HMRCs. An approach is proposed to enhance the consistency when the HMRCs are unacceptably consistent. Thereby, a novel HMBWM is proposed. On basis of HMBWM, this article further develops a novel method for group decision making (GDM) with HMPRs. The decision makers’ weights are determined by consistency ratio and the group PWs of alternatives are obtained by minimum relative entropy model. Four examples show that HMBWM possesses higher consistency and the proposed GDM method has stronger distinguishing ability, less computation workload and fewer modifications of elements.}
}
@article{FOLLEY2003467,
title = {Psychoses and creativity: is the missing link a biological mechanism related to phospholipids turnover?},
journal = {Prostaglandins, Leukotrienes and Essential Fatty Acids},
volume = {69},
number = {6},
pages = {467-476},
year = {2003},
note = {Recent Advances of Membran e Pathology in Schizophrenia},
issn = {0952-3278},
doi = {https://doi.org/10.1016/j.plefa.2003.08.019},
url = {https://www.sciencedirect.com/science/article/pii/S0952327803001716},
author = {Bradley S Folley and Mikisha L Doop and Sohee Park},
keywords = {Creativity, Norepinephrine, Fatty acids, Schizophrenia, Psychoses},
abstract = {Recent evidence suggests that genetic and biochemical factors associated with psychoses may also provide an increased propensity to think creatively. The evolutionary theories linking brain growth and diet to the appearance of creative endeavors have been made recently, but they lack a direct link to research on the biological correlates of divergent and creative thought. Expanding upon Horrobin's theory that changes in brain size and in neural microconnectivity came about as a result of changes in dietary fat and phospholipid incorporation of highly unsaturated fatty acids, we propose a theory relating phospholipase A2 (PLA2) activity to the neuromodulatory effects of the noradrenergic system. This theory offers probable links between attention, divergent thinking, and arousal through a mechanism that emphasizes optimal individual functioning of the PLA2 and NE systems as they interact with structural and biochemical states of the brain. We hope that this theory will stimulate new research in the neural basis of creativity and its connection to psychoses.}
}
@article{MATT2013420,
title = {Implementation of Lean Production in Small Sized Enterprises},
journal = {Procedia CIRP},
volume = {12},
pages = {420-425},
year = {2013},
note = {Eighth CIRP Conference on Intelligent Computation in Manufacturing Engineering},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2013.09.072},
url = {https://www.sciencedirect.com/science/article/pii/S2212827113007130},
author = {D.T. Matt and E. Rauch},
keywords = {Manufacturing, Productivity, Small Enterprises},
abstract = {The introduction and implementation of Lean Production Principles over the last twenty years has had a notable impact on many manufacturing enterprises. The practice shows that lean production methods and instruments are not equally applicable to large and small companies. After the implementation in large enterprises belonging to the automotive sector the concept of lean thinking was introduced successfully in medium sized enterprises. Small enterprises have been ignored for a long time and special investigations about this topic are rarely. Considering statistical data and analysis about the economic importance of small enterprises we can see, that they are numerous and create a notable part of the total value added in the non-financial business economy. This paper analysis in a first step the role and potential of small enterprises – especially in Italy – and shows then a preliminary study of the suitability of existing lean methods for the application in this type of organization. The research was combined with an industrial case study in a small enterprise to analyse the difficulties in the implementation stage and to identify the critical success factors. The results of this preliminary study should illustrate the existing hidden potential in small enterprises as well as a selection of suitable methods for productivity improvements. This research will be the base for a further and more detailed research project.}
}
@article{GOLTSOS2022397,
title = {Inventory – forecasting: Mind the gap},
journal = {European Journal of Operational Research},
volume = {299},
number = {2},
pages = {397-419},
year = {2022},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2021.07.040},
url = {https://www.sciencedirect.com/science/article/pii/S0377221721006500},
author = {Thanos E. Goltsos and Aris A. Syntetos and Christoph H. Glock and George Ioannou},
keywords = {Forecasting, Inventory control, Inventory forecasting, Literature review},
abstract = {We are concerned with the interaction and integration between demand forecasting and inventory control, in the context of supply chain operations. The majority of the literature is fragmented. Forecasting research more often than not assumes forecasting to be an end in itself, disregarding any subsequent stages of computation that are needed to transform forecasts into replenishment decisions. Conversely, most contributions in inventory theory assume that demand (and its parameters) are known, in effect disregarding any preceding stages of computation. Explicit recognition of these shortcomings is an important step towards more realistic theoretical developments, but still not particularly helpful unless they are somehow addressed. Even then, forecasts often constitute exogenous variables that serially feed into a stock control model. Finally, there is a small but growing stream of research that is explicitly built around jointly tackling the inventory forecasting question. We introduce a framework to define four levels of integration: from disregarding, to acknowledging, to partly addressing, to fully understanding the interactions. Focusing on the last two, we conduct a structured review of relevant (integrated) academic contributions in the area of forecasting and inventory control and argue for their classification with regard to integration. We show that the development from one level to another is in many cases chronological in order, but also associated with specific schools of thought. We also argue that although movement from one level to another adds realism, it also adds complexity in terms of actual implementations, and thus a trade-off exists. The article makes a contribution into an area that has always been fragmented despite the importance of bringing the forecasting and inventory communities together to solve problems of common interest. We close with an indicative agenda for further research and a call for more theoretical contributions, but also more work that would help to expand the empirical knowledge base in this area.}
}
@article{FABOLUDE2025100246,
title = {Smart cities, smart systems: A comprehensive review of system dynamics model applications in urban studies in the big data era},
journal = {Geography and Sustainability},
volume = {6},
number = {1},
pages = {100246},
year = {2025},
issn = {2666-6839},
doi = {https://doi.org/10.1016/j.geosus.2024.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S2666683924000993},
author = {Gift Fabolude and Charles Knoble and Anvy Vu and Danlin Yu},
keywords = {Urban sustainability, Smart cities, System dynamics models, Big data analytics, Urban system complexity, Data-driven urbanism},
abstract = {This paper addresses urban sustainability challenges amid global urbanization, emphasizing the need for innovative approaches aligned with the Sustainable Development Goals. While traditional tools and linear models offer insights, they fall short in presenting a holistic view of complex urban challenges. System dynamics (SD) models that are often utilized to provide holistic, systematic understanding of a research subject, like the urban system, emerge as valuable tools, but data scarcity and theoretical inadequacy pose challenges. The research reviews relevant papers on recent SD model applications in urban sustainability since 2018, categorizing them based on nine key indicators. Among the reviewed papers, data limitations and model assumptions were identified as major challenges in applying SD models to urban sustainability. This led to exploring the transformative potential of big data analytics, a rare approach in this field as identified by this study, to enhance SD models’ empirical foundation. Integrating big data could provide data-driven calibration, potentially improving predictive accuracy and reducing reliance on simplified assumptions. The paper concludes by advocating for new approaches that reduce assumptions and promote real-time applicable models, contributing to a comprehensive understanding of urban sustainability through the synergy of big data and SD models.}
}
@article{JOGO2025106357,
journal = {Environmental Modelling & Software},
volume = {186},
pages = {106357},
year = {2025},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2025.106357},
url = {https://www.sciencedirect.com/science/article/pii/S1364815225000416},
author = {Fransiskus Serfian Jogo and Hanum Khairana Fatmah and Aufaclav Zatu {Kusuma Frisky}}
}
@article{BOHLENDER2018428,
title = {Compositional Verification of PLC Software using Horn Clauses and Mode Abstraction},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {7},
pages = {428-433},
year = {2018},
note = {14th IFAC Workshop on Discrete Event Systems WODES 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.06.336},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318306669},
author = {Dimitri Bohlender and Stefan Kowalewski},
keywords = {Formal verification, Programmable logic controllers, Industry automation, Software tools, Software safety},
abstract = {Real-world PLC software is modular and composed of many different function blocks. Due to its cyclic execution, an engineer’s mental model of a single function block often exhibits state machine semantics – partitioning a block’s behaviours into different modes of operation. We propose a technique called mode abstraction for automatic computation of such high-level models from program semantics and investigate its impact on the overall model checking performance. To this end, we use constrained horn clauses to characterise the program semantics, mode transitions and safety goals in a compositional way. The resulting verification conditions are discharged using an SMT solver. Evaluation of our prototypical implementation on examples from the PLCopen Safety library shows the effectiveness of both the chosen formalism and mode abstraction.}
}
@incollection{HASKELL200195,
title = {Chapter 6 - Knowledge Base and Transfer: On the Usefulness of Useless Knowledge},
editor = {Robert E. Haskell},
booktitle = {Transfer of Learning},
publisher = {Academic Press},
address = {San Diego},
pages = {95-113},
year = {2001},
series = {Educational Psychology},
issn = {18716148},
doi = {https://doi.org/10.1016/B978-012330595-4/50007-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012330595450007X},
author = {Robert E. Haskell},
abstract = {Publisher Summary
Knowledge base broadly includes knowledge acquired by reading, personal experience, careful listening, and astute observing. It also includes thinking and it is the absolute requirement not only for transfer but for thinking and reasoning. This chapter presents the general knowledge-base conditions necessary for optimal transfer to occur and explain its importance. Transfer of learning requires more than quick-fix strategies and learners must have a knowledge base in a subject in order to even know enough to ask questions about it. Without a sufficient knowledge base, novices do not have a framework within which to formulate adequate questions. Possessing a large knowledge base enables a learner to think about the subject in depth. To achieve general transfer, one often requires much more than immediately useful knowledge. It requires learning that may be considered useless knowledge. There are several examples of knowledge that appeared to have absolutely no use, but years later, these "useless" knowledge was applied to other areas, which later had major applied importance.}
}
@article{KOSTERHALE2013836,
title = {Theory of Mind: A Neural Prediction Problem},
journal = {Neuron},
volume = {79},
number = {5},
pages = {836-848},
year = {2013},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2013.08.020},
url = {https://www.sciencedirect.com/science/article/pii/S089662731300754X},
author = {Jorie Koster-Hale and Rebecca Saxe},
abstract = {Predictive coding posits that neural systems make forward-looking predictions about incoming information. Neural signals contain information not about the currently perceived stimulus, but about the difference between the observed and the predicted stimulus. We propose to extend the predictive coding framework from high-level sensory processing to the more abstract domain of theory of mind; that is, to inferences about others’ goals, thoughts, and personalities. We review evidence that, across brain regions, neural responses to depictions of human behavior, from biological motion to trait descriptions, exhibit a key signature of predictive coding: reduced activity to predictable stimuli. We discuss how future experiments could distinguish predictive coding from alternative explanations of this response profile. This framework may provide an important new window on the neural computations underlying theory of mind.}
}
@incollection{CORTELLNICOLAU20241090,
title = {Agent-Based Modeling},
editor = {Efthymia Nikita and Thilo Rehren},
booktitle = {Encyclopedia of Archaeology (Second Edition) (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {1090-1098},
year = {2024},
isbn = {978-0-323-91856-5},
doi = {https://doi.org/10.1016/B978-0-323-90799-6.00094-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032390799600094X},
author = {Alfredo Cortell-Nicolau},
keywords = {Agency, Agent-based models, Coding, Complexity, Heuristic modeling, Hypothesis testing, Netlogo, Python, R, Reproducibility, Tactical modeling},
abstract = {This work constitutes a very brief overview of Agent-Based Modeling applied to archaeology. The aim is to provide a synthetic overview of the most fundamental concepts, so that researchers interested in starting to explore this methodological tool have a first contact with it. The text covers basic concepts, as well as offers an example of a simple simulation so that interested readers gain initial insight to this topic.}
}
@article{FAIRHURST1980447,
title = {Towards a rationale for neural stability: A model of neural computation and network architecture},
journal = {International Journal of Bio-Medical Computing},
volume = {11},
number = {6},
pages = {447-459},
year = {1980},
issn = {0020-7101},
doi = {https://doi.org/10.1016/0020-7101(80)90012-4},
url = {https://www.sciencedirect.com/science/article/pii/0020710180900124},
author = {M.C. Fairhurst and G.P. Goutos},
abstract = {Networks of Boolean processing cells with low connectivity are known to be inherently stable, in the sense that they exhibit only limited reverberatory activity among their possible state transitions. This paper discusses the value of such a network as a functional model of a neural system and, in the light of the observed decrease in stability with increasing cell connectivity, seeks to identify the features of network architecture and cell computation which act to protect network stability, thereby providing a framework for an understanding of neural stability.}
}
@article{MENENDEZHERRERO2024210,
title = {Persistence of atoms in molecules: there is room beyond electron densities},
journal = {IUCrJ},
volume = {11},
number = {2},
pages = {210-223},
year = {2024},
issn = {2052-2525},
doi = {https://doi.org/10.1107/S2052252524000915},
url = {https://www.sciencedirect.com/science/article/pii/S2052252524000198},
author = {María Menéndez-Herrero and Ángel {Martín Pendás} and X. Zhang},
keywords = {computational modeling, density functional theory, molecular simulations, energy minimization, electron densities, Born maxima},
abstract = {The 3N-dimensional maxima of the square of the wavefunction, the so-called Born maxima, show beyond doubt that the electronic structure of atoms persists in molecules, either in their original ground state or some low-lying excited state. The electron density is only a low-dimensional projection of this much richer landscape.
Evidence that the electronic structure of atoms persists in molecules to a much greater extent than has been usually admitted is presented. This is achieved by resorting to N-electron real-space descriptors instead of one- or at most two-particle projections like the electron or exchange-correlation densities. Here, the 3N-dimensional maxima of the square of the wavefunction, the so-called Born maxima, are used. Since this technique is relatively unknown to the crystallographic community, a case-based approach is taken, revisiting first the Born maxima of atoms in their ground state and then some of their excited states. It is shown how they survive in molecules and that, beyond any doubt, the distribution of electrons around an atom in a molecule can be recognized as that of its isolated, in many cases excited, counterpart, relating this fact with the concept of energetic promotion. Several other cases that exemplify the applicability of the technique to solve chemical bonding conflicts and to introduce predictability in real-space analyses are also examined.}
}
@article{LI2025100338,
title = {RICE AlgebraBot: Lessons learned from designing and developing responsible conversational AI using induction, concretization, and exemplification to support algebra learning},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100338},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100338},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24001413},
author = {Chenglu Li and Wanli Xing and Yukyeong Song and Bailing Lyu},
keywords = {Conversational AI, Large language models, FAccT AI in education, Technology design and development, Math learning},
abstract = {The importance and challenge of Algebra learning is widely recognized, with students across the U.S. facing difficulties due to the subject's complexity. While extensive research has focused on enhancing Algebra learning in K-12 education, the reusability, scalability, and effectiveness of the strategies employed (e.g., manual interventions and digital tutoring platforms) remain limited. Conversational AI (ConvAI), enabled by the advancement of large language models (LLMs), emerges as a potential tool for automatic, personalized, and effective student support. However, ethical concerns surrounding diversity, safety, sentiment, and stereotype associated with ConvAI are prominent, and empirical studies examining its application in education are scarce. The purpose of this study is to develop a ConvAI system that mitigates the potential ethical concerns and empirically evaluate the effect of such a system for math learning. Specifically, we first examined computational strategies to mitigate the ethical concerns of ConvAI in educational setting with educational big data (npretraining = 2,097,139) and found that researchers could effectively enhance ConvAI responsibility through the investigated algorithmic strategies. Then, a ConvAI system was constructed using these strategies, guided by learning sciences principles. Lastly, we examined students' eye-tracking patterns, acceptance, and learning processes when using this ConvAI system to learn Algebra through a random experiment (nparticipant = 151). Participants using the developed ConvAI demonstrated generally increased visual attention levels as compared to the control group. Moreover, participants expressed a positive acceptance towards the ConvAI technology. Finally, participants' interaction patterns with the ConvAI technology influenced their Algebra learning. These results provide insights for both educational researchers and practitioners to integrate ConvAI in learning environments.}
}
@article{LOCHAB202116,
title = {An improved flux limiter using fuzzy modifiers for Hyperbolic Conservation Laws},
journal = {Mathematics and Computers in Simulation},
volume = {181},
pages = {16-37},
year = {2021},
issn = {0378-4754},
doi = {https://doi.org/10.1016/j.matcom.2020.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S0378475420303207},
author = {Ruchika Lochab and Vivek Kumar},
keywords = {Flux limiters, Hyperbolic equations, High resolution methods, Fuzzy logic},
abstract = {The objective of the work in this paper is to computationally tackle a range of problems in hyperbolic conservation laws, which is an interesting branch of computational fluid dynamics. For the simulation of issues in hyperbolic conservation laws, this work explores the concept of fuzzy logic-based operators. This research presents a unique mixture of fuzzy sets and logic with a new branch of conservation laws from fluid dynamics. The approach considers a computational procedure based on the reconstruction of several high-order numerical methods termed as flux-limited methods using some fuzzy logic operators. With the aid of fuzzy modifiers, these flux limiters are further modified. This approach results in improved convergence of approximations and maintains the problem’s basic properties to be solved. Additionally, to ensure improved results, modified flux-limited methods are imposed on some famous test problems. The application results are provided wherever required. The work has demonstrated that it is possible to use such technique and apply it to complex areas of computational fluid dynamics to produce a more straightforward approach to studying other topics like flux-limited methods and hence opens up an exciting gateway for future work.}
}
@article{GRAY2000401,
title = {Objects, Actions, and Images: A Perspective on Early Number Development},
journal = {The Journal of Mathematical Behavior},
volume = {18},
number = {4},
pages = {401-413},
year = {2000},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(00)00025-0},
url = {https://www.sciencedirect.com/science/article/pii/S0732312300000250},
author = {Eddie Gray and Demetra Pitta and David Tall},
abstract = {It is the purpose of this article to present a review of research evidence that indicates the existence of qualitatively different thinking in elementary number development. In doing so, the article summarizes empirical evidence obtained over a period of 10 years. This evidence first signaled qualitative differences in numerical processing, and was seminal in the development of the notion of procept. More recently, it examines the role of imagery in elementary number processing. Its conclusions indicate that in the abstraction of numerical concepts from numerical processes qualitatively different outcomes may arise because children concentrate on different objects or different aspects of the objects, which are components of numerical processing.}
}
@article{HUANG2025135787,
title = {Integrated economic and environmental optimization for industrial consumers: A dual-objective approach with multi-carrier energy systems and fuzzy decision-making},
journal = {Energy},
volume = {324},
pages = {135787},
year = {2025},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2025.135787},
url = {https://www.sciencedirect.com/science/article/pii/S036054422501429X},
author = {Anzhong Huang and Qiuxiang Bi and Luote Dai},
keywords = {Multi-carrier energy hub, Bi-objective optimization, Peak load management, Fuzzy-decision making, Stability of industrial consumers},
abstract = {Industrial energy systems require innovative optimization strategies to simultaneously minimize costs and reduce environmental impact. This study presents a dual-objective optimization model that integrates economic and environmental considerations within a multi-carrier energy hub framework. The proposed approach incorporates a peak load management strategy to optimize energy consumption patterns, a fuzzy decision-making method to handle operational uncertainties, and an enhanced non-dominated sorting genetic algorithm II to improve multi-objective optimization efficiency. The model employs Pareto-optimal solutions, offering decision-makers a structured method to balance cost reduction and emissions minimization. The effectiveness of the proposed framework is validated through case studies, demonstrating that peak load management significantly flattens load profiles, optimizes distributed energy resources, and reduces reliance on grid electricity during peak hours. The enhanced non-dominated sorting genetic algorithm II ensures better convergence toward optimal trade-offs, improving computational performance. Results indicate that the integration of peak load management leads to a 0.93 % reduction in operational costs and a 0.79 % decrease in carbon emissions compared to conventional energy management approaches. These findings underscore the potential of the developed model in enhancing industrial energy efficiency and sustainability, providing a robust and adaptable solution for modern industrial consumers.}
}
@article{LI2022296,
title = {The role of information structures in game-theoretic multi-agent learning},
journal = {Annual Reviews in Control},
volume = {53},
pages = {296-314},
year = {2022},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2022.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S1367578822000086},
author = {Tao Li and Yuhan Zhao and Quanyan Zhu},
keywords = {Multi-agent learning, Information structures, Reinforcement learning, Belief generation, Game theory, Value of Information},
abstract = {Multi-agent learning (MAL) studies how agents learn to behave optimally and adaptively from their experience when interacting with other agents in dynamic environments. The outcome of a MAL process is jointly determined by all agents’ decision-making. Hence, each agent needs to think strategically about others’ sequential moves, when planning future actions. The strategic interactions among agents makes MAL go beyond the direct extension of single-agent learning to multiple agents. With the strategic thinking, each agent aims to build a subjective model of others decision-making using its observations. Such modeling is directly influenced by agents’ perception during the learning process, which is called the information structure of the agent’s learning. As it determines the input to MAL processes, information structures play a significant role in the learning mechanisms of the agents. This review creates a taxonomy of MAL and establishes a unified and systematic way to understand MAL from the perspective of information structures. We define three fundamental components of MAL: the information structure (i.e., what the agent can observe), the belief generation (i.e., how the agent forms a belief about others based on the observations), as well as the policy generation (i.e., how the agent generates its policy based on its belief). In addition, this taxonomy enables the classification of a wide range of state-of-the-art algorithms into four categories based on the belief-generation mechanisms of the opponents, including stationary, conjectured, calibrated, and sophisticated opponents. We introduce Value of Information (VoI) as a metric to quantify the impact of different information structures on MAL. Finally, we discuss the strengths and limitations of algorithms from different categories and point to promising avenues of future research.}
}
@article{BOWLER2021104535,
title = {Children perform extensive information gathering when it is not costly},
journal = {Cognition},
volume = {208},
pages = {104535},
year = {2021},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2020.104535},
url = {https://www.sciencedirect.com/science/article/pii/S0010027720303541},
author = {Aislinn Bowler and Johanna Habicht and Madeleine E. Moses-Payne and Niko Steinbeis and Michael Moutoussis and Tobias U. Hauser},
keywords = {Cognitive development, Information gathering, Computational modelling},
abstract = {Humans often face decisions where little is known about the choice options. Gathering information prior to making a choice is an important strategy to improve decision making under uncertainty. This is of particular importance during childhood and adolescence, when knowledge about the world is still limited. To examine how much information youths gather, we asked 107 children (8–9 years, N = 30), early (12–13 years, N = 41) and late adolescents (16–17 years, N = 36) to perform an information sampling task. We find that children gather significantly more information before making a decision compared to adolescents, but only if it does not come with explicit costs. Using computational modelling, we find that this is because children have reduced subjective costs for gathering information. Our findings thus demonstrate how children overcome their limited knowledge and neurocognitive constraints by deploying excessive information gathering, a developmental feature that could inform aberrant information gathering in psychiatric disorders.}
}
@article{ZHANG201313,
title = {A semantic representation model for design rationale of products},
journal = {Advanced Engineering Informatics},
volume = {27},
number = {1},
pages = {13-26},
year = {2013},
note = {Modeling, Extraction, and Transformation of Semantics in Computer Aided Engineering Systems},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2012.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S1474034612000985},
author = {Yingzhong Zhang and Xiaofang Luo and Jian Li and Jennifer J. Buis},
keywords = {Design rationale, Design semantics, Representation model, Ontology},
abstract = {Design rationale (DR) is crucial information in product design decision support, design analysis and design reuse. In this paper, based on the Issue-based Information System (IBIS) model, a new ontology-based semantic representation model for DR information; the integrated issue, solution, artifact and argument (ISAA) model; is proposed. The ISAA model introduces the ontology-based semantic representation mode to the DR representation and expands the concept elements of IBIS. The class of concept elements and the semantic relationships among them are defined by Web Ontology Language (OWL). The axioms and rules which are used to reason and analyze DR are defined and encoded with Semantic Web Rule Language (SWRL), which enrich the semantic relations defined by OWL. The ISAA model represents the directed graph of IBIS to the Resource Description Framework (RDF) graph and serializes to an RDF/XML document which lays the foundation for retrieving and reasoning semantic information of DR. A semantic annotator integrating with the visual product design model was developed, by which the discrete information of thinking is captured and abstracted to a conceptual representation of the ISAA model. Finally, an example of DR representation for the spring operating mechanism of a high-voltage circuit breaker product is given.}
}
@article{ATKINSON20137060,
title = {Evolutionary optimization for ranking how-to questions based on user-generated contents},
journal = {Expert Systems with Applications},
volume = {40},
number = {17},
pages = {7060-7068},
year = {2013},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2013.06.017},
url = {https://www.sciencedirect.com/science/article/pii/S0957417413003977},
author = {John Atkinson and Alejandro Figueroa and Christian Andrade},
keywords = {Community question-answering, Question-answering systems, Concept clustering, Evolutionary computation, HPSG parsing},
abstract = {In this work, a new evolutionary model is proposed for ranking answers to non-factoid (how-to) questions in community question-answering platforms. The approach combines evolutionary computation techniques and clustering methods to effectively rate best answers from web-based user-generated contents, so as to generate new rankings of answers. Discovered clusters contain semantically related triplets representing question–answers pairs in terms of subject-verb-object, which is hypothesized to improve the ranking of candidate answers. Experiments were conducted using our evolutionary model and concept clustering operating on large-scale data extracted from Yahoo! Answers. Results show the promise of the approach to effectively discovering semantically similar questions and improving the ranking as compared to state-of-the-art methods.}
}
@article{MEJIA201899,
title = {The Power of Writing, a Pebble Hierarchy and a Narrative for the Teaching of Automata Theory},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {339},
pages = {99-110},
year = {2018},
note = {The XLII Latin American Computing Conference},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2018.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S1571066118300513},
author = {Carolina Mejía and J. {Andres Montoya} and Christian Nolasco},
keywords = {Pebble automata, finite state automata},
abstract = {In this work we study pebble automata. Those automata constitute an infinite hierarchy of discrete models of computation. The hierarchy begins at the level of finite state automata (0-pebble automata) and approaches the model of one-tape Turing machines. Thus, it can be argued that it is a complete hierarchy that covers, in a continuous way, all the models of automata that are important in the theory of computation. We investigate the use of this hierarchy as a narrative for the teaching of automata theory. We also investigate some fundamental questions concerning the power of pebble automata.}
}
@article{VUONG2025,
title = {Critical remarks on current practices of data article publishing: Issues, challenges, and recommendations},
journal = {Data Science and Informetrics},
year = {2025},
issn = {2694-6106},
doi = {https://doi.org/10.1016/j.dsim.2025.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S2694610625000098},
author = {Quan-Hoang Vuong and Viet-Phuong La and Minh-Hoang Nguyen},
keywords = {Data paper, FAIR Principles, Editing and reviewing processes, Quality control, Open science, Mindsponge Theory},
abstract = {ABSTRACT
The contribution of the data paper publishing paradigm to the knowledge generation and validation processes is becoming substantial and pivotal. In this paper, through the information-processing perspective of Mindsponge Theory, we discuss how the data article publishing system serves as a filtering mechanism for quality control of the increasingly chaotic datasphere. The overemphasis on machine-actionality and technical standards presents some shortcomings and limitations of the data article publishing system, such as the lack of consideration of humanistic values, radical race for big data, and inadequate use of expertise in data evaluation. Without addressing the shortcomings and limitations, the reusability of data will be hindered, and scientific investment to facilitate data sharing will be wasted. Thus, we suggest that the current data paper publishing paradigm needs to be updated with a new philosophy of data.}
}
@article{PADILLARIVERA201979,
title = {A systematic review of the sustainability assessment of bioenergy: The case of gaseous biofuels},
journal = {Biomass and Bioenergy},
volume = {125},
pages = {79-94},
year = {2019},
issn = {0961-9534},
doi = {https://doi.org/10.1016/j.biombioe.2019.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0961953419301102},
author = {Alejandro Padilla-Rivera and María Guadalupe Paredes and Leonor Patricia Güereca},
keywords = {Sustainability assessment, Systematic review, Bioenergy, Gaseous biofuels, Renewable energy, Sustainability of bioenergy},
abstract = {In recent years, achieving sustainability in renewable energy systems has become important for achieving future economic prosperity and energy security all over the world; therefore, multiple attempts have been made to assess their sustainability. This means that in addition to considering the technological and economic factors, environmental and social aspects should also be considered. However, the wide-ranging concept of sustainability and the various methodological frameworks presented make their interpretation and correct implementation difficult. In this research, through a systematic literature review, we summarize and analyze the current research on the sustainability assessment of bioenergy production/use (also referred as gaseous biofuels) for electricity and heat generation. Sustainability approaches and their underlying factors from the three dimensions of sustainability were consolidated and structured in this systematic review. In addition, a set of indicators (environmental, social and economic) is provided based on the literature analyzed that decision makers can use to evaluate the sustainability performance of bioenergy systems. The main finding indicates that although there are various international efforts on measuring sustainability, only 32 of studies of the 8542 works initially screened (less than 1%) have an integrated approach that considers all three aspects of sustainability, i.e., environmental, economic and social aspects. In most cases, the focus is on one of the three aspects. Additionally, 50% of the studies evaluated included another dimension, i.e., a cultural, institutional or technical dimension. These results support the idea that a multidimensional sustainability assessment is feasible and facilitates decision-making processes towards a sustainable energy future.}
}
@article{SOLEIMANIJAVID2024113958,
title = {Challenges and opportunities of occupant-centric building controls in real-world implementation: A critical review},
journal = {Energy and Buildings},
volume = {308},
pages = {113958},
year = {2024},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.113958},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824000744},
author = {Atiye Soleimanijavid and Iason Konstantzos and Xiaoqi Liu},
keywords = {Occupant-centric control, Comfort, Sensing, Controls, Learning, Energy efficiency, Smart buildings},
abstract = {Over the past few decades, attention in buildings’ design and operation has gradually shifted from promoting only energy efficiency objectives to also addressing human comfort and well-being. Researchers have developed a wide range of control algorithms ranging from rule-based controls to complex learning approaches that can fully capture occupants’ personalized preferences in smart buildings. This direction of occupant-centric building controls can bridge the gap between occupants’ satisfaction and sustainability objectives. However, most of these promising technologies have not yet found their way into real-world applications. This study will perform a critical review on occupant-centric thermal and lighting control studies aiming to (i) analyze the strengths and weaknesses of different approaches; (ii) identify the requirements for these techniques to be implemented in real-world systems; and (iii) propose new research directions that will promote the usability of such controls and will be a catalyst towards their wide adoption. Computational complexity, integration with Building Automation Systems (BAS), data availability and data quality, scalability, and the lack of more research featuring actual building implementation emerge as critical barriers. Addressing these challenges is imperative for the successful deployment of occupant-centric controls in real-world applications.}
}
@article{WANG2022477,
title = {Methodology of network pharmacology for research on Chinese herbal medicine against COVID-19: A review},
journal = {Journal of Integrative Medicine},
volume = {20},
number = {6},
pages = {477-487},
year = {2022},
issn = {2095-4964},
doi = {https://doi.org/10.1016/j.joim.2022.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S2095496422000966},
author = {Yi-xuan Wang and Zhen Yang and Wen-xiao Wang and Yu-xi Huang and Qiao Zhang and Jia-jia Li and Yu-ping Tang and Shi-jun Yue},
keywords = {Chinese traditional medicine, Herbal medicine, Network pharmacology, Compound identification, COVID-19},
abstract = {Traditional Chinese medicine, as a complementary and alternative medicine, has been practiced for thousands of years in China and possesses remarkable clinical efficacy. Thus, systematic analysis and examination of the mechanistic links between Chinese herbal medicine (CHM) and the complex human body can benefit contemporary understandings by carrying out qualitative and quantitative analysis. With increasing attention, the approach of network pharmacology has begun to unveil the mystery of CHM by constructing the heterogeneous network relationship of “herb-compound-target-pathway,” which corresponds to the holistic mechanisms of CHM. By integrating computational techniques into network pharmacology, the efficiency and accuracy of active compound screening and target fishing have been improved at an unprecedented pace. This review dissects the core innovations to the network pharmacology approach that were developed in the years since 2015 and highlights how this tool has been applied to understanding the coronavirus disease 2019 and refining the clinical use of CHM to combat it.}
}
@article{ZHANG2025100978,
title = {The paradox of self-efficacy and technological dependence: Unraveling generative AI's impact on university students' task completion},
journal = {The Internet and Higher Education},
volume = {65},
pages = {100978},
year = {2025},
issn = {1096-7516},
doi = {https://doi.org/10.1016/j.iheduc.2024.100978},
url = {https://www.sciencedirect.com/science/article/pii/S109675162400040X},
author = {Ling Zhang and Junzhou Xu},
keywords = {Generative AI, Technological dependence, Self-efficacy, Learning efficiency, Educational application, Paradoxical relationship},
abstract = {In the era of proliferating artificial intelligence (AI) technology, generative AI is reshaping educational landscapes, prompting a critical examination of its influence on students' learning processes and their self-efficacy amid concerns over growing technological dependence. This study investigates the nuanced relationship between generative AI use and university students' self-efficacy and technological dependence, illuminating the underlying paradoxes and implications for inclusive education practices. Through a survey of 348 university students, with 200 valid responses analyzed, we uncover the direct and indirect impacts of generative AI usage frequency on AI dependence. Our findings reveal a paradoxical effect: enhanced AI usage significantly amplifies students' confidence and efficiency in learning, yet simultaneously intensifies their dependence on AI. This dual impact both supports and complicates the incorporation of AI technologies into educational settings, underscoring the need for a balanced approach to leveraging AI in teaching and learning. Our study underscores the critical importance of a nuanced understanding of AI's role in education. It highlights the necessity of crafting an educational landscape where technology augments learning processes without compromising independent learning capabilities. By navigating the complex interplay between technological advancement and educational inclusivity, our findings guide the development of AI-assisted learning environments that are not only effective but also equitable and accessible.}
}
@article{AUGSBURGER2009270,
title = {Methodologies to assess blood flow in cerebral aneurysms: Current state of research and perspectives},
journal = {Journal of Neuroradiology},
volume = {36},
number = {5},
pages = {270-277},
year = {2009},
issn = {0150-9861},
doi = {https://doi.org/10.1016/j.neurad.2009.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0150986109000479},
author = {L. Augsburger and P. Reymond and E. Fonck and Z. Kulcsar and M. Farhat and M. Ohta and N. Stergiopulos and D. {A. Rüfenacht}},
keywords = {Cerebral aneurysms, Blood flow assessment, Particle image velocimetry, Computational fluid dynamics},
abstract = {Summary
With intracranial aneurysms disease bringing a weakened arterial wall segment to initiate, grow and potentially rupture an aneurysm, current understanding of vessel wall biology perceives the disease to follow the path of a dynamic evolution and increasingly recognizes blood flow as being one of the main stakeholders driving the process. Although currently mostly morphological information is used to decide on whether or not to treat a yet unruptured aneurysm, among other factors, knowledge of blood flow parameters may provide an advanced understanding of the mechanisms leading to further aneurismal growth and potential rupture. Flow patterns, velocities, pressure and their derived quantifications, such as shear and vorticity, are today accessible by direct measurements or can be calculated through computation. This paper reviews and puts into perspective current experimental methodologies and numerical approaches available for such purposes. In our view, the combination of current medical imaging standards, numerical simulation methods and endovascular treatment methods allow for thinking that flow conditions govern more than any other factor fate and treatment in cerebral aneurysms. Approaching aneurysms from this perspective improves understanding, and while requiring a personalized aneurysm management by flow assessment and flow correction, if indicated.}
}
@article{PINTOCARVALHO2022107255,
title = {An efficient multiscale strategy to predict the evolution of the real contact area between rough surfaces},
journal = {Tribology International},
volume = {165},
pages = {107255},
year = {2022},
issn = {0301-679X},
doi = {https://doi.org/10.1016/j.triboint.2021.107255},
url = {https://www.sciencedirect.com/science/article/pii/S0301679X21004035},
author = {R. {Pinto Carvalho} and A.M. {Couto Carneiro} and F.M. {Andrade Pires} and T. Doca},
keywords = {Contact area, Roughness, Contact homogenisation, Multiscale modelling},
abstract = {An efficient and general multiscale strategy to model rough contact and determine the evolution of the real contact area is proposed, based on the splitting of the surface power spectrum. A multiplicative homogenisation scheme is developed to evaluate the contact area fraction effectively, by incorporating statistical information of the contact pressure field in the scale transitions. A strategy for separating the roughness frequencies and for generating the topography at each scale is proposed. The comparison between direct numerical simulation and the proposed multiscale strategy demonstrates the capability to predict the contact area evolution of multiscale rough topographies. The significant reduction of the computational cost endorses the applicability of the proposed multiscale framework to practical circumstances.}
}
@article{BRIERLEY2021107870,
title = {The dark art of interpretation in geomorphology},
journal = {Geomorphology},
volume = {390},
pages = {107870},
year = {2021},
issn = {0169-555X},
doi = {https://doi.org/10.1016/j.geomorph.2021.107870},
url = {https://www.sciencedirect.com/science/article/pii/S0169555X21002786},
author = {Gary Brierley and Kirstie Fryirs and Helen Reid and Richard Williams},
keywords = {Landform, Landscape, Explanation, Prediction, Big Data, Fieldwork, Modelling},
abstract = {The process of interpretation, and the ways in which knowledge builds upon interpretations, has profound implications in scientific and managerial terms. Despite the significance of these issues, geomorphologists typically give scant regard to such deliberations. Geomorphology is not a linear, cause-and-effect science. Inherent complexities and uncertainties prompt perceptions of the process of interpretation in geomorphology as a frustrating form of witchcraft or wizardry — a dark art. Alternatively, acknowledging such challenges recognises the fun to be had in puzzle-solving encounters that apply abductive reasoning to make sense of physical landscapes, seeking to generate knowledge with a reliable evidence base. Carefully crafted approaches to interpretation relate generalised understandings derived from analysis of remotely sensed data with field observations/measurements and local knowledge to support appropriately contextualised place-based applications. In this paper we develop a cognitive approach (Describe-Explain-Predict) to interpret landscapes. Explanation builds upon meaningful description, thereby supporting reliable predictions, in a multiple lines of evidence approach. Interpretation transforms data into knowledge to provide evidence that supports a particular argument. Examples from fluvial geomorphology demonstrate the data-interpretation-knowledge sequence used to analyse river character, behaviour and evolution. Although Big Data and machine learning applications present enormous potential to transform geomorphology into a data-rich, increasingly predictive science, we outline inherent dangers in allowing prescriptive and synthetic tools to do the thinking, as interpreting local differences is an important element of geomorphic enquiry.}
}
@article{KOU2023109788,
title = {Infrared small target segmentation networks: A survey},
journal = {Pattern Recognition},
volume = {143},
pages = {109788},
year = {2023},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2023.109788},
url = {https://www.sciencedirect.com/science/article/pii/S0031320323004867},
author = {Renke Kou and Chunping Wang and Zhenming Peng and Zhihe Zhao and Yaohong Chen and Jinhui Han and Fuyu Huang and Ying Yu and Qiang Fu},
keywords = {Infrared small target, Characteristic analysis, Segmentation network, Deep learning, Collaborative technology, Data-driven, False alarm, Missed detection},
abstract = {Fast and robust small target detection is one of the key technologies in the infrared (IR) search and tracking systems. With the development of deep learning, there are many data-driven IR small target segmentation algorithms, but they have not been extensively surveyed; we believe our proposed survey is the first to systematically survey them. Focusing on IR small target segmentation tasks, we summarized 7 characteristics of IR small targets, 3 feature extraction methods, 8 design strategies, 30 segmentation networks, 8 loss functions, and 13 evaluation indexes. Then, the accuracy, robustness, and computational complexities of 18 segmentation networks on 5 public datasets were compared and analyzed. Finally, we have discussed the existing problems and future trends in the field of IR small target detection. The proposed survey is a valuable reference for both beginners adapting to current trends in IR small target detection and researchers already experienced in this field.}
}
@article{IGAMBERDIEV201715,
title = {The quantum basis of spatiotemporality in perception and consciousness},
journal = {Progress in Biophysics and Molecular Biology},
volume = {130},
pages = {15-25},
year = {2017},
note = {Quantum information models in biology: from molecular biology to cognition},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2017.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0079610716301687},
author = {Abir U. Igamberdiev and Nikita E. Shklovskiy-Kordi},
keywords = {Cytoskeleton, Molecular computation, Neuron, Quantum measurement, Spatiotemporality},
abstract = {Living systems inhabit the area of the world which is shaped by the predictable space-time of physical objects and forces that can be incorporated into their perception pattern. The process of selecting a “habitable” space-time is the internal quantum measurement in which living systems become embedded into the environment that supports their living state. This means that living organisms choose a coordinate system in which the influence of measurement is minimal. We discuss specific roles of biological macromolecules, in particular of the cytoskeleton, in shaping perception patterns formed in the internal measurement process. Operation of neuron is based on the transmission of signals via cytoskeleton where the digital output is generated that can be decoded through a reflective action of the perceiving agent. It is concluded that the principle of optimality in biology as formulated by Liberman et al. (BioSystems 22, 135–154, 1989) is related to the establishment of spatiotemporal patterns that are maximally predictable and can hold the living state for a prolonged time. This is achieved by the selection of a habitable space approximated to the conditions described by classical physics.}
}
@article{WANG2023170277,
title = {MLKCA-Unet: Multiscale large-kernel convolution and attention in Unet for spine MRI segmentation},
journal = {Optik},
volume = {272},
pages = {170277},
year = {2023},
issn = {0030-4026},
doi = {https://doi.org/10.1016/j.ijleo.2022.170277},
url = {https://www.sciencedirect.com/science/article/pii/S0030402622015352},
author = {Biao Wang and Juan Qin and Lianrong Lv and Mengdan Cheng and Lei Li and Dan Xia and Shike Wang},
keywords = {Deep learning, Spine segmentation, Receptive fields, Multiscale large-kernel convolution, Attention},
abstract = {Medical image segmentation plays a key role in the diagnosis of spinal diseases. Unet has become a universal structure for image segmentation because of its unique skip connection structure in recent years. However, since Unet uses small-kernel convolution, the relationship between remote features is difficult to obtain due to the small receptive fields, and the key information cannot be highlighted, resulting in insufficient edge information. To overcome these problems, this paper proposes multiscale large-kernel convolution Unet (MLKCA-Unet), which develops MLKC block for effective feature extraction. Large-kernel convolution with different convolution kernels is used according to the feature map. For large feature maps, smaller large- kernel convolution is used, and for small feature maps, larger large-kernel convolution is used. All large-kernel convolution can be reduced the dimension by 1 × 1 convolution kernel. This method has a significant reduction in computation. By paralleling each large kernel convolution branch with the 3 × 3 convolution branch, it helps to capture detailed information. At the same time, an attention mechanism is added to the network to emphasize rich feature areas and enhance useful information. Finally, various indicators are employed to evaluate the network’s accuracy, similarity and speed, including IOU, DSC, TPR, PPV, and ET. The published spinesagt2wdataset3 spinal MRI image dataset is adopted in the experiment. The IOU, DSC, TPR, PPV, and ET on the test set are 0.8302, 0.9017, 0.9000, 0.9051 and 70 s/epoch respectively. The experimental result shows that MLKCA-Unet demonstrates superior segmentation performance and robustness, which can be well extended to other medical image segmentation.}
}
@article{KADERAVEK201527,
title = {SCIIENCE: The creation and pilot implementation of an NGSS-based instrument to evaluate early childhood science teaching},
journal = {Studies in Educational Evaluation},
volume = {45},
pages = {27-36},
year = {2015},
issn = {0191-491X},
doi = {https://doi.org/10.1016/j.stueduc.2015.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0191491X15000218},
author = {Joan N. Kaderavek and Tamala North and Regina Rotshtein and Hoangha Dao and Nicholas Liber and Geoff Milewski and Scott C. Molitor and Charlene M. Czerniak},
keywords = {Discourse analysis, Teacher assessment, Language of science in classrooms, Validity/reliability},
abstract = {This paper describes the development, testing and implementation of the Systematic Characterization of Inquiry Instruction in Early LearNing Classroom Environments (SCIIENCE). The SCIIENCE instrument was designed to capture best practices outlined in the National Research Council's Framework for K-12 Science Education as they occur within a science lesson. The goals of the SCIIENCE instrument are to (a) assess the quality of science instruction in PK-3 classrooms, (b) capture teacher behaviors and instructional practices that engage students in the lesson, promote scientific studies, encourage higher-level thinking, and (c) provide a feedback mechanism for guiding professional development of PK-3 teachers. Science educators can apply this instrument to teacher behaviors and use the data to improve classroom inquiry instructional methodology.}
}
@article{JEREMIAH2025103529,
title = {The human-AI dyad: Navigating the new frontier of entrepreneurial discourse},
journal = {Futures},
volume = {166},
pages = {103529},
year = {2025},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2024.103529},
url = {https://www.sciencedirect.com/science/article/pii/S001632872400212X},
author = {Faith Jeremiah},
keywords = {AI, Entrepreneurship, Human, New technologies, Self-identity},
abstract = {The rapid progression of Artificial Intelligence (AI) is further solidifying the importance of redefining entrepreneurship and reshaping how ventures and innovations are conceived, developed, and managed. This paper investigates how AI is transforming entrepreneurship by reshaping traditional business paradigms and entrepreneurs’ roles. It examines AI's influence on entrepreneurial decision-making, innovation, and identity. Through a comprehensive literature review and thematic analysis of previously published data, the research identifies emergent themes in AI's integration into entrepreneurial discourse. The findings indicate that AI enhances operational efficiency and decision-making but challenges traditional notions of entrepreneurial identity and creativity. Furthermore, entrepreneurs increasingly depend on AI for data-driven insights, strategic foresight, and personalised customer interactions, reshaping business strategies and competitive landscapes. This article emphasises the importance of AI literacy and adaptive strategies for entrepreneurs to leverage the human-AI dyad effectively while maintaining values and ethics. It also highlights the significance of concentrating research efforts on entrepreneurs as they are the very cohort to shape new norms and pioneer new business models and innovations. Future research should explore AI's long-term impacts on entrepreneurial ecosystems, including psychological, ethical, and socio-cultural dimensions, with comparative studies across industries and regions providing further insights.}
}
@article{COOPER200672,
title = {Definability as hypercomputational effect},
journal = {Applied Mathematics and Computation},
volume = {178},
number = {1},
pages = {72-82},
year = {2006},
note = {Special Issue on Hypercomputation},
issn = {0096-3003},
doi = {https://doi.org/10.1016/j.amc.2005.09.072},
url = {https://www.sciencedirect.com/science/article/pii/S0096300305008350},
author = {S. Barry Cooper},
keywords = {Computability, Definability, Hypercomputation},
abstract = {The classical simulation of physical processes using standard models of computation is fraught with problems. On the other hand, attempts at modelling real-world computation with the aim of isolating its hypercomputational content have struggled to convince. We argue that a better basic understanding can be achieved through computability theoretic deconstruction of those physical phenomena most resistant to classical simulation. From this we may be able to better assess whether the hypercomputational enterprise is proleptic computer science, or of mainly philosophical interest.}
}
@article{ALEMANY2018429,
title = {Effects of binary variables in mixed integer linear programming based unit commitment in large-scale electricity markets},
journal = {Electric Power Systems Research},
volume = {160},
pages = {429-438},
year = {2018},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2018.03.019},
url = {https://www.sciencedirect.com/science/article/pii/S0378779618300919},
author = {Juan Alemany and Leszek Kasprzyk and Fernando Magnago},
keywords = {Binary variables relaxation, Branch and cut algorithm, Day-ahead electricity market clearing, Mixed integer linear programming, Unit commitment},
abstract = {Mixed integer linear programming is one of the main approaches used to solve unit commitment problems. Due to the computational complexity of unit commitment problems, several researches remark the benefits of using less binary variables or relaxing them for the branch-and-cut algorithm. However, integrality constraints relaxation seems to be case dependent because there are many instances where applying it may not improve the computational burden. In addition, there is a lack of extensive numerical experiments evaluating the effects of the relaxation of binary variables in mixed integer linear programming based unit commitment. Therefore, the primary purpose of this work is to analyze the effects of binary variables and compare different relaxations, supported by extensive computational experiments. To accomplish this objective, two power systems are used for the numerical tests: the IEEE118 test system and a very large scale real system. The results suggest that a direct link between the relaxation of binary variables and computational burden cannot be easily assured in the general case. Therefore, relaxing binary variables should not be used as a general rule-of-practice to improve computational burden, at least, until each particular model is tested under different load scenarios and formulations to quantify the final effects of binary variables on the specific UC implementation. The secondary aim of this work is to give some preliminary insight into the reasons that could be supporting the binary relaxation in some UC instances.}
}
@article{CAKIROGLU2023101279,
title = {A model to develop activities for teaching programming through metacognitive strategies},
journal = {Thinking Skills and Creativity},
volume = {48},
pages = {101279},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101279},
url = {https://www.sciencedirect.com/science/article/pii/S1871187123000494},
author = {Ünal ÇAKIROĞLU and Betül ER},
keywords = {Programming, Teaching programming, Metacognition, Problem solving},
abstract = {Among many endeavors to explore ways to effective teaching programming in classrooms, focusing on metacognition is somewhat neglected. This study suggests a framework for integrating metacognitive strategies to the programming teaching process. In order to achieve this, the knowledge and skills required for writing quality programs and the nature of the metacognition is interrelated with each other encompassing problem solving strategies with programming. The framework has two dimensions; one is metacognitive knowledge including conditional, procedure and declarative knowledge and the other is metacognitive regulation including planning, monitoring and evaluating activities embodied in a specialized worksheet. The activities were designed to be implemented in three phases of programming: at the beginning of the programming, during the programming and after finishing the programming. The suggested framework with the worksheet is pilot tested and validated to be used in the undergraduate introductory programming classrooms. We hope that the suggested roadmap may contribute to the instructional designers and researchers in the field of programming teaching and evaluation.}
}
@article{GRABOWSKI201921,
title = {A Primer on Data Analytics in Functional Genomics: How to Move from Data to Insight?},
journal = {Trends in Biochemical Sciences},
volume = {44},
number = {1},
pages = {21-32},
year = {2019},
issn = {0968-0004},
doi = {https://doi.org/10.1016/j.tibs.2018.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S0968000418302263},
author = {Piotr Grabowski and Juri Rappsilber},
keywords = {data integration, data science, functional genomics, machine learning, systems biology},
abstract = {High-throughput methodologies and machine learning have been central in developing systems-level perspectives in molecular biology. Unfortunately, performing such integrative analyses has traditionally been reserved for bioinformaticians. This is now changing with the appearance of resources to help bench-side biologists become skilled at computational data analysis and handling large omics data sets. Here, we show an entry route into the field of omics data analytics. We provide information about easily accessible data sources and suggest some first steps for aspiring computational data analysts. Moreover, we highlight how machine learning is transforming the field and how it can help make sense of biological data. Finally, we suggest good starting points for self-learning and hope to convince readers that computational data analysis and programming are not intimidating.}
}
@article{GAO2022112486,
title = {Regarding the shallow water in an ocean via a Whitham-Broer-Kaup-like system: hetero-Bäcklund transformations, bilinear forms and M solitons},
journal = {Chaos, Solitons & Fractals},
volume = {162},
pages = {112486},
year = {2022},
issn = {0960-0779},
doi = {https://doi.org/10.1016/j.chaos.2022.112486},
url = {https://www.sciencedirect.com/science/article/pii/S0960077922006944},
author = {Xin-Yi Gao and Yong-Jiang Guo and Wen-Rui Shan},
keywords = {Ocean, Shallow water, Whitham-Broer-Kaup-like system, Hetero-Bäcklund transformations, Bilinear forms,  solitons, Symbolic computation},
abstract = {Considering the water waves, people have investigated many systems. In this paper, what we study is a Whitham-Broer-Kaup-like system for the dispersive long waves in the shallow water in an ocean. With respect to the water-wave horizontal velocity and deviation height from the equilibrium of the water, we construct (A) two branches of the hetero-Bäcklund transformations, from that system to a known constant-coefficient nonlinear dispersive-wave system, (B) two branches of the bilinear forms and (C) two branches of the M-soliton solutions, with M as a positive integer. Results rely upon the oceanic shallow-water coefficients in that system.}
}
@article{MANNUCCI2023103265,
title = {Exploring potential futures: Evaluating the influence of deep uncertainties in urban planning through scenario planning: A case study in Rome, Italy},
journal = {Futures},
volume = {154},
pages = {103265},
year = {2023},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2023.103265},
url = {https://www.sciencedirect.com/science/article/pii/S0016328723001696},
author = {Simona Mannucci and Jan H. Kwakkel and Michele Morganti and Marco Ferrero},
keywords = {Scenario planning, Deep uncertainties, Urban planning, Decision-making, Climate adaptation},
abstract = {Cities play a critical role in developing adaptable strategies to address the challenges posed by climate change. However, the inherent complexity of urban environments and their uncertain future conditions necessitate exploring innovative approaches and tools to assist the current planning practices. This paper presents a workflow rooted in model-based scenario planning for long-term adaptation planning given uncertain futures. To demonstrate the workflow’s effectiveness, a pertinent case study was conducted in a flood-prone area of Rome. The study employed a land-use change model to examine potential urban growth patterns, considering the uncertain implementation of new poles of attraction. This interdisciplinary study constitutes an initial stride toward implementing uncertainty within urban planning frameworks. Future prospects encompass the integration of multiple models for cross-scale analysis, embracing further critical environmental and social aspects. This research contributes to advancing urban resilience strategies. It enhances the understanding of adapting to an uncertain future in the face of climate change, as urban areas must embrace comprehensive planning to ensure flexible adaptation when faced with climate-driven uncertainties in long-term planning. In conclusion, the study underscores that embracing uncertainty is a challenge and a pivotal opportunity to shape resilient and adaptable urban futures.}
}
@article{SHAHRYARI2021101395,
title = {Energy and task completion time trade-off for task offloading in fog-enabled IoT networks},
journal = {Pervasive and Mobile Computing},
volume = {74},
pages = {101395},
year = {2021},
issn = {1574-1192},
doi = {https://doi.org/10.1016/j.pmcj.2021.101395},
url = {https://www.sciencedirect.com/science/article/pii/S1574119221000535},
author = {Om-Kolsoom Shahryari and Hossein Pedram and Vahid Khajehvand and Mehdi Dehghan TakhtFooladi},
keywords = {Internet of Things, Fog computing, Task offloading, Genetic algorithm, Particle swarm optimization, Resource allocation},
abstract = {In order to improve the quality of experience in executing computation-intensive tasks of real-time IoT applications in a fog-enabled IoT network, resource-constrained IoT devices can offload the tasks to resource-rich nearby fog nodes. It causes a reduction in energy consumption compared with local processing, although it extends task completion time due to communication latency. In this paper, we propose a task offloading scheme that optimizes task offloading decision, fog node selection, and computation resource allocation, investigating the trade-off between task completion time and energy consumption. Weighting coefficients of time and energy consumption are determined based on specific demands of the user and residual energy of devices’ battery. Accordingly, we formulate the task offloading problem as a mixed-integer nonlinear program (MINLP), which is NP-hard. A sub-optimal algorithm based on the hybrid of genetic algorithm and particle swarm optimization is designed to solve the formulated problem. Extensive simulations prove the convergence of the proposed algorithm and its superior performance in comparison with baseline schemes.}
}
@article{PAVLOVA2024103631,
title = {A dual process model of spontaneous conscious thought},
journal = {Consciousness and Cognition},
volume = {118},
pages = {103631},
year = {2024},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2023.103631},
url = {https://www.sciencedirect.com/science/article/pii/S105381002300168X},
author = {Maria K. Pavlova},
keywords = {Automatic processing, Cognitive control, Executive failure, Involuntary attention, Mental effort, Mind wandering, Meta-awareness, Modality and interference in working memory, Process–occurrence framework, Spontaneous thought},
abstract = {In the present article, I review theory and evidence on the psychological mechanisms of mind wandering, paying special attention to its relation with executive control. I then suggest applying a dual-process framework (i.e., automatic vs. controlled processing) to mind wandering and goal-directed thought. I present theoretical arguments and empirical evidence in favor of the view that mind wandering is based on automatic processing, also considering its relation to the concept of working memory. After that, I outline three scenarios for an interplay between mind wandering and goal-directed thought during task performance (parallel automatic processing, off-task thought substituting on-task thought, and non-disruptive mind wandering during controlled processing) and address the ways in which the mind-wandering and focused-attention spells can terminate. Throughout the article, I formulate empirical predictions. In conclusion, I discuss how automatic and controlled processing may be balanced in human conscious cognition.}
}
@article{KIM2021188548,
title = {A primer on applying AI synergistically with domain expertise to oncology},
journal = {Biochimica et Biophysica Acta (BBA) - Reviews on Cancer},
volume = {1876},
number = {1},
pages = {188548},
year = {2021},
issn = {0304-419X},
doi = {https://doi.org/10.1016/j.bbcan.2021.188548},
url = {https://www.sciencedirect.com/science/article/pii/S0304419X21000457},
author = {Jason Kim and Rebecca Kusko and Benjamin Zeskind and Jenny Zhang and Renan Escalante-Chong},
abstract = {Background
The concurrent growth of large-scale oncology data alongside the computational methods with which to analyze and model it has created a promising environment for revolutionizing cancer diagnosis, treatment, prevention, and drug discovery. Computational methods applied to large datasets have accelerated the drug discovery process by reducing bottlenecks and widening the search space beyond what is experimentally tractable. As the research community gains understanding of the myriad genetic underpinnings of cancer via sequencing, imaging, screens, and more that are ingested, transformed, and modeled by top open-source machine learning and artificial intelligence tools readily available, the next big drug candidate might seem merely an “Enter” key away. Of course, the reality is more convoluted, but still promising.
Scope of review
We present methods to approach the process of building an AI model, with strong emphasis on the aspects of model development we believe to be crucial to success but that are not commonly discussed: diligence in posing questions, identifying suitable datasets and curating them, and collaborating closely with biology and oncology experts while designing and evaluating the model. Digital pathology, Electronic Health Records, and other data types outside of high-throughput molecular data are reviewed well by others and outside of the scope of this review. This review emphasizes the importance of considering the limitations of the datasets, computational methods, and our minds when designing AI models. For example, datasets can be biased towards areas of research interest, funding, and particular patient populations. Neural networks may learn representations and correlations within the data that are grounded not in biological phenomena, but statistical anomalies erroneously extracted from the training data. Researchers may mis-interpret or over-interpret the output, or design and evaluate the training process such that the resultant model generalizes poorly. Fortunately, awareness of the strengths and limitations of applying data analytics and AI to drug discovery enables us to leverage them carefully and insightfully while maximizing their utility. These applications when performed in close collaboration with domain experts, together with continuous critical evaluation, generation of new data to minimize known blind spots as they are found, and rigorous experimental validation, increases the success rate of the study. We will discuss applications including AI-assisted target identification, drug repurposing, patient stratification, and gene prioritization.
Major conclusions
Data analytics and AI have demonstrated capabilities to revolutionize cancer research, prevention, and treatment by maximizing our understanding and use of the expanding panoply of experimental data. However, to separate promise from true utility, computational tools must be carefully designed, critically evaluated, and constantly improved. Once that is achieved, a human-computer hybrid discovery process will outperform one driven by each alone.
General significance
This review highlights the challenges and promise of synergizing predictive AI models with human expertise towards greater understanding of cancer.}
}
@article{GAL2014246,
title = {Novel approaches to address challenges in modelling aquatic ecosystems},
journal = {Environmental Modelling & Software},
volume = {61},
pages = {246-248},
year = {2014},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2014.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S1364815214002321},
author = {Gideon Gal and Matthew Hipsey and Karsten Rinke and Barbara Robson},
keywords = {Ecological modelling, Freshwater, Marine, Biogeochemistry, Lake, Estuary},
abstract = {Aquatic ecosystems are under increasing stress due to direct and indirect human activities. In response to this increased stress, aquatic ecosystems models are increasingly used to simulate water quality responses to changes. The increasing use of these models has not come without challenges. This thematic issue brings together examples of the latest thinking and novel approaches addressing key areas across a range of aquatic ecosystems, from lakes to rivers to marine waters. Topics include approaches applied to cover the full range of activities from methodological and technical developments of model-driven research of aquatic ecosystem functioning to model applications in lake management and decision-making. This thematic issue will provide additional momentum towards the ongoing development and improvement of aquatic models and their application.}
}
@article{GACS19811,
title = {Causal nets or what is a deterministic computation?},
journal = {Information and Control},
volume = {51},
number = {1},
pages = {1-19},
year = {1981},
issn = {0019-9958},
doi = {https://doi.org/10.1016/S0019-9958(81)90058-9},
url = {https://www.sciencedirect.com/science/article/pii/S0019995881900589},
author = {Péter Gács and Leonid A. Levin},
abstract = {The network approach to computation is more direct and “physical” than the one based on some specific computing devices (like Turing machines). However, the size of a usual—e.g., Boolean—network does not reflect the complexity of computing the corresponding function, since a small network may be very hard to find even if it exists. A history of the work of a particular computing device can be described as a network satisfying some restrictions. The size of this network reflects the complexity of the problem, but the restrictions are usually somewhat arbitrary and even awkward. Causal nets are restricted only by determinism (causality) and locality of interaction. Their geometrical characteristics do reflect computational complexities. And various imaginary computer devices are easy to express in their terms. The elementarity of this concept may help bringing geometrical and algebraic (and maybe even physical) methods into the theory of computations. This hope is supported by the group-theoretical criterion given in this paper for computability from symmetrical initial configurations.}
}
@article{LI20103574,
title = {The process model to aid innovation of products conceptual design},
journal = {Expert Systems with Applications},
volume = {37},
number = {5},
pages = {3574-3587},
year = {2010},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2009.10.034},
url = {https://www.sciencedirect.com/science/article/pii/S0957417409009002},
author = {Wenqiang Li and Yan Li and Jian Wang and Xiaoying Liu},
keywords = {Conceptual design, Innovative strategies, Process mapping, Extension reasoning, Mathematical model},
abstract = {Currently, designers often pay little attention to integrated innovation during the design process of products. In addition, the product assistance design systems mainly focus on the detailed design phrase and the construction function of mathematics models are often been neglected. In order to solve these problems, this paper proposes a conceptual design process model to aid multi-stage innovation of product design based on the integration of the essential rules of the Axiomatic Design (AD) model, Function–Behaviour–Structure (FBS) model, and the guideline of functional creative thinking logics. By utilising the function tree and functional structure tree as the mediums to express the design information and by applying the conflict solving strategies of Extensic theory, the conceptual design process is defined as an integrated system with five stages and four mappings. The integrated logical processes of this model are described with mathematical language. Thus, the whole transformation from design experiences to design principles and to mathematical model finally to aided design system is realized perfectly in the proposed process model. The meaningful exploration on the nature and practical processes of product conceptual design is carried out in this research.}
}
@article{GOSWAMI2024111921,
title = {Real-time evaluation of object detection models across open world scenarios},
journal = {Applied Soft Computing},
volume = {163},
pages = {111921},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111921},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624006951},
author = {Puneet Goswami and Lakshita Aggarwal and Arun Kumar and Rahul Kanwar and Urvi Vasisht},
keywords = {Computer vision, Faster R-CNN, DETR, YOLO, Resnet 50, Resnet 101, Object detection, Model comparison, Evaluation metrices},
abstract = {Object detection models have been experiencing significant improvements over the years due to advancements in deep learning techniques, increased availability of large-scale annotated datasets, and computational resources. Different object detection models have varying levels of accuracy, speed, and robustness. With the increasing complexity and diversity of object detection models, it becomes a problem for researchers and practitioners to choose the most suitable model for their specific needs. This research paper outlines the escalating demand for robust comparison of object detection models in response to rapidly advancing technology. This evaluation helps in identifying the strengths and weaknesses of these models and selecting the most suitable one for a specific task. This highlights a significant challenge stemming from the lack of recent comparative studies on object detection models across various image qualities, object sizes, and training data sizes. The above challenges are tackled by a meticulous evaluation of three state-of-the-art object detection models: YOLO-v8, Faster R-CNN with ResNet 50 and 101 backbones, and End-to-End Object Detection Transformers (DETR) utilizing ResNet 50 and 101 backbones by employing a rigorous assessment framework encompassing mean Average Precision (mAP), accuracy, and inference speed. This study focuses on thoroughly examining how well the models perform across three different datasets: TACO, PlastOPol, and TACO 4.5. These datasets consist of open-world images captured in real-time from various locations. They include 1500, 2500, and 6500 images respectively, depicting real-world environments with varying lighting conditions and complex backgrounds. The results identify YOLOv8 as the superior model for high and medium-quality images, while Faster R-CNN performs better for low-quality images. However, DETR's accuracy falls short compared to other models. The paper fills a crucial gap in understanding model performance across varying image qualities and object sizes and helps in taking informed decisions in object detection systems.}
}
@article{CASTILLOFELISOLA2023108748,
title = {Cadabra and Python algorithms in general relativity and cosmology II: Gravitational waves},
journal = {Computer Physics Communications},
volume = {289},
pages = {108748},
year = {2023},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2023.108748},
url = {https://www.sciencedirect.com/science/article/pii/S0010465523000930},
author = {Oscar Castillo-Felisola and Dominic T. Price and Mattia Scomparin},
keywords = {Computer algebra system, , Gravitation, Gravitational waves, Perturbative field theory, Python, Cosmology, General relativity},
abstract = {Computer Algebra Systems (CASs) like Cadabra Software play a prominent role in a wide range of research activities in physics and related fields. We show how Cadabra language is easily implemented in the well established Python programming framework, gaining excellent flexibility and customization to address the issue of tensor perturbations in General Relativity. We obtain a performing algorithm to decompose tensorial quantities up to any perturbative order of the metric. The features of our code are tested by discussing some concrete computational issues in research activities related to first/higher-order gravitational waves.}
}