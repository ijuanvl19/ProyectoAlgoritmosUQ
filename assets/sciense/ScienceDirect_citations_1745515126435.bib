@article{GENT202238,
title = {Making a mind},
journal = {New Scientist},
volume = {253},
number = {3374},
pages = {38-41},
year = {2022},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(22)00294-9},
url = {https://www.sciencedirect.com/science/article/pii/S0262407922002949},
author = {Edd Gent},
abstract = {In the push to make artificial intelligence that thinks like humans, many researchers are focused on fresh insights from neuroscience. Should they be looking to psychology instead, asks Edd Gent}
}
@article{MUNUZURI202264,
title = {Unified representation of Life's basic properties by a 3-species Stochastic Cubic Autocatalytic Reaction-Diffusion system of equations},
journal = {Physics of Life Reviews},
volume = {41},
pages = {64-83},
year = {2022},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2022.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S1571064522000185},
author = {Alberto P. Muñuzuri and Juan Pérez-Mercader},
keywords = {Living systems, Turing instability, Top-down approach, Bottom-up approach, Non-linear reaction-diffusion equations, Properties of life},
abstract = {Today we can use physics to describe in great detail many of the phenomena intervening in the process of life. But no analogous unified description exists for the phenomenon of life itself. In spite of their complexity, all living creatures are out of equilibrium chemical systems sharing four fundamental properties: they (1) handle information, (2) metabolize, (3) self-reproduce and (4) evolve. This small number of features, which in terran life are implemented with biochemistry, point to an underlying simplicity that can be taken as a guide to motivate and implement a theoretical physics style unified description of life using tools from the non-equilibrium physical-chemistry of extended systems. Representing a system with general rules is a well stablished approach to model building and unification in physics, and we do this here to provide an abstract mathematical description of life. We start by reviewing the work of previous authors showing how the properties in the above list can be individually represented with stochastic reaction-diffusion kinetics using polynomial reaction terms. These include “switches” and computation, the kinetic representation of autocatalysis, Turing instability and adaptation in the presence of both deterministic and stochastic environments. Thinking of these properties as existing on a space-time lattice each of whose nodes are subject to a common mass-action kinetics compatible with the above, leads to a very rich dynamical system which, just as natural life, unifies the above properties and can therefore be interpreted as a high level or “outside-in” theoretical physics representation of life. Taking advantage of currently available advanced computational techniques and hardware, we compute the phase plane for this dynamical system both in the deterministic and stochastic cases. We do simulations and show numerically how the system works. We review how to extract useful information that can be mapped into emergent physical phenomena and attributes of importance in life such as the presence of a “membrane” or the time evolution of an individual system's negentropy or mass. Once these are available, we illustrate how to perform some basic phenomenology based on the model's numerical predictions. Applying the above to the idealization of the general Cell Division Cycle (CDC) given almost 25 years ago by Hunt and Murray, we show from the numerical simulations how this system executes a form of the idealized CDC. We also briefly discuss various simulations that show how other properties of living systems such as migration towards more favorable regions or the emergence of effective Lotka-Volterra populations are accounted for by this general and unified view from the “top” of the physics of life. The paper ends with some discussion, conclusions, and comments on some selected directions for future research. The mathematical techniques and powerful simulation tools we use are all well established and presented in a “didactical” style. We include a very rich but concise SI where the numerical details are thoroughly discussed in a way that anyone interested in studying or extending the results would be able to do so.}
}
@article{FERNANDES202191,
title = {Pruning of generative adversarial neural networks for medical imaging diagnostics with evolution strategy},
journal = {Information Sciences},
volume = {558},
pages = {91-102},
year = {2021},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.12.086},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521000189},
author = {Francisco Erivaldo Fernandes and Gary G. Yen},
keywords = {Deep Neural Networks, Convolutional Neural Networks, Generative Adversarial Networks, Medical Imaging Diagnostics, Evolution Strategy, Pruning},
abstract = {Deep Convolutional Neural Networks (DCNNs) have the potential to revolutionize the field of Medical Imaging Diagnostics due to their capabilities of learning by using only raw data. However, DCNNs can only learn when trained using thousands of data points, which is not always available when dealing with medical data. Moreover, due to patient privacy concerns and the small prevalence of certain diseases in the population, medical data often presents unbalanced classes and fewer data points than other data types. Researchers often rely on Generative Adversarial Networks (GANs) to synthesize more data from a given distribution to solve this problem. Nevertheless, GANs are computationally intensive models requiring the use of powerful hardware to run. In the present work, an algorithm for pruning GANs based on Evolution Strategy (ES) and Multi-Criteria Decision Making (MCDM) is proposed in which a model with the best trade-off between computational complexity and synthesis performance can be found without the use of any trade-off parameter. In the proposed algorithm, the model with the best trade-off is defined geometrically as the candidate solution with the minimum Manhattan distance (MMD) in a two-dimensional objective space established by the number of Floating-Point Operations (FLOPs) and the Wasserstein distance of all candidate solutions, also known as the knee solution. The results show that the pruned GAN model achieves similar performance compared with the original model with up to 70% fewer Floating-Point Operations.}
}
@article{KAPETANIOU2025106115,
title = {Beyond impulse control – toward a comprehensive neural account of future-oriented decision making},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {172},
pages = {106115},
year = {2025},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2025.106115},
url = {https://www.sciencedirect.com/science/article/pii/S0149763425001150},
author = {Georgia E. Kapetaniou and Alexander Soutschek},
keywords = {Self-control, Delay discounting, Dorsolateral prefrontal cortex, Construal theory, Prospection, Metacognition},
abstract = {The dominant focus of current neural models of future-oriented decision making is on the interplay between the brain’s reward system and a frontoparietal network thought to implement impulse control. Here, we propose a re-interpretation of the contribution of frontoparietal activation to future-oriented behavior and argue that future-oriented decisions are influenced by a variety of psychological mechanisms implemented by dissociable brain mechanisms. We review the literature on the neural mechanisms underlying the influence of prospection, retrospection, framing, metacognition, and automatization on future-oriented decisions. We propose that the prefrontal cortex contributes to future-oriented decisions not by exerting impulse control but by constructing and updating the value of abstract future rewards. These prefrontal value representations interact with regions involved in reward processing (neural reward system), prospection (hippocampus, temporal cortex), metacognition (frontopolar cortex), and habitual behavior (dorsal striatum). The proposed account of the brain mechanisms underlying future-oriented decisions has several implications for both basic and clinical research: First, by reconciling the idea of frontoparietal control processes with construal accounts of intertemporal choice, we offer an alternative interpretation of the canonical prefrontal activation during future-oriented decisions. Second, we highlight the need for obtaining a better understanding of the neural mechanisms underlying future-oriented decisions beyond impulse control and of their contribution to myopic decisions in clinical disorders. Such a widened focus may, third, stimulate the development of novel neural interventions for the treatment of pathological impulsive decision making.}
}
@article{TURNER2025100843,
title = {Meet the author: Tychele N. Turner, PhD},
journal = {Cell Genomics},
volume = {5},
number = {4},
pages = {100843},
year = {2025},
issn = {2666-979X},
doi = {https://doi.org/10.1016/j.xgen.2025.100843},
url = {https://www.sciencedirect.com/science/article/pii/S2666979X25000990},
author = {Tychele N. Turner},
abstract = {Dr. Tychele Turner is an assistant professor of genetics at the Washington University in St. Louis. This issue of Cell Genomics presents research from her lab in “Proteome-wide assessment of differential missense variant clustering in neurodevelopmental disorders and cancer” by Ng et al. This paper used their newly developed program, 3D-CLUMP, which can perform proteome-wide significant case-control analysis and clustering of protein-coding variants related to disease. Using 3D-CLUMP, they examine how patients with mostly cancer or neurodevelopmental disorders (NDDs) are observed to have the same gene, even if the specific mutations causing them are not shared.}
}
@article{KEESTRA2009531,
title = {Foundationalism and neuroscience; silence and language},
journal = {Language Sciences},
volume = {31},
number = {4},
pages = {531-552},
year = {2009},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2007.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0388000107001040},
author = {Machiel Keestra and Stephen J. Cowley},
keywords = {Distributed cognition, Foundationalism and coherentism, Language and cognition, Mereology and dualism, Neuroscience and philosophy},
abstract = {Neuroscience offers more than new empirical evidence about the details of cognitive functions such as language, perception and action. Since it also shows many functions to be highly distributed, interconnected and dependent on mechanisms at different levels of processing, it challenges concepts that are traditionally used to describe these functions. The question is how to accommodate these concepts to the recent evidence. A recent proposal, made in Philosophical Foundations of Neuroscience (2003) by Bennett and Hacker, is that concepts play a foundational role in neuroscience, that empirical research needs to presuppose them and that changing concepts is a philosophical task. In defending this perspective, PFN shows much neuroscientific writing to be dualistic in nature due to our poor grasp of its foundations. In our review article we take a different approach. Instead of foundationalism we plead for a mild coherentism, which allows for a gradual and continuous alteration of concepts in light of new evidence. Following this approach it is also easier to deal with some neurological conditions (like blindsight, synaesthesia) that pose difficulties for our concepts. Finally, although words and concepts seem to seduce us to thinking that many skills and tasks function separately, it is language skill that – as neuroscientific evidence shows – co-emerges with action/perception cycles and thus seems to require revision of some of our central concepts.}
}
@article{KROGER200886,
title = {Distinct neural substrates for deductive and mathematical processing},
journal = {Brain Research},
volume = {1243},
pages = {86-103},
year = {2008},
issn = {0006-8993},
doi = {https://doi.org/10.1016/j.brainres.2008.07.128},
url = {https://www.sciencedirect.com/science/article/pii/S000689930801929X},
author = {James K. Kroger and Leigh E. Nystrom and Jonathan D. Cohen and Philip N. Johnson-Laird},
keywords = {Logic, Reasoning, Math, Cortex, fMRI, Frontal pole},
abstract = {In an effort to clarify how deductive reasoning is accomplished, an fMRI study was performed to observe the neural substrates of logical reasoning and mathematical calculation. Participants viewed a problem statement and three premises, and then either a conclusion or a mathematical formula. They had to indicate whether the conclusion followed from the premises, or to solve the mathematical formula. Language areas of the brain (Broca's and Wernicke's area) responded as the premises and the conclusion were read, but solution of the problems was then carried out by non-language areas. Regions in right prefrontal cortex and inferior parietal lobe were more active for reasoning than for calculation, whereas regions in left prefrontal cortex and superior parietal lobe were more active for calculation than for reasoning. In reasoning, only those problems calling for a search for counterexamples to conclusions recruited right frontal pole. These results have important implications for understanding how higher cognition, including deduction, is implemented in the brain. Different sorts of thinking recruit separate neural substrates, and logical reasoning goes beyond linguistic regions of the brain.}
}
@article{RODRIGUEZPLANAS2022429,
title = {Gender norms in high school: Impacts on risky behaviors from adolescence to adulthood},
journal = {Journal of Economic Behavior & Organization},
volume = {196},
pages = {429-456},
year = {2022},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2022.01.015},
url = {https://www.sciencedirect.com/science/article/pii/S016726812200021X},
author = {Nuria Rodríguez-Planas and Anna Sanz-de-Galdeano and Anastasia Terskaya},
keywords = {Gender norms, short-, medium- and long-run effects, risky behaviors and labor market outcomes, Add health},
abstract = {Engagement in risky behaviors is traditionally more prevalent among males than females, and the gap increases as youths move from adolescence to adulthood. Using the National Longitudinal Study of Adolescent to Adult Health, we identify a causal effect of exposure to high-school grade-mates with mothers who think that important skills for both boys and girls to possess are traditionally masculine ones (such as to think for oneself or work hard) as opposed to traditionally feminine ones (namely, to be well-behaved, popular, or help others) on the gender gap in teenagers’ engagement in risky behaviors. We find that a higher proportion of grade-mates’ mothers with non-traditional or non-stereotypical gender views who believe that independent thinking and working hard matter for either gender is associated with a reduction of the gender gap in risky behaviors both in the short and medium run. These results are driven by males curbing risky behaviors, suggesting that the relaxation of gender stereotypes results in boys behaving “more like girls”. In the long run, being exposed to grade-mates whose mothers have non-stereotypical gender beliefs reduces the gender gap in labor market outcomes by improving women's performance. This evidence, together with our exploration of several potential mechanisms, suggests that the transmission of gender norms is driving our results.}
}
@article{WU2023101906,
title = {Human–machine hybrid intelligence for the generation of car frontal forms},
journal = {Advanced Engineering Informatics},
volume = {55},
pages = {101906},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.101906},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623000344},
author = {Yu Wu and Lisha Ma and Xiaofang Yuan and Qingnan Li},
keywords = {Car frontal form, Creative generation, Human–machine hybrid intelligence, Human–machine shared knowledge base, generative adversarial network (GAN)},
abstract = {With the acceleration of the upgrading of the automobile consumption market, artificial intelligence has become an increasingly effective means of enhancing the creative design of automobile appearance modeling. However, when artificial intelligence processes specific design tasks, creativity is primarily based on data drive, resulting in machine-generated design schemes that do not match human-specific psychological intentions. Due to the absence of design knowledge in the process of machine design, there is a data gap between human cognitive thought and machine information processing. This paper aims to structure the human's complex cognitive knowledge of car frontal form, establish the consistency between human and machine cognitive structures, and reduce communication barriers in the process of human–machine hybrid creative design. To achieve this objective, a human–machine hybrid intelligence methodology – a combination of human cognitive mental model, human–machine shared knowledge base, and Generative Adversarial Networks (GAN) – was developed to generate a large number of car frontal forms that are consistent with the design intent. First, we constructeda mental model of human cognition based on three dimensions: design intent, drawing behavior, and functional structure. Second, we created a shared human–machine knowledge base with design Knowledge. This knowledge base contains 12,560 images of car frontal form designs with corresponding morphological semantic labels and 3,140 sketches of car frontal forms drawn by hand. Human–machine shared knowledge base datawasutilized in a machine learning training network. In addition, a conditional cross-domain generative adversarial network was developed to investigate the implicit relationship between sketch characteristics, morphological semantics, and image visual effects. Using the suggested method, a large number of images with the specified morphological semantic category and resembling the hand-drawn sketch of a car frontal form can be generated rapidly. In terms of the quality of car frontal form generation, our research is superior to the baseline model according to qualitative and quantitative assessments. In comparison to the designer's output, the human–machine hybrid intelligent generation also demonstrates excellent creative performance.}
}
@article{BAUERNEGRINI2022105785,
title = {Usability evaluation of circRNA identification tools: Development of a heuristic-based framework and analysis},
journal = {Computers in Biology and Medicine},
volume = {147},
pages = {105785},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105785},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522005522},
author = {Guilherme Bauer-Negrini and Guilherme {Cordenonsi da Fonseca} and Carmem Gottfried and Juliana Herbert},
keywords = {Usability, Bioinformatics, circRNA, Heuristic evaluation, Command-line interface},
abstract = {Background and objective
Circular RNAs (circRNAs) are endogenous molecules of non-coding RNA that form a covalently closed loop at the 3′ and 5′ ends. Recently, the role of these molecules in the regulation of gene expression and their involvement in several human pathologies has gained notoriety. The identification of circRNAs is highly dependent on computational methods for analyzing RNA sequencing data. However, bioinformatics software is known to be problematic in terms of usability. Evidence points out that tools for identifying circRNAs can have such problems, negatively impacting researchers in this field. Here we present a heuristic-based framework for evaluating the usability of command-line circRNA identification software.
Methods
We used heuristics evaluation to comprehensively identify the usability issues in a sample of circRNA identification tools.
Results
We identified 46 usability issues presented individually in four tools. Most of the issues had cosmetic or minor severity. These are unlikely to challenge experienced users but may cause inconvenience for novice users. We also identified severe issues with the potential to harm users regardless of their experience. The areas most affected were the documentation and the installability of the tools.
Conclusions
With the proposed framework, we formally describe, for the first time, the usability problems that can affect users in this area of circRNA research. We hope that our framework can help researchers evaluate their software's usability during development.}
}
@article{TILLS2025111783,
title = {Bioimaging and the future of whole-organismal developmental physiology},
journal = {Comparative Biochemistry and Physiology Part A: Molecular & Integrative Physiology},
volume = {300},
pages = {111783},
year = {2025},
issn = {1095-6433},
doi = {https://doi.org/10.1016/j.cbpa.2024.111783},
url = {https://www.sciencedirect.com/science/article/pii/S1095643324002101},
author = {Oliver Tills and Ziad Ibbini and John I. Spicer},
keywords = {Imaging, Phenomics, Deep learning, Embryonic development, Developmental physiology, Comparative developmental physiology, Computer vision},
abstract = {While omics has transformed the study of biology, concomitant advances made at the level of the whole organism, i.e. the phenome, have arguably not kept pace with lower levels of biological organisation. In this personal commentary we evaluate the importance of imaging as a means of measuring whole organismal developmental physiology. Image acquisition, while an important process itself, has become secondary to image analysis as a bottleneck to the use of imaging in research. Here, we explore the significant potential for increasingly sophisticated approaches to image analysis, including deep learning, to advance our understanding of how developing animals grow and function. Furthermore, unlike many species-specific methodologies, tools and technologies, we explore how computer vision has the potential to be transferable between species, life stages, experiments and even taxa in which embryonic development can be imaged. We identify what we consider are six of the key challenges and opportunities in the application of computer vision to developmental physiology carried out in our lab, and more generally. We reflect on the tangibility of transferrable computer vision models capable of measuring the integrative physiology of a broad range of developing organisms, and thereby driving the adoption of phenomics for developmental physiology. We are at an exciting time of witnessing the move from computer vision as a replacement for manual observation, or manual image analysis, to it enabling a fundamentally more powerful approach to exploring and understanding the complex biology of developing organisms, the quantification of which has long posed a challenge to researchers.}
}
@article{HAVENS2020104571,
title = {Automated Water Supply Model (AWSM): Streamlining and standardizing application of a physically based snow model for water resources and reproducible science},
journal = {Computers & Geosciences},
volume = {144},
pages = {104571},
year = {2020},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2020.104571},
url = {https://www.sciencedirect.com/science/article/pii/S0098300420305598},
author = {Scott Havens and Danny Marks and Micah Sandusky and Andrew Hedrick and Micah Johnson and Mark Robertson and Ernesto Trujillo},
keywords = {Hydrology, Computational method, Software engineering, Data assimilation},
abstract = {Reproducible science requires a shift in thinking and application for how data, code and analysis are shared. Now, scientists must act more like software engineers to design models and perform analysis that use principles and techniques pioneered by software developers. Creating reproducible models that are easy to use and understand is in the best interest for the snow and hydrology community, enabling studies by other researchers and facilitating technology transfer to operational applications. Here, we present the Automated Water Supply Model (AWSM) that streamlines and standardizes the workflow of a physically based snow model to create fully reproducible model simulations that can be utilized by researchers and operational water resource managers. AWSM orchestrates four core components that historically required significant, ad-hoc modeler interaction to load the input data, spatially interpolate to the modeling domain, run the models and process the outputs. Because AWSM was developed using principles and techniques from software engineering, users can quickly perform reproducible simulations on any operating system, from a laptop to the cloud. The three fully reproducible example case studies showcase the simplicity and flexibility of using AWSM to perform simulations from small research catchments to simulations that aid in real time water management decisions.}
}
@article{VEKONY2025111703,
title = {Mind wandering enhances statistical learning},
journal = {iScience},
volume = {28},
number = {2},
pages = {111703},
year = {2025},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2024.111703},
url = {https://www.sciencedirect.com/science/article/pii/S2589004224029304},
author = {Teodóra Vékony and Bence C. Farkas and Bianka Brezóczki and Matthias Mittner and Gábor Csifcsák and Péter Simor and Dezső Németh},
keywords = {Psychology},
abstract = {Summary
The human brain spends 30–50% of its waking hours engaged in mind-wandering (MW), a common phenomenon in which individuals either spontaneously or deliberately shift their attention away from external tasks to task-unrelated internal thoughts. Despite the significant amount of time dedicated to MW, its underlying reasons remain unexplained. Our pre-registered study investigates the potential adaptive aspects of MW, particularly its role in predictive processes measured by statistical learning. We simultaneously assessed visuomotor task performance as well as the capability to extract probabilistic information from the environment while assessing task focus (on-task vs. MW). We found that MW was associated with enhanced extraction of hidden, but predictable patterns. This finding suggests that MW may have functional relevance in human cognition by shaping behavior and predictive processes. Overall, our results highlight the importance of considering the adaptive aspects of MW, and its potential to enhance certain fundamental cognitive abilities.}
}
@article{ROLAND2002183,
title = {Dynamic depolarization fields in the cerebral cortex},
journal = {Trends in Neurosciences},
volume = {25},
number = {4},
pages = {183-190},
year = {2002},
issn = {0166-2236},
doi = {https://doi.org/10.1016/S0166-2236(00)02125-1},
url = {https://www.sciencedirect.com/science/article/pii/S0166223600021251},
author = {Per E. Roland},
keywords = {general computational elements, voltage sensitive dyes, cortical dynamics, layer II-III neurons, memory, cognition},
abstract = {Recent physiological evidence shows that in response to stimuli and preceding motor activity, large fields of the upper layers of the cerebral cortex depolarize. It is argued that this finding is a general one and that these dynamic depolarization fields represent the computational elements of the cerebral cortex. Each depolarization field engages many more neurons than do columns and hyper-columns. These fields can be explained by cooperative neuronal computing in layers I–III of the cortex. In these layers, the computing modes might be general for all parts of the cerebral cortex and be sufficiently flexible to handle all sorts of cortical computations, including perception, memory storage, memory retrieval, thought and the production of behavior.}
}
@article{URKEN2012553,
title = {Designing evolvable systems in a framework of robust, resilient and sustainable engineering analysis},
journal = {Advanced Engineering Informatics},
volume = {26},
number = {3},
pages = {553-562},
year = {2012},
note = {Evolvability of Complex Systems},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2012.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S1474034612000535},
author = {Arnold B. Urken and Arthur {“Buck” Nimz} and Tod M. Schuck},
keywords = {Error Resilient Data Fusion (ERDF), Artificial systems, Cognitive radio, Electrical grids, Reflexive behaviors, System dynamics},
abstract = {“Evolvability” is a concept normally associated with biology or ecology, but recent work on control of interdependent critical infrastructures reveals that network informatics systems can be designed to enable artificial, human systems to “evolve”. To explicate this finding, we draw on an analogy between disruptive behavior and stable variation in the history of science and the adaptive patterns of robustness and resilience in engineered systems. We present a definition of an evolvable system in the context of a model of robust, resilient and sustainable systems. Our review of this context and standard definitions indicates that many analysts in engineering (as well as in biology and ecology) do not differentiate Resilience from Robustness. Neither do they differentiate overall dependable system adaptability from a multi-phase process that includes graceful degradation and time-constrained recovery, restabilization, and prevention of catastrophic failure. We analyze how systemic Robustness, Resilience, and Sustainability are related to Evolvability. Our analysis emphasizes the importance of Resilience as an adaptive capability that integrates Sustainability and Robustness to achieve Evolvability. This conceptual framework is used to discuss nine engineering principles that should frame systems thinking about developing evolvable systems. These principles are derived from Kevin Kelly’s book: Out of Control, which describes living and artificial self-sustaining systems. Kelly’s last chapter, “The Nine Laws of God,” distills nine principles that govern all life-like systems. We discuss how these principles could be applied to engineering evolvability in artificial systems. This discussion is motivated by a wide range of practical problems in engineered artificial systems. Our goal is to analyze a few examples of system designs across engineering disciplines to explicate a common framework for designing and testing artificial systems. This framework highlights managing increasing complexity, intentional evolution, and resistance to disruptive events. From this perspective, we envision a more imaginative and time-sensitive appreciation of the evolution and operation of “reliable” artificial systems. We conclude with a short discussion of two hypothetical examples of engineering evolvable systems in network-centric communications using Error Resilient Data Fusion (ERDF) and cognitive radio.}
}
@article{COMPANY2016108,
title = {A mixed derivative terms removing method in multi-asset option pricing problems},
journal = {Applied Mathematics Letters},
volume = {60},
pages = {108-114},
year = {2016},
issn = {0893-9659},
doi = {https://doi.org/10.1016/j.aml.2016.04.011},
url = {https://www.sciencedirect.com/science/article/pii/S0893965916301252},
author = {R. Company and V.N. Egorova and L. Jódar and F. Soleymani},
keywords = {Multiasset option pricing, Multidimensional partial differential equations, Mixed derivative terms,  factorization, Bunch–Kaufman factorization},
abstract = {The challenge of removing the mixed derivative terms of a second order multidimensional partial differential equation is addressed in this paper. The proposed method, which is based on proper algebraic factorization of the so-called diffusion matrix, depends on the semidefinite or indefinite character of this matrix. Computational cost of the transformed equation is considerably reduced and well-known numerical drawbacks are avoided.}
}
@article{FIORELLI2025150,
title = {Digital models and 3D biomechanics analysis in orthodontics. Part 1: Vector calculations},
journal = {Seminars in Orthodontics},
volume = {31},
number = {1},
pages = {150-157},
year = {2025},
issn = {1073-8746},
doi = {https://doi.org/10.1053/j.sodo.2024.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S1073874624001348},
author = {Giorgio Fiorelli},
keywords = {Biomechanics, Vectors, 3D Force System},
abstract = {Biomechanics is essential for optimizing orthodontic appliances and controlling dental movement. Charles J. Burstone pioneered a three-dimensional (3D) approach in orthodontics, advocating for a shift beyond appliance-focused methods. Initially, biomechanics studies were constrained to two-dimensional (2D) analysis due to the complexities of 3D evaluation. Despite progress in computational tools and digital modeling, orthodontic biomechanics has largely maintained a 2D orientation. This paper advances orthodontic biomechanics into 3D, re-evaluating concepts previously limited to 2D frameworks. A dedicated software, DDP-Ortho (Ortolab, Poland), is introduced to enable orthodontists to analyze and resolve biomechanical challenges in 3D, facilitating appliance designs with precise 3D force systems. The representation and calculation of force vectors and moments in 3D are detailed, emphasizing the inherent complexity absent computational support. Key processes such as vector subtraction and addition, fundamental for assessing and refining orthodontic force systems, are explained. Additionally, the vector split (couple replacement) method, previously described in 2D, is extended to 3D, addressing the unique constraints and challenges of this approach. These tools promise to refine the accuracy and effectiveness of orthodontic treatments, setting the stage to examine the interactions between 3D force systems and dental movement, which will be addressed in a subsequent paper, to broaden the potential of contemporary orthodontic therapy.}
}
@article{BUAH2021103269,
title = {Augmenting the communication and engagement toolkit for CO2 capture and storage projects},
journal = {International Journal of Greenhouse Gas Control},
volume = {107},
pages = {103269},
year = {2021},
issn = {1750-5836},
doi = {https://doi.org/10.1016/j.ijggc.2021.103269},
url = {https://www.sciencedirect.com/science/article/pii/S1750583621000219},
author = {Eric Buah and Lassi Linnanen and Huapeng Wu},
keywords = {Artificial intelligence, CO capture and storage, CCS communication and engagement, CCS toolkit, CCS SWOT, Deep neural network algorithm, Fuzzy logic, Fuzzy deep learning},
abstract = {This paper revisits the Communication and Engagement Toolkit for CO2 Capture and Storage (CCS) projects proposed by Ashworth and colleagues in collaboration with the Global CCS Institute. The paper proposes a new method for understanding the social context where CCS will be deployed based on the toolkit. In practice, the proposed method can be used to harness social data collected on the CCS project. The outcome of this application is a development of a predictive tool for gaining insight into the future, to guide strategic decisions that may enhance deployment. Methodologically, the proposed predictive tool is an artificial intelligence (AI) tool. It uses fuzzy deep neural network to develop computational ability to reason about the social behavior. The hybridization of fuzzy logic and deep neural network algorithms make the predictive tool an explainable AI system. It means that the prediction of the algorithm is interpretable using fuzzy logical rules. The practical feasibility of the proposed system has been demonstrated using an experimental sample of 198 volunteers. Their perceptions, emotions and sentiments were tested using a standard questionnaire from the literature, on a hypothetical CCS project based on 26 predictors. The generalizability of the algorithm to predict future reactions was tested on, 84 out-of-sample respondents. In the simulation experiment, we observed an approximately 90 % performance. This performance was measured when the algorithm's predictions were compared to the self- reported reactions of the out of sample subjects. The implication of the proposed tool to enhance the predictive power of the conventional CCS Communication and Engagement tool is discussed © 2020 xx. Hosting by Elsevier B.V. All rights reserved.}
}
@incollection{BERGER20234,
title = {3.02 - Electronic structure of oxide and halide perovskites},
editor = {Jan Reedijk and Kenneth R. Poeppelmeier},
booktitle = {Comprehensive Inorganic Chemistry III (Third Edition)},
publisher = {Elsevier},
edition = {Third Edition},
address = {Oxford},
pages = {4-25},
year = {2023},
isbn = {978-0-12-823153-1},
doi = {https://doi.org/10.1016/B978-0-12-823144-9.00102-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128231449001023},
author = {Robert F. Berger},
keywords = {Band structure, Density functional theory, Perovskite, Photocatalyst, Photovoltaic},
abstract = {Compounds crystallizing in the ABX3 perovskite structure are studied for a remarkable variety of technologies. Particularly for applications such as photovoltaics and photocatalysis, it is crucial to understand the key features of perovskite electronic structure and how they can be tuned by modifying the composition and crystal structure. This chapter begins with an overview of the compositional and structural diversity of perovskites. Then, density functional theory-based computational methods that have been used to study perovskite compounds are described. Next, the electronic band structures of an undistorted oxide (SrTiO3) and halide (CsPbI3) perovskite are explained in detail, merging the viewpoints of crystal wavefunctions as both linear combinations of atomic orbitals and perturbed plane waves. Finally, routes toward the tunability of perovskite electronic structure and properties are reviewed for various modifications: changes in elemental composition, various modes of geometric distortion, the application of high pressure or strain, and the formation of superstructures with reduced dimensionality. While the concepts and discussion herein are relevant to all perovskite compounds, the examples described in this chapter are mainly d0 oxide perovskite photocatalysts and halide perovskite photovoltaics.}
}
@article{SIMEONOV2017193,
title = {Some resonances between Eastern thought and Integral Biomathics in the framework of the WLIMES formalism for modeling living systems},
journal = {Progress in Biophysics and Molecular Biology},
volume = {131},
pages = {193-212},
year = {2017},
note = {Integral Biomathics 2017: The Necessary Conjunction of Western and Eastern Thought Traditions for Exploring the Nature of Mind and Life},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2017.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S0079610717301141},
author = {Plamen L. Simeonov and Andrée C. Ehresmann},
keywords = {Integral Biomathics, Artificial/synthetic and natural life, Phenomenology, Eastern philosophy, Higher-order logic, Wandering Logic Intelligence, Memory Evolutive Systems},
abstract = {Forty-two years ago, Capra published “The Tao of Physics” (Capra, 1975). In this book (page 17) he writes: “The exploration of the atomic and subatomic world in the twentieth century has …. necessitated a radical revision of many of our basic concepts” and that, unlike ‘classical’ physics, the sub-atomic and quantum “modern physics” shows resonances with Eastern thoughts and “leads us to a view of the world which is very similar to the views held by mystics of all ages and traditions.“ This article stresses an analogous situation in biology with respect to a new theoretical approach for studying living systems, Integral Biomathics (IB), which also exhibits some resonances with Eastern thought. Stepping on earlier research in cybernetics1 and theoretical biology,2 IB has been developed since 2011 by over 100 scientists from a number of disciplines who have been exploring a substantial set of theoretical frameworks. From that effort, the need for a robust core model utilizing advanced mathematics and computation adequate for understanding the behavior of organisms as dynamic wholes was identified. At this end, the authors of this article have proposed WLIMES (Ehresmann and Simeonov, 2012), a formal theory for modeling living systems integrating both the Memory Evolutive Systems (Ehresmann and Vanbremeersch, 2007) and the Wandering Logic Intelligence (Simeonov, 2002b). Its principles will be recalled here with respect to their resonances to Eastern thought.}
}
@incollection{RAHI2025597,
title = {Chapter 56 - Crowdsourcing and artificial intelligence based modeling framework for effective Public Healthcare Informatics and Smart eHealth System},
editor = {M.A. Ansari and R.S. Anand and Pragati Tripathi and Rajat Mehrotra and Md Belal Bin Heyat},
booktitle = {Artificial Intelligence in Biomedical and Modern Healthcare Informatics},
publisher = {Academic Press},
pages = {597-608},
year = {2025},
isbn = {978-0-443-21870-5},
doi = {https://doi.org/10.1016/B978-0-443-21870-5.00056-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044321870500056X},
author = {Pankaj Rahi and Monika Dandotiya and Santi Basa and Souvik Sen and Mayur D. Jakhete and P. Vijayakumar},
keywords = {Artificial intelligence, Crowdsourcing, IoT, Machine learning, Public health informatics, Smart eHealth system},
abstract = {With the help of a multiagent interactive healthcare plan, healthy and independent aging is possible. Testing one's fitness and keeping tabs on one's health can both benefit from an awareness of one's regular routine. The Healthcare Strategies Partnership is introduced here. Geographic and economic factors, including the prevalence of infectious tropical diseases and an increasing number of chronic illnesses, have contributed to a shift in the region's medical requirements. This takes place in a world that is not only difficult but also intricate. This system employs a smartphone's sensor, a machine learning algorithm, multiple agents (including a doctor, fitness instructor, guardian, and intelligent ranker agent), and a smartphone itself to increase a user's sense of independence. A group of health professionals collaborate to assess the patient's day-to-day activities and offer suggestions for improvement. The algorithm figures out a typical day in the life of an adult. A smart autonomous agent using crowdsourced data recommends the best possible treatment plan. In contrast to crowdsourcing, which places value on the abilities of people to generate, aggregate, or filter original data, automatic tools make use of information retrieval techniques to analyze publicly available information. Crowdsourcing, which facilitates collaboration among numerous individuals, is increasingly being used in a variety of industries. Methodical crowdsourcing of useful data and human computation of interchangeable knowledge will aid future advancements in public health informatics. The disease burden on any country can and will be decreased by these efforts the share of healthcare costs borne by individuals and their families. The work also aids in achieving the most crucial objectives along the path to the Sustainable Development Goals. To improve public health surveillance for the prevention and control of communicable and noncommunicable diseases, this chapter presents the fundamental modeling for its design and development. So that a smart autonomous agent can recommend the best course of treatment, lowering the disease burden in any country. Crowdsourcing is defined, and how it can be used to better public health, in this chapter of a book. They are better able to achieve a healthy lifestyle exit, and the statistics and indicators that are tracked primarily under the Sustainable Development Goals Framework are improved as a result. The goal of this chapter is to provide a high-level overview of recent developments in applying artificial intelligence techniques to public health surveillance and response, which is the single most important step toward preventing the spread of disease and saving the lives of humans and other sentient beings.}
}
@article{VELICHKOVSKY2020547,
title = {New Insights into the Human Brain’s Cognitive Organization: Views from the Top, from the Bottom, from the Left and, particularly, from the Right},
journal = {Procedia Computer Science},
volume = {169},
pages = {547-557},
year = {2020},
note = {Postproceedings of the 10th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2019 (Tenth Annual Meeting of the BICA Society), held August 15-19, 2019 in Seattle, Washington, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.211},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920303343},
author = {Boris Velichkovsky and Artem Nedoluzhko and Elkhonon Goldberg and Olga Efimova and Fedor Sharko and Sergey Rastorguev and Anna Krasivskaya and Maxim Sharaev and Anastasia Korosteleva and Vadim Ushakov},
keywords = {fMRI, effective connectivity, levels of cognitive organization, dynamic causal modeling (DCM), protein-coding genes, microRNA, gene expression, modes of attention, hemispheric lateralization},
abstract = {The view that the left cerebral hemisphere in humans “dominates” over the “subdominant” right hemisphere has been so deeply entrenched in neuropsychology that no amount of evidence seems able to overcome it. In this article, we examine inhibitory cause-and-effect connectivity among human brain structures related to different parts of the triune evolutionary stratification —archicortex, paleocortex and neocortex— in relation to early and late phases of a prolonged resting-state functional magnetic resonance imaging (fMRI) experiment. With respect to the evolutionarily youngest parts of the human cortex, the left and right frontopolar regions, we also provide data on the asymmetries in underlying molecular mechanisms, namely on the differential expression of the protein-coding genes and regulatory microRNA sequences. In both domains of research, our results contradict the established view by demonstrating a pronounced right-to-left vector of causation in the hemispheric interaction at multiple levels of brain organization. There may be several not mutually exclusive explanations for the evolutionary significance of this pattern of lateralization. One of the explanations emphasizes the computational advantage of separating the neural substrates for processing novel information ("exploration") mediated predominantly by the right hemisphere, and processing with reliance on established cognitive routines and representations ("exploitation") mediated predominantly by the left hemisphere.}
}
@article{YOUSIF20241342,
title = {Safety 4.0: Harnessing computer vision for advanced industrial protection},
journal = {Manufacturing Letters},
volume = {41},
pages = {1342-1356},
year = {2024},
note = {52nd SME North American Manufacturing Research Conference (NAMRC 52)},
issn = {2213-8463},
doi = {https://doi.org/10.1016/j.mfglet.2024.09.161},
url = {https://www.sciencedirect.com/science/article/pii/S2213846324002451},
author = {Ibrahim Yousif and Jad Samaha and JuHyeong Ryu and Ramy Harik},
keywords = {Smart Manufacturing, Safety 4.0, Computer Vision, Industrial Protection, Musculoskeletal disorders (MSD), Effective Functional Training},
abstract = {In the pursuit of enhanced productivity, reduced costs, and minimized lead times, manufacturers are transitioning from traditional systems to autonomous systems. This shift, driven by the emergence of smart manufacturing and technological advancements such as robotics, collaborative robots (Cobots), automation, and digitalization, necessitates a parallel evolution in safety protocols—termed Safety 4.0—to mitigate the risks associated with such dynamic environments. The integration of smart technologies within manufacturing significantly transforms traditional workflows and intensifies the need for comprehensive safety training and guidelines. Innovations like smart personal protective equipment (PPE) and wearable sensors are pivotal in this transition, yet they often prove financially burdensome for manufacturers due to high costs and the scale of workforce deployment. Moreover, the effective use of these technologies requires continuous monitoring and data analysis, further straining resources. To address these challenges, this paper proposes the adoption of computer vision technology to enhance safety measures within manufacturing facilities, focusing on human and PPE detection. It details a holistic methodology encompassing data collection, preprocessing, training, and execution. The discussion extends to the implementation framework of this technology, emphasizing its role in enabling autonomous decision-making—a crucial step beyond mere detection. Furthermore, the paper explores the utilization of the accumulated data to develop immersive training modules employing Mixed Reality, thereby reinforcing safety protocols and fostering an environment of continuous learning and adaptation. This approach not only contributes to safeguarding personnel but also aligns with the financial and reputational interests of forward-thinking manufacturers.}
}
@article{WANG2023e131,
title = {Interbody Fusion Cage Design Driven by Topology Optimization},
journal = {World Neurosurgery},
volume = {174},
pages = {e131-e143},
year = {2023},
issn = {1878-8750},
doi = {https://doi.org/10.1016/j.wneu.2023.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S1878875023003042},
author = {Zuowei Wang and Jun Jiang and Fengzeng Jian and Zan Chen and Xingwen Wang and Wanru Duan and Weisheng Zhang},
keywords = {Fusion cage design, Interbody fusion, Moving morphable void approach, Topology optimization},
abstract = {Objective
We used topology optimization technology to explore the new theory and method of interbody fusion cage design and realized an innovative design of interbody cages.
Methods
The lumbar spine of a normal healthy volunteer was scanned to perform reverse modeling. Based on the scan data for the L1-L2 segments of the lumbar spine, a three dimensional model was reconstructed to obtain the complete simulation model of the L1-L2 segment. The boundary inversion method was used to obtain approximately isotropic material parameters that can effectively characterize the mechanical behavior of vertebrae, thereby reducing the computational complexity. The topology description function was used to model the clinically used traditional fusion cage to obtain Cage A. The moving morphable void-based topology optimization method was used for the integrated design of size, shape, and topology to obtain the optimized fusion cage, Cage B.
Results
The volume fraction of the bone graft window in Cage B was 74.02%, which was 60.67% higher than that (46.07%) in Cage A. Additionally, the structural strain energy in the design domain of Cage B was 1.48 mJ, which was lower than that of Cage A (satisfying the constraints). The maximum stress in the design domain of Cage B was 5.336 Mpa, which was 35.6% lower than that (8.286 Mpa) of Cage A. In addition, the surface stress distribution of Cage B was more uniform than that of Cage A.
Conclusions
This study proposed a new innovative design method for interbody fusion cages, which not only provides new insights into the innovative design of interbody fusion cages but may also guide the customized design of interbody fusion cages in different pathological environments.}
}
@article{DANILOV2019108891,
title = {On the geometric origin of spurious waves in finite-volume discretizations of shallow water equations on triangular meshes},
journal = {Journal of Computational Physics},
volume = {398},
pages = {108891},
year = {2019},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2019.108891},
url = {https://www.sciencedirect.com/science/article/pii/S0021999119305893},
author = {S. Danilov and A. Kutsenko},
keywords = {Triangular meshes, Finite volume discretization, Computational dispersion branches},
abstract = {Computational wave branches are common to linearized shallow water equations discretized on triangular meshes. It is demonstrated that for standard finite-volume discretizations these branches can be traced back to the structure of the unit cell of triangular lattice, which includes two triangles with a common edge. Only subsets of similarly oriented triangles or edges possess the translational symmetry of unit cell. As a consequence, discrete degrees of freedom placed on triangles or edges are geometrically different, creating an internal structure inside unit cells. It implies a possibility of oscillations inside unit cells seen as computational branches in the framework of linearized shallow water equations, or as grid-scale noise generally. Adding dissipative operators based on smallest stencils to discretized equations is needed to control these oscillations in solutions. A review of several finite-volume discretization is presented with focus on computational branches and dissipative operators.}
}
@article{PATTON20051082,
title = {The role of scanning in open intelligence systems},
journal = {Technological Forecasting and Social Change},
volume = {72},
number = {9},
pages = {1082-1093},
year = {2005},
note = {New Horizons and Challenges for Future-Oriented Technology Analysis: The 2004 EU-US Seminar},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2004.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0040162504001441},
author = {Kermit M. Patton},
keywords = {Scanning process, Open intelligence systems, SRIC-BI},
abstract = {Every month, SRI Consulting Business Intelligence (SRIC-BI) professionals assemble more than 100 short abstracts of developments that they perceive to be signals of change, discontinuities, inflection points, outliers, or disruptive developments. The effort is part of a continuous scanning process and Scan program that allows SRIC-BI to gauge the ongoing turbulent confluence of culture, commerce, and technology that defines today's business environment. For more than 25 years, scanning has played an essential role in SRIC-BI's and SRI International's foresight capabilities by providing a systematic means for surveying the broad external environment for change vectors. Traditional monitoring processes in most organizations are largely arbitrary, depending on what concerned individuals or leaders in the organization are reading, thinking about, and sharing informally with each other. But in today's world, arbitrary is insufficient. No foresight function can operate with confidence without a disciplined process for spotting new patterns of change and bringing those issues into the organization for early consideration and action. This article describes the scanning process as SRIC-BI practices it, the importance of open intelligence systems, what benefits the scanning process can provide to organizations, and what problems organizations typically run into when setting up scanning systems.}
}
@article{FAVERO2023112755,
title = {Analysis of subjective thermal comfort data: A statistical point of view},
journal = {Energy and Buildings},
volume = {281},
pages = {112755},
year = {2023},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2022.112755},
url = {https://www.sciencedirect.com/science/article/pii/S0378778822009264},
author = {Matteo Favero and Antonio Luparelli and Salvatore Carlucci},
keywords = {Subjective thermal comfort data, Rating scales, Level of measurement, Ordinal regression, Bayesian analysis, Statistical thinking},
abstract = {Thermal comfort research aims to determine the relationship between the thermal environment and the human sense of warmth. This is usually achieved by measuring the subjective human thermal response to different thermal environments. However, it is common practice to use simple linear regression to analyse data collected using ordinal scales. This practice may lead to severe errors in inference. This study first set the methodological foundations to analyse subjective thermal comfort data from a statistical perspective. Subsequently, we show the practical consequences of fallacious assumptions by utilising a Bayesian approach and show, through an illustrative example, that a linear regression model applied to ordinal data suggests results different from those obtained using ordinal regression. Specifically, linear regression found no difference in means and effect size between genders, while the ordinal regression model led to the opposite conclusion. In addition, the linear regression model distorts the estimated regression coefficient for air temperature compared to the ordinal model. Finally, the ordinal model shows that the distance between adjacent response categories of the ASHRAE 7-point thermal sensation scale is not equidistant. Given the abovementioned issues, we advocate utilising ordinal models instead of metric models to analyse ordinal data.}
}
@article{DUMONTHEIL20101574,
title = {Taking perspective into account in a communicative task},
journal = {NeuroImage},
volume = {52},
number = {4},
pages = {1574-1583},
year = {2010},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2010.05.056},
url = {https://www.sciencedirect.com/science/article/pii/S1053811910007895},
author = {Iroise Dumontheil and Olivia Küster and Ian A. Apperly and Sarah-Jayne Blakemore},
keywords = {Theory of mind, Perspective taking, Social brain, Social cognition, Decision making, Inhibition},
abstract = {Previous neuroimaging studies of spatial perspective taking have tended not to activate the brain's mentalising network. We predicted that a task that requires the use of perspective taking in a communicative context would lead to the activation of mentalising regions. In the current task, participants followed auditory instructions to move objects in a set of shelves. A 2×2 factorial design was employed. In the Director factor, two directors (one female and one male) either stood behind or next to the shelves, or were replaced by symbolic cues. In the Object factor, participants needed to use the cues (position of the directors or symbolic cues) to select one of three possible objects, or only one object could be selected. Mere presence of the Directors was associated with activity in the superior dorsal medial prefrontal cortex (MPFC) and the superior/middle temporal sulci, extending into the extrastriate body area and the posterior superior temporal sulcus (pSTS), regions previously found to be responsive to human bodies and faces respectively. The interaction between the Director and Object factors, which requires participants to take into account the perspective of the director, led to additional recruitment of the superior dorsal MPFC, a region activated when thinking about dissimilar others' mental states, and the middle temporal gyri, extending into the left temporal pole. Our results show that using perspective taking in a communicative context, which requires participants to think not only about what the other person sees but also about his/her intentions, leads to the recruitment of superior dorsal MPFC and parts of the social brain network.}
}
@incollection{CHRISTAKOS2004661,
title = {The cognitive basis of physical modelling},
editor = {Cass T. Miller and Matthew W. Farthing and William G. Gray and George F. Pinder},
series = {Developments in Water Science},
publisher = {Elsevier},
volume = {55},
pages = {661-669},
year = {2004},
booktitle = {Computational Methods in Water Resources: Volume 1},
issn = {0167-5648},
doi = {https://doi.org/10.1016/S0167-5648(04)80089-3},
url = {https://www.sciencedirect.com/science/article/pii/S0167564804800893},
author = {G. Christakos},
abstract = {We revisit the meaning of the term “solution” with regards to a physical model representing a natural system. We suggest that a (non-conventional) epistemic cognition solution (assuming that the model describes incomplete knowledge about nature, and focusing on conceptual mechanisms of scientific thinking) can lead to more realistic results than a (conventional) ontologic solution (assuming that the model describes nature as is, and focusing on form manipulations).}
}
@incollection{MOTTAMONTESERRAT202189,
title = {Chapter 5 - Computer language and linguistics},
editor = {Dioneia {Motta Monte-Serrat} and Carlo Cattani},
booktitle = {The Natural Language for Artificial Intelligence},
publisher = {Academic Press},
pages = {89-120},
year = {2021},
series = {Cognitive Data Science in Sustainable Computing},
isbn = {978-0-12-824118-9},
doi = {https://doi.org/10.1016/B978-0-12-824118-9.00005-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128241189000059},
author = {Dioneia {Motta Monte-Serrat} and Carlo Cattani},
keywords = {Computer language, Linguistics, Symbolic language, Generative model, Algorithmic core, Axiomatic-logical structure},
abstract = {Language is the ability to use complex communication systems and a set of signals used to encode and decode information. Mathematics and linguistics deal with the representation of thought through symbolic language under a guiding structure of language functioning. Computer language is formal, symbolic, and depends on linguistic principles of natural language. Computing regulates the behavior of the machine in the execution of specific tasks and, for that, generative models were developed. The underlying structure of the latter would be adequate to incorporate complex and high-dimensional data in a latent space with a supposed ability to replicate the original data. The distribution in probabilistic terms, however, does not reflect reality. We suggest the logical-axiomatic principle of natural language as an algorithmic core of computational methods capable of generalizing the execution of a set of factors.}
}
@article{LIN202052,
title = {A novel deep neural network based approach for sparse code multiple access},
journal = {Neurocomputing},
volume = {382},
pages = {52-63},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.11.066},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219316686},
author = {Jinzhi Lin and Shengzhong Feng and Yun Zhang and Zhile Yang and Yong Zhang},
keywords = {Sparse code multiple access, Non-orthogonal multiple access, Machine learning, Dense code multiple access},
abstract = {Sparse code multiple access (SCMA) has been one of the non-orthogonal multiple access (NOMA) schemes aiming to support high spectral efficiency and ubiquitous access requirements for 5G communication networks. Conventional SCMA approaches are confronting challenges in designing low-complexity high-accuracy decoding algorithm and constructing optimum codebooks. Fortunately, the recent spotlighted deep learning technologies are of significant potentials in solving many communication engineering problems. Inspired by this, we propose and train a deep neural network (DNN) called DL-SCMA to learn to decode SCMA modulated signals corrupted by additive white Gaussian noise (AWGN). An autoencoder called AE-SCMA is established and trained to generate optimal SCMA codewords and reconstruct original bits. Furthermore, by manipulating the mapping vectors, an autoencoder is able to generalize SCMA, thus a dense code multiple access (DCMA) scheme is proposed. Simulations show that the DNN SCMA decoder significantly outperforms the conventional message passing algorithm (MPA) in terms of bit error rate (BER), symbol error rate (SER) and computational complexity, and AE-SCMA also demonstrates better performances via constructing better SCMA codebooks. The performance of deep learning aided DCMA is superior to the SCMA.}
}
@article{MISCHKOWSKI201885,
title = {Think it through before making a choice? Processing mode does not influence social mindfulness},
journal = {Journal of Experimental Social Psychology},
volume = {74},
pages = {85-97},
year = {2018},
issn = {0022-1031},
doi = {https://doi.org/10.1016/j.jesp.2017.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0022103117302858},
author = {Dorothee Mischkowski and Isabel Thielmann and Andreas Glöckner},
keywords = {Social mindfulness, Processing mode, Intuition versus deliberation, Spontaneous cooperation, Prosocial personality},
abstract = {Social mindfulness has recently been introduced as a type of prosocial behavior that emphasizes the importance of a skill to see other people's needs beyond the will to act accordingly. Correspondingly, social mindfulness has been proposed to involve processes of executive functioning and thus of deliberate thinking. In four studies, we tested the influence of processing mode on social mindfulness using different experimental manipulations (i.e., instructions to decide intuitively vs. deliberately, time pressure, and cognitive load). Contrary to the idea that social mindfulness requires conscious processing – and unlike recent findings suggesting intuitive cooperation – we consistently found negligible effect sizes for the influence of processing mode on social mindfulness. This was observable for both, prosocial and selfish individuals alike (i.e., those with high vs. low levels in Social Value Orientation or Honesty-Humility, respectively). Overall, the findings suggest that social mindfulness constitutes a general tendency to perceive and act prosocially in social situations that is unaffected by processing mode and, by implication, distinguishable from other types of prosocial behavior.}
}
@article{OKAMURA2021,
title = {NMB4.0: development of integrated nuclear fuel cycle simulator from the front to back-end},
journal = {EPJ - Nuclear Sciences & Technologies},
volume = {7},
year = {2021},
issn = {2491-9292},
doi = {https://doi.org/10.1051/epjn/2021019},
url = {https://www.sciencedirect.com/science/article/pii/S2491929221000145},
author = {Tomohiro Okamura and Ryota Katano and Akito Oizumi and Kenji Nishihara and Masahiko Nakase and Hidekazu Asano and Kenji Takeshita},
abstract = {Nuclear Material Balance code version 4.0 (NMB4.0) has been developed through collaborative R&D between TokyoTech&JAEA. Conventional nuclear fuel cycle simulation codes mainly analyze actinides and are specialized for front-end mass balance analysis. However, quantitative back-end simulation has recently become necessary for considering R&D strategies and sustainable nuclear energy utilization. Therefore, NMB4.0 was developed to realize the integrated nuclear fuel cycle simulation from front- to back-end. There are three technical features in NMB4.0: 179 nuclides are tracked, more than any other code, throughout the nuclear fuel cycle; the Okamura explicit method is implemented, which contributes to reducing the numerical cost while maintaining the accuracy of depletion calculations on nuclides with a shorter half-life; and flexibility of back-end simulation is achieved. The main objective of this paper is to show the newly developed functions, made for integrated back-end simulation, and verify NMB4.0 through a benchmark study to show the computational performance.}
}
@article{COOPER20091351,
title = {Emergence as a computability-theoretic phenomenon},
journal = {Applied Mathematics and Computation},
volume = {215},
number = {4},
pages = {1351-1360},
year = {2009},
note = {Physics and Computation},
issn = {0096-3003},
doi = {https://doi.org/10.1016/j.amc.2009.04.050},
url = {https://www.sciencedirect.com/science/article/pii/S0096300309004159},
author = {S. Barry Cooper},
keywords = {Computability, Emergence, Definability, Turing invariance},
abstract = {In dealing with emergent phenomena, a common task is to identify useful descriptions of them in terms of the underlying atomic processes, and to extract enough computational content from these descriptions to enable predictions to be made. Generally, the underlying atomic processes are quite well understood, and (with important exceptions) captured by mathematics from which it is relatively easy to extract algorithmic content. A widespread view is that the difficulty in describing transitions from algorithmic activity to the emergence associated with chaotic situations is a simple case of complexity outstripping computational resources and human ingenuity. Or, on the other hand, that phenomena transcending the standard Turing model of computation, if they exist, must necessarily lie outside the domain of classical computability theory. In this talk we suggest that much of the current confusion arises from conceptual gaps and the lack of a suitably fundamental model within which to situate emergence. We examine the potential for placing emergent relations in a familiar context based on Turing’s 1939 model for interactive computation over structures described in terms of reals. The explanatory power of this model is explored, formalising informal descriptions in terms of mathematical definability and invariance, and relating a range of basic scientific puzzles to results and intractable problems in computability theory.}
}
@article{OLUBAMBI2025,
title = {Conceptualising a dynamic BIM-based waste management system in enabling net-zero cities},
journal = {Proceedings of the Institution of Civil Engineers - Waste and Resource Management},
year = {2025},
issn = {1747-6534},
doi = {https://doi.org/10.1680/jwarm.23.00043},
url = {https://www.sciencedirect.com/science/article/pii/S1747653425000026},
author = {Ademilade Olubambi and Clinton Aigbavboa and Bolanle Ikotun},
keywords = {building information modelling (BIM), life cycle assessment, life cycle analysis, LCA, sustainable cities and communities, sustainable development, waste disposal system},
abstract = {This study investigates the possibility of applying building information modelling as a tool for eliminating waste throughout the building life cycle toward achieving net-zero waste in the construction industry. To accomplish this goal, literature was reviewed to identify aspects that necessitate using mechanism in optimising a sustainable waste management system. A building information modelling-based conceptual framework with a high capacity for implementing net-zero waste management action throughout the building development stages was developed. The results indicate that the tool can generate an effective programme for ordering materials, assembling, and supplying all building components during the design phase. Any alterations to the building information model during the design phase are updated automatically. During the procurement phase, three-dimensional geometry can be used for project sequencing, take-offs, and integrating energy analyses. Virtual construction modelling, which is extremely cost-effective, can be used during the construction phase. Furthermore, the tools can be utilised as a waste estimation tool before any demolition or remodelling, as well as to help determine a reasonable price for waste disposal rates, throughout the construction phase. In conclusion, this study demonstrates how waste is minimised and consequently prevented in building construction by integrating dynamic building information modelling tools while enabling the possibility of a net-zero cities.}
}
@article{KUIPERS2008155,
title = {Drinking from the firehose of experience},
journal = {Artificial Intelligence in Medicine},
volume = {44},
number = {2},
pages = {155-170},
year = {2008},
note = {Artificial Consciousness},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2008.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0933365708000985},
author = {Benjamin Kuipers},
keywords = {Consciousness, Sensory trackers, Information content, Dynamical systems},
abstract = {Summary
Objective
Computational concepts from robotics and computer vision hold great promise to account for major aspects of the phenomenon of consciousness, including philosophically problematical aspects such as the vividness of qualia, the first-person character of conscious experience, and the property of intentionality.
Methods
We present a dynamical systems model describing human or robotic agents and their interaction with the environment. In order to cope with the enormous information content of the sensory stream, this model includes trackers for selected coherent spatio–temporal portions of the sensory input stream, and a self-constructed plausible coherent narrative describing the recent history of the agent’s sensorimotor interaction with the world.
Results
We describe how an agent can autonomously learn its own intentionality by constructing computational models of hypothetical entities in the external world. These models explain regularities in the sensorimotor interaction, and serve as referents for the agent’s symbolic knowledge representation. The high information content of the sensory stream allows the agent to continually evaluate these hypothesized models, refuting those that make poor predictions. The high information content of the sensory input stream also accounts for the vividness and uniqueness of subjective experience. We then evaluate our account against 11 features of consciousness “that any philosophical–scientific theory should hope to explain”, according to the philosopher and prominent AI critic John Searle.
Conclusion
The essential features of consciousness can, in principle, be implemented on a robot with sufficient computational power and a sufficiently rich sensorimotor system, embodied and embedded in its environment.}
}
@article{HE2023126590,
title = {Global priors guided modulation network for joint super-resolution and SDRTV-to-HDRTV},
journal = {Neurocomputing},
volume = {554},
pages = {126590},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126590},
url = {https://www.sciencedirect.com/science/article/pii/S0925231223007130},
author = {Gang He and Shaoyi Long and Li Xu and Chang Wu and Wenxin Yu and Jinjia Zhou},
keywords = {Convolutional neural network, Super-resolution, SDRTV-to-HDRTV, High dynamic range},
abstract = {Watching low resolution standard dynamic range (LR SDR) video on a 4K high dynamic range (HDR) TV is not the best viewing experience. Joint super-resolution (SR) and SDRTV-to-HDRTV aims to enhance the visual quality of LR SDR videos that have quality deficiencies in resolution and dynamic range. Previous methods that rely on learning local information typically cannot do well in preserving color conformity and long-range structural similarity, resulting in unnatural color transition and texture artifacts. In order to tackle these challenges, we propose a global priors guided modulation network (GPGMNet). In particular, we design a global priors extraction module (GPEM) to extract color conformity prior and structural similarity prior that are beneficial for SDRTV-to-HDRTV and SR tasks, respectively. To further exploit the global priors and preserve spatial information, we devise multiple global priors-guided spatial-wise modulation blocks (GSMBs) with a few parameters for intermediate feature modulation. In these GSMBs, the modulation parameters are generated by the shared global priors and the spatial features map from the spatial pyramid convolution block (SPCB). With these elaborate designs, the GPGMNet can achieve higher visual quality with lower computational complexity. Extensive experiments demonstrate that our proposed GPGMNet is superior to the state-of-the-art methods. Specifically, our proposed model exceeds the state-of-the-art by 0.64 dB in PSNR, with 69% fewer parameters and 3.1× speedup.}
}
@article{BECK2024545,
title = {Understanding the cell: Future views of structural biology},
journal = {Cell},
volume = {187},
number = {3},
pages = {545-562},
year = {2024},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2023.12.017},
url = {https://www.sciencedirect.com/science/article/pii/S0092867423013491},
author = {Martin Beck and Roberto Covino and Inga Hänelt and Michaela Müller-McNicoll},
keywords = {structural biology, digital twin, computational modeling, cellular self-organization},
abstract = {Summary
Determining the structure and mechanisms of all individual functional modules of cells at high molecular detail has often been seen as equal to understanding how cells work. Recent technical advances have led to a flush of high-resolution structures of various macromolecular machines, but despite this wealth of detailed information, our understanding of cellular function remains incomplete. Here, we discuss present-day limitations of structural biology and highlight novel technologies that may enable us to analyze molecular functions directly inside cells. We predict that the progression toward structural cell biology will involve a shift toward conceptualizing a 4D virtual reality of cells using digital twins. These will capture cellular segments in a highly enriched molecular detail, include dynamic changes, and facilitate simulations of molecular processes, leading to novel and experimentally testable predictions. Transferring biological questions into algorithms that learn from the existing wealth of data and explore novel solutions may ultimately unveil how cells work.}
}
@article{PICCININI2008311,
title = {Some neural networks compute, others don’t},
journal = {Neural Networks},
volume = {21},
number = {2},
pages = {311-321},
year = {2008},
note = {Advances in Neural Networks Research: IJCNN ’07},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2007.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S089360800700250X},
author = {Gualtiero Piccinini},
keywords = {Connectionism, Neural network, Computation, Mechanism, Cognition, Brain},
abstract = {I address whether neural networks perform computations in the sense of computability theory and computer science. I explicate and defend the following theses. (1) Many neural networks compute—they perform computations. (2) Some neural networks compute in a classical way. Ordinary digital computers, which are very large networks of logic gates, belong in this class of neural networks. (3) Other neural networks compute in a non-classical way. (4) Yet other neural networks do not perform computations. Brains may well fall into this last class.}
}
@article{BELL200163,
title = {Futures studies comes of age: twenty-five years after The limits to growth},
journal = {Futures},
volume = {33},
number = {1},
pages = {63-76},
year = {2001},
issn = {0016-3287},
doi = {https://doi.org/10.1016/S0016-3287(00)00054-9},
url = {https://www.sciencedirect.com/science/article/pii/S0016328700000549},
author = {Wendell Bell},
abstract = {Twenty-five years ago, the publication of The limits to growth marked a period of accomplishments in the futures field. Today, futures studies is experiencing another burst of development and is ready to move more fully into mainstream intellectual life and the standard educational curriculum. In addition to continued work on methods, theory, and empirical research, the resolution of three issues might help persuade established academic communities of the serious purposes and sound intellectual contributions of futurists. They are (1) the adoption of an adequate theory of knowledge (critical realism is proposed), (2) the recognition that prediction does play a role in futures studies (so we can deal explicitly with the philosophical challenges it poses), and (3) the formulation and justification of core values (so we have a valid basis by which to judge the desirability of alternative futures). I propose a critical discourse among futurists in order to resolve each issue. The desire to make futures thinking a part of everyone's education is not, of course, mere futurist chauvinism, but is based on the conviction that futures studies has important contributions to make to human well-being.}
}
@article{OUYANG2024e29176,
title = {Unmasking the challenges in ideological and political education in China: A thematic review},
journal = {Heliyon},
volume = {10},
number = {8},
pages = {e29176},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e29176},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024052071},
author = {Sha Ouyang and Wei Zhang and Jian Xu and Abdullah {Mat Rashid} and Shwu Pyng How and Aminuddin {Bin Hassan}},
keywords = {Ideological and political education, China, Thematic review},
abstract = {China's distinctive educational approach, particularly its emphasis on ideological and political education, has garnered considerable academic attention for its impact on shaping individual values, fostering citizenship, and maintaining social stability. Despite the Chinese government's prioritization of ideological and political education, academic research in this field appears constrained, with existing studies predominantly focusing on normative and descriptive aspects. Normative research delineates how ideological and political education should be executed, while descriptive research illustrates its practical implementation. The effectiveness of these approaches is significantly diminished if they are not adequately interconnected—when only the current reality is explained without providing tools for improvement or when prescribed steps for improvement lack a basis in specific contexts. This paper conducts a comprehensive review of research on ideological and political education using ATLAS. ti 9 for thematic analysis. The review aims to unveil the intricate landscape of current research in China and address key questions: What are the primary trends in the literature on ideological and political education between 2021 and July 2023? What challenges does ideological and political education face? Through a direct exploration of these issues, this paper seeks to optimize the ideological and political education system, elevate its adaptability and effectiveness, and open avenues for research, fostering a more dynamic, inclusive, and resilient development of ideological and political education.}
}
@article{2025335,
title = {In This Issue},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {10},
number = {4},
pages = {335-336},
year = {2025},
note = {Cognitive Neuroscience of Mindfulness},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2025.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S2451902225000692}
}
@article{HOPPENSTEADT201599,
title = {Spin torque oscillator neuroanalog of von Neumann's microwave computer},
journal = {Biosystems},
volume = {136},
pages = {99-104},
year = {2015},
note = {Selected papers presented at the Eleventh International Workshop on Neural Coding, Versailles, France, 2014},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2015.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S0303264715000891},
author = {Frank Hoppensteadt},
keywords = {Neuromorphic computing, Spintronics, Spin torque oscillator, Spin waves, Logic machine, Nanomagnetics, Von Neumann microwave computer},
abstract = {Frequency and phase of neural activity play important roles in the behaving brain. The emerging understanding of these roles has been informed by the design of analog devices that have been important to neuroscience, among them the neuroanalog computer developed by O. Schmitt and A. Hodgkin in the 1930s. Later J. von Neumann, in a search for high performance computing using microwaves, invented a logic machine based on crystal diodes that can perform logic functions including binary arithmetic. Described here is an embodiment of his machine using nano-magnetics. Electrical currents through point contacts on a ferromagnetic thin film can create oscillations in the magnetization of the film. Under natural conditions these properties of a ferromagnetic thin film may be described by a nonlinear Schrödinger equation for the film's magnetization. Radiating solutions of this system are referred to as spin waves, and communication within the film may be by spin waves or by directed graphs of electrical connections. It is shown here how to formulate a STO logic machine, and by computer simulation how this machine can perform several computations simultaneously using multiplexing of inputs, that this system can evaluate iterated logic functions, and that spin waves may communicate frequency, phase and binary information. Neural tissue and the Schmitt-Hodgkin, von Neumann and STO devices share a common bifurcation structure, although these systems operate on vastly different space and time scales; namely, all may exhibit Andronov-Hopf bifurcations. This suggests that neural circuits may be capable of the computational functionality as described by von Neumann.}
}
@article{HUANG202219,
title = {Numerical simulation and comparative study for the zinc smelting furnaces at the Tongmuling site in Qing Dynasty, Hunan Province, China},
journal = {Advances in Archaeomaterials},
volume = {3},
number = {1},
pages = {19-27},
year = {2022},
issn = {2667-1360},
doi = {https://doi.org/10.1016/j.aia.2022.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S2667136022000061},
author = {Xing Huang and Linheng Mo and Wenli Zhou and Shengqiang Luo and Ya Xiao and Jianli Chen},
keywords = {Zinc smelting furnace, Furnace profile, Retort, Numerical simulation, Tongmuling site},
abstract = {Brass, which appears golden in color, used to be a valuable alloy in ancient times. During the Ming and Qing Dynasties, the Chinese used special furnaces to smelt zinc for minting and exporting to overseas in large quantities. Archeological findings have revealed the overall structure of the zinc smelting furnaces at the Tongmuling site during the Qing Dynasty. In this study, computational fluid dynamics software was employed to simulate airflow fields within a furnace. Consequently, we observed that airflows were concentrated at the center of the lower chamber, after which they dispersed into the upper chamber through ceramic pads and finally were evenly distributed between the retorts. Increasing furnace height and improving thermal convection in the lower chamber helped increase the furnace temperature. The ceramic pads adjusted the airflow to ensure that temperature distribution in the upper chamber was uniform, and they supported burning in the upper chamber by preventing collapse. Compared with the heap smelting process recorded in Heavenly Creations and the large crucible furnaces used in modern times, zinc smelting furnaces at the Tongmuling site possess a unique structure. They serve as a link between preceding and subsequent technologies, offering important evidence for exploring the development of ancient Chinese zinc smelting technologies.}
}
@incollection{MACLENNAN201584,
title = {Cognitive Modeling: Connectionist Approaches},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {84-89},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.43021-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868430217},
author = {Bruce MacLennan},
keywords = {Artificial intelligence, Backpropagation, Computability, Computational map, Connectionism, Correlational learning, Dynamic systems approach, Embodiment, Language of thought, Machine learning, Neural network, Perceptron, Representation, Situatedness, Subsymbolic},
abstract = {Connectionist approaches to cognitive modeling make use of large networks of simple computational units, which communicate by means of simple quantitative signals. Higher-level information processing emerges from the massively parallel interaction of these units by means of their connections, and a network may adapt its behavior by means of local changes in the strength of the connections. Connectionist approaches are related to neural networks and provide a distinct alternative to cognitive models inspired by the digital computer.}
}
@article{MICHIE19931,
title = {Turing's test and conscious thought},
journal = {Artificial Intelligence},
volume = {60},
number = {1},
pages = {1-22},
year = {1993},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(93)90032-7},
url = {https://www.sciencedirect.com/science/article/pii/0004370293900327},
author = {Donald Michie},
abstract = {Over forty years ago A.M. Turing proposed a test for intelligence in machines. Based as it is solely on an examinee's verbal responses, the Test misses some important components of human thinking. To bring these manifestations within its scope, the Turing Test would require substantial extension. Advances in the application of AI methods in the design of improved human-computer interfaces are now focussing attention on machine models of thought and knowledge from the altered standpoint of practical utility.}
}
@incollection{VALLERO2014929,
title = {Chapter 32 - Sustainable Approaches},
editor = {Daniel Vallero},
booktitle = {Fundamentals of Air Pollution (Fifth Edition)},
publisher = {Academic Press},
edition = {Fifth Edition},
address = {Boston},
pages = {929-952},
year = {2014},
isbn = {978-0-12-401733-7},
doi = {https://doi.org/10.1016/B978-0-12-401733-7.00032-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780124017337000323},
author = {Daniel Vallero},
keywords = {American society of mechanical engineers (ASME), Aquatic toxicity, Atmospheric oxidation, Bathtub curve, Benchmark, Benefit-cost ratio, Bioconcentration, Decision force field, Dematerialization, Design failure, Design for disassembly (DfD), Design for the environment (DfE), Design for the recyling (DfR), Energy return on the energy investment (EROEI), Ethanol, Exposure, Fuel change, Green algae, Green engineering, Green technology, Half-life, Hazard, Hazardous air pollutant (HAP), Henry's law, Holdup, Industrial ecology, Life cycle analysis (LCA), Life cycle assessment, Material, energy and waste (MEW) flow, Operation and maintenance (O&M), Oxides of nitrogen (NO), Pollution prevention, Pollution prevention act, Process change, Relative risk, Reliability, Remedial action, Resource conservation and recovery act (RCRA), Risk trade-off, Sludge sorption, Solubility, Sustainability, Thermodynamic efficiency, Utility, Vapor pressure, Waste audit, Waste minimization, WWT, WWTP},
abstract = {This chapter considers alternative means of achieving acceptable air quality in addition to or instead of the controls discussed in previous chapters. Air quality is considered from a systems thinking perspective. The concept of engineering reliability is applied to air pollution. The advantages and disadvantages of utility and benefit-cost analyses are discussed, with recommendations of other means of determining environmental value and evaluating risk trade-offs, especially life cycle analysis, green engineering and industrial ecology tools.}
}
@article{GOTHAM2016705,
title = {A Suitable Approach in Extracting Non Event Related Potential Sources from Brain of Disabled Patients},
journal = {Procedia Computer Science},
volume = {85},
pages = {705-712},
year = {2016},
note = {International Conference on Computational Modelling and Security (CMS 2016)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.05.257},
url = {https://www.sciencedirect.com/science/article/pii/S187705091630607X},
author = {Solomon Gotham and G. Sasibushana Rao},
keywords = {Electroencephalogram (EEG) ;Magneto encephalogram (EMG) ;Non Gaussianity, Evoked potentials (EVP) ;Non Event Related Potentials (NERP)},
abstract = {Brain is the most important, astonishing and complicated part of human body which is responsible for controlling and functioning of all other human organs. The physical movements and thinking capability (Cognition) of humans depend on the brain activity. Based on certain changes that occur within the brain, electric fields will be generated within the brain. Analyzing brain signals plays vital role in diagnosis and treatment of brain disorders. Brain signals are obtained from electrodes of Electroencephalogram (EEG) or Magneto encephalogram (EMG). These are linear mixture of evoked potentials (EVP) of large number of neurons due to variations in conductive and geometric properties in the layers of 3 layer head model or 4 layer head model. Earlier work1-5 considered processing these mixed signals for analyzing brain functioning of brain disabled patients. But working on the source signals gives an authoritative result. Hence there is a need to separate the source signals from the measured (electrode) signals. This work will suggest a suitable approach in extracting source signals of disabled patients while they were used as subjects under experiment of retrieving event related potentials (ERP). This work retrieved the signals of non target trails i.e., non event related potentials (NERP) and extracted original source signals by the best Gaussian estimate and the algorithm proposed.}
}
@article{NADERPOUR2014209,
title = {The explosion at institute: Modeling and analyzing the situation awareness factor},
journal = {Accident Analysis & Prevention},
volume = {73},
pages = {209-224},
year = {2014},
issn = {0001-4575},
doi = {https://doi.org/10.1016/j.aap.2014.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S0001457514002644},
author = {Mohsen Naderpour and Jie Lu and Guangquan Zhang},
keywords = {Situation awareness, Situation assessment, Abnormal situations, Methomyl unit, Accident analysis},
abstract = {In 2008 a runaway chemical reaction caused an explosion at a methomyl unit in West Virginia, USA, killing two employees, injuring eight people, evacuating more than 40,000 residents adjacent to the facility, disrupting traffic on a nearby highway and causing significant business loss and interruption. Although the accident was formally investigated, the role of the situation awareness (SA) factor, i.e., a correct understanding of the situation, and appropriate models to maintain SA, remain unexplained. This paper extracts details of abnormal situations within the methomyl unit and models them into a situational network using dynamic Bayesian networks. A fuzzy logic system is used to resemble the operator’s thinking when confronted with these abnormal situations. The combined situational network and fuzzy logic system make it possible for the operator to assess such situations dynamically to achieve accurate SA. The findings show that the proposed structure provides a useful graphical model that facilitates the inclusion of prior background knowledge and the updating of this knowledge when new information is available from monitoring systems.}
}
@incollection{BISHT2022277,
title = {Chapter Twelve - Perceiving the level of depression from web text},
editor = {Shikha Jain and Kavita Pandey and Princi Jain and Kah Phooi Seng},
booktitle = {Artificial Intelligence, Machine Learning, and Mental Health in Pandemics},
publisher = {Academic Press},
pages = {277-298},
year = {2022},
isbn = {978-0-323-91196-2},
doi = {https://doi.org/10.1016/B978-0-323-91196-2.00008-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323911962000089},
author = {Sankalp Singh Bisht and Herumb Shandilya and Vaibhav Gupta and Shriyansh Agrawal and Shikha Jain},
keywords = {Depression, Loneliness, Mental disorder, Solitude, Suicide},
abstract = {Depression is one of the deadliest diseases found in today's world, and unfortunately, it is also one of the most ignored problems. Depression is a fact that is very hard to accept for any individual and is always a multistep process. The initial stage of Depression is Loneliness, and thus the information about these emotions can be leveraged and can help in the early detection of Depression, which in turn leads to suicidal thoughts. Tweet data analysis is one of the most popular ways to determine the presence of depression and suicidal thoughts, through the concepts of Machine Learning. Twitter proves to be a very rich source of data, as their user base is potentially large enough, but is also increasing in a fast manner. For the scope of this paper, we predicted from a user's specific tweet, which is categorized for loneliness. These tweets are analyzed to check the level of depression as moderate or severe when people start thinking of suicide. The simulation is carried out using four different models for one level of classification and eight models are used at the second level of classification. It is observed that Gated Recurrent Unit with BERT outperformed all the models and showed the accuracy of 99% and 97%. However, for class-1 recall with XLNet gave the best result with class-1 recall being 0.99. This application can help the individual in early detection of depression without any human intervention and seek medical help. Moreover, it also provides an insight about the feelings of the individual to the medical practitioners, which, in turn, can help them provide better decision-making.}
}
@article{WEI202238,
title = {Promoter prediction in nannochloropsis based on densely connected convolutional neural networks},
journal = {Methods},
volume = {204},
pages = {38-46},
year = {2022},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2022.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S1046202322000846},
author = {Pi-Jing Wei and Zhen-Zhen Pang and Lin-Jie Jiang and Da-Yu Tan and Yan-Sen Su and Chun-Hou Zheng},
keywords = {Nannochloropsis, Promoter, Deep learning, Densely connected convolutional neural networks, Within-group scrambling},
abstract = {Promoter is a key DNA element located near the transcription start site, which regulates gene transcription by binding RNA polymerase. Thus, the identification of promoters is an important research field in synthetic biology. Nannochloropsis is an important unicellular industrial oleaginous microalgae, and at present, some studies have identified some promoters with specific functions by biological methods in Nannochloropsis, whereas few studies used computational methods. Here, we propose a method called DNPPro (DenseNet-Predict-Promoter) based on densely connected convolutional neural networks to predict the promoter of Nannochloropsis. First, we collected promoter sequences from six Nannochloropsis strains and removed 80% similarity using CD-HIT for each strain to yield a reliable set of positive datasets. Then, in order to construct a robust classifier, within-group scrambling method was used to generate negative dataset which overcomes the limitation of randomly selecting a non-promoter region from the same genome as a negative sample. Finally, we constructed a densely connected convolutional neural network, with the sequence one-hot encoding as the input. Compared with commonly used sequence processing methods, DNPPro can extract long sequence features to a greater extent. The cross-strain experiment on independent dataset verifies the generalization of our method. At the same time, T-SNE visualization analysis shows that our method can effectively distinguish promoters from non-promoters.}
}
@article{2024100670,
title = {Erratum regarding missing declaration of competing interest statements in previously published articles},
journal = {International Journal of Child-Computer Interaction},
volume = {41},
pages = {100670},
year = {2024},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100670},
url = {https://www.sciencedirect.com/science/article/pii/S2212868924000382}
}
@article{KHAN2023110525,
title = {AAD-Net: Advanced end-to-end signal processing system for human emotion detection & recognition using attention-based deep echo state network},
journal = {Knowledge-Based Systems},
volume = {270},
pages = {110525},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110525},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123002757},
author = {Mustaqeem Khan and Abdulmotaleb {El Saddik} and Fahd Saleh Alotaibi and Nhat Truong Pham},
keywords = {Affective computing, Attention mechanism, Convolution neural network, Echo state networks, Emotion recognition, Human–computer interaction, Audio speech signals},
abstract = {Speech signals are the most convenient way of communication between human beings and the eventual method of Human–Computer Interaction (HCI) to exchange emotions and information. Recognizing emotions from speech signals is a challenging task due to the sparse nature of emotional data and features. In this article, we proposed a Deep Echo-State-Network (DeepESN) system for emotion recognition with a dilated convolution neural network and multi-headed attention mechanism. To reduce the model complexity, we incorporate a DeepESN that combines reservoir computing for higher-dimensional mapping. We also used fine-tuned Sparse Random Projection (SRP) to reduce dimensionality and adopted an early fusion strategy to fuse the extracted cues and passed the joint feature vector via a classification layer to recognize emotions. Our proposed model is evaluated on two public speech corpora, EMO-DB and RAVDESS, and tested for subject/speaker-dependent/independent performance. The results show that our proposed system achieves a high recognition rate, 91.14, 85.57 for EMO-DB, and 82.01, 77.02 for RAVDESS, using speaker-dependent and independent experiments, respectively. Our proposed system outperforms the State-of-The-Art (SOTA) while requiring less computational time.}
}
@article{MARSHALL2023131,
title = {The role of quantum mechanics in cognition-based evolution},
journal = {Progress in Biophysics and Molecular Biology},
volume = {180-181},
pages = {131-139},
year = {2023},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2023.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S007961072300041X},
author = {Perry Marshall},
abstract = {In 2021 I noted that in all information-based systems we understand, Cognition creates Code, which controls Chemical reactions. Known agents write software which controls hardware, and not the other way around. I proposed the same is true in all of biology. Though the textbook description of cause and effect in biology proposes the reverse, that Chemical reactions produce Code from which Cognition emerges, there are no examples in the literature demonstrating either step. A mathematical proof for the first step, cognition generating code, is based on Turing's halting problem. The second step, code controlling chemical reactions, is the role of the genetic code. Thus a central question in biology: What is the nature and source of cognition? In this paper I propose a relationship between biology and Quantum Mechanics (QM), hypothesizing that the same principle that enables an observer to collapse a wave function also grants biology its agency: the organism's ability to act on the world instead of merely being a passive recipient. Just as all living cells are cognitive (Shapiro 2021, 2007; McClintock 1984; Lyon 2015; Levin 2019; Pascal and Pross, 2022), I propose humans are quantum observers because we are made of cells and all cells are observers. This supports the century-old view that in QM, the observer does not merely record the event but plays a fundamental role in its outcome.The classical world is driven by laws, which are deductive; the quantum world is driven by choices, which are inductive. When the two are combined, they form the master feedback loop of perception and action for all biology. In this paper I apply basic definitions of induction, deduction and computation to known properties of QM to show that the organism altering itself (and its environment) is a whole shaping its parts. It is not merely parts comprising a whole. I propose that an observer collapsing the wave function is the physical mechanism for producing negentropy. The way forward in solving the information problem in biology is understanding the relationship between cognition and QM.}
}
@article{ALBERT2008401,
title = {A formal framework for modelling the developmental course of competence and performance in the distance, speed, and time domain},
journal = {Developmental Review},
volume = {28},
number = {3},
pages = {401-420},
year = {2008},
issn = {0273-2297},
doi = {https://doi.org/10.1016/j.dr.2008.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0273229708000257},
author = {Dietrich Albert and Michael D. Kickmeier-Rust and Fumiko Matsuda},
keywords = {Distance–speed–time system, Cognitive development, Overgeneralization, Competence-based Knowledge Space Theory},
abstract = {The developmental course in the distance–speed–time domain is still a matter of debate. Traditional stage models are contested by theories of continuous development and adaptive thinking. In the present work, we introduce a formal framework for modelling the developmental course in this domain, grounding on Competence-based Knowledge Space Theory. This framework, as a more general case, widely includes assumptions and facets of previous models and covers empirical findings collected based on different experimental paradigms. By a distinction of latent competences and observable performance, model validation is not bound to a certain experimental paradigm and no one-to-one correspondence between competences and tasks is required. Therefore, the framework has the potential to bridge the gap between stage models and models of continuous development. The approach also precisely defines misconceptions, for example overgeneralization, and empirically investigates their occurrence. In the present work, we established a prototypical model for the development of understanding the distance–speed–time system. We extended this model with definitions based on different perspectives of overgeneralization. The assumptions of the model and its extensions were examined on the basis of the results of two empirical investigations using six judgment task types. The results yielded a reasonably good fit of model and data. No evidence was found for the occurrence of overgeneralization in this domain. The theoretical model and empirical results are discussed with respect to their relationship to other developmental models and theories.}
}
@article{JURKOVA2023105046,
title = {Turing and von Neumann machines: Completing the new mechanism},
journal = {Biosystems},
volume = {234},
pages = {105046},
year = {2023},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2023.105046},
url = {https://www.sciencedirect.com/science/article/pii/S0303264723002216},
author = {Barbora Jurková and Lukáš Zámečník},
keywords = {Turing machine, von Neumann probe, New mechanism, Code biology, Extended mechanism},
abstract = {Turing (1937) introduces a model of code that is followed by other pioneers of computing machines (such as Flowers 1983, Eckert, Mauchly, Brainerd 1945 and others). One of them is John von Neumann, who defines the concept of optimal code in the context of the conception of EDVAC. He later uses it to build on in his theoretical considerations of the universal constructor (von Neumann 1966). Von Neumann (1963) further presents one of the first neural network models, in relation to the work of McCulloch and Pitts (1943), for both theoretical purposes (von Neumann probe) and practical applications (computer architecture of EDVAC). The aim of this paper is (1) to describe the differences between Turingʼs and von Neumannʼs conceptualizations of code and the mechanical computing model. Between von Neumann's abstract technical conception (von Neumann 1963 and 1966) and Turingʼs more concrete biochemical conception (Turing 1952). Furthermore, (2) we want to answer the question why these influential models of mechanisms (predominantly in computer science) have so far been ignored by philosophers of the new mechanism (Machamer, Darden, Craver 2000, Glennan 2017). We will show that these classical models of machines are not only compatible with the new mechanism, but moreover complement it, since they represent a completely separate type of model of mechanism, alongside producing, maintaining and underlying (Zámečník 2021). The final (3) and main goal of our paper will be an attempt to relate von Neumannʼs and Turingʼs notion of mechanism to Barbieriʼs notion of extended mechanism (Barbieri 2015).}
}
@article{ROYCHOWDHURY2004105,
title = {Diagnosis of the diseases––using a GA-fuzzy approach},
journal = {Information Sciences},
volume = {162},
number = {2},
pages = {105-120},
year = {2004},
note = {Medical Expert Systems},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2004.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0020025504000660},
author = {Anish Roychowdhury and Dilip Kumar Pratihar and Nilav Bose and K.P Sankaranarayanan and N Sudhahar},
keywords = {Diagnosis, Jaundice, Pneumonia, GA-fuzzy approach},
abstract = {The objective of our study is to design an expert system by modelling the knowledge and thinking process of a doctor. A fuzzy logic controller (FLC) is used to model the process and a genetic algorithm (GA) helps to select a number of good rules from a manually constructed large rule base of an FLC, based on the opinion of 10 doctors. The GA-based tuning is done off-line. Once the optimized rule base of the FLC is obtained, it can diagnose the disease, on-line. The scope of the present work has been extended to two diseases, namely Pneumonia and Jaundice. The symptoms of each disease are fed as inputs to the FLC and the output, i.e., grade of a disease is determined.}
}
@article{GROSS20173,
title = {Prospects and problems for standardizing model validation in systems biology},
journal = {Progress in Biophysics and Molecular Biology},
volume = {129},
pages = {3-12},
year = {2017},
note = {Validation of Computer Modelling},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2017.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0079610716300177},
author = {Fridolin Gross and Miles MacLeod},
keywords = {Systems biology, Modeling, Standardization, Validation, Model selection},
abstract = {There are currently no widely shared criteria by which to assess the validity of computational models in systems biology. Here we discuss the feasibility and desirability of implementing validation standards for modeling. Having such a standard would facilitate journal review, interdisciplinary collaboration, model exchange, and be especially relevant for applications close to medical practice. However, even though the production of predictively valid models is considered a central goal, in practice modeling in systems biology employs a variety of model structures and model-building practices. These serve a variety of purposes, many of which are heuristic and do not seem to require strict validation criteria and may even be restricted by them. Moreover, given the current situation in systems biology, implementing a validation standard would face serious technical obstacles mostly due to the quality of available empirical data. We advocate a cautious approach to standardization. However even though rigorous standardization seems premature at this point, raising the issue helps us develop better insights into the practices of systems biology and the technical problems modelers face validating models. Further it allows us to identify certain technical validation issues which hold regardless of modeling context and purpose. Informal guidelines could in fact play a role in the field by helping modelers handle these.}
}
@article{RAMAN2002135,
title = {Coordinating informal and formal aspects of mathematics: student behavior and textbook messages},
journal = {The Journal of Mathematical Behavior},
volume = {21},
number = {2},
pages = {135-150},
year = {2002},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(02)00119-0},
url = {https://www.sciencedirect.com/science/article/pii/S0732312302001190},
author = {Manya Raman},
keywords = {Informal mathematics, Formal mathematics, Precalculus textbooks, Calculus textbooks},
abstract = {In this paper I illustrate difficulties students have coordinating informal and formal aspects of mathematics. I also discuss two ways in which precalculus and calculus textbooks treat mathematics that may make this coordination difficult: emphasizing the informal at the expense of the formal and emphasizing the formal at the expense of the informal. By looking at student difficulties in light of textbook treatments, we see evidence that student difficulties are not merely developmental. Students are not given many opportunities to make the kinds of connections which, while difficult, are an essential component of mathematical thinking.}
}
@article{DELIMANETO2018225,
title = {A semiotic-inspired machine for personalized multi-criteria intelligent decision support},
journal = {Data & Knowledge Engineering},
volume = {117},
pages = {225-238},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2018.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X17300757},
author = {Fernando Buarque {de Lima Neto} and Denis Mayr {Lima Martins} and Gottfried Vossen},
keywords = {Multi-criteria decision support, Computational intelligence, Computational semiotics, Intelligent semiotic machine},
abstract = {The need for appropriate decisions to tackle complex problems increases every day. Selecting destinations for vacation, comparing and optimizing resources to create valuable products, or purchasing a suitable car are just a few examples of puzzling situations in which there is no standard form to find an appropriate solution. Such scenarios become arduous when the number of possibilities, restrictions, and factors affecting the decision rise, thereby turning decision makers into almost mere spectators. In such circumstances, decision support systems (DSS) can play an important role in guiding people and organizations towards more accurate decision making. However, conventional DSS lack the necessary adaptability to account for dynamic changes and are frequently inadequate to tackle the subjectivity inherent in decision-maker's preferences and intention. We argue that these shortcomings can be addressed by a suitable combination of Semiotic Theory and Computational Intelligence algorithms, which together can make up a new generation of DSS. In this article, a formal description of an Intelligent Semiotic Machine is provided and tried out in practical decision contexts. The results obtained show that our approach can provide well-suited decisions based on user preferences, achieving appropriateness while fanning out subjective options without losing decision context, objectivity, or accuracy.}
}
@article{VAKSER20141785,
title = {Protein-Protein Docking: From Interaction to Interactome},
journal = {Biophysical Journal},
volume = {107},
number = {8},
pages = {1785-1793},
year = {2014},
issn = {0006-3495},
doi = {https://doi.org/10.1016/j.bpj.2014.08.033},
url = {https://www.sciencedirect.com/science/article/pii/S0006349514009382},
author = {Ilya A. Vakser},
abstract = {The protein-protein docking problem is one of the focal points of activity in computational biophysics and structural biology. The three-dimensional structure of a protein-protein complex, generally, is more difficult to determine experimentally than the structure of an individual protein. Adequate computational techniques to model protein interactions are important because of the growing number of known protein structures, particularly in the context of structural genomics. Docking offers tools for fundamental studies of protein interactions and provides a structural basis for drug design. Protein-protein docking is the prediction of the structure of the complex, given the structures of the individual proteins. In the heart of the docking methodology is the notion of steric and physicochemical complementarity at the protein-protein interface. Originally, mostly high-resolution, experimentally determined (primarily by x-ray crystallography) protein structures were considered for docking. However, more recently, the focus has been shifting toward lower-resolution modeled structures. Docking approaches have to deal with the conformational changes between unbound and bound structures, as well as the inaccuracies of the interacting modeled structures, often in a high-throughput mode needed for modeling of large networks of protein interactions. The growing number of docking developers is engaged in the community-wide assessments of predictive methodologies. The development of more powerful and adequate docking approaches is facilitated by rapidly expanding information and data resources, growing computational capabilities, and a deeper understanding of the fundamental principles of protein interactions.}
}
@article{DAN2025100814,
title = {Social robot assisted music course based on speech sensing and deep learning algorithms},
journal = {Entertainment Computing},
volume = {52},
pages = {100814},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100814},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124001824},
author = {Xiao Dan},
keywords = {Speech sensing, Deep learning algorithms, Social robots, Course in music},
abstract = {In the field of social robot teaching, research has focused on how to use technological means to provide better learning support and personalized interactive experiences. Social robots can interact with students and provide personalized learning support, thereby improving their learning effectiveness and engagement. The speech sensing model of social robots can perceive students’ emotions and feedback in real-time through technologies such as speech recognition and sentiment analysis, thereby providing intelligent responses and guidance. The deep learning recommendation model for music course resources extracts music features through deep learning techniques, and combines session interest extraction techniques to personalized recommend music resources suitable for students’ interests and abilities. By analyzing students’ interests and learning goals, robots can provide music learning resources that meet their needs based on recommendation algorithms, further stimulating their learning interest and enthusiasm. The experimental results show that the use of social robots in the learning environment significantly improves the learning effectiveness and participation of students. Through personalized interaction and intelligent response guidance, students are more likely to understand and master music knowledge, while experiencing joyful and positive learning emotions. The study validated the effectiveness of social robot assisted music courses based on speech sensing and deep learning algorithms, demonstrating its advantages in improving student learning effectiveness and engagement.}
}
@article{NARASIMHAN197879,
title = {Modelling behaviour: the need for a computational approach},
journal = {Journal of Social and Biological Structures},
volume = {1},
number = {1},
pages = {79-94},
year = {1978},
issn = {0140-1750},
doi = {https://doi.org/10.1016/0140-1750(78)90020-9},
url = {https://www.sciencedirect.com/science/article/pii/0140175078900209},
author = {R. Narasimhan},
abstract = {The principal objective of this paper is to argue the thesis that a science concerned with the study of behaviour requires the computational approach in a serious way for its theoretical advancement. It is pointed out that modelling behaviour requires the articulation of explanations at three levels. The methodology of computational simulations is indispensable to articulating explanations at the first level which underlie explanations at the other two levels. The paper contrasts the currently fashionable approaches in artificial intelligence studies to the kind of constraints viable behavioural models must satisfy. Teachability and open-endedness are two of the essential characteristics of organisms that any satisfactory model must have. It is argued that analogy-based computational techniques and paradigmatic learning/teaching techniques are two modelling aspects that require imaginative study.}
}
@article{LIM2024127512,
title = {Progressive expansion: Cost-efficient medical image analysis model with reversed once-for-all network training paradigm},
journal = {Neurocomputing},
volume = {581},
pages = {127512},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.127512},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224002832},
author = {Shin Wei Lim and Chee Seng Chan and Erma Rahayu {Mohd Faizal} and Kok Howg Ewe},
keywords = {Medical image analysis, Machine learning, Model optimization, Cost-effective model},
abstract = {Low computational cost artificial intelligence (AI) models are vital in promoting the accessibility of real-time medical services in underdeveloped areas. The recent Once-For-All (OFA) network (without retraining) can directly produce a set of sub-network designs with Progressive Shrinking (PS) algorithm; however, the training resource and time inefficiency downfalls are apparent in this method. In this paper, we propose a new OFA training algorithm, namely the Progressive Expansion (ProX) to train the medical image analysis model. It is a reversed paradigm to PS, where technically we train the OFA network from the minimum configuration and gradually expand the training to support larger configurations. Empirical results showed that the proposed paradigm could reduce training time up to 68%; while still being able to produce sub-networks that have either similar or better accuracy compared to those trained with OFA-PS on ROCT (classification), BRATS and Hippocampus (3D-segmentation) public medical datasets. The code implementation for this paper is accessible at: https://github.com/shin-wl/ProX-OFA.}
}
@article{VARAS2023101289,
title = {Teachers’ strategies and challenges in teaching 21st century skills: Little common understanding},
journal = {Thinking Skills and Creativity},
volume = {48},
pages = {101289},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101289},
url = {https://www.sciencedirect.com/science/article/pii/S1871187123000597},
author = {Diego Varas and Macarena Santana and Miguel Nussbaum and Susana Claro and Patricia Imbarack},
keywords = {21st century skills, In-service teacher perceptions, Teacher education, Teaching practice, 6 Cs, 4 Cs},
abstract = {Faced with a world of accelerating change and rapidly-evolving technology, education systems must provide students with the skills they need to succeed in the 21st century. However, many countries have failed to incorporate the teaching of these skills within their schools. Our study therefore looks to portray teachers' understanding, strategies and obstacles in teaching these skills across Latin American classrooms. To do so, we analyzed the responses to an online survey from 1391 active teachers across 20 countries in the region. This revealed varying understandings of 21st century skills, with little common understanding. Most teachers failed to mention the skills included in the most popular framework (the 4 Cs); those who did reported using the same strategies, regardless of the skill being taught. These strategies included project-based learning, oracy activities, literacy strategies, and teamwork. We conclude that there is little or no common understanding around these skills, nor the best strategies for developing them. Our study helps understand the potential causes preventing the teaching of these skills in the classroom, a problem that extends beyond Latin America.}
}
@article{DAVID2022132522,
title = {Integrating fourth industrial revolution (4IR) technologies into the water, energy & food nexus for sustainable security: A bibliometric analysis},
journal = {Journal of Cleaner Production},
volume = {363},
pages = {132522},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.132522},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622021230},
author = {Love O. David and Nnamdi I. Nwulu and Clinton O. Aigbavboa and Omoseni O. Adepoju},
keywords = {WEF Nexus, Fourth industrial revolution, Industry 4.0, Cleaner production, Internet of things (IoT)},
abstract = {The technologies of the fourth Industrial Revolution (4IR/Industry 4.0) have been a technological catalyst for all fields of human endeavor, permeating the water, energy, and food (WEF) nexus. However, there is no empirical evidence of the extent of applications and the permeability level in ensuring the three resources’ security. This study explored the relationship of the fourth industrial revolution technologies and the water, energy, and food nexus by evaluating the applications of the various technologies of 4IR on WEF nexus and examined the effect of 4IR on WEF nexus. The objectives were achieved using the qualitative methodology and bibliometric analysis of content analysis. The result showed that most fourth industrial revolution technologies had not been integrated with the WEF nexus. The result showed that only the Internet of Things (IoT) and Big Data analytics had permeated the nexus, which shows that data of the resources will be the foundation of the nexus. The systematic collection, accuracy of data, and empirical analysis of data will determine the level of security of WEF nexus. The qualitative results show that there are applications of the fourth industrial revolution technologies to the individual sectors of the nexus, birthing Water 4.0, Energy 4.0, and Food 4.0. The Bibliometric analysis result shows that the integration of the fourth industrial revolution with the WEF nexus will lead to cleaner production practices relating to the technological processes of water, energy, and food resources. These practices will ensure the environment's safety from WEF wastes and the water, energy, and food security in production processes. The empirical research and bibliometric analysis result, rooted in the concept of cleaner production, shows that the fourth industrial revolution affected the WEF nexus. The effects are; the birth of clean technologies & industrial applications, the catalyst for sustainability security of WEF nexus leveraging on life cycle thinking, enablement of technological transfer, enhancement of economic growth, and urban planning. The study concludes that the fourth industrial revolution technologies affect WEF nexus, ensuring the popularization of cleaner production strategies and processes of the resources during trade-offs and synergies. The study recommends the integration of a cleaner production concept in WEF processing. It should follow the innovation diffusion theory (IDT) and Technology acceptance theory (TAM) when applying 4IR technologies to the nexus of water, energy, and food resources, for their sustainable security.}
}
@article{20241231,
title = {In This Issue},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {9},
number = {12},
pages = {1231},
year = {2024},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2024.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S2451902224003070}
}
@article{BARKE2023618,
title = {Linking life cycle sustainability assessment and the sustainable development goals – Calculation of goal achievement},
journal = {Procedia CIRP},
volume = {116},
pages = {618-623},
year = {2023},
note = {30th CIRP Life Cycle Engineering Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.02.104},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123000999},
author = {Alexander Barke and Manbir S. Sodhi and Christian Thies and Thomas S. Spengler},
keywords = {Sustainable development goals (SDGs), SDG quantification, Sustainable development, Life cycle sustainability assessment},
abstract = {In 2015, the United Nations General Assembly proposed seventeen Sustainable Development Goals (SDGs) intended to ensure sustainable development worldwide at the economic, environmental, and social levels. SDGs are now being used by some corporations in formulating and expressing business strategies. However, assessing the effects of corporate activities and products regarding their contribution to SDGs is difficult. In this paper, we have developed a method for linking life cycle sustainability assessment (LCSA) with the SDGs and calculating the contribution to SDG achievement. An essential part of this approach is the weighting of LCSA impact categories, which is typically done using equal weighting. This weighting method enables compensation of negative contributions by positive contributions in different impact categories but results in ambiguity in the results. This article identifies alternative weighting methods, integrates them into a computational approach, and determines their influence on the SDG contribution scores. The analysis shows that the use of alternative weights changes SDG contribution scores. However, the same product always has the highest SDG contribution score, regardless of the weighting method used. Nonetheless, the recommendations for action with regard to the total product alternatives would change depending on the weighting method.}
}
@article{RAMIREZPEDRAZA2020293,
title = {A bio-inspired model of behavior considering decision-making and planning, spatial attention and basic motor commands processes},
journal = {Cognitive Systems Research},
volume = {59},
pages = {293-303},
year = {2020},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2019.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S138904171930508X},
author = {Raymundo Ramirez-Pedraza and Natividad Vargas and Carlos Sandoval and Juan Luis {del Valle-Padilla} and Félix Ramos},
keywords = {Brain model, Decision-making, Planning, Spatial attention, Motor system, Goal-driven},
abstract = {Cognitive architectures (CA) are an IA approach to implement computer systems with human-like behavior. Fundamental exhibited human capabilities include planning and decision-making. In that regard, numerous AI systems successfully exhibit human-like behavior but are limited to either achieving specific objectives or are restrained to too heavily constrained environments, which makes them unsuitable in the presence of unforeseen situations where autonomy is required. To try to alleviate the problem, we present a bio-inspired computational model to solve the autonomous navigation problem of a computational entity in a controlled context. This proposal is the result of the interaction between planning and decision-making, spatial attention and the motor cognitive functions. The proposed model is based on neuroscientific evidence concerning the involved cognitive functions and is part of a more general cognitive architecture. In the case study developed to validate our idea, we can see that the processes previously identified play an important role to accomplish spatial navigation. In the case study presented, an agent achieves the navigation over an unexplored maze from an initial to a final position successfully. The reunited results motivate us to continue improving our model considering attentional information to influence the agent’s motor behavior.}
}
@incollection{KOKINOV200699,
title = {Chapter 4 A Cognitive Approach to Context Effects on Individual Decision Making under Risk},
editor = {Richard Topol and Bernard Walliser},
series = {Contributions to Economic Analysis},
publisher = {Elsevier},
volume = {280},
pages = {99-116},
year = {2006},
booktitle = {Cognitive Economics},
issn = {0573-8555},
doi = {https://doi.org/10.1016/S0573-8555(06)80005-2},
url = {https://www.sciencedirect.com/science/article/pii/S0573855506800052},
author = {Boicho Kokinov and Daniela Raeva},
keywords = {choice under risk, computational models, context effects},
abstract = {This chapter compares and contrasts various approaches to understanding human decision making under risk, and is trying to formulate requirements for a cognitive economics theory of risky decision making. Then a first attempt is made to put forward such a theory by proposing a cognitive model JUDGEMAP based on the general cognitive architecture DUAL. This allows the model to be integrated with other cognitive processes such as perception, analogical reasoning, spreading activation memory retrieval, etc. The fact that all processes in DUAL are based on local computations and parallel processing allows for modelling the interplay between various cognitive processes during the decision-making process, in particular the model predicts that the unconscious and automatic process of spreading activation will influence the conscious process of argument building and comparison. This prediction is tested and confirmed by a psychological experiment that demonstrates that seemingly remote and irrelevant aspects of the environment can change the decision we make.}
}
@article{AYDIN2013173,
title = {A swarm intelligence based sample average approximation algorithm for the capacitated reliable facility location problem},
journal = {International Journal of Production Economics},
volume = {145},
number = {1},
pages = {173-183},
year = {2013},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2012.10.019},
url = {https://www.sciencedirect.com/science/article/pii/S0925527312004604},
author = {Nezir Aydin and Alper Murat},
keywords = {Reliable, Facility location, Stochastic programming, Sample average approximation, Swarm intelligence},
abstract = {We present a novel hybrid method, swarm intelligence based sample average approximation (SIBSAA), for solving the capacitated reliable facility location problem (CRFLP). The CRFLP extends the well-known capacitated fixed-cost facility problem by accounting for the unreliability of facilities. The standard SAA procedure, while effectively used in many applications, can lead to poor solution quality if the selected sample sizes are not sufficiently large. With larger sample sizes, however, the SAA method is not practical due to the significant computational effort required. The proposed SIBSAA method addresses this limitation by using smaller samples and repetitively applying the SAA method while injecting social learning in the solution process inspired by the swarm intelligence of particle swarm optimization. We report on experimental study results showing that the SIBSAA improves the computational efficiency significantly while attaining same or better solution quality than the SAA method.}
}
@article{LUND2012192,
title = {The economic crisis and sustainable development: The design of job creation strategies by use of concrete institutional economics},
journal = {Energy},
volume = {43},
number = {1},
pages = {192-200},
year = {2012},
note = {2nd International Meeting on Cleaner Combustion (CM0901-Detailed Chemical Models for Cleaner Combustion)},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2012.02.075},
url = {https://www.sciencedirect.com/science/article/pii/S0360544212001892},
author = {Henrik Lund and Frede Hvelplund},
keywords = {Sustainable energy planning, Energy and Job creation, Renewable energy and economic growth},
abstract = {This paper presents Concrete Institutional Economics as an economic paradigm to understand how the wish for sustainable energy in times of economic crisis can be used to generate jobs as well as economic growth. In most countries, including European countries, the USA and China, the implementation of sustainable energy solutions involves the replacement of imported fossil fuels by substantial investments in energy conservation and renewable energy (RE). In such situation, it becomes increasingly essential to develop economic thinking and economic models that can analyse the concrete institutions in which the market is embedded. This paper presents such tools and methodologies and applies them to the case of the Danish heating sector. The case shows how investments in decreasing fossil fuels and CO2 emissions can be made in a way in which they have a positive influence on job creation and economic development as well as public expenditures.}
}
@article{UMNEY2018201,
title = {Designing frames: The use of precedents in parliamentary debate},
journal = {Design Studies},
volume = {54},
pages = {201-218},
year = {2018},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2017.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X17300789},
author = {Darren Umney and Peter Lloyd},
keywords = {design process, design precedents, framing, discourse analysis, legislation},
abstract = {Using the naturally-occurring data of official UK Parliamentary transcripts for the development of a new high speed rail project, this paper takes one characteristic of the design process, the use of precedent, to explore how problems and solutions are framed during discussion. In contrast to accounts of reframing that describe one big insight changing the design process we show how one particular precedent allows a series of attempts at reframing to take place in discussion. We conclude by arguing that precedents enable a diffusion of semi-objective meaning in discussion, similar to a prototype in a more conventional design process. This contrasts with other types of discourse elements, such as storytelling, that function through the subjective accumulation of meaning.}
}
@article{ZHANG202240,
title = {FPFS: Filter-level pruning via distance weight measuring filter similarity},
journal = {Neurocomputing},
volume = {512},
pages = {40-51},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.09.049},
url = {https://www.sciencedirect.com/science/article/pii/S092523122201164X},
author = {Wei Zhang and Zhiming Wang},
keywords = {Model compression, Neural network pruning, Distance and similarity, Deep convolutional neural network (DCNN)},
abstract = {Deep Neural Networks (DNNs) enjoy the welfare of convolution, while also bearing huge computational pressure. Therefore, model compression techniques are used to alleviate this problem, where filter-based neural network has received extensive attention as the research object of this paper. Common approaches treat filters as independent individuals and choose retrained filters by evaluating their performance, while more complex macro methods consider relationship between filters. Therefore, we propose a facile distance-based filter selection method, called FPFS, to visualize the similarity between filters from a global perspective. We calculate and sum the distance between filters to get filters’ “Distance Weight” which is applied as a metric to assess filters. We use four common and appropriate distances for filters evaluation. To verify the performance of our algorithm, we introduce FPFS to classical DCNNs and test it on general classification datasets CIFAR-10, CIFAR-100 and mageNet. For example, FPFS reduces Parameters and FLOPs of the lightweight model DenseNet-40 to about half of the original while maintain accuracy on CIFAR-10 by 94.40% (the original model is 94.80%). To ResNet-56 on CIFAR-100, FPFS compresses FLOPs to less than half of the original, while model accuracy reaches 71.46% (the original model is 71.44%). About ResNet-50 on ImageNet, FPFS achieves 60.3% FLOPs pruning rate accompanied by 0.96% top-1 accuracy loss. We also compare the experimental results with state-of-the-art filter pruning algorithms to highlight the effectiveness of FPFS.}
}
@article{J2023105690,
title = {Deep learning based multi-labelled soil classification and empirical estimation toward sustainable agriculture},
journal = {Engineering Applications of Artificial Intelligence},
volume = {119},
pages = {105690},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2022.105690},
url = {https://www.sciencedirect.com/science/article/pii/S0952197622006807},
author = {Padmapriya J. and Sasilatha T.},
keywords = {Soil classification, Q-HOG, SVM, Deep Neural Network, VGG16},
abstract = {Agriculture is the underlying occupation of the vast people in India and it is a major economic contribution. Soil is prime for the vital nutrient supply to the crops and its yield. Determination of the type of soil which comprises of the clay, sand and silt particles in the respective proportion is indeed significant for the suitable crop selection and to identify the weeds growth. The most commonly utilized soil determination methods were International Pipette method and Pressure-plate apparatus method. In this research work, multiclass soil classification using machine learning and deep learning models for the appropriate determination of the soil type as Multi-Stacking ensemble model and a novel feature selection algorithm Q-HOG is proposed; since the Artificial Intelligence has led to furtherance in the smart agriculture. Besides, the images are collected from the exploration site vriddhachalam along with the soil datasets will increase the classification accuracy. The deep learning models Recurrent Neural Network(RNN), Long Short Term Memory(LSTM), Gated Recurrent Unit(GRU) and VGG16 are considered and the comprehensive evaluation of these different deep learning architectures and also the machine learning algorithms such as Naïve-bayes, KNN, SVM are carried out and the obtained results are tabulated. Multi-stacking ensemble model for multi-classification is proposed with the Machine learning and deep learning algorithms and evaluated the performance with increased computation time. Among these models the proposed model outperformed in soil classification in-terms of accuracy as 98.96 percent, achieved precision as 96.14 percent, recall as 99.65 percent and the achieved F1-Score is 97.87 percent.}
}
@article{SPIVEY2025149477,
title = {A linking hypothesis for eyetracking and mousetracking in the visual world paradigm},
journal = {Brain Research},
volume = {1851},
pages = {149477},
year = {2025},
issn = {0006-8993},
doi = {https://doi.org/10.1016/j.brainres.2025.149477},
url = {https://www.sciencedirect.com/science/article/pii/S0006899325000356},
author = {Michael J. Spivey},
keywords = {Psycholinguistics, Eyetracking, Mousetracking, Spoken word recognition, Action-perception cycle, Perception–action cycle, Dynamical systems, Embodied cognition},
abstract = {For a linking hypothesis in the visual world paradigm to clearly accommodate existing findings and make unambiguous predictions, it needs to be computationally implemented in a fashion that transparently draws the causal connection between the activations of internal representations and the measured output of saccades and reaching movements. Quantitatively implemented linking hypotheses provide an opportunity to not only demonstrate an existence proof of that causal connection but also to test the fidelity of the measuring methods themselves. When a system of interest is measured one way (e.g., ballistic dichotomous outputs) or another way (e.g., smooth graded outputs), the apparent results can differ substantially. What is needed is one linking hypothesis that can produce both types of outputs. The localist attractor network simulation of spoken word recognition demonstrated here recreates eye and mouse movements that capture key findings in the visual world paradigm, and especially relies on one particularly powerful theoretical construct: feedback from the action-perception cycle. Visual feedback from the eye position enhancing the cognitive prominence of the fixated object allows the simulation to fit a wider range of findings, and points to predictions for new experiments. When that feedback is absent, the linking hypothesis simulation no longer fits human data as well. Future experiments, and improvements of this network simulation, are discussed.}
}
@article{PATTEE20025,
title = {The origins of Michael Conrad's research programs (1964–1979)},
journal = {Biosystems},
volume = {64},
number = {1},
pages = {5-11},
year = {2002},
issn = {0303-2647},
doi = {https://doi.org/10.1016/S0303-2647(01)00169-1},
url = {https://www.sciencedirect.com/science/article/pii/S0303264701001691},
author = {H.H Pattee},
keywords = {Artificial ecosystems, Adaptability, Evolvability, Non-programmable computation, Brain-computer disanalogy, Enzymatic neurons},
abstract = {This paper summarizes Michael Conrad's academic and professional career from the time he began his Ph.D. studies in 1964 to his appointment at Wayne State University in 1979. It describes the origins of several of his major research interests and presents a personal evaluation of how this early work continues to be of fundamental importance.}
}
@article{ZHANG2024308,
title = {Empathetic Language in LLMs under Prompt Engineering: A Comparative Study in the Legal Field},
journal = {Procedia Computer Science},
volume = {244},
pages = {308-317},
year = {2024},
note = {6th International Conference on AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.204},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924030059},
author = {Yifan Zhang and Christopher Radishian and Sabine Brunswicker and Dan Whitenack and Daniel W. Linna},
keywords = {LLM, Human-AI Interaction, Empathetic Response},
abstract = {The demand for empathetic conversations increases with conversational AIs’ rise and exponentially spreading applications. In areas like law and healthcare, where professional and empathetic conversations are essential, conversational AIs must strive to retain the correctness of information and logic while improving on empathetic language use. When addressing such an issue, we focus on linguistic empathy, relating only to syntactic and rhetoric choices in language while disregarding the emotional aspect of influence. By performing this study, we are interested in finding whether current open-sourced Large Language Models (LLMs) can match human experts in the legal field by using empathetic language while not compromising facts and logic in responses. We compare responses from three open-sourced LLMs under four prompting strategies with the expert responses. In the comparison, we use metrics from three aspects: text and semantic similarity, factual consistency, and ten rules of linguistic empathy from previous research literature. After statistical tests, the comparison results show that language models can use empathetic language without compromising the default knowledge base of LLMs when properly prompt-engineered. To accomplish this, additional domain knowledge is still needed to match factually. The data supporting this study is publicly available at huggingface.co/datasets/RCODI/empathy-prompt and code is available at github.com/RCODI-ConversationalAI/Empathy-Prompt.}
}
@article{BAJAJ2021117750,
title = {Association between emotional intelligence and effective brain connectome: A large-scale spectral DCM study},
journal = {NeuroImage},
volume = {229},
pages = {117750},
year = {2021},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2021.117750},
url = {https://www.sciencedirect.com/science/article/pii/S1053811921000276},
author = {Sahil Bajaj and William D.S. Killgore},
abstract = {Introduction
Emotional Intelligence (EI) is a well-documented aspect of social and interpersonal functioning, but the underlying neural mechanisms for this capacity remain poorly understood. Here we used advanced brain connectivity techniques to explore the associations between EI and effective connectivity (EC) within four functional brain networks.
Methods
The Mayer-Salovey-Caruso Emotional Intelligence Test (MSCEIT) was used to collect EI data from 55 healthy individuals (mean age = 30.56±8.3 years, 26 males). The MSCEIT comprises two area cores – experiential EI (T1) and strategic EI (T2). The T1 core included two sub-scales – perception of emotions (S1) and using emotions to facilitate thinking (S2), and the T2 core included two sub-scales – understanding of emotions (S3) and management of emotions (S4). All participants underwent structural and resting-state functional magnetic resonance imaging (rsfMRI) scans. The spectral dynamic causal modeling approach was implemented to estimate EC within four networks of interest – the default-mode network (DMN), dorsal attention network (DAN), control-execution network (CEN) and salience network (SN). The strength of EC within each network was correlated with the measures of EI, with correlations at pFDR < 0.05 considered as significant.
Results
There was no significant association between any of the measures of EI and EC strength within the DMN and DAN. For CEN, however, we found that there were significant negative associations between EC strength from the right anterior prefrontal cortex (RAPFC) to the left anterior prefrontal cortex (LAPFC) and both S2 and T1, and significant positive associations between EC strength from LAPFC to RAPFC and S2. EC strength from the right superior parietal cortex (SPC) to RAPFC also showed significant negative association with S4 and T2. For the SN, S3 showed significant negative association with EC strength from the right insula to RAPFC and significant positive association with EC strength from the left insula to dorsal anterior cingulate cortex (DACC).
Conclusions
We provide evidence that the negative ECs within the right hemisphere, and from the right to left hemisphere, and positive ECs within the left hemisphere and from the left to right hemisphere of CEN (involving bilateral frontal and right parietal region) and SN (involving right frontal, anterior cingulate and bilateral insula) play a significant role in regulating and processing emotions. These findings also suggest that measures of EC can be utilized as important biomarkers to better understand the underlying neural mechanisms of EI.}
}
@article{VANLAAR2025106259,
title = {Towards desirable futures for the circular adaptive reuse of buildings: A participatory approach},
journal = {Sustainable Cities and Society},
volume = {122},
pages = {106259},
year = {2025},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2025.106259},
url = {https://www.sciencedirect.com/science/article/pii/S2210670725001362},
author = {Brian {van Laar} and Angela Greco and Hilde Remøy and Vincent Gruis and Mohammad B. Hamida},
keywords = {Adaptive reuse, Scenario development, Cross-impact balance analysis, Participatory scenario workshops, Normative narrative scenarios, Circularity},
abstract = {Adaptive reuse of buildings offers a sustainable strategy for reducing global CO2 emissions by repurposing existing structures, conserving resources, reducing the need to extract new materials, and minimizing waste. However, the decision-making process in adaptive reuse projects is often complex, involving conflicting criteria and diverse stakeholders. Current approaches tend to polarize alternatives, focusing either on broad functional use or specific design options, which can limit decision effectiveness and quality. This study addresses these challenges by developing a participatory mixed-methods approach that integrates Cross-Impact Balance (CIB) analysis with creative scenario-building techniques, including generative AI and participatory workshops. This approach balances the extremes of current decision-making processes, offering a more comprehensive overview of desirable futures for decision-makers. The methodology was applied to create 15 “big picture” circular adaptive reuse scenarios, each incorporating circular building adaptability (CBA) strategies, and enriched with AI generated narratives and visualizations. These scenarios provide stakeholders with a nuanced understanding of potential future pathways, enhancing decision-making processes. This mixed-method approach demonstrates the potential of participatory CIB scenario development in advancing circularity, offering a valuable tool for navigating the complexities of adaptive reuse decision-making.}
}
@incollection{ROSCHELLE20071,
title = {Designing Networked Handheld Devices to Enhance School Learning},
editor = {Marvin V. Zelkowitz},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {70},
pages = {1-60},
year = {2007},
issn = {0065-2458},
doi = {https://doi.org/10.1016/S0065-2458(06)70001-8},
url = {https://www.sciencedirect.com/science/article/pii/S0065245806700018},
author = {Jeremy Roschelle and Charles Patton and Deborah Tatar},
abstract = {Handheld devices, especially networked handheld devices, are growing in importance in education, largely because their affordability and accessibility create an opportunity for educators to transition from occasional, supplemental use of computers, to frequent and integral use of portable computational technology. Why and how might these new devices enhance school learning? We begin by discussing a simple but important factor: networked handhelds can allow a 1:1 student:device ratio for the first time, enabling ready-at-hand access to technology throughout the school day and throughout the learner's personal life. We argue that designers need to understand the capabilities of the new generation of handheld computers and wireless networks that are most relevant for learning. We follow this with a discussion of Learning Science theories that connect those capabilities to enhanced learning. The capabilities and features feed into design practices. We describe a set of example applications that are arising from the capabilities, theories and design practices previously described. Finally, we close with a discussion of the challenge of scale.}
}
@incollection{SAHU2022127,
title = {Artificial Intelligence and Machine Learning: New Age Tools for Augmenting Plastic Materials Designing, Processing, and Manufacturing},
editor = {M.S.J. Hashmi},
booktitle = {Encyclopedia of Materials: Plastics and Polymers},
publisher = {Elsevier},
address = {Oxford},
pages = {127-152},
year = {2022},
isbn = {978-0-12-823291-0},
doi = {https://doi.org/10.1016/B978-0-12-820352-1.00108-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128203521001085},
author = {Kisor Kumar Sahu and Shibu Meher and Abhilash M. Menon and M.K. Sridhar and Gangala V. {Harsha Vardhan} and Saurabh Pandey and Ashutosh Kumar and Shreeja Das},
keywords = {Artificial intelligence (AI), Artificial neural network (ANN), Autoencoders, Deep learning, Machine learning (ML), Principal component analysis (PCA)},
abstract = {Plastic and polymers are late entrants in the repository of engineering materials, compared to bronze and iron (early phases of human civilizations are named after them). However, the extent of usage of the former is growing at an exponential rate because of the near-infinite combinatorial possibilities. In fact, there are hardly few engineering disciplines that can potentially offer so large and endless unexploited possibilities. Plastic industries are widespread across the world due to their easy scalability, favorable economics and extremely diverse applications. Plastic manufacturing and recycling are especially important for the economy of a country and provide livelihood for a large population. It is imperative that the current processes in plastic be improved upon by the advantages offered by computational tools and digital technologies. At this stage, we desperately need new age tools that can properly guide the human enterprise of innovation in designing, perfection in processing while maintaining stringent quality requirements in manufacturing. Artificial intelligence (AI) and machine learning (ML) perfectly fits this bill for the new age tools. We are at a very nascent stage of this AI/ML revolution. This article samples some of the pioneering works from the very discreet space of AI/ML applications in the field of plastic and polymer designing, processing and manufacturing and attempts to tie them up in a cohesive narrative. For the sake of completeness, applications of AI/ML for limiting the adverse environmental impact and future outlook have also been covered.}
}
@article{CAHILL20172131,
title = {Building a Community of Practice to Prepare the HPC Workforce},
journal = {Procedia Computer Science},
volume = {108},
pages = {2131-2140},
year = {2017},
note = {International Conference on Computational Science, ICCS 2017, 12-14 June 2017, Zurich, Switzerland},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.05.059},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917305902},
author = {Katharine J. Cahill and Scott Lathrop and Steven Gordon},
keywords = {HPC workforce, petascale computing, on-line education, graduate education, SPOC course},
abstract = {It has been well documented for more than 30 years, that significantly more effort is needed to prepare the HPC workforce needed today and well into the future. The Blue Waters Virtual School of Computational Science (VSCSE) provides an innovative model for addressing this critical need. The VSCSE uses a Small Private Online Course (SPOC) approach to providing graduate level credit courses to students at multiple institutions. In this paper, we describe the rationale for this approach, a description of the implementation, findings from external evaluations, and lessons learned. The paper concludes with recommendations for future strategies to build on this work to address the workforce needs of our global society.}
}
@article{HOLZINGER2025103032,
title = {Enhancing trust in automated 3D point cloud data interpretation through explainable counterfactuals},
journal = {Information Fusion},
volume = {119},
pages = {103032},
year = {2025},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2025.103032},
url = {https://www.sciencedirect.com/science/article/pii/S1566253525001058},
author = {Andreas Holzinger and Niko Lukač and Dzemail Rozajac and Emile Johnston and Veljka Kocic and Bernhard Hoerl and Christoph Gollob and Arne Nothdurft and Karl Stampfer and Stefan Schweng and Javier {Del Ser}},
keywords = {Explainable AI, Point cloud data, Counterfactual reasoning, Information fusion, Interpretability, Human-centered AI},
abstract = {This paper introduces a novel framework for augmenting explainability in the interpretation of point cloud data by fusing expert knowledge with counterfactual reasoning. Given the complexity and voluminous nature of point cloud datasets, derived predominantly from LiDAR and 3D scanning technologies, achieving interpretability remains a significant challenge, particularly in smart cities, smart agriculture, and smart forestry. This research posits that integrating expert knowledge with counterfactual explanations – speculative scenarios illustrating how altering input data points could lead to different outcomes – can significantly reduce the opacity of deep learning models processing point cloud data. The proposed optimization-driven framework utilizes expert-informed ad-hoc perturbation techniques to generate meaningful counterfactual scenarios when employing state-of-the-art deep learning architectures. The optimization process minimizes a multi-criteria objective comprising counterfactual metrics such as similarity, validity, and sparsity, which are specifically tailored for point cloud datasets. These metrics provide a quantitative lens for evaluating the interpretability of the counterfactuals. Furthermore, the proposed framework allows for the definition of explicit interpretable counterfactual perturbations at its core, thereby involving the audience of the model in the counterfactual generation pipeline and ultimately, improving their overall trust in the process. Results demonstrate a notable improvement in both the interpretability of the model’s decisions and the actionable insights delivered to end-users. Additionally, the study explores the role of counterfactual reasoning, coupled with expert input, in enhancing trustworthiness and enabling human-in-the-loop decision-making processes. By bridging the gap between complex data interpretations and user comprehension, this research advances the field of explainable AI, contributing to the development of transparent, accountable, and human-centered artificial intelligence systems.}
}
@article{MOHIT20221713,
title = {Approach of artificial intelligence for analysing properties of concrete},
journal = {Materials Today: Proceedings},
volume = {48},
pages = {1713-1717},
year = {2022},
note = {SCPINM-2021},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.10.028},
url = {https://www.sciencedirect.com/science/article/pii/S221478532106507X},
author = { Mohit and Balwinder Lallotra},
keywords = {Concrete, Artificial intelligence, Artificial neural network, Workability, Compressive strength},
abstract = {Technological progress is often measured by computation power. At the moment we are in the golden age where we are blessed with a perfect trio, machine learning algorithms, huge datasets across disciplines, and processing hardware. The constant desire to understand the human brain has led us to try mimicking it, thus forming the basis of neural networks creating a way for deep learning algorithms. Such algorithms have proven to work on non-linear data sets effectively, generating results that could find patterns just like our brains. In this paper, we explore a recently rising application for Neural Network frameworks; in particular, concrete in basic designing. We design and implement tests to analyze various properties of concrete of different concrete mixes. Customarily, the ability of concrete to perform is influenced by numerous non-straight factors, and testing its quality includes the destructive procedure of concrete samples.}
}
@incollection{SADEGHI2024457,
title = {Chapter Thirteen - Dynamic framework for large-scale modeling of membranes and peripheral proteins},
editor = {Markus Deserno and Tobias Baumgart},
series = {Methods in Enzymology},
publisher = {Academic Press},
volume = {701},
pages = {457-514},
year = {2024},
booktitle = {Biophysical Approaches for the Study of Membrane Structure—Part B: Theory and Simulations},
issn = {0076-6879},
doi = {https://doi.org/10.1016/bs.mie.2024.03.018},
url = {https://www.sciencedirect.com/science/article/pii/S0076687924001137},
author = {Mohsen Sadeghi and David Rosenberger},
keywords = {Membranes, Mesoscopic modeling, Particle-based modeling, Hydrodynamics, Membrane-protein interaction},
abstract = {In this chapter, we present a novel computational framework to study the dynamic behavior of extensive membrane systems, potentially in interaction with peripheral proteins, as an alternative to conventional simulation methods. The framework effectively describes the complex dynamics in protein-membrane systems in a mesoscopic particle-based setup. Furthermore, leveraging the hydrodynamic coupling between the membrane and its surrounding solvent, the coarse-grained model grounds its dynamics in macroscopic kinetic properties such as viscosity and diffusion coefficients, marrying the advantages of continuum- and particle-based approaches. We introduce the theoretical background and the parameter-space optimization method in a step-by-step fashion, present the hydrodynamic coupling method in detail, and demonstrate the application of the model at each stage through illuminating examples. We believe this modeling framework to hold great potential for simulating membrane and protein systems at biological spatiotemporal scales, and offer substantial flexibility for further development and parametrization.}
}
@article{COLOM2017385,
title = {Collaborative building of behavioural models based on internet of things},
journal = {Computers & Electrical Engineering},
volume = {58},
pages = {385-396},
year = {2017},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2016.08.019},
url = {https://www.sciencedirect.com/science/article/pii/S0045790616302191},
author = {José Francisco Colom and Higinio Mora and David Gil and María Teresa Signes-Pont},
keywords = {Social internet of things, Big data, Embedded systems, Healthcare, Distributed system framework},
abstract = {This paper proposes a new framework that takes advantage of the computing capabilities provided by the Internet of Thing (IoT) paradigm in order to support collaborative applications. It looks at the requirements needed to run a wide range of computing tasks on a set of devices in the user environment with limited computing resources. This approach contributes to building the social dimension of the IoT by enabling the addition of computing resources accessible to the user without harming the other activities for which the IoT devices are intended. The framework mainly includes a model of the computing load, a scheduling mechanism and a handover procedure for transferring tasks between available devices. The experiments show the feasibility of the approach and compare different implementation alternatives.}
}
@article{ZAK2019139,
title = {Multiple Criteria Optimization of the Carpooling Problem},
journal = {Transportation Research Procedia},
volume = {37},
pages = {139-146},
year = {2019},
note = {21st EURO Working Group on Transportation Meeting, EWGT 2018, 17th – 19th September 2018, Braunschweig, Germany},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2018.12.176},
url = {https://www.sciencedirect.com/science/article/pii/S235214651830591X},
author = {Jacek Żak and Maciej Hojda and Grzegorz Filcek},
keywords = {carpooling, multiple criteria optimisation, NSGA II, LBS},
abstract = {The paper presents a multiple criteria (MC) formulation of the carpooling optimization (CO) problem and a solution procedure that allows to solve it. The mathematical model of the MCCO problem includes two major sub-problems, such as planning of the routes and matching carpoolers (drivers and passengers). Different aspects, including: economic, social, technical and market-oriented are considered. The MCCO problem is solved with the application of an original computational procedure based on the multiple criteria genetic algorithm, called NSGA II and the solutions’ analysis and review technique, called Light Beam Search (LBS) method. The former method allows to generate a set of Pareto optimal solutions, while the latter assists the carpoolers in finding the most desired compromise solution (common route and match between carpoolers). The results of computational experiments are presented. We find that solving the formulating carpooling problem in a heuristic manner is possible in reasonable time}
}
@article{COHEN2017208,
title = {Where Does EEG Come From and What Does It Mean?},
journal = {Trends in Neurosciences},
volume = {40},
number = {4},
pages = {208-218},
year = {2017},
issn = {0166-2236},
doi = {https://doi.org/10.1016/j.tins.2017.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0166223617300243},
author = {Michael X Cohen},
keywords = {EEG, neural microcircuit, oscillations, electrophysiology, computation},
abstract = {Electroencephalography (EEG) has been instrumental in making discoveries about cognition, brain function, and dysfunction. However, where do EEG signals come from and what do they mean? The purpose of this paper is to argue that we know shockingly little about the answer to this question, to highlight what we do know, how important the answers are, and how modern neuroscience technologies that allow us to measure and manipulate neural circuits with high spatiotemporal accuracy might finally bring us some answers. Neural oscillations are perhaps the best feature of EEG to use as anchors because oscillations are observed and are studied at multiple spatiotemporal scales of the brain, in multiple species, and are widely implicated in cognition and in neural computations.}
}
@article{STRACHANREGAN2024e28340,
title = {The impact of room shape on affective states, heartrate, and creative output},
journal = {Heliyon},
volume = {10},
number = {6},
pages = {e28340},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e28340},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024043718},
author = {K. Strachan-Regan and O. Baumann},
keywords = {Built environment, Neuroarchitecture, Environmental psychology, Emotion, Creativity},
abstract = {The architectural design of space can deeply impact an individuals' mood, physiology, and mental health. While previous research has predominantly focused on elements like nature and lighting within architectural spaces, there is a growing literature base that also investigates the psychological and neurophysiological impacts of geometrical properties of architectural spaces. Employing virtual reality technology, the study sought to investigate the effects of curved and rectangular architectural spaces on affective states, heart rate, and creativity. A total of 35 participants were exposed to two distinct virtual environments: a curved room and a rectangular room. Participants' self-reported mood was assessed using the Positive and Negative Affect Schedule (PANAS-Long Form). Heart rate was monitored using a pulse oximeter, and creative output was evaluated using the Guilford Alternative Uses Task (GAUT). Statistical comparisons between the two room types indicated that participants experienced higher positive affect and lower negative affect in the curved room condition compared to the rectangular room condition. Furthermore, heart rate measurements revealed lower physiological arousal in the curved room. Additionally, participants exhibited higher creative output in the curved room as opposed to the rectangular room. These findings align with previous literature on the influence of geometric factors on affective responses. The implications of this study are significant as they pertain to individuals' daily environments and their impact on health and well-being. The positive influence of curved room geometry on mood, arousal, and creativity emphasises the importance of considering room layout and design in various settings, such as workplaces and educational environments. Architects and designers can utilise these findings to inform their decisions and promote neuroarchitecture that enhances positive emotional experiences and productivity.}
}
@article{PANG2024121485,
title = {A concept lattice-based expert opinion aggregation method for multi-attribute group decision-making with linguistic information},
journal = {Expert Systems with Applications},
volume = {237},
pages = {121485},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121485},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423019875},
author = {Kuo Pang and Luis Martínez and Nan Li and Jun Liu and Li Zou and Mingyu Lu},
keywords = {Concept lattice, Linguistic truth-valued lattice implication algebra, Linguistic information processing, Multi-attribute group decision-making},
abstract = {During the multi-attribute group decision-making (MAGDM) processing, the individuals often hold different opinions about the alternatives. It is necessary to aggregate the different individual opinions into a unified group opinion. In the real world, experts sometimes use linguistic expressions to evaluate attributes in uncertain environments. To address the problem of reducing the information loss of expert opinion aggregation in MAGDM, this paper proposes a MAGDM approach based on linguistic concept lattices in the context of uncertain linguistic expression. A linguistic concept lattice for multi-expert linguistic formal context is first constructed based on linguistic truth-valued lattice implication algebra, which can express both comparable and incomparable linguistic information in the decision-making process. Different expert opinions are aggregated via the extent of fuzzy linguistic concepts, which can reduce information loss in the aggregation process. Second, meet-irreducible elements in the linguistic concept lattice are introduced to reduce the computational complexity of obtaining all fuzzy linguistic concepts in the decision-making process. the distance between the intents of different fuzzy linguistic concepts is considered to enhance the rationality of linguistic decision results. In addition, the expert’s decision-making process for each alternative is visualized via linguistic concept lattices. Finally, the case study and comparative analysis illustrate the validity and rationality of the proposed approach in MAGDM with linguistic information.}
}
@article{TERZOPOULOU2024104133,
title = {Iterative voting with partial preferences},
journal = {Artificial Intelligence},
volume = {332},
pages = {104133},
year = {2024},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2024.104133},
url = {https://www.sciencedirect.com/science/article/pii/S0004370224000699},
author = {Zoi Terzopoulou and Panagiotis Terzopoulos and Ulle Endriss},
keywords = {Social choice theory, Iterative voting, Partial preferences},
abstract = {Voting platforms can offer participants the option to sequentially modify their preferences, whenever they have a reason to do so. But such iterative voting may never converge, meaning that a state where all agents are happy with their submitted preferences may never be reached. This problem has received increasing attention within the area of computational social choice. Yet, the relevant literature hinges on the rather stringent assumption that the agents are able to rank all alternatives they are presented with, i.e., that they hold preferences that are linear orders. We relax this assumption and investigate iterative voting under partial preferences. To that end, we define and study two families of rules that extend the well-known k-approval rules in the standard voting framework. Although we show that for none of these rules convergence is guaranteed in general, we also are able to identify natural conditions under which such guarantees can be given. Finally, we conduct simulation experiments to test the practical implications of our results.}
}
@article{WU20183,
title = {The five key questions of human performance modeling},
journal = {International Journal of Industrial Ergonomics},
volume = {63},
pages = {3-6},
year = {2018},
note = {Human Performance Modeling},
issn = {0169-8141},
doi = {https://doi.org/10.1016/j.ergon.2016.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0169814116300427},
author = {Changxu Wu},
keywords = {Human performance modeling},
abstract = {Via building computational (typically mathematical and computer simulation) models, human performance modeling (HPM) quantifies, predicts, and maximizes human performance, human-machine system productivity and safety. This paper describes and summarizes the five key questions of human performance modeling: 1) Why we build models of human performance; 2) What the expectations of a good human performance model are; 3) What the procedures and requirements in building and verifying a human performance model are; 4) How we integrate a human performance model with system design; and 5) What the possible future directions of human performance modeling research are. Recent and classic HPM findings are addressed in the five questions to provide new thinking in HPM's motivations, expectations, procedures, system integration and future directions.}
}
@article{KALLEYA2024147,
title = {An innovative artificial intelligence-based visualization and rendering for e-commerce startup booth design},
journal = {Procedia Computer Science},
volume = {245},
pages = {147-154},
year = {2024},
note = {9th International Conference on Computer Science and Computational Intelligence 2024 (ICCSCI 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.238},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924030461},
author = {Calista Kalleya and Andi Pramono and Chelsea Putri Angelina and Wilbert Alvin Cuarista and Riefky Prabowo and Fairuz Iqbal Maulana},
keywords = {Interior Design, Artificial Intelligence, Prototype, Render, Start-up},
abstract = {Marketplace development is increasing, as is the number of people with active internet access. Unive is a startup that runs an e-commerce platform connecting Student Activity Units with consumers. Focuses on fostering student entrepreneurship through practical business experience. For its development, this startup needs an interior for business escalation. Designers need software integrated with artificial intelligence (AI) in interior design. This study explores the integration of AI-powered rendering tools in the design of e-commerce platforms, focusing on how these tools enhance the user experience and satisfaction. The research employs a mixed-method approach, combining qualitative and quantitative analyses. The author collected data through user surveys, expert interviews, and performance metrics of AI rendering tools. The author based the evaluation criteria on adherence to interior design principles and the quality of rendering results. Findings indicate that selecting furniture and supporting components that adhere to interior design principles results in optimal rendering outcomes. These outcomes are close to user expectations, significantly improving user engagement and satisfaction. Detailed analysis shows the effectiveness of AI rendering in various design scenarios. The discussion highlights the implications of integrating AI rendering tools in e-commerce platforms. It addresses the potential of these tools to revolutionize user interaction and provides insights into future developments and applications in the field. This study demonstrates the value of AI-powered rendering tools in enhancing e-commerce platform design. By adhering to interior design principles, these tools can achieve rendering results that meet user expectations, ultimately leading to improved user experiences.}
}
@article{CHEN2022105882,
title = {Neural connectome features of procrastination: Current progress and future direction},
journal = {Brain and Cognition},
volume = {161},
pages = {105882},
year = {2022},
issn = {0278-2626},
doi = {https://doi.org/10.1016/j.bandc.2022.105882},
url = {https://www.sciencedirect.com/science/article/pii/S0278262622000409},
author = {Zhiyi Chen and Tingyong Feng},
keywords = {Procrastination, Neural connectome, Self-control network, DLPFC},
abstract = {Procrastination refers to an irrationally delay for intended courses of action despite of anticipating a negative consequence due to this delay. Previous studies tried to reveal the neural substrates of procrastination in terms of connectome-based biomarkers. Based on this, we proposed a unified triple brain network model for procrastination and pinpointed out what challenges we are facing in understanding neural mechanism of procrastination. Specifically, based on neuroanatomical features, the unified triple brain network model proposed that connectome-based underpinning of procrastination could be ascribed to the abnormalities of self-control network (i.e., dorsolateral prefrontal cortex, DLPFC), emotion-regulation network (i.e., orbital frontal cortex, OFC), and episodic prospection network (i.e., para-hippocampus cortex, PHC). Moreover, based on the brain functional features, procrastination had been attributed to disruptive neural circuits on FPN (frontoparietal network)-SCN (subcortical network) and FPN-SAN (salience network), which led us to hypothesize the crucial roles of interplay between these networks on procrastination in unified triple brain network model. Despite of these findings, poor interpretability and computational model limited further understanding for procrastination from theoretical and neural perspectives. On balance, the current study provided an overview to show current progress on the connectome-based biomarkers for procrastination, and proposed the integrative neurocognitive model of procrastination.}
}
@article{BRADLEY2016400,
title = {Jet flame heights, lift-off distances, and mean flame surface density for extensive ranges of fuels and flow rates},
journal = {Combustion and Flame},
volume = {164},
pages = {400-409},
year = {2016},
issn = {0010-2180},
doi = {https://doi.org/10.1016/j.combustflame.2015.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0010218015003120},
author = {Derek Bradley and Philip H. Gaskell and Xiaojun Gu and Adriana Palacios},
keywords = {Jet flame height, Lift-off distance, Flamelet modelling, “Fracking”, Jet flame stability, Mean flame surface density},
abstract = {An extensive review and re-thinking of jet flame heights and structure, extending into the choked/supersonic regime is presented, with discussion of the limitations of previous flame height correlations. Completely new dimensionless correlations for the plume heights, lift-off distances, and mean flame surface densities of atmospheric jet flames, in the absence of a cross wind, are presented. It was found that the same flow rate parameter could be used to correlate both plume heights and flame lift-off distances. These are related to the flame structure, jet flame instability, and flame extinction stretch rates, as revealed by complementary experiments and computational studies. The correlations are based on a vast experimental data base, covering 880 flame heights. They encompass pool fires and flares, as well as choked and unchoked jet flames of CH4, C2H2, C2H4, C3H8, C4H10 and H2, over a wide range of conditions. Supply pressures range from 0.06 to 90 MPa, discharge diameters from 4 × 10−4 to 1.32 m, and flame heights from 0.08 to 110 m. The computational studies enabled reaction zone volumes to be estimated, as a proportion of the plume volumes, measured from flame photographs, and temperature contours. This enabled mean flame surface densities to be estimated, together with mean volumetric heat releases rates. There is evidence of a “saturation” mean surface density and increases in turbulent burn rates being accomplished by near pro rata increases in the overall volume of reacting mixture.}
}
@article{GOEL2023,
title = {Users’ Concerns About Endometriosis on Social Media: Sentiment Analysis and Topic Modeling Study},
journal = {Journal of Medical Internet Research},
volume = {25},
year = {2023},
issn = {1438-8871},
doi = {https://doi.org/10.2196/45381},
url = {https://www.sciencedirect.com/science/article/pii/S1438887123006179},
author = {Rahul Goel and Vijayachitra Modhukur and Katrin Täär and Andres Salumets and Rajesh Sharma and Maire Peters},
keywords = {endometriosis, latent Dirichlet allocation, pain, Reddit, sentiment analysis, social media, surgery, topic modeling, user engagement},
abstract = {Background
Endometriosis is a debilitating and difficult-to-diagnose gynecological disease. Owing to limited information and awareness, women often rely on social media platforms as a support system to engage in discussions regarding their disease-related concerns.
Objective
This study aimed to apply computational techniques to social media posts to identify discussion topics about endometriosis and to identify themes that require more attention from health care professionals and researchers. We also aimed to explore whether, amid the challenging nature of the disease, there are themes within the endometriosis community that gather posts with positive sentiments.
Methods
We retrospectively extracted posts from the subreddits r/Endo and r/endometriosis from January 2011 to April 2022. We analyzed 45,693 Reddit posts using sentiment analysis and topic modeling–based methods in machine learning.
Results
Since 2011, the number of posts and comments has increased steadily. The posts were categorized into 11 categories, and the highest number of posts were related to either asking for information (Question); sharing the experiences (Rant/Vent); or diagnosing and treating endometriosis, especially surgery (Surgery related). Sentiment analysis revealed that 92.09% (42,077/45,693) of posts were associated with negative sentiments, only 2.3% (1053/45,693) expressed positive feelings, and there were no categories with more positive than negative posts. Topic modeling revealed 27 major topics, and the most popular topics were Surgery, Questions/Advice, Diagnosis, and Pain. The Survey/Research topic, which brought together most research-related posts, was the last in terms of posts.
Conclusions
Our study shows that posts on social media platforms can provide insights into the concerns of women with endometriosis symptoms. The analysis of the posts confirmed that women with endometriosis have to face negative emotions and pain daily. The large number of posts related to asking questions shows that women do not receive sufficient information from physicians and need community support to cope with the disease. Health care professionals should pay more attention to the symptoms and diagnosis of endometriosis, discuss these topics with patients to reduce their dissatisfaction with doctors, and contribute more to the overall well-being of women with endometriosis. Researchers should also become more involved in social media and share new science-based knowledge regarding endometriosis.}
}
@article{MEDFORD201536,
title = {From the Sabatier principle to a predictive theory of transition-metal heterogeneous catalysis},
journal = {Journal of Catalysis},
volume = {328},
pages = {36-42},
year = {2015},
note = {Special Issue: The Impact of Haldor Topsøe on Catalysis},
issn = {0021-9517},
doi = {https://doi.org/10.1016/j.jcat.2014.12.033},
url = {https://www.sciencedirect.com/science/article/pii/S0021951714003686},
author = {Andrew J. Medford and Aleksandra Vojvodic and Jens S. Hummelshøj and Johannes Voss and Frank Abild-Pedersen and Felix Studt and Thomas Bligaard and Anders Nilsson and Jens K. Nørskov},
keywords = {Heterogeneous catalysis, Transition metals, Theory, Computational catalysis, DFT, Sabatier principle, Scaling relation, Descriptor},
abstract = {We discuss three concepts that have made it possible to develop a quantitative understanding of trends in transition-metal catalysis: scaling relations, activity maps, and the d-band model. Scaling relations are correlations between surface bond energies of different adsorbed species including transition states; they open the possibility of mapping the many parameters determining the rate of a full catalytic reaction onto a few descriptors. The resulting activity map can be viewed as a quantitative implementation of the classical Sabatier principle, which states that there is an optimum “bond strength” defining the best catalyst for a given reaction. In the modern version, the scaling relations determine the relevant “bond strengths” and the fact that these descriptors can be measured or calculated makes it a quantitative theory of catalysis that can be tested experimentally by making specific predictions of new catalysts. The quantitative aspect of the model therefore provides new possibilities in catalyst design. Finally, the d-band model provides an understanding of the scaling relations and variations in catalytic activity in terms of the electronic structure of the transition-metal surface.}
}
@article{TAMSTORF2013362,
title = {Discrete bending forces and their Jacobians},
journal = {Graphical Models},
volume = {75},
number = {6},
pages = {362-370},
year = {2013},
issn = {1524-0703},
doi = {https://doi.org/10.1016/j.gmod.2013.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S1524070313000209},
author = {Rasmus Tamstorf and Eitan Grinspun},
keywords = {Discrete shells, Hinge angle Hessian, Bending force Jacobians, Dihedral angle},
abstract = {Computation of bending forces on triangle meshes is required for numerous simulation and geometry processing applications. In particular it is a key component in cloth simulation. A common quantity in many bending models is the hinge angle between two adjacent triangles. This angle is straightforward to compute, and its gradient with respect to vertex positions (required for the forces) is easily found in the literature. However, the Hessian of the bend angle, which is required to compute the associated force Jacobians is not documented in the literature. Force Jacobians are required for efficient numerics (e.g., implicit time stepping, Newton-based energy minimization) and are thus highly desirable. Readily available computations of the force Jacobian, such as those produced by symbolic algebra systems, or by autodifferentiation codes, are expensive to compute and therefore less useful. We present compact, easily reproducible, closed form expressions for the Hessian of the bend angle. Compared to automatic differentiation, we measure up to 7× speedup for the evaluation of the bending forces and their Jacobians.}
}
@article{DRAWEL2017632,
title = {Reasoning about Trust and Time in a System of Agents},
journal = {Procedia Computer Science},
volume = {109},
pages = {632-639},
year = {2017},
note = {8th International Conference on Ambient Systems, Networks and Technologies, ANT-2017 and the 7th International Conference on Sustainable Energy Information Technology, SEIT 2017, 16-19 May 2017, Madeira, Portugal},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.05.369},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917310384},
author = {Nagat Drawel and Jamal Bentahar and Elhadi Shakshuki},
keywords = {Multi-Agent Systems (MASs), trust, temporal logic},
abstract = {Abstract:
The study of trust in Multi-Agent Systems (MASs) has been an area of interest for many researchers over the last years. This is due to the fact that trust is the basis for agent communication wherein entities have to operate in a dynamic and uncertain environment. Several approaches have been proposed to define logical semantics for trust in MASs. However, these approaches are limited to reason about trust based on the sole agents’ mental states. Therefore, this paper considers trust from a high-level abstraction based on the social correct behaviors of agents. Specifically, we propose a logical framework that allows us to reason about unconditional trust and time. In particular, we introduce a new logical language called Trust Computation Tree logic (TCTL) that extends the Computation Tree Logic (CTL) with a new modality to represent trust. We describe the semantics by extending the interpreted systems formalism and consider a set of reasoning rules along with proofs to support our logic. Finally, we evaluate our approach using a real-life case study in the e-business domain to explain our proposed logic in a practical application.}
}