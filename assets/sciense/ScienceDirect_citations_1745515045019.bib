@article{ALI2024101172,
title = {Physics-informed neural networks in groundwater flow modeling: Advantages and future directions},
journal = {Groundwater for Sustainable Development},
volume = {25},
pages = {101172},
year = {2024},
issn = {2352-801X},
doi = {https://doi.org/10.1016/j.gsd.2024.101172},
url = {https://www.sciencedirect.com/science/article/pii/S2352801X2400095X},
author = {Ahmed Shakir Ali Ali and Farhad Jazaei and T. Prabhakar Clement and Brian Waldron},
keywords = {Artificial intelligence, Physics-informed neural network, PINN, Groundwater modeling, MODFLOW},
abstract = {In recent years, there has been enormous development in soft computing, especially artificial intelligence (AI), which has developed robust methods for solving complex engineering problems. Researchers in the field of water resources engineering have applied these AI methods to solve a variety of hydrological problems. Despite their widespread use in the surface and atmospheric hydrology fields, groundwater hydrologists have not widely used AI methods in their routine field-scale modeling efforts. This is because AI models have been primarily considered black box models that lack physical meaning. Furthermore, using AI models to generate the space-time distribution of transient groundwater level variations is challenging and requires further flux balance and mass transport analyses. More recently, a new type of physics-informed neural network (PINN) model has been developed to address several limitations by integrating governing physics (groundwater flow equations) into the AI tools. This study presents the systematic advantages of the PINN algorithm for solving groundwater problems using a set of classic test problems. As discussed in detail in the article, these advantages and potentials are associated with the meshless nature of PINN, its continuous time and space dimensions, its independence from time-stepping and incremental marching in space, and its efficiency in running time. However, despite PINN's promising attributes, it is important to acknowledge its nascent stage of development and the inherent limitations of all neural network models, such as training challenges and hyperparameter selection. Thus, collaborative efforts between groundwater modelers and computer scientists are imperative to explore and exploit the full potential of PINN in tackling increasingly complex groundwater problems and nurturing PINN into a dependable modeling tool in industry and academia.}
}
@article{RONGHUA2024e27753,
title = {Improved ant colony optimization for safe path planning of AUV},
journal = {Heliyon},
volume = {10},
number = {7},
pages = {e27753},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e27753},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024037848},
author = {Meng Ronghua and Cheng Xinhao and Wu Zhengjia and  {Du xuan}},
keywords = {Improved ant colony optimization, Safety factors, Dam inspections},
abstract = {In order to address the autonomous underwater vehicle navigation challenge for dam inspections, with the goal of enabling safe inspections and reliable obstacle avoidance, an improved smooth Ant Colony Optimization algorithm is proposed for path planning. This improved algorithm would optimize the smoothness of the path besides the robustness, avoidance of local optima, and fast computation speed. To achieve the goal of reducing turning time and improving the directional effect of path selection, a corner-turning heuristic function is introduced. Experimental simulation results show that the improved algorithm performs best than other algorithms in terms of path smoothness and iteration stability in path planning.}
}
@article{BISWAS2008127,
title = {Towards an agent-oriented approach to conceptualization},
journal = {Applied Soft Computing},
volume = {8},
number = {1},
pages = {127-139},
year = {2008},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2006.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S1568494606001062},
author = {Pratik K. Biswas},
keywords = {Intelligent agents, Multi-agent systems, Agent-oriented software engineering, Agent-oriented thinking, Agent-oriented modeling, Extended agent model, Agent-oriented analysis, Agent-oriented design},
abstract = {Agent-oriented modeling provides a new technique for the conceptualization of agent-based systems. This paper extends and formalizes this agent-oriented modeling approach to the conceptualization process. It defines agent models and proposes a high-level methodology for agent-oriented analysis and design. It also includes analogies with the object-oriented and other existing agent-oriented methodologies, wherever applicable. The paper is concluded with a case study and an insight to future challenges.}
}
@incollection{SRIPRASADH202545,
title = {Chapter 3 - Review of existing neuromorphic systems},
editor = {Harish Garg and Jyotir {Moy Chatterjee} and R. Sujatha and Shatrughan Modi},
booktitle = {Primer to Neuromorphic Computing},
publisher = {Academic Press},
pages = {45-66},
year = {2025},
isbn = {978-0-443-21480-6},
doi = {https://doi.org/10.1016/B978-0-443-21480-6.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780443214806000018},
author = {K. Sriprasadh},
keywords = {Deep learning, Neural network, Machine learning, Expert systems},
abstract = {Computer systems try to run in a similar manner like human brain. Interfacing the human brain activity with the computer device and making it to learn by thinking as human is coined the name neuromorphic system. Basically, neuromorphic form of computation performance ideology was initiated from the mathematical analysis started from the year of 1936, by mathematician and computer scientist Alan Turing, who created an algorithm to perform mathematical equation or solve mathematical problems through a machine. In 1949, he published the paper in name of intelligent machinery. The machine was named after him as Turing machine, which solved mathematical equations. This format was compared with humans; comparatively, humans were able to perform better than the system. The proposed model was coined the name cognitive modeling machinery. This model was the first step made by humans to create system model like the human brain. In 1949, Canadian psychologist Donald Hebb identified a supportive model of neuroscience correlating synaptic plasticity and learning, connecting human brain activity with an algorithm. In the year of 1950, Turing tested his Turing machine, which rendered his results. Based on the result, US navy created the Perceptron and human brain activity was mapped up to the level. But total activity of human brain cannot map due to lack of technology support. From 1980, neuromorphic research was taken through by Caltech professor Carver Mead. He created a analog silicon retina model and cochlea in 1981. Mead identified and proposed that computers can perform every action that human nervous system is capable of doing. In 2013, Henry Markram launched a system HBP like the human brain form, capable of understanding the human brain activity up to 10years and tried apply this format in science and technology. Recently, the neuromorphic system relies on AI and machine learning models and tries to support humans in detecting and decision-making in some critical situations. Neuromorphic systems basically make the decision through fuzzy neural and deep learning inputs. In this chapter, a review is made on features of trending neuromorphic system, and the future of the neuromorphic system is analyzed. The readers will gain the knowledge about the features of the neuromorphic systems and get an idea how it could be developed for different applications similar to decision-making and as expert system model in the form of humanoid.}
}
@article{VISWAN2023102808,
title = {Understanding molecular signaling cascades in neural disease using multi-resolution models},
journal = {Current Opinion in Neurobiology},
volume = {83},
pages = {102808},
year = {2023},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2023.102808},
url = {https://www.sciencedirect.com/science/article/pii/S0959438823001332},
author = {Nisha Ann Viswan and Upinder Singh Bhalla},
abstract = {If the genome defines the program for the operations of a cell, signaling networks execute it. These cascades of chemical, cell-biological, structural, and trafficking events span milliseconds (e.g., synaptic release) to potentially a lifetime (e.g., stabilization of dendritic spines). In principle almost every aspect of neuronal function, particularly at the synapse, depends on signaling. Thus dysfunction of these cascades, whether through mutations, local dysregulation, or infection, leads to disease. The sheer complexity of these pathways is matched by the range of diseases and the diversity of their phenotypes. In this review, we discuss how to build computational models, how these models are essential to tackle this complexity, and the benefits of using families of models at different levels of detail to understand signaling in health and disease.}
}
@article{ADAMO2024109162,
title = {Crop planting layout optimization in sustainable agriculture: A constraint programming approach},
journal = {Computers and Electronics in Agriculture},
volume = {224},
pages = {109162},
year = {2024},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2024.109162},
url = {https://www.sciencedirect.com/science/article/pii/S0168169924005532},
author = {Tommaso Adamo and Lucio Colizzi and Giovanni Dimauro and Emanuela Guerriero and Deborah Pareo},
keywords = {Constraint programming, Optimization crop planting layout, AI planning, Smart Agriculture, Intercropping systems},
abstract = {In sustainable agriculture, intercropping systems represent a valuable approach. These systems involve placing mutually beneficial plant types in close proximity to each other, with the goal of exploiting biodiversity to reduce pesticide and water usage, as well as improve soil nutrient utilization. Despite its potential, the optimization of intercropping systems has received limited attention in previous studies. One of the first steps in the design of an intercropping system is the solution of the crop planting layout problem, which involves meeting crop demand while maximizing positive interactions between adjacent plants. We perform a complexity analysis of this problem and solve it through constraint programming, an artificial intelligence technique, which relies on automated reasoning, constraint propagation and search heuristics. To this aim, we present two constraint programming models based on integer variables and interval variables, respectively. Through a computational study on real-life instances, we examine the impact of different modelling approaches on the difficulty of solving the crop planting layout problem with standard constraint programming solvers. This research work has also provided the groundwork for a sowing robotic arm (under development), aiming to automate intercropping systems and assist farm workers.}
}
@article{ZHAO2024102465,
title = {GA-GGD: Improving semantic discriminability in graph contrastive learning via Generative Adversarial Network},
journal = {Information Fusion},
volume = {110},
pages = {102465},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102465},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524002434},
author = {Jitao Zhao and Dongxiao He and Meng Ge and Yongqi Huang and Lianze Shan and Yongbin Qin and Zhiyong Feng},
keywords = {Graph representation learning, Generative Adversarial Network, Graph contrastive learning, Adversarial Machine Learning, Semantic discriminability},
abstract = {Graph contrastive learning has garnered considerable research interest due to its ability to effectively embed graph data without manual labels. Among them, methods based on Deep Graph Infomax (DGI) have been widely studied and favored in the industry because of their fast training speed, applicability to large-scale data. DGI-based methods usually obtain a noise graph through node shuffling. The proxy task of these methods encourages the encoder to distinguish whether the nodes come from the original graph or the noise graph, thereby maximizing the mutual information between the node representation and the graph it belongs to, while also maximizing the Jenson–Shannon divergence between the nodes of original graph and noise graph. However, we argue that these approaches only enable the encoder to differentiate between semantically meaningful graphs and noise graphs, but not to effectively identify different semantic graphs. This leads to the inability of the encoder to effectively embed information between different semantics, significantly reducing the robustness and affecting the performance of downstream tasks. In addition, this training mode makes the model more sensitive to attacks. To improve their semantic discriminability, we take advantage of the natural ability of generative adversarial networks to generate semantic data, proposing a method called Generative Adversarial Graph Group Discrimination (GA-GGD). Specifically, it consists of a graph group discriminator and a semantic attack generator. The discriminator aims to encode the graph and identify whether nodes originate from the original graph. The goal of the generator is to use random features and graph structure to find vulnerabilities of discriminator and generate node representations with similar but wrong semantic to confuse the discriminator. GA-GGD can improve the model’s semantic information embedding without significantly increasing computational overhead and memory occupancy. We test the effectiveness of the proposed model on commonly used data sets and large-scale datasets, as well as in various downstream tasks such as classification, clustering, and adversarial attacks defence. A wealth of experimental results confirm the efficacy of the proposed model.}
}
@incollection{CHENG20251,
title = {Chapter 1 - A new milestone in artificial intelligence—ChatGPT},
editor = {Ge Cheng},
booktitle = {ChatGPT},
publisher = {Elsevier},
pages = {1-20},
year = {2025},
isbn = {978-0-443-27436-7},
doi = {https://doi.org/10.1016/B978-0-443-27436-7.00002-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443274367000023},
author = {Ge Cheng},
keywords = {ChatGPT, OpenAI, transformer model, natural language processing (NLP), artificial general intelligence (AGI), GPT series, human feedback reinforcement learning (HFRL), large language models (LLMs), computational power, model limitations},
abstract = {This chapter provides an in-depth exploration of the evolution, capabilities, and impact of ChatGPT, a groundbreaking artificial intelligence (AI) application developed by OpenAI. The chapter traces the development history of ChatGPT from its early predecessors like GPT-1 and GPT-2 to the more advanced GPT-3 and GPT-4 models. It highlights the technological advancements that have made ChatGPT a powerful tool capable of complex tasks such as language comprehension, code generation, and multimodal reasoning. The chapter also discusses the architecture underpinning large language models (LLMs), focusing on the transition from traditional natural language processing techniques to Transformer-based models. Additionally, it addresses the significant computational and data requirements for training these models, alongside challenges such as model interpretability, biases, and privacy concerns. The chapter concludes by examining the broader implications of LLMs across various sectors and anticipates future trends in AI development.}
}
@article{DRACK2011150,
title = {System approaches of Weiss and Bertalanffy and their relevance for systems biology today},
journal = {Seminars in Cancer Biology},
volume = {21},
number = {3},
pages = {150-155},
year = {2011},
note = {Why Systems Biology and Cancer?},
issn = {1044-579X},
doi = {https://doi.org/10.1016/j.semcancer.2011.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S1044579X11000307},
author = {Manfred Drack and Olaf Wolkenhauer},
keywords = {Paul Alfred Weiss, Ludwig von Bertalanffy, Organismic biology, System theory of life, Systems biology},
abstract = {System approaches in biology have a long history. We focus here on the thinking of Paul A. Weiss and Ludwig von Bertalanffy, who contributed a great deal towards making the system concept operable in biology in the early 20th century. To them, considering whole living systems, which includes their organisation or order, is equally important as the dynamics within systems and the interplay between different levels from molecules over cells to organisms. They also called for taking the intrinsic activity of living systems and the conservation of system states into account. We compare these notions with today's systems biology, which is often a bottom-up approach from molecular dynamics to cellular behaviour. We conclude that bringing together the early heuristics with recent formalisms and novel experimental set-ups can lead to fruitful results and understanding.}
}
@article{BUHLER1990577,
title = {The COIN model for concurrent computation and its implementation},
journal = {Microprocessing and Microprogramming},
volume = {30},
number = {1},
pages = {577-584},
year = {1990},
note = {Proceedings Euromicro 90: Hardware and Software in System Engineering},
issn = {0165-6074},
doi = {https://doi.org/10.1016/0165-6074(90)90302-P},
url = {https://www.sciencedirect.com/science/article/pii/016560749090302P},
author = {Peter Buhler},
abstract = {COIN is a model for object-oriented programming with special emphasis on concurrent and distributed systems. It was developed to integrate design, implementation, and visualization of distributed applications. The distinguishing characteristics of COIN are: a) hierarchial object structures; b) multiple explicit object interfaces; c) explicit and dynamic binding of interface operations to operation implementations; d) generation of structures consisting of several objects and their interconnections as an atomic action. The paper gives an overview of the COIN model and its implementation in the COIN/L programming language.}
}
@article{XHAXHIU2024270,
title = {Seaweed boards as value-added natural waste product for insulation and building materials},
journal = {Energy Storage and Saving},
volume = {3},
number = {4},
pages = {270-277},
year = {2024},
issn = {2772-6835},
doi = {https://doi.org/10.1016/j.enss.2024.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S2772683524000359},
author = {Kledi Xhaxhiu and Avni Berisha and Nensi Isak and Besnik Baraj and Adelaida Andoni},
keywords = {Seaweed, Natural waste, Waste recycling, Building material, Insulation, Thermal and mechanical properties calculations},
abstract = {Large amounts of seaweed are deposited on shores worldwide daily. The presence of this natural pollutant on the coast is not only considered an environmental burden but also often hinders the development of tourism in the affected areas. Depending on the beach surface area, local governments worldwide spend considerable portions of their budgets to remove seaweed from beaches. Moreover, the removed seaweed occupies increasing space in landfills where it is disposed. Seaweed is noncombustible and decomposes slowly over long periods. In this study, we consider the use of seaweed (a natural waste) as a value-added product for insulation and building materials. Seaweed (Posidonia Oceanica) boards with dimensions of 250 mm × 60 mm × 10 mm were obtained by pressing a mixture of processed seaweed and an organic binder. The as-prepared boards were analyzed for their physical–mechanical properties according to the British standards. The boards with a mean humidity level of 9.15% and density of 404.5 g·cm−3 demonstrated a maximum bending resistance of 2.72 × 103 N·m−2 and mean expansion upon water adsorption of ∼10% with regards to length and width and ∼30% with regards to height. The tested samples showed significant humidity resistance according to the boiling test and an average thermal conductivity of 0.047 W·m−1·K−1, which is comparable to that of polystyrene. Computational analysis of the “seaweed material” model revealed significant thermal and mechanical properties. The mechanical strength of the computed material, including its high Young’s and shear moduli, renders it a promising candidate in construction.}
}
@article{THANHEISER2024101176,
title = {Introduction to the virtual special issue: Mathematics that underpins social issues},
journal = {The Journal of Mathematical Behavior},
volume = {75},
pages = {101176},
year = {2024},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2024.101176},
url = {https://www.sciencedirect.com/science/article/pii/S0732312324000531},
author = {Eva Thanheiser and Ami Mamolo},
keywords = {Mathematics in Society, Educational Research, Social Issues, Mathematical Worldview, Numeracy, Mathematical Literacy},
abstract = {This Virtual Special Issue on Mathematics in Society: Exploring the Mathematics that Underpins Social Issues features 13 articles which expand our understanding of how people build, retain, communicate, apply, and comprehend mathematical ideas as they relate to social and societal issues. The focus is on education research that explores the ways in which mathematics and a mathematical worldview can influence choices, on educational, personal and societal levels. We take a broad view and raise questions about what it means to be mathematical in society, and we consider the multifaceted ways in which abilities to derive and interpret information presented mathematically are also necessary in and for society.}
}
@article{WU201655,
title = {Vertical position of Chinese power words influences power judgments: Evidence from spatial compatibility task and event-related Potentials},
journal = {International Journal of Psychophysiology},
volume = {102},
pages = {55-61},
year = {2016},
issn = {0167-8760},
doi = {https://doi.org/10.1016/j.ijpsycho.2016.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167876016300253},
author = {Xiangci Wu and Huibin Jia and Enguo Wang and Chenguang Du and Xianghua Wu and Caiping Dang},
keywords = {Conceptual representation, Metaphor, Vertical position, Power},
abstract = {The present study used event-related potentials (ERPs) to explore the influence of vertical position on power judgments. Participants were asked to identify whether a Chinese word represented a powerful or powerless group (e.g., “king” or “servant”), which was presented in the top or bottom of the screen. The behavioral analysis showed that judging the power of powerful words were significantly faster when they were presented at the top position, compared with when they were presented at the bottom position. The ERP analysis showed enhanced N1 amplitude for congruent trials (i.e., the powerful words in the top and the powerless words in the bottom of the screen) and larger P300 and LPC amplitude for incongruent trials (i.e., the powerful words in the bottom and the powerless words in the top of the screen). The present findings provide further electrophysiological evidence that thinking about power can automatically activate the underlying spatial up-down (verticality) image schema and that the influence of vertical position on the power judgments not only occurs at the early perceptual stage of power word processing, but also at the higher cognitive stage (i.e., allocation of attention resources, conflict solving and response selection). This study revealed the neural underpinnings of metaphor congruent effect which have great significance to our understanding of the abstract concept power.}
}
@article{LIU2025104441,
title = {Multi-TuneV: Fine-tuning the fusion of multiple modules for video action recognition},
journal = {Journal of Visual Communication and Image Representation},
volume = {109},
pages = {104441},
year = {2025},
issn = {1047-3203},
doi = {https://doi.org/10.1016/j.jvcir.2025.104441},
url = {https://www.sciencedirect.com/science/article/pii/S1047320325000550},
author = {Xinyuan Liu and Junyong Ye and Jingjing Wang and Guangyi Xu and Youwei Li and Chaoming Zheng},
keywords = {Multiple Fine-tuning, Vision Transformers, Video Action Recognition, Transfer Learning, Deep Learning},
abstract = {The current pre-trained models have achieved remarkable success, but they usually have complex structures and hundreds of millions of parameters, resulting in a huge computational resource requirement to train or fully fine-tune a pre-trained model, which limits its transfer learning on different tasks. In order to migrate pre-trained models to the field of Video Action Recognition (VAR), recent research uses parametric efficient transfer learning (PETL) approaches, while most of them are studied on a single fine-tuning module. For a complex task like VAR, a single fine-tuning method may not achieve optimal results. To address this challenge, we want to study the effect of joint fine-tuning with multiple modules, so we propose a method that merges multiple fine-tuning modules, namely Multi-TuneV. It combines five fine-tuning methods, including ST-Adapter, AdaptFormer, BitFit, VPT and LoRA. We design a particular architecture for Multi-TuneV and integrate it organically into the Video ViT model so that it can coordinate the multiple fine-tuning modules to extract features. Multi-TuneV enables pre-trained models to migrate to video classification tasks while maintaining improved accuracy and effectively limiting the number of tunable parameters, because it combines the advantages of five fine-tuning methods. We conduct extensive experiments with Multi-TuneV on three common video datasets, and show that it surpasses both full fine-tuning and other single fine-tuning methods. When only 18.7 % (16.09 M) of the full fine-tuning parameters are updated, the accuracy of Multi-TuneV on SSv2 and HMDB51 improve by 23.43 % and 16.46 % compared with the full fine-tuning strategy, and improve to 67.43 % and 75.84 %. This proves the effectiveness of joint multi-module fine-tuning. Multi-TuneV provides a new idea for PETL and a new perspective to address the challenge in video understanding tasks. Code is available at https://github.com/hhh123-1/Multi-TuneV.}
}
@article{GIANNAKOS201777,
title = {Entertainment, engagement, and education: Foundations and developments in digital and physical spaces to support learning through making},
journal = {Entertainment Computing},
volume = {21},
pages = {77-81},
year = {2017},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2017.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S1875952117300307},
author = {Michail N. Giannakos and Monica Divitini and Ole Sejer Iversen},
keywords = {Maker movement, Learning technologies, Entertainment technologies, Creativity, Knowledge construction, Technological fluency, Constructionist},
abstract = {Making is a relatively new concept applied to describe the increasing attention paid to constructing activities to enable entertaining, and engaging learning. Making focuses on the process that occurs in digital and/or physical spaces that is not always learning oriented, but enables qualities such as problem solving, design thinking, collaboration, and innovation, to name a few. Contemporary technical and infrastructural developments, such as Hackerspaces, Makerspaces, TechShops, and FabLabs, and the appearance of tools such as wearable computing, robotics, 3D printing, microprocessors, and intuitive programming languages, posit making as a very promising research area to support learning processes, especially towards the acquisition of 21st-century learning competences. Collecting learning evidence via rigorous multidimensional and multidisciplinary case studies will allow us to better understand and improve the value of making and the role of the various digital and physical spaces. Drawing from our experience with a recent workshop that used making as a pathway to foster joyful engagement and creativity in learning (Make2Learn), we present the developments, as well as the four selected contributions of this special issue. The paper further draws attention to the great potential and need for research in the area of making to enable entertaining, and engaging, and learning.}
}
@article{WANG2025111994,
title = {A lightweight progressive joint transfer ensemble network inspired by the Markov process for imbalanced mechanical fault diagnosis},
journal = {Mechanical Systems and Signal Processing},
volume = {224},
pages = {111994},
year = {2025},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2024.111994},
url = {https://www.sciencedirect.com/science/article/pii/S0888327024008926},
author = {Changdong Wang and Jingli Yang and Huamin Jie and Zhen Tao and Zhenyu Zhao},
keywords = {Class imbalance, Ensemble learning, Fault diagnosis, Markov process, Progressive joint-transfer strategy},
abstract = {Owing to safety limitations and data collection costs, scenarios with imbalanced data usually arise, posing a great challenge for precise fault diagnosis. Targeting imbalanced fault diagnosis and the high computational cost of mainstream ensemble learning methods currently used, this article proposes a lightweight and accurate scheme based on a progressive joint-transfer ensemble network (PJTEN) and a Markov-lightweight strategy (MLS). Specifically, a PJTEN is developed, incorporating a multiple excitation-channel attention basic estimator and progressive joint-transfer strategy (PJTS) to maintain diversity of basic estimators better and focus more on key information from minority classes. Besides, the MLS guided by Markov transition probabilities is for the first time constructed for ensemble learning to reduce the network redundancy by alternating optimization. Using a standard dataset and a brand-new dataset of a real ship propulsion system, the proposed method achieves leading results in Accuracy, F1 score and MCC, compared with eight cutting-edge methods, thereby validating its substantial value. In terms of lightweight operation, such as temporal complexity (TC), spatial complexity (SC), and time efficiency, it is also ahead of the latest ensemble-based methods.}
}
@article{EPIOTIS1989213,
title = {Lewis formulae for metallic systems: the Li tetramer paradigm},
journal = {Journal of Molecular Structure: THEOCHEM},
volume = {201},
pages = {213-238},
year = {1989},
issn = {0166-1280},
doi = {https://doi.org/10.1016/0166-1280(89)87077-0},
url = {https://www.sciencedirect.com/science/article/pii/0166128089870770},
author = {N.D. Epiotis},
abstract = {In previous works, we argued that metal atoms bind through a mechanism in which overlap is assisted by some other overlap-independent mode like dispersion or induction. The result is the formation of gas pairs (interstitial electron pairs). Unlike overlap, these mechanisms of bonding can be properly reproduced only at the MCSCF level. To draw the chemist away from one-electron thinking, we propose specific Lewis formulae for small metal clusters and we show how these change as we shift from a lower to a higher level of theory so as to project the key point. A qualitative (let alone quantitative) understanding of metallic bonding can only be achieved from examination of properly correlated wavefunctions. From the practical standpoint, we show how usage of these Lewis formulae can inspire analogies for explaining computational and experimental results.}
}
@article{ZHU2023110006,
title = {Deep reinforcement learning-based edge computing offloading algorithm for software-defined IoT},
journal = {Computer Networks},
volume = {235},
pages = {110006},
year = {2023},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2023.110006},
url = {https://www.sciencedirect.com/science/article/pii/S1389128623004516},
author = {Xiaojuan Zhu and Tianhao Zhang and Jinwei Zhang and Bao Zhao and Shunxiang Zhang and Cai Wu},
keywords = {Edge computing, Computing offloading, Software defined network, Internet of things, Deep reinforcement learning},
abstract = {Edge computing offloading can effectively solve the problem of insufficient computing resources for terminal devices and improve the performance and efficiency of the system. When network states and tasks change rapidly, data-driven intelligent algorithms have difficulty obtaining comprehensive statistics for accurate prediction, resulting in degraded performance of computational offloading and difficulty in adaptive adjustment. It is a current challenge to improve the environment-aware, intelligent optimization so that the computational offloading algorithm can adapt to the dynamic changes in network state and task demands, thus achieving global multi-objective optimization. This paper presents optimized edge computing offloading algorithm for software-defined IoT. First, to provide global state for making decisions, a software defined edge computing (SDEC) architecture is proposed. The edge layer is integrated into the control layer of software-defined IoT, and multiple controllers share the global network state information via east–west message exchange. Moreover, an edge computing offloading algorithm in software-defined IoT (ECO-SDIoT) based on deep reinforcement learning is proposed. It enables the controllers to offload the computing task to the most appropriate edge server according to the global states, task requirements, and reward. Finally, the performance metrics for edge computing offloading were evaluated in terms of unit task processing latency, load balancing of edge servers, task processing energy consumption, and task completion rate, respectively. Simulation results show that ECO-SDIoT can effectively reduce task completion time and energy consumption compared with other strategies.}
}
@article{BHADURI2025100723,
title = {Community partnership design of a maker-related camp for underserved youth: Impacts on youths’ present and future learning trajectories},
journal = {International Journal of Child-Computer Interaction},
volume = {44},
pages = {100723},
year = {2025},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2025.100723},
url = {https://www.sciencedirect.com/science/article/pii/S2212868925000030},
author = {Srinjita Bhaduri and Quentin Biddy and Melissa Rummel and Mimi Recker},
keywords = {Collaborative design, Maker technologies, 3D modeling and printing, Community partnership, Informal learning, Middle school youth STEM activities},
abstract = {This paper describes the design of Science, Technology, Engineering, and Mathematics (STEM) maker-related activities offered to middle school youth as part of a free, four-week summer camp. The camp was aimed at academically at-risk youth in a rural, tourism-oriented mountain community with significant income disparities. Guided by an educational model focused on enhancing youths’ present and future interests in and visions of STEM and computing fields, camp activities were collaboratively designed by a community partnership comprised of a local camp provider, the local school district, and researchers. Situating design in a community partnership helped highlight and integrate locally relevant resources, careers, and community opportunities. The paper also reports findings from a study examining how the STEM maker camp activities, which leveraged 3D modeling and printing practices, impacted youths’ perceptions of their disciplinary identity, engagement, and their present and future visions of the relevance of these STEM practices to themselves and their communities. The study also explores design tensions that emerged during the camp design process and identified barriers and opportunities that arose from balancing the needs of each partner, the research team’s focus on youth-centered learning, and the overall program goals.}
}
@article{DEVGUN2023141,
title = {Pre-cath Laboratory Planning for Left Atrial Appendage Occlusion – Optional or Essential?},
journal = {Cardiac Electrophysiology Clinics},
volume = {15},
number = {2},
pages = {141-150},
year = {2023},
note = {Left Atrial Appendage Occlusion},
issn = {1877-9182},
doi = {https://doi.org/10.1016/j.ccep.2023.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S1877918223000205},
author = {Jasneet Devgun and Tom {De Potter} and Davide Fabbricatore and Dee Dee Wang},
keywords = {Left atrial appendage occlusion, Left atrial appendage, Atrial fibrillation, Cardiac CT, 3D printing, Imaging, Structural heart disease}
}
@incollection{BILLEN2023385,
title = {Chapter 16 - Lithosphere–Mantle Interactions in Subduction Zones},
editor = {João C. Duarte},
booktitle = {Dynamics of Plate Tectonics and Mantle Convection},
publisher = {Elsevier},
pages = {385-405},
year = {2023},
isbn = {978-0-323-85733-8},
doi = {https://doi.org/10.1016/B978-0-323-85733-8.00014-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780323857338000147},
author = {Magali I. Billen},
keywords = {Subduction dynamics, Rheology, Phase transitions, Numerical modeling, Mantle mixing},
abstract = {How does the interaction of sinking lithosphere with the mantle contribute to the motion of tectonics plates at the Earth's surface and to long-term mixing in the deep mantle? In the decades immediately following the acceptance of the theory of plate tectonics, these questions were pursued vigorously using analytical, laboratory, and numerical models. In the past two decades, attention has turned to building on this foundational knowledge using numerical simulations to more fully integrate the complexity of Earth materials including the effects of deformation mechanisms, composition, fluids, melting, and phase transitions. This ongoing transition to a more system-centered view of geodynamics and plate tectonics not only presents many challenges (computational, experimental, and theoretical) but also promises to bridge the gaps in our current understanding and address the still enigmatic behavior of sinking lithosphere.}
}
@article{YANG2010209,
title = {Creativity of student information system projects: From the perspective of network embeddedness},
journal = {Computers & Education},
volume = {54},
number = {1},
pages = {209-221},
year = {2010},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2009.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0360131509001997},
author = {Heng-Li Yang and Hsiu-Hua Cheng},
keywords = {Project team creativity, Network embeddedness, Affiliation network, Innovation climate, Centrality},
abstract = {Many companies have pursued innovation to obtain a competitive edge. Thus, educational reform focuses mainly on training creative students. This study adopted the concept of an affiliated network of projects to investigate how project embeddedness influences project team creativity. This work surveys 60 projects in a Management Information Systems Department of a University. Validity of the specific study hypotheses is tested by using moderate hierarchical regression analysis to determine how project embeddedness affects project team creativity and assess how the team innovation climate moderates the relationships between project embeddedness and project team creativity. Analytical results indicate a positive association between structural embeddedness and project team creativity, a negative relationship between positional embeddedness and project team creativity, and a positive influence of team innovation climate on the relationships between network embeddedness and project team creativity. An attempt is also made to understand the role of positional embeddedness by classifying the interactions based on the content of interactions. According to those results, positional embeddedness is positively related to project team creativity during problem–identification interaction; during solution–design interaction, positional embeddedness is negatively related to project team creativity. Results of this study explain the phenomena of divergent thinking and convergent thinking during creative development.}
}
@article{SEDJELMACI2019101970,
title = {An efficient cyber defense framework for UAV-Edge computing network},
journal = {Ad Hoc Networks},
volume = {94},
pages = {101970},
year = {2019},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2019.101970},
url = {https://www.sciencedirect.com/science/article/pii/S1570870519302136},
author = {Hichem Sedjelmaci and Aymen Boudguiga and Inès Ben Jemaa and Sidi Mohammed Senouci},
keywords = {UAV-Edge computing, Detection, Stackelberg game, Energy consumption, Computation overhead},
abstract = {Mobile Edge Computing (MEC) is usually deployed in energy and delay constrained networks, such as internet of things networks and transportation systems to address the issues of energy consumption, computation capacity and network delay. In this work, we focus on a special case, which is Unmanned Aerial Vehicle Edge Computing (UEC) network. Addressing the security issues in UAV-Edge Computing network is mandatory due to the criticality of UEC services, such as network traffic monitoring, or search and rescue operations. However, cyber defense and protection of UEC network have not yet received sufficient research attention. Thereby, we propose and develop a cyber-defense solution based on a non-cooperative game to protect the UEC from network and offloading attacks, while taking into account nodes’ energy constraints and computation overhead. Simulation results show that, the deployment of our cyber defense system in UEC network requires low energy consumption and low computation overhead to obtain a high protection rate.}
}
@article{YONG2023e13529,
title = {Structure bionic topology design method based on biological unit cell},
journal = {Heliyon},
volume = {9},
number = {2},
pages = {e13529},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e13529},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023007363},
author = {Yang Yong and Jiang Xue-tao and Zhu Qi-xin and Lu En-hui and Dong Xin-feng and Li Jing-bin},
keywords = {Biological unit cell, Substructure, Matter element, TRIZ},
abstract = {The mechanical structure topology design based on substructure always adopts the traditional substructure design method, which often comes from the experience and is limited by the inherent or stereotyped design thinking. A substructure design method based on biological unit cell (UC) is proposed, which draws inspiration from the biological efficient load-bearing topology structure. Especially, the thought of the formalized problem-solving of extension matter-element is introduced. Through the matter-element definition of UC substructure, the process model for the structure bionic topology design method based on biological UC is formed, which avoids the random or wild mental stimulation of the structure topology design method based on traditional substructure. In particular, in this proposed method, aiming at the problem about how to achieve the integration of high-efficiency load-bearing advantage of different organisms, furthermore, a biological UC hybridization method based on the principle of inventive problem solving theory (TRIZ) is proposed. The typical case is used to illustrate the process of this method in detail. The results from simulations and experiments both show that: the load-bearing capacity of structure design based on biology UC is improved than the initial design; on this basis, the load-bearing capacity of structure design is improved further through UC hybridization. All these show the feasibility and correctness of the proposed method.}
}
@article{MURTAGH201637,
title = {Direct Reading Algorithm for Hierarchical Clustering},
journal = {Electronic Notes in Discrete Mathematics},
volume = {56},
pages = {37-42},
year = {2016},
note = {TCDM 2016 - 1st IMA Conference on Theoretical and Computational Discrete Mathematics, University of Derby},
issn = {1571-0653},
doi = {https://doi.org/10.1016/j.endm.2016.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S157106531630213X},
author = {Fionn Murtagh and Pedro Contreras},
keywords = {Analytics, hierarchical clustering, ultrametric topology, p-adic and m-adic number representation, linear time computational complexity},
abstract = {Reading the clusters from a data set such that the overall computational complexity is linear in both data dimensionality and in the number of data elements has been carried out through filtering the data in wavelet transform space. This objective is also carried out after an initial transforming of the data to a canonical order. Including high dimensional, high cardinality data, such a canonical order is provided by row and column permutations of the data matrix. In our recent work, we induce a hierarchical clustering from seriation through unidimensional representation of our observations. This linear time hierarchical classification is directly derived from the use of the Baire metric, which is simultaneously an ultrametric. In our previous work, the linear time construction of a hierarchical clustering is studied from the following viewpoint: representing the hierarchy initially in an m-adic, m = 10, tree representation, followed by decreasing m to smaller valued representations that include p-adic representations, where p is prime and m is a non-prime positive integer. This has the advantage of facilitating a more direct visualization and hence interpretation of the hierarchy. In this work we present further case studies and examples of how this approach is very advantageous for such an ultrametric topological data mapping.}
}
@article{OLADEJO2024111880,
title = {The Hiking Optimization Algorithm: A novel human-based metaheuristic approach},
journal = {Knowledge-Based Systems},
volume = {296},
pages = {111880},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111880},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124005148},
author = {Sunday O. Oladejo and Stephen O. Ekwe and Seyedali Mirjalili},
keywords = {Optimization, Metaheuristics, Hiking, Tobler’s Hiking function, Algorithm, Benchmark, Problem solving},
abstract = {In this paper, a novel metaheuristic called ‘The Hiking Optimization Algorithm’ (HOA) is proposed. HOA is inspired by hiking, a popular recreational activity, in recognition of the similarity between the search landscapes of optimization problems and the mountainous terrains traversed by hikers. HOA’s mathematical model is premised on Tobler’s Hiking Function (THF), which determines the walking velocity of hikers (i.e. agents) by considering the elevation of the terrain and the distance covered. THF is employed in determining hikers’ positions in the course of solving an optimization problem. HOA’s performance is demonstrated by benchmarking with 29 well-known test functions (including unimodal, multimodal, fixed-dimension multimodal, and composite functions), three engineering design problems (EDPs), (including I-beam, tension/compression spring, and gear train problems) and two N-P Hard problems (i.e. Traveling Salesman’s and Knapsack Problems). Moreover, HOA’s results are verified by comparison to 14 other metaheuristics, including Teaching Learning Based Optimization (TLBO), Genetic Algorithm (GA), Differential Evolution (DE), Particle Swarm Optimization, Grey Wolf Optimizer (GWO) as well as newly introduced algorithms such as Komodo Mlipir Algorithm (KMA), Quadratic Interpolation Optimization (QIO), and Coronavirus Optimization Algorithm (COVIDOA). In this study, we employ statistical tests such as the Wilcoxon rank sum, Friedman test, and Dunn’s post hoc test for the performance evaluation. HOA’s results are competitive and, in many instances, outperform the aforementioned well-known metaheuristics. The source codes of HOA and related metaheuristics can be accessed publicly via this link: https://github.com/DayoSun/The-Hiking-Optimization-Algorithm.}
}
@article{KACZYNSKA20214290,
title = {A new multi-criteria model for ranking chess players},
journal = {Procedia Computer Science},
volume = {192},
pages = {4290-4299},
year = {2021},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 25th International Conference KES2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.09.205},
url = {https://www.sciencedirect.com/science/article/pii/S187705092101944X},
author = {Aleksandra Kaczyńska and Joanna Kołodziejczyk and Wojciech Sałabun},
keywords = {Chess, MCDA, COMET, players evaluation, decision making},
abstract = {Chess is a very demanding sport as it requires advanced planning and strategic thinking skills. The degree of difficulty of the game also depends on the time allotted for a game, which can range from a few minutes to several tens of minutes. For this reason, the games are divided into several categories: standard, blitz, and bullet. However, as many chess players specialize in only some of the categories, it is difficult to determine the best chess player. It is very important to keep a proper ranking of the players. One way to recognize their achievements is the FIDE (Fédération Internationale des Échecs) titles awarded to the best players. However, there is still the problem of how to determine the best among the Grandmasters. There are many very talented players competing in chess. Creating a single ranking for all types of chess, regardless of the time allotted for the game, is a difficult challenge, as many undeniably outstanding chess players do not specialize in all types. Creating a ranking for only one type would not accurately describe the level of players. Therefore, a ranking was created based on all of them using the COMET method, which belongs to the multi-criteria decision-making methods (MCDA). It is based on fuzzy logic and uses characteristic objects for the assessment of alternatives, which guarantees immunity to the paradox of reversal rankings. Expert opinion was used for correct evaluation. This article presents the ranking of chess players regardless of the type of game they specialize in, to prove that it should be possible to identify the single best chess player.}
}
@article{IMM2012130,
title = {Talking mathematically: An analysis of discourse communities},
journal = {The Journal of Mathematical Behavior},
volume = {31},
number = {1},
pages = {130-148},
year = {2012},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2011.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S0732312311000630},
author = {Kara Imm and Despina A. Stylianou},
keywords = {Discourse, Cognitively demand tasks, Local authority},
abstract = {Discourse has always been at the heart of teaching. In more recent years, the mathematics education community has also turned its attention towards understanding the role of discourse in mathematics teaching and learning. Using earlier classifications of discourse, in this paper, we looked at three types of classrooms: classrooms that engage in high discourse, low discourse and a hybrid of the two. We aimed to understand how the elements of each discourse affected classroom learning, relationships between teachers and students, and participatory structures for students. Overall, our findings highlight the important relationship between cognitively demanding tasks and mathematical talk, and the power of discourse as a “thinking device” as opposed to mere conduit of knowledge. Our work also points to the under-theorized nature of hybrid discourse in mathematics classrooms, thereby providing some directions for pedagogy and further research.}
}
@article{REVACH2021103229,
title = {Expanding the discussion: Revision of the fundamental assumptions framing the study of the neural correlates of consciousness},
journal = {Consciousness and Cognition},
volume = {96},
pages = {103229},
year = {2021},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2021.103229},
url = {https://www.sciencedirect.com/science/article/pii/S1053810021001550},
author = {Daniel Revach and Moti Salti},
keywords = {Consciousness, Awareness, Conscious perception, Unconscious perception, Cognition, Neuroscience, Assumptions, Premises, Neurobiological, Mechanism, Phenomenology},
abstract = {The way one asks a question is shaped by a-priori assumptions and constrains the range of possible answers. We identify and test the assumptions underlying contemporary debates, models, and methodology in the study of the neural correlates of consciousness, which was framed by Crick and Koch’s seminal paper (1990). These premises create a sequential and passive conception of conscious perception: it is considered the product of resolved information processing by unconscious mechanisms, produced by a singular event in time and place representing the moment of entry. The conscious percept produced is then automatically retained to be utilized by post-conscious mechanisms. Major debates in the field, such as concern the moment of entry, the all-or-none vs graded nature, and report vs no-report paradigms, are driven by the consensus on these assumptions. We show how removing these assumptions can resolve some of the debates and challenges and prompt additional questions. The potential non-sequential nature of perception suggests new ways of thinking about consciousness as a dynamic and dispersed process, and in turn about the relationship between conscious and unconscious perception. Moreover, it allows us to present a parsimonious account for conscious perception while addressing more aspects of the phenomenon.}
}
@article{VANZUNDERT2010270,
title = {Effective peer assessment processes: Research findings and future directions},
journal = {Learning and Instruction},
volume = {20},
number = {4},
pages = {270-279},
year = {2010},
note = {Unravelling Peer Assessment},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2009.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0959475209000814},
author = {Marjo {van Zundert} and Dominique Sluijsmans and Jeroen {van Merriënboer}},
keywords = {Peer assessment, Development of peer assessment skills, Attitudes towards peer assessment, Training of peer assessment skills},
abstract = {Despite the popularity of peer assessment (PA), gaps in the literature make it difficult to describe exactly what constitutes effective PA. In a literature review, we divided PA into variables and then investigated their interrelatedness. We found that (a) PA's psychometric qualities are improved by the training and experience of peer assessors; (b) the development of domain-specific skills benefits from PA-based revision; (c) the development of PA skills benefits from training and is related to students' thinking style and academic achievement, and (d) student attitudes towards PA are positively influenced by training and experience. We conclude with recommendations for future research.}
}
@article{RADTKE2022102355,
title = {Smart energy systems beyond the age of COVID-19: Towards a new order of monitoring, disciplining and sanctioning energy behavior?},
journal = {Energy Research & Social Science},
volume = {84},
pages = {102355},
year = {2022},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2021.102355},
url = {https://www.sciencedirect.com/science/article/pii/S2214629621004461},
author = {Jörg Radtke},
keywords = {Smart city, Smart energy governmentality, Social power framework, Energy transition conflict, Energy communities, Energy democracy, Michel Foucault},
abstract = {The Corona pandemic has led to the increased use of online tools throughout society, whether in business, education, or daily life. This shift to an online society has led social scientists to question the extent to which increased forms of control, surveillance and enforced conformity to ways of thinking, attitudes and behaviors can be promoted through online activities. This question arises overtly amidst a pandemic, but it also lurks behind the widespread diffusion of smart energy systems throughout the world and the increased use of smart meters in those systems. The extent to which forms of monitoring, disciplining and sanctioning of energy behavior and practices could come to reality is thus an important question to consider. This article does so using the ideas of Michel Foucault, together with research on smart energy systems and current trends in energy policy. The article closes with a discussion of energy democracy and democratic legitimacy in the context of possible effects of smart technologies on community energy systems.}
}
@article{ENRIQUEZHIDALGO2025123924,
title = {Evaluation of decision-support tools for coastal flood and erosion control: A multicriteria perspective},
journal = {Journal of Environmental Management},
volume = {373},
pages = {123924},
year = {2025},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2024.123924},
url = {https://www.sciencedirect.com/science/article/pii/S0301479724039112},
author = {Andrés M. Enríquez-Hidalgo and Andrés Vargas-Luna and Andrés Torres},
keywords = {Decision support tool, Coastal erosion and flood management, Development pathways, Coastal archetypes, Multi-criteria decision analysis},
abstract = {Coastal areas face significant challenges due to natural and anthropogenic changes, such as sea level rise, extreme events and coastal erosion. The coastal management requires the consideration of socioeconomic and environmental factors to address these variables. The selection of an appropriate Decision Support Tool (DST) based on decision matrix method plays a crucial role in implementing coastal management strategies to tackle climate change-related issues. This has posed considerable challenges for decision-makers, aligning with the Sustainable Development Goals (SDG). This review provides an overview of the practical experience in the application of DSTs for coastal erosion and flood risk, emphasizing the use of Multi-Criteria Decision Analysis (MCDA). DST choice depends on the coastal archetype, including its geographical features and sociocultural context. The purpose is to clarify how the integration of DSTs maximizes flexibility and supports the implementation of future Decision Support System (DSS) tailored to the needs of coastal cities with development pathways (DP). This review assesses different MCDA methods, highlighting their applicability, utility, and integration in coastal management, while evaluating each method's strengths, weaknesses, and specific applications, with a focus on sustainability and resilience. The review highlights the necessity of expert knowledge in accurately defining criteria and weighting factors to ensure that the chosen MCDA method reflects the complexities of the coastal environment. Depending on the scenario, methods like PROMETHEE and ELECTRE are recommended for their flexibility and robustness in handling complex decision-making processes, especially in data-rich and well-structured environments. In contrast, TOPSIS and AHP are suitable for scenarios with limited information or requiring minimal interaction with decision-makers. For more challenging contexts, where computational resources and expertise are constrained, methods like MAUT, VIKOR, and TODAIM emerge as viable alternatives due to their adaptability and reduced reliance on extensive datasets.}
}
@article{DULIC201654,
title = {Designing futures: Inquiry in climate change communication},
journal = {Futures},
volume = {81},
pages = {54-67},
year = {2016},
note = {Modelling and Simulation in Futures Studies},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2016.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0016328716000057},
author = {Aleksandra Dulic and Jeannette Angel and Stephen Sheppard},
keywords = {Design inquiry, Designing futures, Climate change communication, Interaction design, 3D game simulation, Transdisciplinary research},
abstract = {There are many barriers and challenges associated with climate change communication focused on promoting community-based action for sustainable futures. Of particular interest is the challenge to embed community perspectives in a communication process of climate change solutions. In this paper we argue that 3D interactive simulations using design inquiry as a development process, can be an effective way of communicating climate change solutions and multiple community responses. People are more likely to engage with the challenges associated with complexity of climate change at the local level when their perspectives are integrated into viable and multiple pathways for action. Future scenarios of change processes situated in local experiences in compelling and interactive ways can be disseminated holistically by making links between scientific, social, political, economic and cultural elements. Design inquiry, as a research approach, integrates contextual knowledge into communication processes to aid imagining, re-thinking and reembodying viable pathways that explore the kinds of futures we collectively envision. This paper examines the contributions that design inquiry makes to climate change communication using an interactive simulation environment for designing futures. We discuss these ideas using the example of the Future Delta project, a virtual 3D environment that enables the exploration and simulation of multiple community-based climate change solutions in the Corporation of Delta, British Columbia.}
}
@incollection{JANELLE2015415,
title = {Time-Space in Geography},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {415-420},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.72070-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868720708},
author = {Donald G. Janelle},
keywords = {Activity patterns, Communication, Cyberinfrastructure, Geographic information systems, Human extensibility, Space-time path, Space-time prism, Time geography, Time-space compression, Time-space convergence, Time-space distanciation, Transportation, Travel},
abstract = {This article reviews the development of time-space perspectives in geography and exposes linkages between these perspectives and society's prevailing technologies for travel and communication. Special attention is given to the role of information, computation, and visualization technologies that shape the research practices that advance the potential to understand social organization and human activity behavior in a time-space context.}
}
@article{YANG2023125877,
title = {Multi-parameter controlled mechatronics-electro-hydraulic power coupling electric vehicle based on active energy regulation},
journal = {Energy},
volume = {263},
pages = {125877},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2022.125877},
url = {https://www.sciencedirect.com/science/article/pii/S0360544222027633},
author = {Jian Yang and Bo Liu and Tiezhu Zhang and Jichao Hong and Hongxin Zhang},
keywords = {Mechatronics-electro-hydraulic power coupling, Energy efficiency, K-means clustering analysis, Torque characteristic, Fuzzy control},
abstract = {To enhance the hydraulic energy utilization and torque output stability, a novel mechatronics-electro-hydraulic power coupling electric vehicle (MEH-PCEV) is proposed, integrating a hydraulic pump/motor and a motor into a single device for mutual energy conversion. For MEH-PCEVs equipped with multiple energy sources, a cluster analysis method is used to classify the actual road test dataset and provide guiding ideas for designing rule-based energy management strategies (RB-EMS). Simultaneously, for the output torque anomaly phenomenon in RB-EMS, an inverse thinking fuzzy logic optimization energy management strategy (FLO-EMS) conside ring multi-parameter objectives as input is used to adjust the electromagnetic torque in real-time and reasonably allocate the energy flow. The simulation results demonstrate that the electric and total torque output are more stable. The electric peak torque is relieved, with a corresponding increase in the percentage of electrical energy recovery. With the equal power demand, the overall efficiency of the motor working point is substantially improved, and the energy consumption rate is decreased by 24.42%. Under the active regulation of FLO-EMS, hydraulic energy is more reasonably utilized to meet the vehicle demand power while avoiding energy dissipation and waste. Moreover, this work is expected to reference the development and engineering applications of electro-hydraulic coupling systems.}
}
@article{KONDINSKI20241071,
title = {Hacking decarbonization with a community-operated CreatorSpace},
journal = {Chem},
volume = {10},
number = {4},
pages = {1071-1083},
year = {2024},
issn = {2451-9294},
doi = {https://doi.org/10.1016/j.chempr.2023.12.018},
url = {https://www.sciencedirect.com/science/article/pii/S2451929423006198},
author = {Aleksandar Kondinski and Sebastian Mosbach and Jethro Akroyd and Andrew Breeson and Yong Ren Tan and Simon Rihm and Jiaru Bai and Markus Kraft},
keywords = {decarbonization, chemistry, knowledge graphs, agents, CreatorSpace},
abstract = {Summary
The pressing challenge of decarbonization encompasses a vast combinatorial space of interlinked technologies, thus necessitating an increased reliance on artificial intelligence (AI)-assisted molecular modeling and data analytics. Our backcasting analysis proposes a future rich in efficient decarbonization technologies, such as sustainable fuels for aviation and shipping, as well as carbon capture and utilization. We then retrace the path to this proposed future with the guidance of two constraints: the maximization of scientists’ creative capacities and the evolution of a world-centric AI. Our exploration leads us to the concept of a “CreatorSpace,” a distributed digital system resembling existing hackerspaces and makerspaces known for accelerating the prototyping of new technologies worldwide. The CreatorSpace serves as a virtual, semantic platform where chemists, engineers, and materials scientists can freely collaborate, integrating chemical knowledge with cross-scale, cross-technology tools, and operations. This streamlined molecular-to-process-design pathway facilitates a diverse array of solutions for decarbonization and other sustainability technologies.}
}
@article{YU2023114721,
title = {Numerical investigation of splitter blades on the performance of a forward-curved impeller used in a pump as turbine},
journal = {Ocean Engineering},
volume = {281},
pages = {114721},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.114721},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823011058},
author = {He Yu and Tao Wang and Yuancheng Dong and Qiuqin Gou and Lei Lei and Yunqi Liu},
keywords = {Special impeller, Pump as turbine, Splitter blade, Entropy generation, Computational fluid dynamics},
abstract = {Abstract
As a type of economical energy recovery device, pump as turbine (PAT) is generally used in micro-hydropower plants and energy recovery. To study the influence of the splitter blade on a special impeller used in PAT, impellers without and with splitter blades are designed in this paper. The influences of splitter blade on the energy loss, external characteristics and internal flow field distribution of a PAT were simulated via a verified computational fluid dynamics (CFD) method. The consequences present that the shaft power, efficiency, and head corresponding to the BEP of the PAT with splitter blades are 16.4%, 1.3%, and 8.8% better than those of the PAT without splitter blades. The total entropy production of the PAT without splitter blade is higher than that of the PAT with splitter blades at the same flow rate. Adding splitter blade increased the number of effective blades, made the fluid flow more evenly along the impeller flow passage, and reduced the flow separation inside the impeller. This paper displays that adding splitter blades not only obviously increases hydraulic performance under large flow conditions but also significantly widens the high-efficiency range of PATs.}
}
@article{CHEN2022101380,
title = {An automated quality evaluation framework of psychotherapy conversations with local quality estimates},
journal = {Computer Speech & Language},
volume = {75},
pages = {101380},
year = {2022},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2022.101380},
url = {https://www.sciencedirect.com/science/article/pii/S0885230822000213},
author = {Zhuohao Chen and Nikolaos Flemotomos and Karan Singla and Torrey A. Creed and David C. Atkins and Shrikanth Narayanan},
keywords = {Cognitive behavioral therapy, Computational linguistics, Hierarchical framework, Local quality estimates},
abstract = {Text-based computational approaches for assessing the quality of psychotherapy are being developed to support quality assurance and clinical training. However, due to the long durations of typical conversation based therapy sessions, and due to limited annotated modeling resources, computational methods largely rely on frequency-based lexical features or dialogue acts to assess the overall session level characteristics. In this work, we propose a hierarchical framework to automatically evaluate the quality of transcribed Cognitive Behavioral Therapy (CBT) interactions. Given the richly dynamic nature of the spoken dialog within a talk therapy session, to evaluate the overall session level quality, we propose to consider modeling it as a function of local variations across the interaction. To implement that empirically, we divide each psychotherapy session into conversation segments and initialize the segment-level qualities with the session-level scores. First, we produce segment embeddings by fine-tuning a BERT-based model, and predict segment-level (local) quality scores. These embeddings are used as the lower-level input to a Bidirectional LSTM-based neural network to predict the session-level (global) quality estimates. In particular, we model the global quality as a linear function of the local quality scores, which allows us to update the segment-level quality estimates based on the session-level quality prediction. These newly estimated segment-level scores benefit the BERT fine-tuning process, which in turn results in better segment embeddings. We evaluate the proposed framework on automatically derived transcriptions from real-world CBT clinical recordings to predict session-level behavior codes. The results indicate that our approach leads to improved evaluation accuracy for most codes when used for both regression and classification tasks.}
}
@article{MONNAHAN2024107024,
title = {Toward good practices for Bayesian data-rich fisheries stock assessments using a modern statistical workflow},
journal = {Fisheries Research},
volume = {275},
pages = {107024},
year = {2024},
issn = {0165-7836},
doi = {https://doi.org/10.1016/j.fishres.2024.107024},
url = {https://www.sciencedirect.com/science/article/pii/S0165783624000882},
author = {Cole C. Monnahan},
keywords = {No-U-turn sampler (NUTS), Bayesian integration, Prior predictive checks, Posterior predictive checks, Cross validation},
abstract = {Bayesian inference has long been recognized as useful for fisheries stock assessments but it is less common than maximum likelihood approaches due to long run times and a lack of good practices. Recent computational advances leave developing good practices and user-friendly interfaces as the most important hurdles to wider use of this powerful statistical paradigm. Here, I argue that the modern Bayesian workflow proposed by Gelman et al. (2020) should form the basis for proposed good practices in fisheries sciences. Their workflow is a conceptual roadmap for iterative model building which includes the philosophical role of priors and how to apply statistical tools to construct them, how to validate and compare models, and how to overcome computational problems. Adapted for stock assessment, this leads to the following good practices for analysts. Diagnostics from multiple no-U-turn sampler (NUTS) chains (the recommended MCMC algorithm) should pass and be reported, specifically that the potential scale reduction Rˆ is <1.01 and the effective sample size is >400 for all parameters, and there are no NUTS divergences. When direct a priori information is unavailable on parameters, use prior predictive checking to build, assess, and adjust priors to enforce desired constraints on complexity, or to conform to a priori expectations or physical/biological limitations on derived quantities. Use posterior predictive checks to validate models by confirming simulated data and summaries (e.g., variance of compositional data) are similar to the observed counterparts. Process error variances can be estimated jointly with random effects and other parameters when desired, and should be for important model components. An approximate cross-validation technique called PSIS-LOO is the most practical tool for model selection, but can also provide important insights into model deficiencies. I also recommended that model developers build and parameterize models to have minimal parameter correlations and marginal variances close to one, have options for diverse (multivariate) priors, do predictive modeling, and ensure that the tools comprising a workflow are accessible and straightforward for routine use. I review, adapt, and illustrate a Bayesian workflow on AD Model Builder and Stock Synthesis models, but these good practices apply to models from any software platform, including Template Model Builder and Stan. Finally, I argue that the Bayesian and frequentist paradigms complement each other, with both helping analysts better understand different aspects of their models and data. Wider adoption of Bayesian methods using the good practices proposed here would therefore lead to improved scientific advice used to manage fisheries.}
}
@article{HUGHES2021338,
title = {High-content phenotypic and pathway profiling to advance drug discovery in diseases of unmet need},
journal = {Cell Chemical Biology},
volume = {28},
number = {3},
pages = {338-355},
year = {2021},
issn = {2451-9456},
doi = {https://doi.org/10.1016/j.chembiol.2021.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S2451945621001008},
author = {Rebecca E. Hughes and Richard J.R. Elliott and John C. Dawson and Neil O. Carragher},
keywords = {high-content imaging, machine learning, structural similarity, network pharmacology, esophageal cancer, glioblastoma},
abstract = {Summary
Conventional thinking in modern drug discovery postulates that the design of highly selective molecules which act on a single disease-associated target will yield safer and more effective drugs. However, high clinical attrition rates and the lack of progress in developing new effective treatments for many important diseases of unmet therapeutic need challenge this hypothesis. This assumption also impinges upon the efficiency of target agnostic phenotypic drug discovery strategies, where early target deconvolution is seen as a critical step to progress phenotypic hits. In this review we provide an overview of how emerging phenotypic and pathway-profiling technologies integrate to deconvolute the mechanism-of-action of phenotypic hits. We propose that such in-depth mechanistic profiling may support more efficient phenotypic drug discovery strategies that are designed to more appropriately address complex heterogeneous diseases of unmet need.}
}
@incollection{GIGERENZER2015515,
title = {Computers: Impact on the Social Sciences},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {515-518},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.03202-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868032025},
author = {Gerd Gigerenzer},
keywords = {Charles Babbage, Cognitive revolution, Computer simulation, Division of labor, H.A. Simon, Metaphor, Statistics},
abstract = {The social organization of labor in the nineteenth century served as the model for Babbage's first computers. In the second half of the twentieth century, when working computers were finally constructed and invaded the offices of social scientists, they turned into theories of mind. This mutual inspiration first changed the meaning of calculation and then led to a new understanding of thought as computation based on hierarchically organized subroutines. The computer as a research tool has changed the social sciences in a fundamental way, from enabling large-scale simulations of cognitive and social systems to allowing mindless and mechanical use of statistics.}
}
@article{SULLIVAN2020246,
title = {Maritime 4.0 – Opportunities in Digitalization and Advanced Manufacturing for Vessel Development},
journal = {Procedia Manufacturing},
volume = {42},
pages = {246-253},
year = {2020},
note = {International Conference on Industry 4.0 and Smart Manufacturing (ISM 2019)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.02.078},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920306430},
author = {Brendan P. Sullivan and Shantanoo Desai and Jordi Sole and Monica Rossi and Lucia Ramundo and Sergio Terzi},
keywords = {Maritime 4.0, Digitalization, Maritime Vessel Development, Industry 4.0},
abstract = {Maritime vessels are complex systems that generate and require the utilization of large amounts of data for maximum efficiency. The successful utilization of sensors and IoT in the industry requires a forward-thinking approach to leverage the benefits of Industry 4.0 in a more comprehensive manner. While processes and manufacturing processes can be improved and advanced through such efforts, in order the industry to be able to benefit from data generation, integrated approaches are necessary. In order to develop truly value-added vessels, we introduce a descriptive approach for understanding Maritime 4.0.}
}
@article{ZHANG2020259,
title = {Self-blast state detection of glass insulators based on stochastic configuration networks and a feedback transfer learning mechanism},
journal = {Information Sciences},
volume = {522},
pages = {259-274},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.02.058},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520301419},
author = {Qian Zhang and Weitao Li and Hua Li and Jianping Wang},
keywords = {Insulator self-blast state, Deep learning, Feedback transfer learning mechanism, Semantic error entropy},
abstract = {The self-blast state of a glass insulator directly affects the safety and reliability of transmission lines. To address the insufficient generalization ability of existing detection methods for insulator self-blast states and the drawbacks of deep neural network structures, the theories of transfer learning and closed-loop control are drawn upon to provide an intelligent detection method for the self-blast states of glass insulators. The method proposed in this paper is based on stochastic configuration networks and a feedback transfer learning mechanism. First, to reduce the redundancy of convolutional kernels in the channel extent, the interleaved group convolution strategy is employed to reconstruct the convolutional layers of the Inception network. Second, in view of the different feature applicabilities of different glass insulator images and based on the adaptive convolution module groups, the data structure of the dynamic feature space of insulator images is built with a certain mapping relationship from global to local. Then, the discriminative measure index is used to evaluate the discriminative information of the feature space to enhance the interpretability of the compact feature spance. Third, the fully connected feature vector of the compact feature space is sent to stochastic configuration networks (SCNs), which have universal approximation property to establish the classification criteria of the self-blast states of insulator images with generalization ability. Finally, an imitation of human thinking patterns is employed that exhibits repeated deliberation and comparison. Consequently, based on generalized error and entropy theories, the evaluation index of the objective function is established to evaluate the uncertain detection results of the self-blast states of glass insulator images in real time. Then, the dynamic transfer learning mechanism is constructed based on the constraint of the measurement index of uncertain detection results to realize self-optimizing regulation of the feature space that exhibits multihierarchy and discrimination and reconstructed classification criteria. The experimental results show that compared with other algorithms, the proposed method enhances the generalization ability and detection accuracy of the model.}
}
@article{BASHIRPOURBONAB2023122642,
title = {In complexity we trust: A systematic literature review of urban quantum technologies},
journal = {Technological Forecasting and Social Change},
volume = {194},
pages = {122642},
year = {2023},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2023.122642},
url = {https://www.sciencedirect.com/science/article/pii/S004016252300327X},
author = {Aysan {Bashirpour Bonab} and Maria Fedele and Vincenzo Formisano and Ihor Rudko},
keywords = {Quantum City, Uncertainty, Duality, Parallelism, Quantum mechanics, Systematic literature review},
abstract = {Today's cities are facing increasingly complex challenges. The growing uncertainty and complexity—caused by the unremitted differentiation of social, environmental, and technological orders—call for novel ways of conceptualizing urban reality. Although technology-oriented solutions shape the most efficient strategies to manage complexity in contemporary cities, ensuring an effective transition toward a Quantum City paradigm can grant considerable advantages for city administrators and managers facing looming urban challenges. In this article, we introduce the Quantum City metaphor—grounded in fundamental notions of quantum mechanics—as a new conceptual lens for investigating urban complexity. We then build upon the metaphor, theorizing a set of assumptions grounded in three fundamental concepts of quantum theory: relativity, uncertainty, and duality/parallelism. Finally, we propose an empirical conceptualization of Quantum Cities based on the concrete adoption of quantum technologies to deal with urban complexity. This is achieved through a systematic literature review of scholarly records on quantum technologies in the context of social sciences, emphasizing related urban problematics and challenges. Principal component analysis and agglomerative hierarchical clustering reveal two types of quantum technologies most useful for city planners and managers: quantum communication and quantum computing. Accordingly, we perform a qualitative thematic synthesis of related scholarly records, emphasizing the negative and positive aspects of both types of urban quantum technologies.}
}
@article{NAZI2025100124,
title = {Evaluation of open and closed-source LLMs for low-resource language with zero-shot, few-shot, and chain-of-thought prompting},
journal = {Natural Language Processing Journal},
volume = {10},
pages = {100124},
year = {2025},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2024.100124},
url = {https://www.sciencedirect.com/science/article/pii/S2949719124000724},
author = {Zabir Al Nazi and Md. Rajib Hossain and Faisal Al Mamun},
keywords = {Large language models, Zero-shot, Few-shot, Chain-of-thought, GPT-4, Llama 3, Ablation studies, Prompting, LLM reasoning, Low-resource, Bangla},
abstract = {As the global deployment of Large Language Models (LLMs) increases, the demand for multilingual capabilities becomes more crucial. While many LLMs excel in real-time applications for high-resource languages, few are tailored specifically for low-resource languages. The limited availability of text corpora for low-resource languages, coupled with their minimal utilization during LLM training, hampers the models’ ability to perform effectively in real-time applications. Additionally, evaluations of LLMs are significantly less extensive for low-resource languages. This study offers a comprehensive evaluation of both open-source and closed-source multilingual LLMs focused on low-resource language like Bengali, a language that remains notably underrepresented in computational linguistics. Despite the limited number of pre-trained models exclusively on Bengali, we assess the performance of six prominent LLMs, i.e., three closed-source (GPT-3.5, GPT-4o, Gemini) and three open-source (Aya 101, BLOOM, LLaMA) across key natural language processing (NLP) tasks, including text classification, sentiment analysis, summarization, and question answering. These tasks were evaluated using three prompting techniques: Zero-Shot, Few-Shot, and Chain-of-Thought (CoT). This study found that the default hyperparameters of these pre-trained models, such as temperature, maximum token limit, and the number of few-shot examples, did not yield optimal outcomes and led to hallucination issues in many instances. To address these challenges, ablation studies were conducted on key hyperparameters, particularly temperature and the number of shots, to optimize Few-Shot learning and enhance model performance. The focus of this research is on understanding how these LLMs adapt to low-resource downstream tasks, emphasizing their linguistic flexibility and contextual understanding. Experimental results demonstrated that the closed-source GPT-4o model, utilizing Few-Shot learning and Chain-of-Thought prompting, achieved the highest performance across multiple tasks: an F1 score of 84.54% for text classification, 99.00% for sentiment analysis, a F1bert score of 72.87% for summarization, and 58.22% for question answering. For transparency and reproducibility, all methodologies and code from this study are available on our GitHub repository: https://github.com/zabir-nabil/bangla-multilingual-llm-eval.}
}
@article{LUCKRING2024100998,
title = {Prediction of concentrated vortex aerodynamics: Current CFD capability survey},
journal = {Progress in Aerospace Sciences},
volume = {147},
pages = {100998},
year = {2024},
issn = {0376-0421},
doi = {https://doi.org/10.1016/j.paerosci.2024.100998},
url = {https://www.sciencedirect.com/science/article/pii/S0376042124000241},
author = {James M. Luckring and Arthur Rizzi},
abstract = {Concentrated vortex flows contribute to the aerodynamic performance of aircraft at elevated load conditions. For military interests, the vortex flows are exploited at maneuver conditions of combat aircraft and missiles. For transport interests, the vortex flows are exploited at takeoff and landing conditions as well as at select transonic conditions. Aircraft applications of these vortex flows are reviewed with a historical perspective followed by a discussion of the underlying physics of a concentrated vortex flow. A hierarchy of computational fluid dynamics simulation technology is then presented followed by findings from a capability survey for predicting concentrated vortex flows with computational fluid dynamics. Results are focused on military and civil fixed-wing aircraft; only limited results are included for missiles, and rotary-wing applications are not assessed. Opportunities for predictive capability advancement are then reported with comments related to digital transformation interests. A hierarchical approach that merges a physics-based perspective of the concentrated vortex flows with a systems engineering viewpoint of the air vehicle is also used to frame much of the discussion.}
}
@article{AQDA2011260,
title = {The impact of constructivist and cognitive distance instructional design on the learner’s creativity},
journal = {Procedia Computer Science},
volume = {3},
pages = {260-265},
year = {2011},
note = {World Conference on Information Technology},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2010.12.044},
url = {https://www.sciencedirect.com/science/article/pii/S1877050910004199},
author = {Mahnaz Fatemi Aqda and Farideh Hamidi and Farhad Ghorbandordinejad},
keywords = {Instructional design, Distance education, E-learning, Creativity, Cognitivism, Constructivism},
abstract = {Creativity is at the heart of the 21st century educational work. Learner’ creativity or learner’s creative thinking skills are among the most important skills they need to be prepared for the knowledge society. The rapid development of technology in the modern era sheds light on the place and importance of creativity in education. Technology also has brought change in the way the students learn (collaboration strategy) and recently the computer-based instruction associated with electrical technologies has been a popular way of instruction that learning is no longer confined to classrooms (distance learning). Also in the case of the students if they want to take effective advantage of technology, they have to use the constructivist and cognitive skills (psychological learning theory). Recently education experts have tried to show how a distance instructional can be designed. The main question of this paper is what effects the distance instructional design based on the views of constructivism and cognitivism have on the learners’ creativity.The definition of distance education (e-learning), the instruction design based on constructivist view and its function in education and distance learning (e-learning), the instruction design based on cognitive view and its function in education and distance learning (e-learning), the factors affecting the creativity development and accommodation (comparison) the characteristics of the instructural context, and the impact of the appropriate learning on the creativity development according to these settings are among the other main points of this review.}
}
@article{NORGAARD2023105308,
title = {Linked auditory and motor patterns in the improvisation vocabulary of an artist-level jazz pianist},
journal = {Cognition},
volume = {230},
pages = {105308},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2022.105308},
url = {https://www.sciencedirect.com/science/article/pii/S0010027722002967},
author = {Martin Norgaard and Kevin Bales and Niels Chr. Hansen},
keywords = {Improvisation, Jazz, Music, Audiomotor coupling, Expectation, Entropy},
abstract = {Improvising musicians possess a stored library of musical patterns forming the basis for their improvisations. According to a prominent theoretical framework by Pressing (1988), this library includes linked auditory and motor information. Though examples of libraries of melodic patterns have been shown in extant recordings by some improvising musicians, the underlying motor component has not been experimentally investigated nor related to its auditory counterparts. Here we analyzed a large corpus of ∼100,000 notes from improvisations by one artist-level jazz pianist recorded during 11 live performances with audience. We compared the library identified from these recordings to a control corpus consisting of improvisations by 24 different advanced jazz pianists. In addition to pitch, our recordings included accurate micro-timing and key velocity (i.e., force) data. Following a previously validated procedure, this information was used to identify the underlying motor patterns through correlations between relative timing and velocity between notes in different iterations of the same pitch pattern. A computational model was, furthermore, used to estimate the information content and generated entropy exhibited by recurring pitch patterns with high and low timing and velocity correlations as perceived by a stylistically enculturated expert listener. Though both corpora contained a large number of recurring patterns, the single-player corpus showed stronger evidence that pitch patterns were linked to motor programs in that within-pattern timing and velocity correlations were significantly higher compared to the control corpus. Even when controlling for potentially greater baseline levels of motor self-consistency in the single-player corpus, this effect remained significant for velocity correlations. Amongst recurring 5-tone pitch patterns, those exhibiting more consistent motor schema also used less idiomatic pitch transitions that were both more unexpected and generated more uncertain expectations in enculturated experts than less consistently repeated patterns. Interestingly, we only found partial evidence for fixed pattern boundaries as predicted by the Pressing model and therefore suggest an expanded view in which the beginning and ends of idiomatic audio-motor patterns are not always clear-cut. Our results indicate that the library of melodic patterns may be idiosyncratic to the individual improviser and relies both on motor programming and predictive processing to promote stylistic distinctiveness.}
}
@incollection{QAZI2025245,
title = {Chapter 11 - Deep learning in clinical genomics-based cancer diagnosis},
editor = {Khalid Raza},
booktitle = {Deep Learning in Genetics and Genomics},
publisher = {Academic Press},
pages = {245-259},
year = {2025},
isbn = {978-0-443-27574-6},
doi = {https://doi.org/10.1016/B978-0-443-27574-6.00014-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044327574600014X},
author = {Sahar Qazi and Raiyan Ali and Manoj Kumar Jana and Bimal Prasad Jit and Neeraj Gurung and Ashok Sharma},
keywords = {Artificial intelligence, Bioinformatics, Cancer, Deep learning, Diagnosis, Next generation sequencing, Variant calling},
abstract = {Deep learning, an artificial intelligence facet, has impacted distinct fields, including natural language processing and computer vision. Its advancements have transformed how computational and data scientists approach data, turning unstructured information into valuable insights. This is particularly impactful in clinical genomics, where high-throughput sequencing generates vast amounts of data. Techniques such as whole genome sequencing and transcriptomic profiling produce enormous datasets that are challenging to analyze manually. Deep learning tools like “Deep Variant” enhance accuracy in variant calling, improving diagnostic precision. By adapting to factors such as genetic mutations and disease progression, deep learning aids in early cancer diagnosis and better clinical outcomes. This chapter explores these transformative applications in clinical genomic research.}
}
@article{SOLIMAN2025100131,
title = {A comparative analysis of encoder only and decoder only models for challenging LLM-generated STEM MCQs using a self-evaluation approach},
journal = {Natural Language Processing Journal},
volume = {10},
pages = {100131},
year = {2025},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2025.100131},
url = {https://www.sciencedirect.com/science/article/pii/S294971912500007X},
author = {Ghada Soliman and Hozaifa Zaki and Mohamed Kilany},
keywords = {NLP, LLM, SLM, Self-evaluation, MCQ},
abstract = {Large Language Models (LLMs) have demonstrated impressive capabilities in various tasks, including Multiple-Choice Question Answering (MCQA) evaluated on benchmark datasets with few-shot prompting. Given the absence of benchmark Science, Technology, Engineering, and Mathematics (STEM) datasets on Multiple-Choice Questions (MCQs) created by LLMs, we employed various LLMs (e.g., Vicuna-13B, Bard, and GPT-3.5) to generate MCQs on STEM topics curated from Wikipedia. We evaluated open-source LLM models such as Llama 2-7B and Mistral-7B Instruct, along with an encoder model such as DeBERTa v3 Large, on inference by adding context in addition to fine-tuning with and without context. The results showed that DeBERTa v3 Large and Mistral-7B Instruct outperform Llama 2-7B, highlighting the potential of LLMs with fewer parameters in answering hard MCQs when given the appropriate context through fine-tuning. We also benchmarked the results of these models against closed-source models such as Gemini and GPT-4 on inference with context, showcasing the potential of narrowing the gap between open-source and closed-source models when context is provided. Our work demonstrates the capabilities of LLMs in creating more challenging tasks that can be used as self-evaluation for other models. It also contributes to understanding LLMs’ capabilities in STEM MCQs tasks and emphasizes the importance of context for LLMs with fewer parameters in enhancing their performance.}
}
@article{SEISING2006237,
title = {From vagueness in medical thought to the foundations of fuzzy reasoning in medical diagnosis},
journal = {Artificial Intelligence in Medicine},
volume = {38},
number = {3},
pages = {237-256},
year = {2006},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2006.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0933365706001072},
author = {Rudolf Seising},
keywords = {History of science and technology, Fuzzy set theory, Vagueness, Medical diagnoses, Computer assistance, Medical philosophy, System theory},
abstract = {Summary
Objective
This article delineates a relatively unknown path in the history of medical philosophy and medical diagnosis. It is concerned with the phenomenon of vagueness in the physician's “style of thinking” and with the use of fuzzy sets, systems, and relations with a view to create a model of such reasoning when physicians make a diagnosis. It represents specific features of medical ways of thinking that were mentioned by the Polish physician and philosopher Ludwik Fleck in 1926. The paper links Lotfi Zadeh's work on system theory before the age of fuzzy sets with system-theory concepts in medical philosophy that were introduced by the philosopher Mario Bunge, and with the fuzzy-theoretical analysis of the notions of health, illness, and disease by the Iranian-German physician and philosopher Kazem Sadegh-Zadeh.
Material
Some proposals to apply fuzzy sets in medicine were based on a suggestion made by Zadeh: symptoms and diseases are fuzzy in nature and fuzzy sets are feasible to represent these entity classes of medical knowledge. Yet other attempts to use fuzzy sets in medicine were self-contained. The use of this approach contributed to medical decision-making and the development of computer-assisted diagnosis in medicine.
Conclusion
With regard to medical philosophy, decision-making, and diagnosis; the framework of fuzzy sets, systems, and relations is very useful to deal with the absence of sharp boundaries of the sets of symptoms, diagnoses, and phenomena of diseases. The foundations of reasoning and computer assistance in medicine were the result of a rapid accumulation of data from medical research. This explosion of knowledge in medicine gave rise to the speculation that computers could be used for the medical diagnosis. Medicine became, to a certain extent, a quantitative science. In the second half of the 20th century medical knowledge started to be stored in computer systems. To assist physicians in medical decision-making and patient care, medical expert systems using the theory of fuzzy sets and relations (such as the Viennese “fuzzy version” of the Computer-Assisted Diagnostic System, Cadiag, which was developed at the end of the 1970s) were constructed. The development of fuzzy relations in medicine and their application in computer-assisted diagnosis show that this fuzzy approach is a framework to deal with the “fuzzy mode of thinking” in medicine.}
}
@article{LEALVILLASECA2025105833,
title = {Interpreting Deepkriging for spatial interpolation in geostatistics},
journal = {Computers & Geosciences},
volume = {196},
pages = {105833},
year = {2025},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2024.105833},
url = {https://www.sciencedirect.com/science/article/pii/S0098300424003169},
author = {Fabian Leal-Villaseca and Edward Cripps and Mark Jessell and Mark Lindsay},
keywords = {Spatial statistics, Deep learning interpretability, Shapley values, Deepkriging, Batched Shapley},
abstract = {In the current era marked by an unprecedented abundance of data, the usage of conventional methods such as kriging persists in some applications of geostatistics, despite their limitations in adequately capturing the intricate relationships found in contemporary, multivariate datasets. Although deep neural networks (DNNs) have demonstrated remarkable efficacy in capturing complex nonlinear feature relationships across various domains, their success in geostatistical applications has been limited. This can be partly attributed to two significant challenges. Firstly, the opaque nature of these black box models raises concerns about the dependability of their outputs for critical decision-making, as the inner workings of the model remain less interpretable. Secondly, DNNs do not explicitly capture spatial dependencies within data. To address these shortcomings, we employ a methodology to interpret the recently proposed spatial DNNs known as Deepkriging, and we apply it to dry bulk rock density estimation, an often-overlooked aspect in mineral resource estimation. Through our adaptation of Shapley values—Batched Shapley—we overcome significant computational challenges to quantify feature importance for Deepkriging. This approach takes into account feature interactions, which is crucial for DNNs, as they rely on high-order interactions, especially in a complex application like mineral resource estimation. Additionally, we demonstrate in the 3D case that Deepkriging outperforms ordinary kriging and regression kriging in terms of mean squared errors, in both the purely spatial case and in the presence of auxiliary variables. Our study produces the first methodology to interpret Deepkriging, which is suitable for any model with a large number of features; it reaffirms the efficacy of Deepkriging through several comparisons in a 3D application, and most importantly; it underscores the adaptability and broader potential of DNNs to cater to various challenges in geostatistics.}
}
@article{MCLEAN20248,
title = {Autoantibodies against acetylcholine receptors are increased in archived serum samples from patients with schizophrenia},
journal = {Schizophrenia Research},
volume = {267},
pages = {8-13},
year = {2024},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2024.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0920996424001129},
author = {Ryan Thomas McLean and Elizabeth Buist and David {St. Clair} and Jun Wei},
keywords = {Neurotransmitter receptor, CHRM4, GRM3, CHRNA4, CHRNA5 neuroimmunology},
abstract = {Previous studies have demonstrated that the levels of IgG against neurotransmitter receptors are increased in patients with schizophrenia. Genome-wide association (GWA) studies of schizophrenia confirmed that 108 loci harbouring over 300 genes were associated with schizophrenia. Although the functional implications of genetic variants are unclear, theoretical functional alterations of these genes could be replicated by the presence of autoantibodies. This study examined the levels of plasma IgG antibodies against four neurotransmitter receptors, CHRM4, GRM3, CHRNA4 and CHRNA5, using an in-house ELISA in 247 patients with schizophrenia and 344 non-psychiatric controls. Four peptides were designed based on in silico analysis with computational prediction of HLA-DRB1 restricted and B-cell epitopes. The relationship between plasma IgG levels and psychiatric symptoms, as defined by the Operational Criteria Checklist for Psychotic Illness and Affective Illness (OPCRIT), were examined. The results showed that the levels of plasma IgG against peptides derived from CHRM4 and CHRNA4 were significantly increased in patients with schizophrenia compared with control subjects, but there was no significant association of plasma IgG levels with any symptom domain or any specific symptoms. These preliminary results suggest that CHRM4 and CHRNA4 may be novel targets for autoantibody responses in schizophrenia, although the pathogenic relationship between increased serum autoantibody levels and schizophrenia symptoms remains unclear.}
}
@article{BELABES2015639,
title = {Designing Islamic Finance Programmes in a Competitive Educational Space: The Islamic Economics Institute Experiment},
journal = {Procedia - Social and Behavioral Sciences},
volume = {191},
pages = {639-643},
year = {2015},
note = {The Proceedings of 6th World Conference on educational Sciences},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2015.04.300},
url = {https://www.sciencedirect.com/science/article/pii/S1877042815025604},
author = {Abderrazak Belabes and Ahmed Belouafi and Mohamed Daoudi},
keywords = {Curricula design, glocalization, competitiveness, Islamic finance, Islamic Economics Institute},
abstract = {This paper aims at exploring the experiment of the Islamic Economics Institute (IEI) of King Abdulaziz University in the design of the first ever Islamic finance higher educational programme at a Saudi Public University. An evaluative analytical framework has been utilized to meet this goal. Results show that the Institute has pursued a ‘glocalization’; thinking globally and acting locally approach in designing the programme. This approach aims at providing learners with ‘cutting-edge’ skills that will enhance their chances for employment at the local as well as regional markets. What are the advantages of this approach? And how can the Institute preserve its ‘distinctive research’ positioning that it has gained over the years, at the same time, being able to provide ‘world-class’ educational programmes?}
}
@article{FELSCHE2023101530,
title = {Evidence for abstract representations in children but not capuchin monkeys},
journal = {Cognitive Psychology},
volume = {140},
pages = {101530},
year = {2023},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2022.101530},
url = {https://www.sciencedirect.com/science/article/pii/S0010028522000664},
author = {Elisa Felsche and Patience Stevens and Christoph J. Völter and Daphna Buchsbaum and Amanda M. Seed},
keywords = {Overhypotheses, Abstraction, Generalization, Animal cognition, Computational modeling, Cognitive development},
abstract = {The use of abstract higher-level knowledge (also called overhypotheses) allows humans to learn quickly from sparse data and make predictions in new situations. Previous research has suggested that humans may be the only species capable of abstract knowledge formation, but this remains controversial. There is also mixed evidence for when this ability emerges over human development. Kemp et al. (2007) proposed a computational model of how overhypotheses could be learned from sparse examples. We provide the first direct test of this model: an ecologically valid paradigm for testing two species, capuchin monkeys (Sapajus spp.) and 4- to 5-year-old human children. We presented participants with sampled evidence from different containers which suggested that all containers held items of uniform type (type condition) or of uniform size (size condition). Subsequently, we presented two new test containers and an example item from each: a small, high-valued item and a large but low-valued item. Participants could then choose from which test container they would like to receive the next sample – the optimal choice was the container that yielded a large item in the size condition or a high-valued item in the type condition. We compared performance to a priori predictions made by models with and without the capacity to learn overhypotheses. Children's choices were consistent with the model predictions and thus suggest an ability for abstract knowledge formation in the preschool years, whereas monkeys performed at chance level.}
}
@article{HOLYOAK2021118,
title = {Emergence of relational reasoning},
journal = {Current Opinion in Behavioral Sciences},
volume = {37},
pages = {118-124},
year = {2021},
note = {Same-different conceptualization},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2020.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S2352154620301716},
author = {Keith J Holyoak and Hongjing Lu},
abstract = {We review recent theoretical and empirical work on the emergence of relational reasoning, drawing connections among the fields of comparative psychology, developmental psychology, cognitive neuroscience, cognitive science, and machine learning. Relational learning appears to involve multiple systems: a suite of Early Systems that are available to human infants and are shared to some extent with nonhuman animals; and a Late System that emerges in humans only, at approximately age three years. The Late System supports reasoning with explicit role-governed relations, and is closely tied to the functions of a frontoparietal network in the human brain. Recent work in cognitive science and machine learning suggests that humans (and perhaps machines) may acquire abstract relations from nonrelational inputs by means of processes that enable re-representation.}
}
@article{KATONA2023115228,
title = {Accuracy of the robust design analysis for the flux barrier modelling of an interior permanent magnet synchronous motor},
journal = {Journal of Computational and Applied Mathematics},
volume = {429},
pages = {115228},
year = {2023},
issn = {0377-0427},
doi = {https://doi.org/10.1016/j.cam.2023.115228},
url = {https://www.sciencedirect.com/science/article/pii/S0377042723001723},
author = {Mihály Katona and Miklós Kuczmann and Tamás Orosz},
keywords = {Electrical machines, Optimisation, Finite element method, Robust design analysis, Design of Experiment methods},
abstract = {Mass-produced electrical machines are subjected to manufacturing uncertainties in terms of geometry. A robust design is inevitable to ensure the consistent performance of the electric motor. Some parts of the rotor geometry are often simplified, like the flux barrier at the end of the magnets. This paper presents a design optimisation regarding the torque ripple and the average torque. The aim is to assess the effects of the flux barrier on the main properties of a permanent magnet synchronous motor. Also, robust design analysis is presented on the flux barrier. The computational burden of the robust design analysis is immense, even if uniform uncertainties are assumed. In this case, different Design of Experiment (DoE) methods reduce the number of simulations. The efficiency of the DoE methods is compared in terms of simulation number and extreme value approximation. We found that the Central Composite method is the most accurate, while the Plackett–Burman is the most efficient in this particular case.}
}
@article{HUANG2025134690,
title = {Operation optimization of Combined Heat and Power microgrid in buildings consider renewable energy, electric vehicles and hydrogen fuel},
journal = {Energy},
volume = {319},
pages = {134690},
year = {2025},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2025.134690},
url = {https://www.sciencedirect.com/science/article/pii/S0360544225003329},
author = {Yongyi Huang and Shoaib Ahmed and Soichiro Ueda and Xunyu Liang and Harun Or Rashid Howlader and Mohammed Elsayed Lotfy and Tomonobu Senjyu},
keywords = {Microgrid, Renewable energy, Electric vehicles, Monte Carlo simulation, K-means, Real-time pricing, Particle swarm optimization, Chance-constrained programming, Combined heat and power},
abstract = {This paper introduces a forward-thinking framework that integrates renewable energy, Electric Vehicles (EVs), and hydrogen within Combined Heat and Power (CHP) microgrids (MGs) for effective building energy management. By utilizing Particle Swarm Optimization (PSO) to find the optimal solution and incorporating Chance-Constrained Programming (CCP) to handle uncertainties in renewable energy generation and EV loads, this framework addresses the complexities of modern energy systems. The study employs Monte Carlo (MC) to simulate the EV load profile, applies K-means clustering to categorize load and renewable generation patterns, and uses a Sigmoid function-based model for Real-Time Pricing (RTP). The combination of PSO and CCP is used to optimize the system’s operating strategy. This evaluates the system’s economic benefits and impact on carbon emissions by analyzing different scenarios, such as weekdays versus weekends and various weather conditions (sunny, cloudy, rainy). The results show that due to the high price of hydrogen, it is currently costly to replace hydrogen completely. However, this integrated approach not only improves energy efficiency and reduces carbon footprint but also ensures system reliability under uncertain conditions, contributing to broader environmental sustainability.}
}
@article{LI2024120,
title = {Conformal structure-preserving SVM methods for the nonlinear Schrödinger equation with weakly linear damping term},
journal = {Applied Numerical Mathematics},
volume = {205},
pages = {120-136},
year = {2024},
issn = {0168-9274},
doi = {https://doi.org/10.1016/j.apnum.2024.06.024},
url = {https://www.sciencedirect.com/science/article/pii/S0168927424001727},
author = {Xin Li and Luming Zhang},
keywords = {Damped nonlinear Schrödinger equation, Conformal properties, Supplementary variable method, High-order accuracy, Optimization model},
abstract = {In this paper, by applying the supplementary variable method (SVM), some high-order, conformal structure-preserving, linearized algorithms are developed for the damped nonlinear Schrödinger equation. We derive the well-determined SVM systems with the conformal properties and they are then equivalent to nonlinear equality constrained optimization problems for computation. The deduced optimization models are discretized by using the Gauss type Runge-Kutta method and the prediction-correction technique in time as well as the Fourier pseudo-spectral method in space. Numerical results and some comparisons between this method and other reported methods are given to favor the suggested method in the overall performance. It is worthwhile to emphasize that the numerical strategy in this work could be extended to other conservative or dissipative system for designing high-order structure-preserving algorithms.}
}
@article{LESSARD20071754,
title = {Complexity and reflexivity: Two important issues for economic evaluation in health care},
journal = {Social Science & Medicine},
volume = {64},
number = {8},
pages = {1754-1765},
year = {2007},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2006.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0277953606006356},
author = {Chantale Lessard},
keywords = {Health economics, Complexity theory, Reflexivity, Economic evaluation in health care},
abstract = {Economic evaluations are analytic techniques to assess the relative costs and consequences of health care programmes and technologies. Their role is to provide rigorous data to inform the health care decision-making process. Economic evaluation may oversimplify complex health care decisions. These analyses often ignore important health consequences, contextual elements, relationships or other relevant modifying factors, which might not be appropriate in a multi-objective, multi-stakeholder issue. One solution would be to develop a new paradigm based on the issues of perspective and context. Complexity theory may provide a useful conceptual framework for economic evaluation in health care. Complexity thinking develops an awareness of issues including uncertainty, contextual issues, multiple perspectives, broader societal involvement, and transdisciplinarity. This points the economic evaluation field towards an accountability and epistemology based on pluralism and uncertainty, requiring new forms of lay-expert engagement and roles of lay knowledge into decision-making processes. This highlights the issue of reflexivity in economic evaluation in health care. A reflexive approach would allow economic evaluators to analyze how objective structures and subjective elements influence their practices. In return, this would point increase the integrity and reliability of economic evaluations. Reflexivity provides opportunities for critically thinking about the organization and activities of the intellectual field, and perhaps the potential of moving in new, creative directions. This paper argues for economic evaluators to have a less positivist attitude towards what is useful knowledge, and to use more imagination about the data and methodologies they use.}
}
@incollection{KUMARI2025219,
title = {Chapter Nine - Harnessing artificial intelligence in identifying and isolation of marine peptides},
editor = {Akanksha Srivastava and Vaibhav Mishra},
series = {Methods in Microbiology},
publisher = {Academic Press},
volume = {56},
pages = {219-242},
year = {2025},
booktitle = {Artificial Intelligence in Microbiology: Scope and Challenges Volume 2},
issn = {0580-9517},
doi = {https://doi.org/10.1016/bs.mim.2024.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0580951724000461},
author = {Priyanshi Kumari and Bhavya Gaur and Vaibhav Mishra},
keywords = {Marine peptides, Artificial intelligence, Therapeutic values, Machine learning, Deep learning model, AMP's discovery},
abstract = {Marine ecosystem is a vast and relatively unexplored environment, where innumerable resources reside, including marine microbes, animals, algae, and other organisms' those have potential to produce different bioactive microbial peptides. Moreover, marine peptides are structurally unique and known for their exceptional bioactivity, with minimal to no harmful side effects. These bioactive peptides, isolated from marine sources, exhibit various properties, including antimicrobial, antiviral, anti-obesity, antioxidant, anti-inflammatory, and more hence, are deemed as future drugs. Furthermore, discovery of potential active peptides is exorbitant and laborious with traditional methods. Whereas, advanced computational techniques like Artificial Intelligence (AI) and their prime models make easier in the prediction and detection of important marine peptides. In this chapter we are highlighting modern AI based Machine Learning (ML) and Deep Learning (DL) models including k-Nearest Neighbour (kNN), Random Forest (RF), Artificial Neural Networks (ANNs) as ML, Fuzzy Logic or Adaptive Neuro-Fuzzy Inference System (ANFIS), Support Vector Machine (SVMs), and many more other DL models. Moreover, employing these advanced AI models to ease the isolation and identification of the bioactive microbial peptides from marine environments.}
}
@article{LOPEZ2023104398,
title = {Facets of social problem-solving as moderators of the real-time relation between social rejection and negative affect in an at-risk sample},
journal = {Behaviour Research and Therapy},
volume = {169},
pages = {104398},
year = {2023},
issn = {0005-7967},
doi = {https://doi.org/10.1016/j.brat.2023.104398},
url = {https://www.sciencedirect.com/science/article/pii/S0005796723001468},
author = {Roberto López and Christianne Esposito-Smythers and Annamarie B. Defayette and Katherine M. Harris and Lauren F. Seibel and Emma D. Whitmyre},
keywords = {Social problem-solving, Social rejection, Negative affect},
abstract = {Social rejection predicts negative affect, and theoretical work suggests that problem-solving deficits strengthen this relation in real-time. Nevertheless, few studies have explicitly tested this relation, particularly in samples at risk for suicide. This may be particularly important as social rejection and negative affect are significant predictors of suicide. The aim of the current study was to examine whether cognitive (i.e., perceiving problems as threats) and behavioral (i.e., avoidance) facets of problem-solving deficits moderated the real-time relation between social rejection and negative affect. The sample consisted of 49 young adults with past-month suicidal ideation. Demographic information, social problem-solving deficits, as well as depressive/anxiety symptoms and stress levels were assessed at baseline. Social rejection and negative affect were assessed using ecological momentary assessment over the following 28 days. Dynamic structural equation modeling was used to assess relations among study variables. After accounting for depressive/anxiety symptoms, stress levels, sex, and age, only avoidance of problems bolstered the real-time positive relation between social rejection severity and negative affect (b = 0.04, 95% credibility interval [0.003, 0.072]). Individuals with suicidal ideation who possess an avoidant problem-solving style may be particularly likely to experience heightened negative affect following social rejection and may benefit from instruction in problem-solving skills.}
}
@article{WU2025104172,
title = {TrustCNAV: Certificateless aggregate authentication of civil navigation messages in GNSS},
journal = {Computers & Security},
volume = {148},
pages = {104172},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2024.104172},
url = {https://www.sciencedirect.com/science/article/pii/S0167404824004772},
author = {Zhijun Wu and Yun Bai and Yuan Zhang and Liang Liu and Meng Yue},
keywords = {Satellite navigation, Spoofing attacks, Elliptic curve, Aggregate authentication, Authentication protocol},
abstract = {The Global Navigation Satellite System (GNSS) is capable of accurate positioning because it can provide high-precision data. These data are transmitted to the receiver in the form of navigation messages, called civil navigation messages (CNAV). As it is transmitted in an open, transparent environment without data integrity protection mechanisms and secure data transmission measures, the CNAV is suspected to spoofing attacks. In 2023, the OPSGROUP has received approximately 50 reports of GPS spoofing activity. A spoofed plane's navigation system will show it as being in a different place - a security risk if a jet is guided to fly into a hostile country's airspace. To prevent the forging of GNSS positioning data by spoofing attacks targeting CNAV, we propose a certificateless aggregation authentication for CNAV by using the elliptic curve discrete logarithm problem and the combination of the GNAV structural characteristics, called TrustCNAV. Security proof and performance analysis indicate that this authentication scheme can resist spoofing attacks and ensure data security of CNAV, also it avoids pairing operations with high computational complexity, thus meeting security requirements without causing too much time and communication consumption.}
}
@article{DAEMS2019101110,
title = {Building communities. Presenting a model of community formation and organizational complexity in southwestern Anatolia},
journal = {Journal of Anthropological Archaeology},
volume = {56},
pages = {101110},
year = {2019},
issn = {0278-4165},
doi = {https://doi.org/10.1016/j.jaa.2019.101110},
url = {https://www.sciencedirect.com/science/article/pii/S027841651830237X},
author = {Dries Daems},
keywords = {Social complexity, Community formation, Sagalassos, Anatolian archaeology, Social interaction, Organizational complexity},
abstract = {In this paper, a model of community formation and organizational complexity is presented, focusing on the fundamental role of social interactions and information transmission for the development of complex social organisation. The model combines several approaches in complex systems thinking which has garnered increasing attention in archaeology. It is then outlined how this conceptual model can be applied in archaeology. In the absence of direct observations of constituent social interactions, archaeologists study the past through material remnants found in the archaeological record. People used their material surroundings to shape, structure and guide social interactions and practices in various ways. The presented framework shows how dynamics of social organisation and community formation can be inferred from these material remains. The model is applied on a case study of two communities, Sagalassos and Düzen Tepe, located in southwestern Anatolia during late Achaemenid to middle Hellenistic times (fifth to second centuries BCE). It is suggested that constituent interactions and practices can be linked to the markedly different forms of organizational structures and material surroundings attested in both communities. The case study illustrates how the presented model can help understand trajectories of socio-political structures and organizational complexity on a community level.}
}
@article{CUFFARO201235,
title = {Many worlds, the cluster-state quantum computer, and the problem of the preferred basis},
journal = {Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics},
volume = {43},
number = {1},
pages = {35-42},
year = {2012},
issn = {1355-2198},
doi = {https://doi.org/10.1016/j.shpsb.2011.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S1355219811000694},
author = {Michael E. Cuffaro},
keywords = {Quantum computation, Quantum mechanics, Many worlds, Everettian interpretation, Quantum parallelism, Quantum speed-up, Cluster state, Measurement-based, One-way, Preferred basis problem},
abstract = {I argue that the many worlds explanation of quantum computation is not licensed by, and in fact is conceptually inferior to, the many worlds interpretation of quantum mechanics from which it is derived. I argue that the many worlds explanation of quantum computation is incompatible with the recently developed cluster state model of quantum computation. Based on these considerations I conclude that we should reject the many worlds explanation of quantum computation.}
}
@article{BUI1997575,
title = {Computational modelling of thermophysical processes in the light metals industry},
journal = {Revue Générale de Thermique},
volume = {36},
number = {8},
pages = {575-591},
year = {1997},
issn = {0035-3159},
doi = {https://doi.org/10.1016/S0035-3159(97)89985-0},
url = {https://www.sciencedirect.com/science/article/pii/S0035315997899850},
author = {Rung T Bui},
keywords = {computer modelling, light metals, thermophysical processes, electrolysis, casting, furnaces, modèles numériques, métaux légers, procédés thermophysiques, électrolyse, coulée, fours},
abstract = {This survey focuses on the aluminium industry, mostly on process aspects as opposed to metallurgical aspects. It covers recent work on process models involving fluid flow and heat transfer, and extends to all important categories of processes encountered in the primary aluminium industry, from raw materials and reduction to cast shop and recycling. This includes a wide variety of processes from precipitators, calciners, rotary kilns, baking furnaces, reduction cells, casting and mixing furnaces to recycling furnaces and metal filtration. A review is carried out on the modelling work, the applications, and the needs expressed not only in analysis and design but also in process control, optimization and supervision, as well as operator training. A summary is given of the problems perceived, mainly in the field of model parameters and model validation. Indications on future trends are also given. Conclusions are drawn from the survey of this fast-expanding body of knowledge that suggests tough challenges as well as unprecedented opportunities. Suggestions are made as to how some of those challenges could be met.
Résumé
Cette synthèse concerne l'industrie de l'aluminium et s'intéresse surtout aux procédés de fabrication, par opposition aux aspects métallurgiques. Elle couvre les travaux récents sur les modèles de procédés impliquant les écoulements et le transfert de chaleur. Elle inclut toutes les catégories de procédés rencontrées dans l'industrie de l'aluminium de première fusion, allant des matières premières à la réduction, la coulée et le recyclage. On y retrouve les précipitateurs, les calcinateurs, les fours rotatifs, les fours de cuisson, les cuves d'électrolyse, les fours de coulée ou de mélange, les fours de recyclage, ainsi que la filtration du métal. Les travaux de modélisation et leurs applications sont brièvement énumérés, les besoins exprimés, concernant non seulement l'analyse et la conception des procédés, mais aussi le contrôle, sont examinés, ainsi que des aspects touchant à l'optimisation, à la supervision et à la formation du personnel. Un résumé est fait des problèmes, notamment en ce qui a trait à la détermination des paramètres de modélisation et à la validation des modèles. On tente de dégager les tendances d'avenir. Des conclusions sont tirées de cette étude, qui semble mettre en lumière des défis de taille aussi bien que des opportunités sans précédent ; quelques suggestions visant à relever certains de ces défis sont proposées.}
}
@article{CHERNYSHOV20117408,
title = {System Identification Technique Application to Revealing Human-Operator Skills},
journal = {IFAC Proceedings Volumes},
volume = {44},
number = {1},
pages = {7408-7413},
year = {2011},
note = {18th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20110828-6-IT-1002.00077},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016447968},
author = {K.R. Chernyshov},
keywords = {Human factors and errors, Identification, Information correlation, Sampled data, Skills, Stochastic systems},
abstract = {Abstract
A new approach to abnormal situations with regard for the heuristic regularities of human-operator thinking process is proposed. The regularities are revealed on basis of recording the motions of the human-operator eyes over the information field of the control board and processing the experimental data obtained. For data processing, a probability theoretical approach is utilized. Such an approach is based on involving the notion of consistency of measures of dependence of random variables. Within the approach, a set of the so called information correlations has been proposed to serve as a quantitative performance index of human-operator skills.}
}
@article{NAARANOJA2015611,
title = {Multi-ontology Sense Making – Decision Making of Project Core Team},
journal = {Procedia Manufacturing},
volume = {3},
pages = {611-617},
year = {2015},
note = {6th International Conference on Applied Human Factors and Ergonomics (AHFE 2015) and the Affiliated Conferences, AHFE 2015},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2015.07.280},
url = {https://www.sciencedirect.com/science/article/pii/S2351978915002814},
author = {Marja Naaranoja},
keywords = {Sense making, Decision making, Ontology, Project, Core team, Construction industry},
abstract = {In order to understand core team's management task this paper studies the landscape of the decision making of the construction project core team. This paper uses multi-ontology sense making framework developed by Snowden. The four described situation illustrate the use of this framework. Firstly, a project core team create a project plan –timetable and cost estimate, that is supposed to be followed (rules and order) when making investment decision. Secondly, a project core team uses the plan but since the plan cannot be followed due to an unexpected situation the team changes the plan by calculating an optimal solution. In other words the team uses heuristic thinking when they change the rule (heuristics and order). Thirdly, the design group guides the design process by rules to get information for designing new facilities (rules and un-order). Fourthly, there are situations when the stakeholders have different kind of opinions in crisis and team cannot follow the preset orderly way of working (heuristics and un-order).}
}
@article{RICHEY2014857,
title = {A Complex Sociotechnical Systems Approach to Provisioning Educational Policies for Future Workforce},
journal = {Procedia Computer Science},
volume = {28},
pages = {857-864},
year = {2014},
note = {2014 Conference on Systems Engineering Research},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.03.102},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914001653},
author = {Michael Richey and Marcus Nance and Leroy Hanneman and William Hubbard and Azad M. Madni and Marc Spraragen},
keywords = {Socio-technical Systems, Systems Engineering Education, Engineering, Worldbuilding and Workforce Development},
abstract = {Reforming the U.S. educational system and workforce is a national challenge. Both industry leaders 30 and academics 2,28 concur that improving the quality, quantity [and alignment] of U.S STEM graduates are national imperatives. Models of the U.S. educational system, using complex sociotechnical systems’ approaches and tools that instill systems thinking, offer a holistic perspective to the educational and workforce challenges we face as a nation and allow us to identify and understand challenges associated with workforce preparedness, and increasing the number and technical excellence of STEM graduates 9,13,29. These models represent a sociotechnical system of systems with various sub-systems, each one representing an inherently complex and interdisciplinary problem of maintaining bi-directional, non-linear feedback relationships between one another. Each system involves multiple disparate stakeholders that need to collaborate within a time-and resource-intensive process while embedded in a larger sociotechnical system, aligned with the people, ideas, and support required to support desired global outcomes, of the system of systems, society and industry in particular11.}
}
@article{OLTEANU20161,
title = {Opportunity to communicate: The coordination between focused and discerned aspects of the object of learning},
journal = {The Journal of Mathematical Behavior},
volume = {44},
pages = {1-12},
year = {2016},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2016.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S073231231630116X},
author = {Lucian Olteanu},
keywords = {Algebra, Communication, Experience, Critical aspects, Opportunity to communicate},
abstract = {There are extensive concerns pertaining to the idea that students do not develop sufficient communication abilities in algebra and in mathematics more generally. This problem is at least partially related to their algebraic thinking. Although teaching should give students the opportunity to develop their ability to communicate, there are limited research insights as to why some forms of communication work better than others, and how and why instruction influences such communication. Two case studies are reported on in this article. The analysis of the opportunity to communicate was grounded in variation theory. Differences between focused aspects and discerned aspects of the object of learning are described. The results show that the coordination between the aspects focused on by the teacher and discerned by the students provides students with the opportunity to successfully communicate the content in algebra. In addition, the structure of the lesson influences the opportunity to communicate aspects of the content.}
}
@article{BRODO202025,
title = {A Constraint-based Language for Multiparty Interactions},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {351},
pages = {25-50},
year = {2020},
note = {Proceedings of LSFA 2020, the 15th International Workshop on Logical and Semantic Frameworks, with Applications (LSFA 2020)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2020.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S1571066120300396},
author = {Linda Brodo and Carlos Olarte},
keywords = {Concurrency theory, constraints, multiparty interactions},
abstract = {Multiparty interactions are common place in today's distributed systems. An agent usually communicates, in a single session, with other agents to accomplish a given task. Take for instance an online transaction including the vendor, the client, the credit card system and the bank. When specifying this kind of system, we probably observe a single transaction including several (binary) communications leading to changes in the state of all the involved agents. Multiway synchronization process calculi, that move from a binary to a multiparty synchronization discipline, have been proposed to formally study the behavior of those systems. However, adopting models such as Bodei, Brodo, and Bruni's Core Network Algebra (CNA), where the number of participants in an interaction is not fixed a priori, leads to an exponential blow-up in the number of states/behaviors that can be observed from the system. In this paper we explore mechanisms to tackle this problem. We extend CNA with constraints that declaratively allow the modeler to restrict the interaction that should actually happen. Our extended process algebra, called CCNA, finds application in balancing the interactions in a concurrent system, leading to a simple, deadlock-free and fair solution for the Dinning Philosopher problem. Our definition of constraints is general enough and it offers the possibility of accumulating costs in a multiparty negotiation. Hence, only computations respecting the thresholds imposed by the modeler are observed. We use this machinery to neatly model a Service Level Agreement protocol. We develop the theory of CCNA including its operational semantics and a behavioral equivalence that we prove to be a congruence. We also propose a prototypical implementation that allows us to verify, automatically, some of the systems explored in the paper.}
}
@article{GOLDMAN2025104323,
title = {The value of real-time automated explanations in stochastic planning},
journal = {Artificial Intelligence},
volume = {343},
pages = {104323},
year = {2025},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2025.104323},
url = {https://www.sciencedirect.com/science/article/pii/S0004370225000426},
author = {Claudia V. Goldman and Ronit Bustin and Wenyuan Qi and Zhengyu Xing and Rachel McPhearson-White and Sally Rogers},
keywords = {Explainable AI, Decision-Making, Human-Computer interaction},
abstract = {Recently, we are witnessing an increase in computation power and memory, leading to strong AI algorithms becoming applicable in areas affecting our daily lives. We focus on AI planning solutions for complex, real-life decision-making problems under uncertainty, such as autonomous driving. Human trust in such AI-based systems is essential for their acceptance and market penetration. Moreover, users need to establish appropriate levels of trust to benefit the most from these systems. Previous studies have motivated this work, showing that users can benefit from receiving (handcrafted) information about the reasoning of a stochastic AI planner, for example, controlling automated driving maneuvers. Our solution to automating these hand-crafted notifications with explainable AI algorithms, XAI, includes studying: (1) what explanations can be generated from an AI planning system, applied to a real-world problem, in real-time? What is that content that can be processed from a planner's reasoning that can help users understand and trust the system controlling a behavior they are experiencing? (2) when can this information be displayed? and (3) how shall we display this information to an end user? The value of these computed XAI notifications has been assessed through an online user study with 800 participants, experiencing simulated automated driving scenarios. Our results show that real time XAI notifications decrease significantly subjective misunderstanding of participants compared to those that received only a dynamic HMI display. Also, our XAI solution significantly increases the level of understanding of participants with prior ADAS experience and of participants that lack such experience but have non-negative prior trust to ADAS features. The level of trust significantly increases when XAI was provided to a more restricted set of the participants, including those over 60 years old, with prior ADAS experience and non-negative prior trust attitude to automated features.}
}
@article{XU2025112998,
title = {Discrete Differentiated Creative Search for traveling salesman problem},
journal = {Applied Soft Computing},
volume = {174},
pages = {112998},
year = {2025},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2025.112998},
url = {https://www.sciencedirect.com/science/article/pii/S1568494625003096},
author = {Qi Xu and Kewen Xia and Xiaoyu Chu},
keywords = {Population-based algorithm, Traveling salesman problem, Edge-based operations, Random nearest neighbor replacement, Greedy beam search},
abstract = {A novel population-based Discrete Differentiated Creative Search (DDCS) is proposed in this paper for solving the traveling salesman problem (TSP). DDCS introduces greedy beam search to adaptively initialize the population and improve the quality of the initial solutions. Second, a multi-edge construction operator, edge-based mathematical operations and a similarity attraction operator are used to guide individuals from different population categories towards higher-quality solutions based on the current solutions. Finally, a random nearest neighbor replacement strategy is used to replace individuals with the same distance heuristically, reducing the assimilation rate of the population. DDCS is tested with 50 instances from TSPLIB and compared with a variety of state-of-the-art and variants of classical algorithms. The results demonstrate that DDCS exhibits superior optimization capability and higher stability.}
}
@article{RASS202385,
title = {Adaptive dynamical systems modelling of transformational organizational change with focus on organizational culture and organizational learning},
journal = {Cognitive Systems Research},
volume = {79},
pages = {85-108},
year = {2023},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723000049},
author = {Lars Rass and Jan Treur and Wioleta Kucharska and Anna Wiewiora},
keywords = {Transformational Change, Organizational Culture, Organizational Learning, Safety Culture},
abstract = {Transformative Organizational Change becomes more and more significant both practically and academically, especially in the context of organizational culture and learning. However computational modeling and a formalization of organizational change and learning processes are still largely unexplored. This paper aims to provide an adaptive network model of transformative organizational change and translate a selection of organizational learning and change processes into computationally modelled processes. Additionally, it sets out to connect the dynamic systems view of organizations to self-modelling network models. The creation of the model and the implemented mechanisms of organizational processes are based on extrapolations of an extensive literature study and grounded in related work in this field, and then applied to a specified hospital-related case scenario in the context of safety culture. The model was evaluated by running several simulations and variations thereof. The results of these were investigated by qualitative analysis and comparison to expected emergent behaviour based on related available academic literature. The simulations performed confirmed the occurrence of an organizational transformational change towards a constant learning culture by offering repeated and effective learning and changes to organizational processes. Observations about various interplays and effects of the mechanism have been made, and they exposed that acceptance of mistakes as a part of learning culture facilitates transformational change and may foster sustainable change in the long run. Further, the model confirmed that the self-modelling network model approach applies to a dynamic systems view of organizations and a systems perspective of organizational change. The created model offers the basis for the further creation of self-modelling network models within the field of transformative organizational change and the translated mechanisms of this model can further be extracted and reused in a forthcoming academic exploration of this field.}
}
@article{GHABOUSSI201275,
title = {Unifying Principles for Sudden Transitions in All Systems},
journal = {Procedia Computer Science},
volume = {8},
pages = {75-80},
year = {2012},
note = {Conference on Systems Engineering Research},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2012.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S1877050912000178},
author = {Jamshid Ghaboussi},
keywords = {Complex systems, Sudden transitions, System properties, Tipping points},
abstract = {All physical, natural, biological and socio-economic systems – also referred to as complex systems - have system-level properties that result from the interactions between their components. In most cases it is not possible to determine the complete system-level properties with the current state of our knowledge. Where there are system-level properties, the uncoupled form of those properties is the eigen-system consisting of system eigenvalues and eigenfunctions, even though at the present we are not able to determine them through modelling or observation. All systems operate in equilibrium states; small perturbations cause small changes. While these systems normally undergo gradual changes in their system-level properties, they can also undergo sudden transitions to new equilibrium states. New insights into these important transitions are proposed in this paper. In some mechanical systems transition occur when the smallest system eigenvalue goes to zero. It is proposed that the same principles apply to all systems. Transitions in all systems occur when at least one system eigenvalue goes to zero. Generalization of these principles to all systems will encourage new ways of thinking about systems and will suggest new research directions in studying these important major transitions, potentially leading to reliable methods for predicting their onset.}
}
@article{GUEST1989560,
title = {An overview of vector and parallel processors in scientific computation},
journal = {Computer Physics Communications},
volume = {57},
number = {1},
pages = {560},
year = {1989},
issn = {0010-4655},
doi = {https://doi.org/10.1016/0010-4655(89)90285-3},
url = {https://www.sciencedirect.com/science/article/pii/0010465589902853},
author = {M. Guest}
}
@article{SHER1992505,
title = {A computational normative theory of scientific evidence},
journal = {International Journal of Approximate Reasoning},
volume = {6},
number = {4},
pages = {505-524},
year = {1992},
issn = {0888-613X},
doi = {https://doi.org/10.1016/0888-613X(92)90002-H},
url = {https://www.sciencedirect.com/science/article/pii/0888613X9290002H},
author = {David B. Sher},
keywords = {interval probability, evidence combination, experimental evidence, probabilistic logic, statistical inference},
abstract = {A scientific reasoning system makes decisions using objective evidence in the form of independent experimental trials, propositional axioms, and constraints on the probabilities of events. I propose a collection of algorithms that derive probability intervals and estimate conditional probabilities from objective evidence in those forms. This reasoning system can manage uncertainty about data and rules in a rule-based expert system. I expect that the system will be particularly applicable to diagnosis and analysis in domains with a wealth of experimental evidence such as medicine. The algorithms currently apply to systems with arbitrary amounts of experimental evidence but with less than 20 variables. I discuss limitations of this solution and propose future directions for this research. This work can be considered a generalization of Nilsson's “probabilistic logic” to intervals and experimental observations.}
}
@article{GILL2019556,
title = {Holons on the Horizon: Re-Understanding Automation and Control},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {25},
pages = {556-561},
year = {2019},
note = {19th IFAC Conference on Technology, Culture and International Stability TECIS 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.12.605},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319325261},
author = {Karamjit S Gill},
keywords = {data science, human-machine interaction, cybernetics, systems architecture, holon, symbiosis, valorisation, cultural architectures},
abstract = {In Re-Understanding Automation and Control in the era of digital automation of societal systems, we need to understand the inter-connected relations between knowledge, culture, technology and society. This in turn demands the exploration of social and cultural architectures, which facilitate them. Whilst computational model of data systems is built upon the bottom-up architecture, it is the top-down architecture of social and cultural contexts that synchronises the processing and outcomes of data systems, may they relate to organisational systems, heath and welfare systems, or institutional systems. It is this notion of the inter-dependence of the bottom-up and top-down architectures that makes us act beyond the linear gaze worldview of automation and control of production systems, and explore the multiplicity of interconnections between and across societal systems. In these horizons, we see the inter-connectedness between the unit and the whole, between the horizontal and vertical, and a symbiosis of hand and brain- an augmentation of the human and the machine. The ideas of inter-connectedness, augmentation and symbiosis lie at the core of holonic horizons. These horizons allow us to transcend the limit of the calculation and control model of automation, and enable the design of human-centred systems that valorise differences whilst utilising the richness and diversity of human-machine collaborations. When we envision these interactions and collaborations as a systems developmental process, we begin to visualise systems design from an interdependent perspective, which goes beyond the linear gaze of “utility”. The paper explores the ways holonic architectures engage us in the design process.}
}
@article{GAO2025111002,
title = {Learning and knowledge-guided evolutionary algorithm for the large-scale buffer allocation problem in production lines},
journal = {Computers & Industrial Engineering},
volume = {203},
pages = {111002},
year = {2025},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2025.111002},
url = {https://www.sciencedirect.com/science/article/pii/S0360835225001482},
author = {Sixiao Gao and Fan Zhang and Shuo Shi},
keywords = {Buffer allocation, Large-scale, Evolutionary algorithm, Learning-guided, Knowledge-guided},
abstract = {The large-scale buffer allocation problem (LBAP) in production lines represents a significant optimization challenge, centered on the efficient allocation of limited temporary storage areas. Prior research has predominantly addressed the LBAP through dynamic programming, search algorithms, and metaheuristics. However, these methodologies are often problem-specific and inefficient when applied to large-scale scenarios. Consequently, there is a pressing need to investigate innovative algorithms beyond existing approaches. This paper presents a novel learning and knowledge-guided evolutionary algorithm designed for the LBAP in production lines. The proposed algorithm develops an adaptive genetic algorithm and a variable neighborhood search algorithm, incorporating a simulated annealing-based strategy. An online Q-learning algorithm is employed to dynamically select the more effective of the two preceding algorithms for solution updates, while the simulated annealing-based strategy regulates the acceptance of these updated solutions. Furthermore, The proposed algorithm dynamically adjusts crossover, mutation, and shaking rates to adapt to the neighborhood structure. It also leverages conflict knowledge obtained from prior update experiences to inform the search process, thereby enhancing solution quality and computational efficiency. Numerical results indicate that the proposed algorithm surpasses state-of-the-art methods in addressing the LBAP. Additionally, empirical ablation studies demonstrate that the knowledge-guided approach efficiently explores promising solution regions by eliminating low-value solutions, while the learning-guided approach effectively generates improved solutions by selecting optimal strategies. This proposed algorithm significantly advances dynamic production resource allocation in large-scale systems.}
}
@article{HAPPE2025112240,
title = {Authentic interdisciplinary online courses for alternative pathways into computer science},
journal = {Journal of Systems and Software},
volume = {219},
pages = {112240},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112240},
url = {https://www.sciencedirect.com/science/article/pii/S016412122400284X},
author = {Lucia Happe and Kai Marquardt},
keywords = {Interdisciplinary teaching, e-learning, Interest, Engagement, Diversity, Gender, Computer science education},
abstract = {The field of computer science (CS) is facing a crucial challenge in broadening participation and embracing diversity, especially among underrepresented gender groups. The presented interdisciplinary educational program is an efficient response to this challenge, designed to catalyze diversity in CS through engagement with complex, interest-driven problems. This paper outlines the program’s structure, elucidates the pedagogical underpinnings, and reflects on the emergent challenges and opportunities. We delve into how the fusion of CS with other academic disciplines can allure a more varied demographic, emphasizing the engagement of female high school students—a demographic pivotally positioned yet significantly untapped in CS. Through a systematic survey analysis, we measure the program’s efficacy in increasing interest in CS and in cultivating an appreciation for its application in addressing real-world, cross-disciplinary challenges. Our findings affirm the program’s success in bridging the engagement gap by leveraging students’ intrinsic interests, thus charting alternative pathways into the CS field. These insights underscore the critical role of interdisciplinary approaches, establishing a new standard for transformative CS educational methods.}
}
@article{MILANEZ200793,
title = {A new method for real time computation of power quality indices based on instantaneous space phasors},
journal = {Electric Power Systems Research},
volume = {77},
number = {1},
pages = {93-98},
year = {2007},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2006.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0378779606000265},
author = {Dalgerti L. Milanez and Rade M. Ciric},
keywords = {Instantaneous space phasors, Instantaneous complex power, Buchholz–Goodhue effective apparent power, Power definitions, Unbalance, Dispersed generators},
abstract = {One of the important issues about using renewable energy is the integration of dispersed generation in the distribution networks. Previous experience has shown that the integration of dispersed generation can improve voltage profile in the network, decrease loss, etc. but can create safety and technical problems as well. This work report the application of the instantaneous space phasors and the instantaneous complex power in observing performances of the distribution networks with dispersed generators in steady state. New IEEE apparent power definition, the so-called Buchholz–Goodhue effective apparent power, as well as new proposed power quality (oscillation) index in the three-phase distribution systems with unbalanced loads and dispersed generators, are applied. Results obtained from several case studies using IEEE 34 nodes test network are presented and discussed.}
}
@article{MANORAT2025100403,
title = {Artificial intelligence in computer programming education: A systematic literature review},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100403},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100403},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000438},
author = {Pisut Manorat and Suppawong Tuarob and Siripen Pongpaichet},
keywords = {Artificial intelligence, Machine learning, Computer programming education, Systematic literature review},
abstract = {The demand for skilled programmers and the increasing complexity of coding skills have led to a rise in the adoption of artificial intelligence (AI) and machine learning (ML) technologies in computer programming education. Previous research has explored the potential of AI in aspects such as grading assignments, generating feedback, detecting plagiarism, and identifying at-risk students, but there is a lack of systematic reviews focused on AI-powered teaching processes in computer programming classes. To provide a more comprehensive understanding of AI and ML's role in computer programming education, this systematic review examines a wider range of applications across the entire pedagogical process. Analyzing 119 relevant research papers published between 2012 and 2024, this review offers an overview of AI and ML tools and techniques used in various educational contexts. Aligned with instructional design models, the reviewed literature is categorized into four key areas: course design, classroom implementation, assessment and feedback, and performance monitoring. This systematic review not only highlights the practical tools available to instructors but also identifies research trends and potential areas for future exploration in the field of computer programming education.}
}
@article{SMIRNOV20142507,
title = {Domain Ontologies Integration for Virtual Modelling and Simulation Environments},
journal = {Procedia Computer Science},
volume = {29},
pages = {2507-2514},
year = {2014},
note = {2014 International Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.05.234},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914004116},
author = {Pavel A. Smirnov and Sergey V. Kovalchuk and Alexey V. Dukhanov},
keywords = {Virtual simulation objects, semantic technologies, computational experiment, knowledge base},
abstract = {This paper presents a model of semantic ontologies integration into workflow co mposition design process via Virtual Simu lation Objects (VSO) concept and technology. Doma in knowledge distributed over open linked data sources may be usefully applied for new VSO-images design and used for organization co mputational-intensive simulation e xpe riments. In this paper we describe the VSO- architecture e xtended with novel functionality regarding integration with lin ked open data sources. We also provide a computational-scientific e xa mp le of do ma in-specific use-case offering a solution for some public-transportation domain problem.}
}
@article{TSUTSUI201856,
title = {A Bayesian network model for supporting the formation of PSS design knowledge},
journal = {Procedia CIRP},
volume = {73},
pages = {56-60},
year = {2018},
note = {10th CIRP Conference on Industrial Product-Service Systems, IPS2 2018, 29-31 May 2018, Linköping, Sweden},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2018.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212827118305171},
author = {Yusuke Tsutsui and Yosuke Kubota and Yoshiki Shimomura},
keywords = {Product-service systems, Design knowledge, Bayesian network},
abstract = {Recently, product-service systems (PSS) have drawn the interest of the manufacturing industry. Designing PSS to enhance the value of their core products, manufacturers should assume that their products are their strength or constraint and also derive the service solution logically. However, PSS design knowledge to determine the services suitable for manufacturers’ core products is unclear. As a result, determining a service solution that is compatible with their core products is difficult. This difficulty consequently prevents the manufacturing industry from realising high-quality PSS. To form PSS design knowledge efficiently, this study aims to support the analysis of the complicated and diverse relationships between product characteristics and service contents. Specifically, a Bayesian network model that represents the logical structure between the product characteristics and service contents common among existing PSS cases is constructed through computational learning based on statistical data on PSS cases.}
}
@article{CARLSON201888,
title = {Ghosts in machine learning for cognitive neuroscience: Moving from data to theory},
journal = {NeuroImage},
volume = {180},
pages = {88-100},
year = {2018},
note = {New advances in encoding and decoding of brain signals},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2017.08.019},
url = {https://www.sciencedirect.com/science/article/pii/S1053811917306663},
author = {Thomas Carlson and Erin Goddard and David M. Kaplan and Colin Klein and J. Brendan Ritchie},
keywords = {Multivariate pattern analysis, Brain decoding, Exploratory methods, fMRI, Magnetoencephalography},
abstract = {The application of machine learning methods to neuroimaging data has fundamentally altered the field of cognitive neuroscience. Future progress in understanding brain function using these methods will require addressing a number of key methodological and interpretive challenges. Because these challenges often remain unseen and metaphorically “haunt” our efforts to use these methods to understand the brain, we refer to them as “ghosts”. In this paper, we describe three such ghosts, situate them within a more general framework from philosophy of science, and then describe steps to address them. The first ghost arises from difficulties in determining what information machine learning classifiers use for decoding. The second ghost arises from the interplay of experimental design and the structure of information in the brain – that is, our methods embody implicit assumptions about information processing in the brain, and it is often difficult to determine if those assumptions are satisfied. The third ghost emerges from our limited ability to distinguish information that is merely decodable from the brain from information that is represented and used by the brain. Each of the three ghosts place limits on the interpretability of decoding research in cognitive neuroscience. There are no easy solutions, but facing these issues squarely will provide a clearer path to understanding the nature of representation and computation in the human brain.}
}
@article{MCLEAN2020,
title = {Simulation Modeling as a Novel and Promising Strategy for Improving Success Rates With Research Funding Applications: A Constructive Thought Experiment},
journal = {JMIR Nursing},
volume = {3},
number = {1},
year = {2020},
issn = {2562-7600},
doi = {https://doi.org/10.2196/18983},
url = {https://www.sciencedirect.com/science/article/pii/S2562760020000125},
author = {Allen McLean and Wade McDonald and Donna Goodridge},
keywords = {simulation modeling, computational science, funding application, grant funding, grant writing, nursing, research, thought experiment, persuasive technology, peripheral vascular disease},
abstract = {Writing a successful grant or other funding applications is a requirement for continued employment, promotion, and tenure among nursing faculty and researchers. Writing successful applications is a challenging task, with often uncertain results. The inability to secure funding not only threatens the ability of nurse researchers to conduct relevant health care research but may also negatively impact their career trajectories. Many individuals and organizations have offered advice for improving success with funding applications. While helpful, those recommendations are common knowledge and simply form the basis of any well-considered, well-formulated, and well-written application. For nurse researchers interested in taking advantage of innovative computational methods and leading-edge analytical techniques, we propose adding the results from computer-based simulation modeling experiments to funding applications. By first conducting a research study in a virtual space, nurse researchers can refine their study design, test various assumptions, conduct experiments, and better determine which elements, variables, and parameters are necessary to answer their research question. In short, simulation modeling is a learning tool, and the modeling process helps nurse researchers gain additional insights that can be applied in their real-world research and used to strengthen funding applications. Simulation modeling is well-suited for answering quantitative research questions. Still, the design of these models can benefit significantly from the addition of qualitative data and can be helpful when simulating the results of mixed methods studies. We believe this is a promising strategy for improving success rates with funding applications, especially among nurse researchers interested in contributing new knowledge supporting the paradigm shift in nursing resulting from advances in computational science and information technology.}
}
@article{HONG2015671,
title = {Free will: A case study in reconciling phenomenological philosophy with reductionist sciences},
journal = {Progress in Biophysics and Molecular Biology},
volume = {119},
number = {3},
pages = {671-727},
year = {2015},
note = {Integral Biomathics: Life Sciences, Mathematics, and Phenomenological Philosophy},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2015.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S0079610715001212},
author = {Felix T. Hong},
keywords = {Free will, Determinism, Quantum indeterminacy, Downward causation, Naturalizing phenomenology, Visual thinking},
abstract = {Phenomenology aspires to philosophical analysis of humans' subjective experience while it strives to avoid pitfalls of subjectivity. The first step towards naturalizing phenomenology — making phenomenology scientific — is to reconcile phenomenology with modern physics, on the one hand, and with modern cellular and molecular neuroscience, on the other hand. In this paper, free will is chosen for a case study to demonstrate the feasibility. Special attention is paid to maintain analysis with mathematical precision, if possible, and to evade the inherent deceptive power of natural language. Laplace's determinism is re-evaluated along with the concept of microscopic reversibility. A simple and transparent version of proof demonstrates that microscopic reversibility is irreconcilably incompatible with macroscopic irreversibility, contrary to Boltzmann's claim. But the verdict also exalts Boltzmann's statistical mechanics to the new height of a genuine paradigm shift, thus cutting the umbilical cord linking it to Newtonian mechanics. Laplace's absolute determinism must then be replaced with a weaker form of causality called quasi-determinism. Biological indeterminism is also affirmed with numerous lines of evidence. The strongest evidence is furnished by ion channel fluctuations, which obey an indeterministic stochastic phenomenological law. Furthermore, quantum indeterminacy is shown to be relevant in biology, contrary to the opinion of Erwin Schrödinger. In reconciling phenomenology of free will with modern sciences, three issues — alternativism, intelligibility and origination — of free will must be accounted for. Alternativism and intelligibility can readily be accounted for by quasi-determinism. In order to account for origination of free will, the concept of downward causation must be invoked. However, unlike what is commonly believed, there is no evidence that downward causation can influence, shield off, or overpower low-level physical forces already known to physicists. Quasi-determinism offers an escape route: The possibility that downward causation arising from hierarchical organization of biological structures can modify dispersions of physical laws remains open. Empirical evidence in support of downward causation is scanty but nevertheless exists. Still, origination of free will must be considered an unsolved problem at present. It is demonstrated that objectivity does not guarantee scientific rigor in the study of complex phenomena, such as human creativity. In its replacement, universality and overall consistency between a theory and empirical evidence must be maintained. Visual thinking is proposed as a reasoning tool to ensure universality and overall consistency through inference to the best explanation.}
}
@article{KARVONEN2023101166,
title = {Fundamental concepts of cognitive mimetics},
journal = {Cognitive Systems Research},
volume = {82},
pages = {101166},
year = {2023},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.101166},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723001006},
author = {Antero Karvonen and Tuomo Kujala and Tommi Kärkkäinen and Pertti Saariluoma},
keywords = {AI design, Cognitive mimetics, Design, Artificial intelligence},
abstract = {The rapid development and widespread adoption of Artificial Intelligence (AI) technologies have made the development of AI-specific design methods an important topic to advance. In recent decades, the centre of gravity in AI has shifted away from cognitive science and related fields like psychology. However, there is a clear need and potential for added value in returning to stronger interaction. One potential challenge for this interaction may be the lack of common conceptual grounds and design languages. In this article, we aim to contribute to the development of conceptual interfaces for human-based AI-specific design methods through the idea of cognitive mimetics. We begin by introducing basic concepts from mimetic design and interpret them in the context of this thematic area. These provide some of the basic building blocks for a design language and bring to the surface key questions. These in turn provide a ground for explicating cognitive mimetics. In the second part of this paper, we focus on specifying a key aspect in cognitive mimetics: the contents of information processes. Others engaged in this field can derive value from using or developing the basic conceptual machinery to specify their own approaches in this interdisciplinary field that is still shaping itself. Furthermore, those who resonate with the idea of cognitive mimetics, as specified here, can join in taking this particular approach further.}
}
@article{GERSHMAN2023104825,
title = {The molecular memory code and synaptic plasticity: A synthesis},
journal = {Biosystems},
volume = {224},
pages = {104825},
year = {2023},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2022.104825},
url = {https://www.sciencedirect.com/science/article/pii/S0303264722002064},
author = {Samuel J. Gershman},
keywords = {Memory, Free energy, Synaptic plasticity, Learning, Inference},
abstract = {The most widely accepted view of memory in the brain holds that synapses are the storage sites of memory, and that memories are formed through associative modification of synapses. This view has been challenged on conceptual and empirical grounds. As an alternative, it has been proposed that molecules within the cell body are the storage sites of memory, and that memories are formed through biochemical operations on these molecules. This paper proposes a synthesis of these two views, grounded in a computational model of memory. Synapses are conceived as storage sites for the parameters of an approximate posterior probability distribution over latent causes. Intracellular molecules are conceived as storage sites for the parameters of a generative model. The model stipulates how these two components work together as part of an integrated algorithm for learning and inference.}
}
@article{KELLYPITOU201723,
title = {Microgrids and resilience: Using a systems approach to achieve climate adaptation and mitigation goals},
journal = {The Electricity Journal},
volume = {30},
number = {10},
pages = {23-31},
year = {2017},
issn = {1040-6190},
doi = {https://doi.org/10.1016/j.tej.2017.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S1040619017303007},
author = {Katrina M. Kelly-Pitou and Anais Ostroski and Brandon Contino and Brandon Grainger and Alexis Kwasinski and Gregory Reed},
keywords = {Sustainable energy, Microgrid, Sustainable development, Energy policy, Climate change policy, Climate change adaptation, Adaptive capacity, Ecological modernization},
abstract = {Although energy resource sustainability has been researched extensively, the understanding of how we use and interact with electricity sustainably is less understood. New electrical designs, like microgrids, provide opportunities to better address the immediate needs of electrical sustainability and urban development. This paper analyzes the role of microgrids in urban development and examines how greater systemic thinking between infrastructure planning and energy policymaking can increase a city’s resilience.}
}
@article{MA2024100647,
title = {Design of online teaching interaction mode for vocational education based on gamified-learning},
journal = {Entertainment Computing},
volume = {50},
pages = {100647},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100647},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124000156},
author = {Zhongbao Ma and Wei Li},
keywords = {Gamified-learning, Traveler-type problems, Genetic-based algorithms, Game design},
abstract = {Along with the process of building China's modern vocational education system, China's higher vocational education has made great progress. With the development of computer and Internet technology, gamified learning, as a new way of learning, combines the advantages of computer games and online learning, which not only meets the needs of people to learn anytime and anywhere, but also increases the fun of learning activities. In this paper, we developed a gamified learning software with traveler-type problems as the research content, through the interaction with the game, so that students can think in the game and learn knowledge through the game. Through the questionnaire for research and analysis, this game is good game fun and can stimulate learning interest well. In addition, this paper carries out an in-depth study of the game's help system, optimizes the algorithm for the help system, and proposes an improved genetic algorithm. The reverse learning method is adopted to improve the accuracy and convergence speed of the optimal solution; then the Metropolis criterion is used to improve the crossover and mutation operators to enhance the local search ability of the algorithm; finally, the concept of realistic elite learning is introduced to further enhance the local search ability of the algorithm. The simulation results show that the algorithm is effectively improved in convergence performance and solution accuracy, which can significantly improve the response speed of the help system, effectively improve the game's fun, and improve the game's playability.}
}
@article{KANWISHER2025102969,
title = {Animal models of the human brain: Successes, limitations, and alternatives},
journal = {Current Opinion in Neurobiology},
volume = {90},
pages = {102969},
year = {2025},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2024.102969},
url = {https://www.sciencedirect.com/science/article/pii/S0959438824001314},
author = {Nancy Kanwisher},
abstract = {The last three decades of research in human cognitive neuroscience have given us an initial “parts list” for the human mind in the form of a set of cortical regions with distinct and often very specific functions. But current neuroscientific methods in humans have limited ability to reveal exactly what these regions represent and compute, the causal role of each in behavior, and the interactions among regions that produce real-world cognition. Animal models can help to answer these questions when homologues exist in other species, like the face system in macaques. When homologues do not exist in animals, for example for speech and music perception, and understanding of language or other people's thoughts, intracranial recordings in humans play a central role, along with a new alternative to animal models: artificial neural networks.}
}
@incollection{MONTEIRO202253,
title = {4 - An artificial intelligent cognitive approach for classification and recognition of white blood cells employing deep learning for medical applications},
editor = {Deepak Gupta and Utku Kose and Ashish Khanna and Valentina Emilia Balas},
booktitle = {Deep Learning for Medical Applications with Unique Data},
publisher = {Academic Press},
pages = {53-69},
year = {2022},
isbn = {978-0-12-824145-5},
doi = {https://doi.org/10.1016/B978-0-12-824145-5.00012-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128241455000125},
author = {Ana Carolina Borges Monteiro and Reinaldo Padilha França and Rangel Arthur and Yuzo Iano},
keywords = {Artificial intelligence, Biomedical signals, CNN, Cognitive computing, Cognitive health care, Cognitive models, Deep learning, Digital image, Erythrocytes, Health care, Health care data, Health care informatics, Image processing, Leukocytes, Python},
abstract = {Cognitive computing aims to implement a unified computational theory similar to human thought, consisting of systems whose objective is to mimic human mental tasks based on the concepts of artificial intelligence and machine learning generating and understanding knowledge. Deep learning is an abstraction of the biological neural network, and can understood as a complex structure interconnected by simple processing elements (neurons), can perform operations as calculations in parallel, for data processing and representation of knowledge. Convolutional neural networks for image recognition through deep learning has been modeled to determine good performance with respect to digital image recognition, especially in medical areas, which is a classic problem of computational classification. In this context, an artificial intelligent cognitive approach was developed achieving an accuracy of 84.19%, which used Python employing Jupyter Notebook, with a dataset of 12,500 medical digital images of human blood smear fields of nonpathologic leukocytes.}
}
@article{YEAP1988297,
title = {Towards a computational theory of cognitive maps},
journal = {Artificial Intelligence},
volume = {34},
number = {3},
pages = {297-360},
year = {1988},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(88)90064-1},
url = {https://www.sciencedirect.com/science/article/pii/0004370288900641},
author = {Wai K. Yeap},
abstract = {A computational theory of cognitive maps is developed which can explain some of the current findings about cognitive maps in the psychological literature and which provides a coherent framework for future development. The theory is tested with several computer implementations which demonstrate how the shape of the environment is computed and how one's conceptual representation of the environment is derived. We begin with the idea that the cognitive mapping process should be studied as two loosely coupled modules: The first module, known as the raw cognitive map, is computed from information made explicit in Marr's 212-D sketch and not from high-level descriptions of what we perceive. The second module, known as the full cognitive map, takes the raw cognitive map as input and produces different “abstract representations” for solving high-level spatial tasks faced by the individual.}
}
@article{STRYCKER2020e04358,
title = {K-12 art teacher technology use and preparation},
journal = {Heliyon},
volume = {6},
number = {7},
pages = {e04358},
year = {2020},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2020.e04358},
url = {https://www.sciencedirect.com/science/article/pii/S2405844020312020},
author = {Jesse Strycker},
keywords = {Applications in the subject area, Art education, Educational technology, Elementary education, Instructional technology, Post-secondary education, Secondary education, Teaching/learning strategies, Educational development, Evaluation in education, Media education, Pedagogy, Teaching research, Education},
abstract = {Largely absent from educational/instructional technology journals, this study focused on how K-12 art teachers in a southern state used technology to support teaching and learning, uses they found to be the best, and what kinds of technology training they received as part of their initial teacher preparation. Findings indicated that presentation and resource access technologies had transformed the way art teachers in the study work with students and materials. They also had little use of technology to support students with special needs and had limited technology experiences in their own training. Elementary art teachers were found to have more examples of student higher-order thinking skills promoting technology use, while secondary art teachers had more student media creation and a desire to implement digital portfolios. Additional findings and interpretations are offered.}
}
@article{GROSS2019116125,
title = {Is perception the missing link between creativity, curiosity and schizotypy? Evidence from spontaneous eye-movements and responses to auditory oddball stimuli},
journal = {NeuroImage},
volume = {202},
pages = {116125},
year = {2019},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2019.116125},
url = {https://www.sciencedirect.com/science/article/pii/S1053811919307165},
author = {Madeleine E. Gross and Draulio B. Araujo and Claire M. Zedelius and Jonathan W. Schooler},
keywords = {Creativity, Curiosity, Schizotypy, Eye-tracking, Eye-gaze, Salience, Perception},
abstract = {What is the relationship between creativity, curiosity, and schizotypy? Schizophrenia-spectrum conditions and creativity have been linked to deficits in filtering sensory information, and curiosity is associated with information-seeking. This raises the possibility of a perception-based link between all three concepts. Here, we investigated whether the same individual differences in perceptual encoding explain variance in creativity, curiosity, and schizotypy. We administered an active auditory oddball task and a free viewing eye-tracking paradigm (N = 88). Creativity was measured with the figural portion of the Torrance Tests of Creative Thinking (TTCT) and two self-report scales. Schizotypy and curiosity were measured with self-reports. We found that creativity was associated with increased reaction time to the rare tone in the oddball task and was positively associated with the number and duration of fixations in the free viewing task. Schizotypy, on the other hand, showed a negative trend with the number and duration of fixations. Both creativity and curiosity were positively associated with explorative eye movements (unique number of regions visited) and Shannon entropy, while schizotypy was negatively associated with entropy. We further compared saliency maps finding that individuals high versus low in creativity and curiosity, respectively, exhibit differences in where they look. These findings may suggest a perception-based link between creativity and curiosity, but not schizotypy. Implications and limitations of these findings are discussed.}
}
@article{CLANCY2008248,
title = {Applications of complex systems theory in nursing education, research, and practice},
journal = {Nursing Outlook},
volume = {56},
number = {5},
pages = {248-256.e3},
year = {2008},
note = {Special Issue Informatics: Science and Practice},
issn = {0029-6554},
doi = {https://doi.org/10.1016/j.outlook.2008.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S0029655408001619},
author = {Thomas R. Clancy and Judith A. Effken and Daniel Pesut},
abstract = {The clinical and administrative processes in today's healthcare environment are becoming increasingly complex. Multiple providers, new technology, competition, and the growing ubiquity of information all contribute to the notion of health care as a complex system. A complex system (CS) is characterized by a highly connected network of entities (e.g., physical objects, people or groups of people) from which higher order behavior emerges. Research in the transdisciplinary field of CS has focused on the use of computational modeling and simulation as a methodology for analyzing CS behavior. The creation of virtual worlds through computer simulation allows researchers to analyze multiple variables simultaneously and begin to understand behaviors that are common regardless of the discipline. The application of CS principles, mediated through computer simulation, informs nursing practice of the benefits and drawbacks of new procedures, protocols and practices before having to actually implement them. The inclusion of new computational tools and their applications in nursing education is also gaining attention. For example, education in CSs and applied computational applications has been endorsed by The Institute of Medicine, the American Organization of Nurse Executives and the American Association of Colleges of Nursing as essential training of nurse leaders. The purpose of this article is to review current research literature regarding CS science within the context of expert practice and implications for the education of nurse leadership roles. The article focuses on 3 broad areas: CS defined, literature review and exemplars from CS research and applications of CS theory in nursing leadership education. The article also highlights the key role nursing informaticists play in integrating emerging computational tools in the analysis of complex nursing systems.}
}
@article{HEINZE2021R1381,
title = {Fly navigation: Yet another ring},
journal = {Current Biology},
volume = {31},
number = {20},
pages = {R1381-R1383},
year = {2021},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2021.09.009},
url = {https://www.sciencedirect.com/science/article/pii/S0960982221012495},
author = {Stanley Heinze},
abstract = {Summary
Flies keep track of a food site by path integration. A novel behavioral paradigm has been combined with computational models to show that Drosophila can track at least three food patches simultaneously by using the center of gravity of all food sites as the reference point for their path integrator.}
}
@article{MACLENNAN2015410,
title = {Living science: Science as an activity of living beings},
journal = {Progress in Biophysics and Molecular Biology},
volume = {119},
number = {3},
pages = {410-419},
year = {2015},
note = {Integral Biomathics: Life Sciences, Mathematics, and Phenomenological Philosophy},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2015.08.009},
url = {https://www.sciencedirect.com/science/article/pii/S0079610715001224},
author = {Bruce J. MacLennan},
keywords = {Philosophy of science, Phenomenology, Embodied cognition, Causality, Analytical psychology, Goethe},
abstract = {The philosophy of science should accommodate itself to the facts of human existence, using all aspects of human experience to adapt more effectively, as individuals, species, and global ecosystem. This has several implications: (1) Our nature as sentient beings interacting with other sentient beings requires the use of phenomenological methods to investigate consciousness. (2) Our embodied, situated, purposeful physical interactions with the world are the foundation of scientific understanding. (3) Aristotle's four causes are essential for understanding living systems and, in particular, the final cause aids understanding the role of humankind, and especially science, in the global ecosystem. (4) In order to fulfill this role well, scientists need to employ the full panoply of human faculties. These include the consciousness faculties (thinking, sensation, feeling, intuition), and therefore, as advocated by many famous scientists, we should cultivate our aesthetic sense, emotions, imagination, and intuition. Our unconscious faculties include archetypal structures common to all humans, which can guide scientific discovery. By striving to engage the whole of human nature, science will fulfill better its function for humans and the global ecosystem.}
}
@article{OSMAN2013188,
title = {21st Century Biology: An Interdisciplinary Approach of Biology, Technology, Engineering and Mathematics Education},
journal = {Procedia - Social and Behavioral Sciences},
volume = {102},
pages = {188-194},
year = {2013},
note = {6th International Forum on Engineering Education (IFEE 2012)},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2013.10.732},
url = {https://www.sciencedirect.com/science/article/pii/S1877042813042687},
author = {Kamisah Osman and Lee Chuo Hiong and Rian Vebrianto},
keywords = {interdisciplinary, BTEM (Biology, Technology, Engineering, Mathematics), inquiry-discovery, 21st century skills},
abstract = {The principal goal of interdisciplinary approach for Biology, Technology, Engineering and Mathematics (BTEM) is to cultivate scientific inquiry that requires coordination of both knowledge and skills simultaneously. The dominant activity for BTEM is inquiry-discovery on the authentic problems. This is intended to enhance the students’ abilities to construct their own knowledge through the relevant hands-on and minds-on activities. The essence of engineering is inventive problem solving. The Integration of advanced information communication technologies believed to be able to fulfill current Net Generation learning styles. Mathematics plays an important role as computational tools. The expected outcome of BTEM implementation is the inculcation of 21st century skills.}
}