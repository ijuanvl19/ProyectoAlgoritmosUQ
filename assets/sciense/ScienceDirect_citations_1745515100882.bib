@article{PENG2025104791,
title = {From GPT to DeepSeek: Significant gaps remain in realizing AI in healthcare},
journal = {Journal of Biomedical Informatics},
volume = {163},
pages = {104791},
year = {2025},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2025.104791},
url = {https://www.sciencedirect.com/science/article/pii/S1532046425000206},
author = {Yifan Peng and Bradley A. Malin and Justin F. Rousseau and Yanshan Wang and Zihan Xu and Xuhai Xu and Chunhua Weng and Jiang Bian},
keywords = {DeepSeek, ChatGPT, AI in Healthcare}
}
@article{MAJID2004108,
title = {Can language restructure cognition? The case for space},
journal = {Trends in Cognitive Sciences},
volume = {8},
number = {3},
pages = {108-114},
year = {2004},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2004.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S1364661304000208},
author = {Asifa Majid and Melissa Bowerman and Sotaro Kita and Daniel B.M. Haun and Stephen C. Levinson},
abstract = {Frames of reference are coordinate systems used to compute and specify the location of objects with respect to other objects. These have long been thought of as innate concepts, built into our neurocognition. However, recent work shows that the use of such frames in language, cognition and gesture varies cross-culturally, and that children can acquire different systems with comparable ease. We argue that language can play a significant role in structuring, or restructuring, a domain as fundamental as spatial cognition. This suggests we need to rethink the relation between the neurocognitive underpinnings of spatial cognition and the concepts we use in everyday thinking, and, more generally, to work out how to account for cross-cultural cognitive diversity in core cognitive domains.}
}
@incollection{ESCHE2014699,
title = {Systematic Modeling for Optimization},
editor = {Mario R. Eden and John D. Siirola and Gavin P. Towler},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {34},
pages = {699-704},
year = {2014},
booktitle = {Proceedings of the 8th International Conference on Foundations of Computer-Aided Process Design},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-63433-7.50101-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780444634337501012},
author = {Erik Esche and David Müller and Günter Wozny},
keywords = {Multiple-Scale Modeling, Optimization, Convexification, Linearization},
abstract = {Optimization usually requires models, which are computationally speaking less expensive than models commonly used for simulations. At the same time, process optimization and model predictive control etc. require dependable accuracies in addition to the fastness. To demystify the art of preparing process models for optimization, a workflow is presented in this contribution, which systematically deduces models based on simplification of existing models and experiment based deduction of computationally inexpensive correlations.}
}
@article{PANETSOS2011314,
title = {Physical measurement of brain perception abilities. Foundations of a working methodology for the design of “intelligent” beings},
journal = {Procedia Computer Science},
volume = {7},
pages = {314-316},
year = {2011},
note = {Proceedings of the 2nd European Future Technologies Conference and Exhibition 2011 (FET 11)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2011.09.052},
url = {https://www.sciencedirect.com/science/article/pii/S1877050911006120},
author = {F. Panetsos and S.L. Andino Gonzalez and P.C. Marijuan and C. Herrera-Rincon},
keywords = {Emergent properties, complexity, artificial brain, synthetic approach},
abstract = {Most of the important properties of the brain (thinking, consciousness, music, etc.) are severely ill-defined. They are not the direct output of biological sensors or their combinations but emerge from complex computations at the network level and are not necessarily represented in the sensory input or the activity of individual cells. They are emergent properties arising from dynamic interactions between neurons in the different relay stations of the sensory pathways where recognition of basic physical properties of incoming stimuli take place. Emergent properties and interactions between them range from physical properties of stimuli to cognitive operations as emotions or consciousness and gradually involve interactions between sensory pathways, associative cortexes, hippocampus, or the amygdala. Here we propose to build neural tissues from embryonic stem cells in “in vitro” controlled environments to determine the way physical inputs are transformed into what humans perceive and measure. We will start with “low complexity” tissues able to perform low level recognition of physical properties, to gradually increase the complexity of the tissue to investigate how the physical characteristics of the incoming stimuli correspond at higher levels to the emergent properties of the system. Mathematical methods based on networks theory, nonlinear dynamics, fractal theory and chaos among other will be used to determine and measure the emergent properties of the nervous tissue at different complexity levels. We expected to provide criteria and methodologies to measure human-like perception variables and use them for the design of future living artifacts (autonomous robots, intelligent sensors, hybrid systems, etc.).}
}
@incollection{BIR202197,
title = {Chapter Four - Generic quantum hardware accelerators for conventional systems},
editor = {Shiho Kim and Ganesh Chandra Deka},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {122},
pages = {97-133},
year = {2021},
booktitle = {Hardware Accelerator Systems for Artificial Intelligence and Machine Learning},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2021.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0065245821000322},
author = {Parth Bir},
keywords = {Quantum mechanics, Computational basis, State space, Deterministic model, Probabilistic model, QA, GQHA},
abstract = {Quantum mechanics proposes, universe is a sum of a generic building block. Different orientation (i.e., angle, phase, amplitude, etc.) and summation of blocks forms entities. When differentiated, building blocks used for formation of entity are termed as basis. Following computational theory, these basis are termed as computational basis. Classical computers possess binary basis. Quantum system possess exponential computational power because of infinite computational basis. When computing solution to a problem, it's found in state space. Deterministic model (Conventional) requires both correct and incorrect solution set. For problems of probabilistic nature with plenty of variables (NP and P problems), computing solution requires exponential time, as entire state space is scanned. Furthermore, if solution is incomputable, the computation will never complete as solution is missing from both sets. Probabilistic model (Quantum) conducts a guided state space search and possess greater information carrying capacity per bit. Therefore, Quantum Accelerators (QA) are ideal for solving such problems. Resulting implementation of a Generic Quantum Hardware Accelerator (GQHA) is described via algorithms, mathematical models and microarchitecture. Next, a competitive industrial analysis and virtual implementation in a cloud environment is defined. Finally, it's proven that GQHA can replace conventional accelerators to produce faster and reliable results.}
}
@article{DELLACQUA2021199,
title = {Increased functional connectivity within alpha and theta frequency bands in dysphoria: A resting-state EEG study},
journal = {Journal of Affective Disorders},
volume = {281},
pages = {199-207},
year = {2021},
issn = {0165-0327},
doi = {https://doi.org/10.1016/j.jad.2020.12.015},
url = {https://www.sciencedirect.com/science/article/pii/S0165032720331049},
author = {Carola Dell'Acqua and Shadi Ghiasi and Simone {Messerotti Benvenuti} and Alberto Greco and Claudio Gentili and Gaetano Valenza},
keywords = {depression, depressive symptoms, dysphoria, functional connectivity, EEG, vulnerability},
abstract = {Background: The understanding of neurophysiological correlates underlying the risk of developing depression may have a significant impact on its early and objective identification. Research has identified abnormal resting-state electroencephalography (EEG) power and functional connectivity patterns in major depression. However, the entity of dysfunctional EEG dynamics in dysphoria is yet unknown. Methods: 32-channel EEG was recorded in 26 female individuals with dysphoria and in 38 age-matched, female healthy controls. EEG power spectra and alpha asymmetry in frontal and posterior channels were calculated in a 4-minute resting condition. An EEG functional connectivity analysis was conducted through phase locking values, particularly mean phase coherence. Results: While individuals with dysphoria did not differ from controls in EEG spectra and asymmetry, they exhibited dysfunctional brain connectivity. Particularly, in the theta band (4-8 Hz), participants with dysphoria showed increased connectivity between right frontal and central areas and right temporal and left occipital areas. Moreover, in the alpha band (8-12 Hz), dysphoria was associated with increased connectivity between right and left prefrontal cortex and between frontal and central-occipital areas bilaterally. Limitations: All participants belonged to the female gender and were relatively young. Mean phase coherence did not allow to compute the causal and directional relation between brain areas. Conclusions: An increased EEG functional connectivity in the theta and alpha bands characterizes dysphoria. These patterns may be associated with the excessive self-focus and ruminative thinking that typifies depressive symptoms. EEG connectivity patterns may represent a promising measure to identify individuals with a higher risk of developing depression.}
}
@article{ANDERSON2024108366,
title = {Trichotomy revisited: A monolithic theory of attentional control},
journal = {Vision Research},
volume = {217},
pages = {108366},
year = {2024},
issn = {0042-6989},
doi = {https://doi.org/10.1016/j.visres.2024.108366},
url = {https://www.sciencedirect.com/science/article/pii/S0042698924000105},
author = {Brian A. Anderson},
keywords = {Attentional control, Visual attention, Selection history, Memory, Learning},
abstract = {The control of attention was long held to reflect the influence of two competing mechanisms of assigning priority, one goal-directed and the other stimulus-driven. Learning-dependent influences on the control of attention that could not be attributed to either of those two established mechanisms of control gave rise to the concept of selection history and a corresponding third mechanism of attentional control. The trichotomy framework that ensued has come to dominate theories of attentional control over the past decade, replacing the historical dichotomy. In this theoretical review, I readily affirm that distinctions between the influence of goals, salience, and selection history are substantive and meaningful, and that abandoning the dichotomy between goal-directed and stimulus-driven mechanisms of control was appropriate. I do, however, question whether a theoretical trichotomy is the right answer to the problem posed by selection history. If we reframe the influence of goals and selection history as different flavors of memory-dependent modulations of attentional priority and if we characterize the influence of salience as a consequence of insufficient competition from such memory-dependent sources of priority, it is possible to account for a wide range of attention-related phenomena with only one mechanism of control. The monolithic framework for the control of attention that I propose offers several concrete advantages over a trichotomy framework, which I explore here.}
}
@incollection{DEDEOGLU2023251,
title = {Chapter Nine - Blockchain meets edge-AI for food supply chain traceability and provenance},
editor = {Joost Laurus Dinant Nelis and Aristeidis S. Tsagkaris},
series = {Comprehensive Analytical Chemistry},
publisher = {Elsevier},
volume = {101},
pages = {251-275},
year = {2023},
booktitle = {Smartphones for Chemical Analysis: From Proof-of-concept to Analytical Applications},
issn = {0166-526X},
doi = {https://doi.org/10.1016/bs.coac.2022.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S0166526X22001064},
author = {Volkan Dedeoglu and Sidra Malik and Gowri Ramachandran and Shantanu Pal and Raja Jurdak},
keywords = {Blockchain, Edge AI, Traceability, Provenance, Supply Chains},
abstract = {Food supply chains are increasingly digitised and automated through the use of technologies such as Internet-of-Things (IoT), blockchain and Artificial Intelligence (AI). Such digitization efforts often rely on cloud computing, which creates bandwidth overhead, high latency, security and privacy challenges. In this chapter, we propose the use of edge AI, which is a computing paradigm that combines edge computing and AI, to complete computing tasks close to the sensor data sources. Edge AI can promote greater scalability and avoid the security and privacy challenges of centralised cloud computing. The chapter introduces the provenance and traceability requirements of food supply chains and the digitization of these supply chains through blockchain, IoT, and AI. The chapter also proposes the use of smartphone integrated sensors to provide unique physical, chemical, or biological signatures of food supply chain products, and to conduct the necessary computations on the smartphone. The proposed Edge AI approach to supply chain digitization sets the scene for greater resilience in modern digital supply chains.}
}
@article{PLUZHNIKOVA202334,
title = {The Human Factor and the Problem of Transport Safety in Modern Conditions},
journal = {Transportation Research Procedia},
volume = {68},
pages = {34-39},
year = {2023},
note = {XIII International Conference on Transport Infrastructure: Territory Development and Sustainability},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2023.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S2352146523000078},
author = {N.N. Pluzhnikova},
keywords = {Transport, transport robotics, intelligence, IT-technologies, human},
abstract = {The article is devoted to the analysis of the interaction between man and artificial intelligence to ensure the safety of transport. The author analyzes intelligence and IT technologies on the example of the development of transport robotics. To consider this problem the author refers to cognitive developments in this area, and also indicates the philosophical problems of the development of transport robotics. The article uses such methods as comparative and structural analysis.}
}
@incollection{ZELINSKY2005395,
title = {CHAPTER 65 - Specifying the Components of Attention in a Visual Search Task},
editor = {Laurent Itti and Geraint Rees and John K. Tsotsos},
booktitle = {Neurobiology of Attention},
publisher = {Academic Press},
address = {Burlington},
pages = {395-400},
year = {2005},
isbn = {978-0-12-375731-9},
doi = {https://doi.org/10.1016/B978-012375731-9/50069-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780123757319500690},
author = {Gregory J. Zelinsky},
abstract = {ABSTRACT
Although commonly treated as a unitary process, attention is more likely a collection of task-related but separable operations. Three components of attention (set, selection, and movement) are identified and defined within the context of a computationally explicit model of eye movements during visual search. The model compares filter-based representations of the target and search displays to derive a salience map indicating likely target candidates in a scene. Eye position is defined as the centroid of activity on this saliency map. As this map is thresholded over time, the changing centroid produces a sequence of movements that eventually cause simulated gaze to become aligned with the target. By adopting a more computational language and making explicit the underlying operations of the task, visual search, a behavior that has long been hobbled to the concept of attention, can be well described without appeal to an abstracted attention theory.}
}
@article{ROBINSON2009310,
title = {Children's understanding of the inverse relation between multiplication and division},
journal = {Cognitive Development},
volume = {24},
number = {3},
pages = {310-321},
year = {2009},
issn = {0885-2014},
doi = {https://doi.org/10.1016/j.cogdev.2008.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0885201408000889},
author = {Katherine M. Robinson and Adam K. Dubé},
keywords = {Arithmetic, Inversion, Conceptual knowledge, Procedural knowledge, Factual knowledge, Multiplication, Division},
abstract = {Children's understanding of the inversion concept in multiplication and division problems (i.e., that on problems of the form d * e/e no calculations are required) was investigated. Children in Grades 6, 7, and 8 completed an inversion problem-solving task, an assessment of procedures task, and a factual knowledge task of simple multiplication and division. Application of the inversion concept in the problem-solving task was low and constant across grades. Most participants approved of the inversion-based shortcut but only a slight majority preferred it. Three clusters of children were identified based on their performance on the three tasks. The inversion cluster used and approved of the inversion shortcut the most and had high factual knowledge. The negation cluster used the negation strategy, had lower approval of the inversion shortcut, and had medium factual knowledge. The computation cluster used computation and had the lowest approval and the weakest factual knowledge. The findings highlight the importance of addressing the multiplication and division inversion concept in theories of children's mathematical competence.}
}
@article{ALLGOWER2019147,
title = {Position paper on the challenges posed by modern applications to cyber-physical systems theory},
journal = {Nonlinear Analysis: Hybrid Systems},
volume = {34},
pages = {147-165},
year = {2019},
issn = {1751-570X},
doi = {https://doi.org/10.1016/j.nahs.2019.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S1751570X19300603},
author = {Frank Allgöwer and João {Borges de Sousa} and James Kapinski and Pieter Mosterman and Jens Oehlerking and Patrick Panciatici and Maria Prandini and Akshay Rajhans and Paulo Tabuada and Philipp Wenzelburger},
keywords = {cyber–physical systems theory},
abstract = {Cyber-physical systems theory offers a powerful framework for modeling, analyzing, and designing real engineering systems integrating communication, control, and computation functionalities (the cyber part) within a natural and/or man-made system governed by the laws of physics (the physical part). New methodological developments in cyber-physical systems theory are required by traditional application domains such as manufacturing, transportation, and energy systems, which are currently experiencing significant and – to some extent – revolutionary changes to address the needs of our modern society. The goal of this position paper is to provide the cyber-physical systems community, and especially young researchers, a clear view on what are research directions worth pursuing motivated by the challenges posed by modern applications.}
}
@article{DELIGUORO2023114082,
title = {From semantics to types: The case of the imperative λ-calculus},
journal = {Theoretical Computer Science},
volume = {973},
pages = {114082},
year = {2023},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2023.114082},
url = {https://www.sciencedirect.com/science/article/pii/S030439752300395X},
author = {Ugo de'Liguoro and Riccardo Treglia},
keywords = {State monad, Imperative lambda calculus, Type assignment systems, Filter models},
abstract = {We study the logical semantics of an untyped λ-calculus equipped with operators representing read and write operations from and to a global store. Such a logic consists of an intersection type assignment system, which we derive from the denotational semantics of the calculus, based on the monadic approach to model computational λ-calculi. The system is obtained by constructing a filter model in the category of ω-algebraic lattices, such that the typing rules can be recovered out of the term interpretation. By construction, the so-obtained type system satisfies the “type-semantics” property and completeness.}
}
@incollection{YADEN2023849,
title = {Reintroducing “development” into theories of the acquisition and growth of early literacy: developmental science approaches and the cultural-historical perspective of L. S. Vygotsky},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {849-865},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.07103-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305071037},
author = {David B. Yaden and Camille Martinez-Yaden},
keywords = {Microgenetic, Developmental science, Optimizing equilibration, Process-relational, Relational-developmental-system, Backward transition, Early writing, Overlapping waves theory, Prospective models, Retrospective models, Hyperbolic geometry},
abstract = {This chapter contrasts the differences between computational “retrospective” models of early writing achievement whose elements represent static states of being and “prospective” models based upon the principles of developmental science and a process-relational-developmental framework which characterizes early writing performances always in the process of “becoming.” The chapter highlights these differences using examples from Spanish-speaking and Chinese/English emergent bilinguals to illustrate the various patterns of writing development captured in a Piagetian/Vygotskian-inspired early writing assessment. The children's simultaneous display of multiple conceptualizations of the notational system in Spanish, English, and Chinese is interpreted as reflecting aspects of Siegler's “overlapping waves theory” and Piaget's “optimizing equilibration.”}
}
@article{KOULADOUM2024200052,
title = {The role of institutional quality on the impact of Chinese foreign direct investments and human capital development on macroeconomic performance in the CEMAC zone},
journal = {Transnational Corporations Review},
volume = {16},
number = {2},
pages = {200052},
year = {2024},
issn = {1925-2099},
doi = {https://doi.org/10.1016/j.tncr.2024.200052},
url = {https://www.sciencedirect.com/science/article/pii/S1925209924005783},
author = {Jean-Claude Kouladoum},
keywords = {Institutional quality, Chinese foreign direct investments, Human capital development, Macroeconomic performance, CEMAC zone},
abstract = {This paper investigates the role of institutional quality in terms of governance on the impact of Chinese foreign direct investments and human capital development on the macroeconomic performance of the CEMAC zone between 2003 and 2020. The data were analyzed using descriptive statistics and the Correlated Panels Corrected Standard Errors (PCSEs) Approach. The findings indicated that poor governance performance in the CEMAC zone deteriorates the effects of Chinese foreign direct investments and human capital development on the macroeconomic performance of the CEMAC zone. At the same time, poor governance performance also deteriorates the impact of foreign aid and personal remittances received on the macroeconomic performance of the CEMAC zone. This study strongly recommends measures to improve institutional quality such as increased training on ethical thinking in all forms of education, meritocratic recruitment to the civil service, and auditing of public finances and services.}
}
@article{JOHANN2016420,
title = {Soil moisture modeling based on stochastic behavior of forces on a no-till chisel opener},
journal = {Computers and Electronics in Agriculture},
volume = {121},
pages = {420-428},
year = {2016},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2015.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S0168169915004020},
author = {André L. Johann and Augusto G. {de Araújo} and Hevandro C. Delalibera and André R. Hirakawa},
keywords = {Soil physics, Computational models, Precision agriculture, Soft computing, Force sensors},
abstract = {Crop-yield variability is frequently associated with soil moisture and its real-time measurement can be an alternative for the automatic control of no-till seeding to improve soil–crop conditions. Soil moisture has a significant influence on soil behavior, markedly on its temporal and spatial variability; however, the measurement of soil moisture is generally time consuming and expensive. Many studies employ electric, electromagnetic, optical, or radiometric sensors for the direct measurement of soil moisture. It is also possible to develop an estimation method employing existing machinery components using mechanical sensors such as load cells. Auto-regressive error function (AREF) combined with computational models is applied in this study for estimating soil moisture using a data set of forces acting on a chisel and speed as inputs to assess the feasibility of achieving more accurate results than previously obtained by Sakai et al. (2005). AREF is a stochastic method that can be applied to the analysis of soil-force patterns acting on a tool. Three computational models are developed, including two artificial neural networks (a Multi-Layer Perceptron (MLP) and a Radial Basis Function (RBF)) and one Neuro-Fuzzy model (ANFIS). These are compared with two multiple linear regression (MLR) models with two and six independent variables. The models’ performances are evaluated using root mean square error (RMSE), determination coefficient (R2), and average percentage error (APE). The computational models demonstrated superior performance compared to MLR, confirming the hypothesis. The neural network models had similar performances with RMSE between 1.27% and 1.30%, R2 around 0.80, and APE between 3.77% and 3.75% for testing data. These results indicate that using AREF parameters combined with computational models may be a suitable technique to estimate soil moisture and has potential to be used in control systems applied to no-till machinery.}
}
@article{ZOU2024134011,
title = {Synthesis and mechanism of quaternary ammonium salts based on porphyrin as high-performance copper levelers},
journal = {Tetrahedron},
volume = {159},
pages = {134011},
year = {2024},
issn = {0040-4020},
doi = {https://doi.org/10.1016/j.tet.2024.134011},
url = {https://www.sciencedirect.com/science/article/pii/S0040402024001911},
author = {Peikun Zou and Xuyang Li and Xin Chen and Wenhao Zhou and Kexin Du and Limin Wang},
keywords = {Porphyrin, Porphyrin quaternary ammonium salts, Through-hole electroplating, Electroplating leveler, Quantum chemical calculations},
abstract = {The molecular structure and energy distribution of organic compounds have a great influence on their adsorption capacity on the metal surface. However, there are still insufficient researches on the influence of energy distribution on adsorption properties of organic molecules. Herein, a family of porphyrin derivatives (TPyP-Et, TPyP-Oct, TPyP-Bn and TPyP-Al) bearing quaternary ammonium groups were synthesized for the first time as promising levelers for through-hole copper electrodeposition. Electrochemical tests revealed that all four TPyP derivatives displayed enhanced electrochemical properties. Theoretical calculations and molecular dynamics simulations were carried out to investigate the physisorption capacity and chemical reaction activity of the TPyP molecules, as well as the adsorption capacity on the surface of the copper layer. Through optical and scanning electron microscopy as well as X-ray diffractometry, it was demonstrated that TPyP molecules are effective electroplating levelers. TPyP-Oct, with its longer carbon chain substituent, exhibited superior hole-filling performance in practical PCB experiments among the four compounds. This study expandes the application range of porphyrin compounds, analyzes the influence of organic molecular adsorption properties in copper electrodeposition, and provides theoretical guidance for the future study of organic compounds adsorbed on metal surfaces.}
}
@article{FRENCH2023100030,
title = {Reflections on 50 years of MCDM: Issues and future research needs},
journal = {EURO Journal on Decision Processes},
volume = {11},
pages = {100030},
year = {2023},
issn = {2193-9438},
doi = {https://doi.org/10.1016/j.ejdp.2023.100030},
url = {https://www.sciencedirect.com/science/article/pii/S2193943823000031},
author = {Simon French},
keywords = {Behavioural decision studies, Bayesian analysis, Conflicting objectives, Cynefin, Multiple criteria decision-making (MCDM), Uncertainty},
abstract = {Modern discussions of multiple criteria decision-making extend back about half a century. I reflect on key developments, schools of thought and controversies that have taken place over the period, arguing that perhaps those of us in different schools focus too much on our differences and do not capitalise enough on what we share in common. Moreover, the differences between schools are indications of their respective weaknesses and can drive improvements in each. The discussion points to a number of issues and research needs that the community needs to address.}
}
@incollection{WANG2017259,
title = {Chapter 14 - Reason and Emotion in Xunzi’s Moral Psychology},
editor = {T.-W. Hung and T.J. Lane},
booktitle = {Rationality},
publisher = {Academic Press},
address = {San Diego},
pages = {259-276},
year = {2017},
isbn = {978-0-12-804600-5},
doi = {https://doi.org/10.1016/B978-0-12-804600-5.00014-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128046005000143},
author = {E.H. Wang},
keywords = {Xunzi, moral rationalism, emotion},
abstract = {In this paper I explore the extent to which Xunzi may, or may not, be a moral rationalist by investigating the roles reason and emotion play in Xunzi’s moral psychology. To this end, I address Soek’s and Slingerland’s recent work on this subject. Seok (2013) recently characterized two contrasting models of moral psychology: “reason based” and “emotion based”; the former takes the reflective and conscious reasoning ability to be the essence of one’s moral judgment and action, while emotions and affective mechanisms play only minor roles (if any); the latter takes emotional states to be essential or at least necessary. Soek understands Confucian ethics in general to operate with the emotion-based model, but his argument mainly concerns Mencius’ work. Slingerland (2010), on the other hand, categorizes Xunzi’s moral psychology as a theory that presumes what he calls the “high reason model,” which significantly resembles the reason-based model in Soek’s account. Slingerland understands that, on Xunzi’s account, rational faculties and emotional faculties are competitive in the reasoning process. Moreover, he takes Xunzi to prioritize the rational faculties, thinking that they can and should monitor emotional responses, and override them when needed. Slingerland also cites recent empirical studies to criticize this model. I argue that Xunzi’s moral psychology cannot be captured by either of the two models Soek characterizes, but presents to us a third alternative: it gives us a good example of a hybrid model of these two. Indeed, Xunzi’s emphasis on ritual practices in the cultivation of xin and qing toward sagehood sheds light on a possible interplay between reason and emotion in ideal moral judgment/decision. This discussion inspires further consideration of what a moral rationalist may be, and the extent to which Xunzi may, or may not, be a moral rationalist.}
}
@article{SCHOLL201856,
title = {Understanding psychiatric disorder by capturing ecologically relevant features of learning and decision-making},
journal = {Behavioural Brain Research},
volume = {355},
pages = {56-75},
year = {2018},
note = {SI: MCC 2016},
issn = {0166-4328},
doi = {https://doi.org/10.1016/j.bbr.2017.09.050},
url = {https://www.sciencedirect.com/science/article/pii/S0166432817305673},
author = {Jacqueline Scholl and Miriam Klein-Flügge},
keywords = {Reinforcement learning, Decision-making, Computational psychiatry},
abstract = {Recent research in cognitive neuroscience has begun to uncover the processes underlying increasingly complex voluntary behaviours, including learning and decision-making. Partly this success has been possible by progressing from simple experimental tasks to paradigms that incorporate more ecological features. More specifically, the premise is that to understand cognitions and brain functions relevant for real life, we need to introduce some of the ecological challenges that we have evolved to solve. This often entails an increase in task complexity, which can be managed by using computational models to help parse complex behaviours into specific component mechanisms. Here we propose that using computational models with tasks that capture ecologically relevant learning and decision-making processes may provide a critical advantage for capturing the mechanisms underlying symptoms of disorders in psychiatry. As a result, it may help develop mechanistic approaches towards diagnosis and treatment. We begin this review by mapping out the basic concepts and models of learning and decision-making. We then move on to consider specific challenges that emerge in realistic environments and describe how they can be captured by tasks. These include changes of context, uncertainty, reflexive/emotional biases, cost-benefit decision-making, and balancing exploration and exploitation. Where appropriate we highlight future or current links to psychiatry. We particularly draw examples from research on clinical depression, a disorder that greatly compromises motivated behaviours in real-life, but where simpler paradigms have yielded mixed results. Finally, we highlight several paradigms that could be used to help provide new insights into the mechanisms of psychiatric disorders.}
}
@article{FRY2010218,
title = {On the nature of tetraalkylammonium ions in common electrochemical solvents: General and specific solvation – Quantitative aspects},
journal = {Journal of Electroanalytical Chemistry},
volume = {638},
number = {2},
pages = {218-224},
year = {2010},
issn = {1572-6657},
doi = {https://doi.org/10.1016/j.jelechem.2009.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S0022072809004288},
author = {Albert J. Fry and L. Kraig Steffen},
keywords = {Computational electrochemistry, Tetraalkylammonium ions, Specific solvation, Inner sphere solvation, General solvation},
abstract = {The free energies of each of 80 tetraalkylammonium ion/solvent complexes [R4N+/(solv)n], with R ranging from methyl through butyl and n ranging from 1 through 4, were computed by density functional theory (DFT) in five common electrochemical solvents: dimethylformamide (DMF), dimethylsulfoxide (DMSO), acetonitrile (AN), dichloromethane (DCM), and methanol (MeOH). The energies of the complexes were computed both with and without their solvation energies. Additional computations of the energies of the individual components, both solvated and unsolvated, were also carried out. The resulting data permit construction of a thermodynamic cycle for each R4N+/solvent pair that in turn allows the determination of the extent of general and specific solvation energies for that pair. An additional series of computations for pentane as solvent were carried out. Since this solvent should not coordinate with tetraalkylammonium ions, these computations provide a test of the validity of the computational method. This work represents a useful new general protocol for assessing the relative importance of general and specific solvation in chemical systems.}
}
@article{XU2024102292,
title = {Hierarchical spatio-temporal graph convolutional neural networks for traffic data imputation},
journal = {Information Fusion},
volume = {106},
pages = {102292},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2024.102292},
url = {https://www.sciencedirect.com/science/article/pii/S1566253524000708},
author = {Dongwei Xu and Hang Peng and Yufu Tang and Haifeng Guo},
keywords = {Traffic data imputation, Hierarchical representation, Graph convolution network, Spatio-temporal features},
abstract = {The quality of traffic services depends on the accuracy and completeness of the collected traffic data. However,the existing traffic data imputation methods usually only rely on the predefined road network structure to capture the spatio-temporal features and only consider the imputation effect from a single perspective, which are very limited for imputation of different missing patterns of road traffic data. In this paper, we propose a novel deep learning framework called Hierarchical Spatio-temporal Graph Convolutional Neural Networks(HSTGCN) to impute traffic data,through the macro layer and the road layer. The model constructs macro graph of the road network based on the data temporal correlation clustering, which can mine the temporal dependencies of road traffic data from a hierarchical perspective. Besides, a temporal attention mechanism and adaptive adjacency matrix are introduced in the road layer to better extract the spatio-temporal information of the road traffic data. Finally, we use graph convolution neural networks to learn the spatio-temporal feature representations of the road layer and macro layer, which are then fused to achieve data imputation. To illustrate the efficient performance of the model, experiments are conducted on traffic data collected from California and Seattle. The proposed model performs better than the comparison model for traffic data imputation.}
}
@article{ADAMOVIC2024100604,
title = {Streamlined approach to 2nd/3rd graders learning basic programming concepts},
journal = {Entertainment Computing},
volume = {48},
pages = {100604},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2023.100604},
url = {https://www.sciencedirect.com/science/article/pii/S1875952123000599},
author = {Milan Đ. Adamović and Dragan V. Ivetić},
keywords = {Video games, Edutainment, Programming, School},
abstract = {There is a growing need to teach schoolchildren programming at an increasingly younger age. The goal of this study is to determine if it is possible to teach schoolchildren basic programming concepts in a streamlined manner. In order to present the new knowledge in a way schoolchildren could understand easily, analogies between basic programming concepts and traffic were used. A simple video game was developed with this in mind and an effort was made to avoid design pitfalls commonly found in edutainment titles. The study involved 112 schoolchildren ages 7 to 9. Test group and control group were given a pre-test, a re-test and a post-test. The re-test and the post-test respectively showed 16% and 7% score difference in favor of the test group. Focusing on questions featuring content analogous to basic programming concepts showed 36% and 20% difference in scores.}
}
@article{PANTALEON201579,
title = {Taylor series expansion using matrices: An implementation in MATLAB®},
journal = {Computers & Fluids},
volume = {112},
pages = {79-82},
year = {2015},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2015.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0045793015000183},
author = {Carlos Pantaleón and Amitabha Ghosh},
keywords = {Taylor series, Finite differences, Truncation error, Modified equation, Symbolic computation},
abstract = {Taylor series expansions are widely used in engineering approximations, for instance, to develop finite differences schemes or numerical integration methods. This technical note presents a novel technique to generate, display and manipulate Taylor series expansion by using matrices. The resulting approach allows algebraic manipulation as well as differentiation in a very intuitive manner in order to experiment with different numerical schemes, their truncation errors and their structures, while avoiding manual calculation errors. A detailed explanation of the mathematical procedure to generate a matrix form of the Taylor series expansion for a function of two variables is presented along with the algorithm of an implementation in MATLAB®. Example cases of different orders are tabulated to illustrate the generation and manipulation capabilities of this technique. Additionally, an extended application is developed to determine the modified equations of finite difference schemes for partial differential equations, with one-dimensional examples of the wave equation and the heat equation using explicit and implicit schemes.}
}
@article{SUKHOBOKOV2024101279,
title = {A universal knowledge model and cognitive architectures for prototyping AGI},
journal = {Cognitive Systems Research},
volume = {88},
pages = {101279},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101279},
url = {https://www.sciencedirect.com/science/article/pii/S1389041724000731},
author = {Artem Sukhobokov and Evgeny Belousov and Danila Gromozdov and Anna Zenger and Ilya Popov},
keywords = {Cognitive architecture, AGI, Metagraph, Archigraph, Universal knowledge model, Machine consciousness, Machine subconsciousness, Machine reflection, Machine worldview},
abstract = {The article identified 56 cognitive architectures for creating general artificial intelligence (AGI) and proposed a set of interrelated functional blocks that an agent approaching AGI in its capabilities should possess. Since the required set of blocks is not found in any of the existing architectures, the article proposes a reference cognitive architecture for intelligent systems approaching AGI in their capabilities. As one of the key solutions within the framework of the architecture, a universal method of knowledge representation is proposed, which allows combining various non-formalized, partially and fully formalized methods of knowledge representation in a single knowledge base, such as texts in natural languages, images, audio and video recordings, graphs, algorithms, databases, neural networks, knowledge graphs, ontologies, frames, essence-property-relation models, production systems, predicate calculus models, conceptual models, and others. To combine and structure various fragments of knowledge, archigraph model are used, constructed as a development of annotated metagraphs. As other components, the reference cognitive architecture being developed includes following modules: machine consciousness, machine subconsciousness, interaction with the external environment, a goal management, an emotional control, social interaction, reflection, ethics, worldview, learning, monitoring, statement problems, solving problems, self-organization and meta learning. Based on the composition of the proposed reference architecture modules, existing cognitive architectures containing the following modules were analyzed: machine consciousness, machine subconsciousness, reflection, worldview.}
}
@article{PHILLIPS2009597,
title = {Advances in evolution and genetics: Implications for technology strategy},
journal = {Technological Forecasting and Social Change},
volume = {76},
number = {5},
pages = {597-607},
year = {2009},
note = {Two Special Sections: Advances in Evolution and Genetics: Implications for Technology Strategy The Digital Economy in Asia},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2008.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0040162508001522},
author = {Fred Phillips and Yu-Shan Su},
keywords = {Evolution, Selection, Genetics, Technology strategy, Technology forecasting},
abstract = {Genetic and evolutionary principles are of great importance to technology strategists, both directly (as in the forecasting of genetic engineering technologies) and as a source of metaphor and perspective on socio-technical change. Recent rapid progress in the molecular sciences have revealed new genetic mechanisms of evolution, and introduced new controversies of interpretation. How do these recent developments affect technology forecasting and our view of technological evolution? This paper provides a quick primer for TFSC readers on several new developments in evolution and genetics, comments upon a number of common misconceptions and pitfalls in evolutionary thinking, and critically describes some controversies and open questions, introducing key readings and sources. It relates genetic and evolutionary knowledge, analogies and metaphors to areas of interest to researchers in technology forecasting and assessment, noting possible future directions. The paper concludes with an overview of the other papers in this special section.}
}
@article{NOORMAN2017677,
title = {Biochemical engineering’s grand adventure},
journal = {Chemical Engineering Science},
volume = {170},
pages = {677-693},
year = {2017},
note = {13th International Conference on Gas-Liquid and Gas-Liquid-Solid Reactor Engineering},
issn = {0009-2509},
doi = {https://doi.org/10.1016/j.ces.2016.12.065},
url = {https://www.sciencedirect.com/science/article/pii/S0009250916307266},
author = {Henk J. Noorman and Joseph J. Heijnen},
keywords = {Lifeline modeling, Bioprocess design, Scale-down, Bio-economy, Renewable feedstocks, Bio-products},
abstract = {Building on the recent revolution in molecular biology, enabling a wealth of bio-product innovations made from renewable feedstocks, the biotechnology field is in a transition phase to bring the products to the market. This requires a shift from natural sciences to engineering sciences with first conception of new, efficient large-scale bioprocess designs, followed by implementation of the most promising design in practice. Inspired by a former publication by O. Levenspiel in 1988, an outline is presented of main challenges that the field of biochemical engineering is currently facing, in a context of major global sustainability trends. The critical stage is the conceptual design phase. Issues can best be addressed and overcome by adopting an attitude where one begins with the end in mind. This applies to three principal components: 1. the bioprocess value chain, where the product specifications and downstream purification schemes should be set before defining the upstream sections, 2. the time perspective, starting in the future assuming that feedstock and product-market combinations are already in place and then going back to today, and 3. the scale of operation, where the industrial operation sets the boundaries for all labscale research and development, and not vice versa. In this way, and ideal process is defined taking constraints from anticipated manufacturing into account. For illustration, three bioprocess design examples are provided, that show how new, ideal conceptual designs can be generated. These also make clear that the engineering sciences are undergoing a revolution, where bio-based approaches replace fossil routes, and gross simplification is replaced by highly detailed computational methods. For biochemical processes, lifeline modeling frameworks are highlighted as powerful means to reconcile the competing needs for high speed and high quality in biochemical engineering, both in the design and implementation stages, thereby enabling significant growth of the bio-based economy.}
}
@incollection{HERLIHY20211,
title = {Chapter 1 - Introduction},
editor = {Maurice Herlihy and Nir Shavit and Victor Luchangco and Michael Spear},
booktitle = {The Art of Multiprocessor Programming (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {1-18},
year = {2021},
isbn = {978-0-12-415950-1},
doi = {https://doi.org/10.1016/B978-0-12-415950-1.00009-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780124159501000094},
author = {Maurice Herlihy and Nir Shavit and Victor Luchangco and Michael Spear},
keywords = {parallelism, concurrent programming, shared-memory multiprocessors, safety, liveness, mutual exclusion, coordination protocol, producer—consumer problem, readers–writers problem, deadlock-freedom, starvation-freedom, Amdahl's law},
abstract = {This chapter introduces and motivates the study of shared-memory multiprocessor programming, or concurrent programming. It describes the overall plan of the book, and then presents some basic concepts of concurrent computation, and presents some of the fundamental problems—mutual exclusion, the producer–consumer problem, and the readers–writers problem—and some simple approaches to solve these problems. It ends with a brief discussion of Amdahl's law.}
}
@article{OMIZO2020102578,
title = {Machining Topoi: Tracking Premising in Online Discussion Forums with Automated Rhetorical Move Analysis},
journal = {Computers and Composition},
volume = {57},
pages = {102578},
year = {2020},
note = {Composing Algorithms: Writing (with) Rhetorical Machines},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2020.102578},
url = {https://www.sciencedirect.com/science/article/pii/S8755461520300396},
author = {Ryan M. Omizo},
keywords = {computational rhetoric, Docuscope, Faciloscope, discussion forums, topoi},
abstract = {This article interrogates recent computational work on discovering and analyzing topoi through the use of topic modeling in the discipline of the literary digital humanities against the long history of topical research and pedagogy in rhetoric and composition. While significant work has been done in the literary digital humanities to advance the study of texts through topic modeling, this article argues that the emphasis on the textuality of topoi in computational research neglects situated rhetorical actions and the dynamics of audience interaction. In response to this deemphasis, this article proposes an algorithmic alternative to the identification and explanation of the rhetorical topoi through the integrated use of a computational rhetorical move classifier called the Faciloscope (Omizo et al., 2016) and the pattern-matching program, Docuscope (Kaufer and Ishizaki, 1998).}
}
@article{WILLMANN201616,
title = {Robotic timber construction — Expanding additive fabrication to new dimensions},
journal = {Automation in Construction},
volume = {61},
pages = {16-23},
year = {2016},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2015.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0926580515002046},
author = {Jan Willmann and Michael Knauss and Tobias Bonwetsch and Anna Aleksandra Apolinarska and Fabio Gramazio and Matthias Kohler},
keywords = {Non-standard timber structures, Automated assembly, Computational design, Industrial full scale implementation, Additive digital fabrication, Robotic Timber Construction},
abstract = {This paper presents a novel approach to non-standard timber assembly – Robotic Timber Construction (RTC) – where robotic fabrication is used to expand additive digital fabrication techniques towards industrial full scale dimensions. Featuring robotic systems that grasp, manipulate, and finally position building components according to a precise digital blueprint, RTC combines robotic assembly procedures and advanced digital design of non-standard timber structures. The resulting architectural morphologies allow for a convergence of aesthetic and functional concerns, enabling structural optimisation through the locally differentiated aggregation of material. Initiated by the group of Gramazio Kohler Research at ETH Zurich, this approach offers a new perspective on automated timber construction, where the focus is shifted from the processing of single parts towards the assembly of generic members in space. As such, RTC promotes unique advantages over conventional approaches to timber construction, such as, for example, CNC joinery and cutting: through the automated placement of material exactly where it is needed, RTC combines additive and largely waste-free construction with economic assembly procedures, it does not require additional external building reference, and it offers digital control across the entire building process, even when the design and assembly information are highly complex. This paper considers 1) research parameters for the individual components of RTC (such as computational design processes, construction methods and fabrication strategies), and 2) the architectural implications of integrating these components into a systemic, unifying process at the earliest stages of design. Overall, RTC leads to profound changes in the design, performance and expressive language of architecture and thus fosters the creation of architecture that profoundly reinvents its constructive repertoire.}
}
@article{RAMIREZPEDRAZA2021122,
title = {Decision-making bioinspired model for target definition and “satisfactor” selection for physiological needs},
journal = {Cognitive Systems Research},
volume = {66},
pages = {122-133},
year = {2021},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2020.10.020},
url = {https://www.sciencedirect.com/science/article/pii/S1389041720300942},
author = {Raymundo Ramirez-Pedraza and Felix Ramos},
keywords = {Decision-making, Brain model, Satisfactor selection, Physiological need, Goal-driven},
abstract = {Every person, from an early age, has to make decisions to resolve situations that arise in life. In general, different people make different decisions in the same situation, since decision-making takes into account different factors such as age, emotional state, experience, among others. We can make decisions about situations that we classify as: more important than others, routine, unexpected, or trivial. However, making the correct decision(s) in a timely manner for these situations is one of the most complex and delicate challenges that human beings face. This is due to the arduous mental process required to be carried out. Providing such behavior to a virtual entity is possible through the use of Cognitive Architectures (CAs). CAs are an approach for modeling human intelligence and behavior. This paper presents an functional bioinspired computational decision-making model to satisfy the physiological needs of hunger and thirst. Our proposal considers as black boxes other cognitive functions that are part of a general CA (named Cuäyöllötl or brain in Nahuatl). In the proposed case study, it is proved that the decision-making process plays an essential role in determining the objective and selecting the object that satisfies the established need.}
}
@article{SCHMID2019178,
title = {Representing stuff in the human brain},
journal = {Current Opinion in Behavioral Sciences},
volume = {30},
pages = {178-185},
year = {2019},
note = {Visual perception},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2019.10.007},
url = {https://www.sciencedirect.com/science/article/pii/S2352154619300816},
author = {Alexandra C Schmid and Katja Doerschner},
abstract = {Our experience of materials does not merely comprise judgments of single properties such as glossiness or roughness but is rather made up of a multitude of simultaneous impressions of qualities. To understand the neural mechanisms yielding such complex impressions, we suggest that it is necessary to extend existing experimental approaches to those that view material perception as a distributed and dynamic process. A distributed representations framework not only fits better with our perceptual experience of material qualities, it is commensurate with recent psychophysics and neuroimaging results.}
}
@incollection{WALSH2017,
title = {Sensory Systems},
booktitle = {Reference Module in Neuroscience and Biobehavioral Psychology},
publisher = {Elsevier},
year = {2017},
isbn = {978-0-12-809324-5},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.06867-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012809324506867X},
author = {V. Walsh},
keywords = {Auditory system, Multisensory integration, Nerves, Somatosensation, Visual system},
abstract = {Sensory systems have an old school ring to them, a very old school ring to them. In the 15th century Benedetti was able to write, “By means of nerves, the pathways of the senses are distributed like the roots and fibers of a tree” (Alessandro Benedetti, 1497). This is still a good place to start because it gives one a feel for the 3D structure of our sensory apparatus, but the challenge of understanding the senses has, of course, gone well beyond structure (which is not to imply that all structural descriptions are complete or that we have joined all the dots of structure–function relationships), and any serious scholar needs to have a working knowledge of the development, physiology, psychophysics (physiology without the blood), genetics, pathology, and computational models of the senses.}
}
@article{JOHNSON2022105743,
title = {Metacognition for artificial intelligence system safety – An approach to safe and desired behavior},
journal = {Safety Science},
volume = {151},
pages = {105743},
year = {2022},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2022.105743},
url = {https://www.sciencedirect.com/science/article/pii/S0925753522000832},
author = {Bonnie Johnson},
keywords = {Metacognition, Artificial intelligence systems, Machine learning, System safety, Complexity},
abstract = {Advances in computational thinking and data science have led to a new era of artificial intelligence systems being engineered to adapt to complex situations and develop actionable knowledge. These learning systems are meant to reliably understand the essence of a situation and construct critical decision recommendations to support autonomous and human–machine teaming operations. In parallel, the increasing volume, velocity, variety, veracity, value, and variability of data is confounding the complexity of these new systems – creating challenges in terms of their development and implementation. For artificial systems supporting critical decisions with higher consequences, safety has become an important concern. Methods are needed to avoid failure modes and ensure that only desired behavior is permitted. This paper discusses an approach that promotes self-awareness, or metacognition, within the artificial intelligence systems to understand their external and internal operational environments and use this knowledge to identify potential failures and enable self-healing and self-management for safe and desired behavior.}
}
@article{VERDUZCO2022103189,
title = {CALRECOD — A software for Computed Aided Learning of REinforced COncrete structural Design},
journal = {Advances in Engineering Software},
volume = {172},
pages = {103189},
year = {2022},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2022.103189},
url = {https://www.sciencedirect.com/science/article/pii/S0965997822000965},
author = {Luis Fernando Verduzco and Jaime Horta and Miguel A. Pérez Lara y Hernández and Juan Bosco Hernández},
keywords = {CALRECOD, Reinforced concrete structures, High education, Computed aided learning, Optimization methods},
abstract = {It is presented the development and implementation of a new computed aided learning MatLab Toolbox for the design of reinforced concrete structures named as CALRECOD for their abbreviation Computer Aided Learning of Reinforced Concrete Design. Such development emerges as the result of a series of research works in the Autonomous University of Queretaro with the main purpose of improving the way in which the design of reinforced concrete structures is taught in high education institutions. CALRECOD uses optimization methods and algorithms to aid students in their design interaction learning so that they are able to compare their own designs and what commercial software delivers with optimal ones given certain load conditions on the elements or structures. The software consists almost entirely of MatLab functions (.m files) and the ACI 318-19 code is taken as their main design reference to make it internationally useful, although in some cases the Mexican code NTC-17 specifications are used. Besides MatLab functions, the software consists as well of ANSYS SpaceClaim script functions (.scscript files) as an additional tool for the aid in the visualization of design results in a 3D space in the software ANSYS SpaceClaim. CALRECOD has proven to be versatile, flexible and of easy use with a huge potential to increase learning outcomes for students in high education programs related with the design of reinforced concrete structures as well as to enhance the creation of efficient interactive environments for researchers and academics focused on the development of new design and analysis methods for such structures. With their optimization design functions, a solid comparison platform of designs’ performance could be laid out, and with its extended function design packages for structural systems, reinforced concrete design courses could be enhanced in a great deal regarding their program content’s scope. The software can be found at: https://github.com/calrecod/CALRECOD.}
}
@article{RUBIN2023104955,
title = {Cartography of the multiple formal systems of molecular autopoiesis: from the biology of cognition and enaction to anticipation and active inference},
journal = {Biosystems},
volume = {230},
pages = {104955},
year = {2023},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2023.104955},
url = {https://www.sciencedirect.com/science/article/pii/S0303264723001302},
author = {Sergio Rubin},
keywords = {Self-fabrication, Operational closure, Closure to efficient causation, Calculus of self-reference, Non-algorithmic, Enaction, Final cause},
abstract = {A rich literature has grown up over the years that bears with autopoiesis, which tends to assume that it is a model, a theory, a principle, a definition of life, a property, refers to self-organization or even to hastily conclude that it is hylomorphic, hylozoist, in need of reformulation or to be overcome, making its status even more unclear. Maturana insists that autopoiesis is none of these and rather it is the causal organization of living systems as natural systems (NS) such that when it stops, they die. He calls this molecular autopoiesis (MA), which comprises two domains of existence: that of the self-producing organization (self-fabrication) and that of the structural coupling/enaction (cognition). Like all-NS in the universe, MA is amenable to be defined in theoretical terms, i.e. encoded in mathematical models and/or formal systems (FS). Framing the multiple formal systems of autopoiesis (FSA) into the Rosen's modeling relation (a process of bringing into equivalence the causality of NS and the inferential rules of FS), allows a classification of FSA into analytical categories, most importantly Turing machine (algorithmic) vs non-Turing machine (non-algorithmic) based, and FSA with a purely reactive mathematical image as cybernetic systems, i.e. feedbacks based, or conversely, as anticipatory systems making active inferences. It is thus the intent of the present work to advance the precision with which different FS may be observed to comply (preserve correspondence) with MA in its worldly state as a NS. The modeling relation between MA and the range of FS proposed as potentially illuminating their processes forecloses the applicability of Turing-based algorithmic computational models. This outcome indicates that MA, as modelled through Varela's calculus of self-reference or more especially through Rosen's (M,R)-system, is essentially anticipatory without violating structural determinism nor causality whatsoever, hence enaction may involve it. This quality may capture a fundamentally different mode of being in living systems as opposed to mechanical-computational systems. Implications in different fields of biology from the origin of life to planetary biology as well as in cognitive science and artificial intelligence are of interest.}
}
@article{HE2021117140,
title = {Understanding chemical short-range ordering/demixing coupled with lattice distortion in solid solution high entropy alloys},
journal = {Acta Materialia},
volume = {216},
pages = {117140},
year = {2021},
issn = {1359-6454},
doi = {https://doi.org/10.1016/j.actamat.2021.117140},
url = {https://www.sciencedirect.com/science/article/pii/S1359645421005206},
author = {Q.F. He and P.H. Tang and H.A. Chen and S. Lan and J.G. Wang and J.H. Luan and M. Du and Y. Liu and C.T. Liu and C.W. Pao and Y. Yang},
keywords = {Chemical Short Rang Order, High entropy alloy, Solid solution, Lattice distortion},
abstract = {Chemical short-range ordering (CSRO) or demixing in solid solution high entropy alloys (HEAs) is a fundamental issue yet to be fully understood. In this work, we first developed a generalized quasi-chemical solid solution model that enables quantitative computation of the local chemical ordering or demixing in solid solution HEAs. After that, we performed synchrotron diffraction experiments, extensive Reverse Monte Carlo (RMC) simulations, and first principles calculations on the CoCrFeNi model alloy to study the development of local chemical environments after long time thermal annealing. The outcome of the combined research demonstrates that the development of local chemical ordering or demixing in CoCrFeNi is not only affected by the heat of mixing between dislike atoms but also coupled with local lattice distortion.}
}
@article{READ201952,
title = {Using neural networks as models of personality process: A tutorial},
journal = {Personality and Individual Differences},
volume = {136},
pages = {52-67},
year = {2019},
note = {Dynamic Personality Psychology},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2017.11.015},
url = {https://www.sciencedirect.com/science/article/pii/S0191886917306724},
author = {Stephen J. Read and Vita Droutman and Benjamin J. Smith and Lynn C. Miller},
keywords = {Neural networks, Computational modeling, Within-subjects variability, Connectionist modeling, Personality dynamics},
abstract = {This paper presents a tutorial for creating neural network models of personality processes. Such models enable researchers to create explicit models of both personality structure and personality dynamics, and to address issues of recent concern in personality, such as, “If personality is stable, then how is it possible that within subject variability in personality states can be as large as or larger than between subject variability in personality?” or “Is it possible to understand personality dynamics and personality structure within a common framework?” We discuss why one should want to use neural networks, review what a neural network model is, review a previous model we have constructed, discuss how to conceptualize issues in such a way that they can be computationally modeled, show how that conceptualization can be translated into a model, and discuss the utility of such models for understanding personality structure and personality dynamics. To build our model we use a neural network modeling package called emergent that is freely available, and a specific architecture called Leabra to build a runnable model that addresses one of the questions posed above: How can within subject variability in personality related states be as large as between subject variability in personality?}
}
@article{BREIGER2018104,
title = {Capturing distinctions while mining text data: Toward low-tech formalization for text analysis},
journal = {Poetics},
volume = {68},
pages = {104-119},
year = {2018},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2018.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X17301584},
author = {Ronald L. Breiger and Robin Wagner-Pacifici and John W. Mohr},
keywords = {Text mining, Hermeneutics, National security, Computational sociology, Big data, Close reading},
abstract = {In this article we consider some low-tech approaches to text mining. Our goal is to articulate a RiCH (Reader in Control of Hermeneutics) style of text analysis that takes advantage of the digital affordances of modern reading practices and easily deployable computational tools while also preserving the primacy of the interpretive lens of the human reader. In the article we offer three analytical interventions that are suitable to the low-tech formalizations we propose: the first and most developed intervention tracks the (normally computationally ignored) “stop” words; the second identifies the use of strategic anxiety terms in the texts; and the third (less developed in this article) introduces the grammatical features of modality (including modalization statements of probability and usuality, and modulation statements regarding degrees of obligation and inclination). All three analytical interventions provide a productive tracking of various modes and degrees of strategic decisiveness, contradiction, uncertainty and indeterminacy in a corpus of recent U.S. National Security Strategy reports.}
}
@article{DELLACORTE2016209,
title = {Referential description of the evolution of a 2D swarm of robots interacting with the closer neighbors: Perspectives of continuum modeling via higher gradient continua},
journal = {International Journal of Non-Linear Mechanics},
volume = {80},
pages = {209-220},
year = {2016},
note = {Dynamics, Stability, and Control of Flexible Structures},
issn = {0020-7462},
doi = {https://doi.org/10.1016/j.ijnonlinmec.2015.06.016},
url = {https://www.sciencedirect.com/science/article/pii/S0020746215001468},
author = {Alessandro {Della Corte} and Antonio Battista and Francesco dell׳Isola},
keywords = {Swarm robot, Second gradient continua, Generalized continua, Deformable bodies},
abstract = {In the present paper a discrete robotic system model whose elements interact via a simple geometric law is presented and some numerical simulations are provided and discussed. The main idea of the work is to show the resemblance between the cases of first and second neighbors interaction with (respectively) first and second gradient continuous deformable bodies. Our numerical results showed indeed that the interaction and the evolution process described is suitable to closely reproduce some basic characteristics of the behavior of bodies whose deformation energy depends on first or on higher gradients of the displacement. Moreover, some specific qualitative characteristics of the continuous deformation are also reproduced. The model introduced here will need further investigation and generalization in both theoretical and numerical directions.}
}
@incollection{GROSSBERG19873,
title = {The Qijantized Geometry of Visual Space: The Coherent Computation of Depth, Form and Lightness},
editor = {Stephen Grossberg},
series = {Advances in Psychology},
publisher = {North-Holland},
volume = {43},
pages = {3-79},
year = {1987},
booktitle = {The Adaptive Brain II},
issn = {0166-4115},
doi = {https://doi.org/10.1016/S0166-4115(08)61756-2},
url = {https://www.sciencedirect.com/science/article/pii/S0166411508617562},
author = {Stephen Grossberg},
keywords = {binocular vision, brightness perception, figure-ground, feature extraction, form perception, neural network, nonlinear resonance, receptive field, short-term memory, spatial scales, visual completion},
abstract = {A theory is presented of how global visual interactions between depth, length, lightness, and form percepts can occur. The theory suggests how quantized activity patterns which reflect these visual properties can coherently fill-in, or complete, visually ambiguous regions starting with visually informative data features. Phenomena such as the Cornsweet and Craik-O'Brien effects, phantoms and subjective contours, binocular brightness summation, the equidistance tendency, Emmert's law, allelotropia, multiple spatial frequency scaling and edge detection, figure-ground completion, coexistence of depth and binocular rivalry, reflectance rivalry, Fechner's paradox, decrease of threshold contrast with increased number of cycles in a grating pattern, hysteresis, adaptation level tuning, Weber law modulation, shift of sensitivity with background luminance, and the finite capacity of visual short term memory are discussed in terms of a small set of concepts and mechanisms. Limitations of alternative visual theories which depend upon Fourier analysis, Laplacians, zero-crossings, and cooperative depth planes are described. Relationships between monocular and binocular processing of the same visual patterns are noted, and a shift in emphasis from edge and disparity computations toward the characterization of resonant activity-scaling correlations across multiple spatial scales is recommended. This recommendation follows from the theory's distinction between the concept of a structural spatial scale, which is determined by local receptive field properties, and a functional spatial scale, which is defined by the interaction between global properties of a visual scene and the network as a whole. Functional spatial scales, but not structural spatial scales, embody the quantization of network activity that reflects a scene's global visual representation. A functional scale is generated by a filling-in resonant exchange, or FIRE, which can be ignited by an exchange of feedback signals among the binocular cells where monocular patterns are binocularly matched.}
}
@incollection{GRANGER1986137,
title = {The Computation Of Contingency In Classical Conditioning},
editor = {Gordon H. Bower},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {20},
pages = {137-192},
year = {1986},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(08)60018-3},
url = {https://www.sciencedirect.com/science/article/pii/S0079742108600183},
author = {Richard H. Granger and Jeffrey C. Schlimmer},
abstract = {Publisher Summary
This chapter discusses a unified framework, which encompasses the computations, algorithms, and neurobiological implementations underlying classical conditioning. It presents an extensive mathematical analysis of the constraints on classical conditioning—that is, the precise contingency conditions under which mammals may and may not learn a particular association between two events in a classical conditioning situation. In classical conditioning, an unconditional stimulus (US)—that is, a cue, which is inherently biologically salient to an animal (such as an electric shock), is repeatedly paired with a conditional stimulus (CS), a cue that initially has no special significance to the animal over repeated trials, the animal can learn that the CS is predictive of or associated with the US. This phenomenon of associative learning is subject to laws and constraints: An association is learned to some extent in some conditions and to a lesser extent in others.}
}
@incollection{YANG202333,
title = {Chapter 2 - Machine learning for solid mechanics},
editor = {Yuebing Zheng and Zilong Wu},
booktitle = {Intelligent Nanotechnology},
publisher = {Elsevier},
pages = {33-45},
year = {2023},
series = {Materials Today},
isbn = {978-0-323-85796-3},
doi = {https://doi.org/10.1016/B978-0-323-85796-3.00002-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323857963000020},
author = {Charles Yang and Zhizhou Zhang and Grace X. Gu},
keywords = {Solid mechanics, Inverse design, Physics-informed deep learning, Graph neural networks},
abstract = {Solid mechanics is an important field responsible for the robust designs of humanity's greatest engineering accomplishments, from skyscrapers to airplanes to the space shuttle. A burst of new manufacturing techniques and novel next-generation materials is ushering in a new age of engineering revolving around sustainable development. In this chapter, we outline how artificial intelligence (AI) can help scientists and engineers manage the increasing complexity and computational requirements in solid mechanics fields. Two common problem-solving frameworks, forward and inverse design, as well as two promising new AI-based approaches, physics-informed deep learning and graph neural networks, are covered.}
}
@article{UMAIR2024106224,
title = {Emotion Fusion-Sense (Emo Fu-Sense) – A novel multimodal emotion classification technique},
journal = {Biomedical Signal Processing and Control},
volume = {94},
pages = {106224},
year = {2024},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2024.106224},
url = {https://www.sciencedirect.com/science/article/pii/S1746809424002829},
author = {Muhammad Umair and Nasir Rashid and Umar {Shahbaz Khan} and Amir Hamza and Javaid Iqbal},
keywords = {EEG, ECG, GSR, Respiration amplitude, Body temperature, LSTM, Feature fusion, Modality biasing, Multimodal emotion classification},
abstract = {Human emotions play a vital role in overall well-being. With the advent of advance technologies growing interest has been observed in developing a multimodal emotion classification system that can accurately interpret human emotions. The article presents a comprehensive overview of a multimodal emotion classification system (Emo Fu-Sense) designed to capture the rich and nuanced nature of human emotions. Objective of Emo Fu-Sense is to integrates information from Electrocardiogram (ECG), Galvanic Skin Response (GSR), Electroencephalograph (EEG), respiration amplitude and body temperature to achieve holistic understanding of emotional states. To effectively extract information from multimodal data, designed system employs conventional methods with sophisticated machine learning algorithms including Long Short-Term Memory (LSTM), a variety of Recurrent Neural Network (RNN). Recommended solution extracts column wise features independently from different modalities based on the windowing operation. Finally, feature fusion and modality biasing were used to combine the information from different modalities. The proposed method has not only highlighted the limitations of unimodal system but has achieved a classification accuracy of 92.62 %, with an average F1-Score of 93 % and 9.2 % of Mean Absolute Error (MAE). Obtained results are better than existing state-of-the-art approaches. Evaluation of the multimodal emotion classification system was conducted on MAHNOB-HCI dataset, which encompasses a wide range of emotional expressions across various contexts and individuals. The integration of multiple modalities and advanced machine learning techniques enables a more comprehensive understanding of emotional states and highlights the significance of research and development in the field of affective computing.}
}
@article{KOTYRA2023105613,
title = {High-performance watershed delineation algorithm for GPU using CUDA and OpenMP},
journal = {Environmental Modelling & Software},
volume = {160},
pages = {105613},
year = {2023},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105613},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222003139},
author = {Bartłomiej Kotyra},
keywords = {Watershed delineation, GIS, Parallel algorithms, GPU, CUDA, OpenMP},
abstract = {Watershed delineation is one of the fundamental tasks in hydrological studies. Tools for extracting watersheds from digital elevation models and flow direction rasters are commonly implemented in GIS software packages. However, the performance of available techniques and algorithms often turns out to be far from sufficient, especially when working with large datasets. While modern hardware offers high computing performance through massive parallelism, there is still a need for algorithms that can effectively use these capabilities. This paper proposes an algorithm for rapid watershed delineation directly from flow direction rasters, using the possibilities offered by modern GPU devices. Performance measurements show a significant reduction in execution time compared to other parallel solutions proposed for this task in the literature. Moreover, this implementation makes it possible to delineate multiple watersheds from the same dataset simultaneously, each having one or more outlet cells, with virtually no additional computational cost.}
}
@article{LI2025112016,
title = {The neural correlates of logical-mathematical symbol systems processing resemble those of spatial cognition more than language processing},
journal = {iScience},
volume = {28},
number = {4},
pages = {112016},
year = {2025},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2025.112016},
url = {https://www.sciencedirect.com/science/article/pii/S2589004225002767},
author = {Yuannan Li and Shan Xu and Jia Liu},
keywords = {Neuroscience, Cognitive neuroscience},
abstract = {Summary
The ability to use logical-mathematical symbols (LMS), encompassing tasks such as calculation, reasoning, and programming, is special to humans with recent emergence. LMS processing was suggested to build upon fundamental cognitive systems through neuronal recycling, with natural language processing and spatial cognition as key candidates. This study used meta-analyses and synthesized neural maps of representative LMS tasks, including reasoning, calculation, and mental programming, to compare their neural correlates with those of the two systems. Our results revealed greater activation overlap and multivariate similarity between LMS and spatial cognition than with language processing. Hierarchical clustering further indicated that LMS tasks were indistinguishable from spatial tasks at the neural level, suggesting an inherent connection. Our findings support the hypothesis that spatial cognition is the basis of LMS processing, shedding light on the logical reasoning limitations of large language models, particularly those lacking explicit spatial representations.}
}
@article{TRONCOSOGARCIA2023108387,
title = {Explainable hybrid deep learning and Coronavirus Optimization Algorithm for improving evapotranspiration forecasting},
journal = {Computers and Electronics in Agriculture},
volume = {215},
pages = {108387},
year = {2023},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2023.108387},
url = {https://www.sciencedirect.com/science/article/pii/S0168169923007755},
author = {A.R. Troncoso-García and I.S. Brito and A. Troncoso and F. Martínez-Álvarez},
keywords = {XAI, Deep learning, Evapotranspiration forecasting, Hyperparameter optimization},
abstract = {Reference evapotranspiration is a critical hydrological measurement closely associated with agriculture. Accurate forecasting is vital in effective water management and crop planning in sustainable agriculture. In this study, the future values of reference evapotranspiration are forecasted by applying a recurrent long short-term memory neural network optimized using the Coronavirus Optimization Algorithm, a novel bioinspired metaheuristic based on the spread of COVID-19. The input data is sourced from the Sistema Agrometeorológico para a Gestão da Rega no Alentejo, in Portugal, with meteorological data such as air temperature or wind speed. Several baseline models are applied to the same problem to facilitate comparisons, including support vector machines, multi-layer perceptron, Lasso and decision tree. The results demonstrate the successful forecasting performance of the proposed model and its potential in this field. In turn, to gain deeper insights into the model’s inner workings, the SHapley Additive exPlanation tool is applied for explainability. Consequently, the study identifies the most relevant variables for reference evapotranspiration forecasting, including previously measured evapotranspiration values. Additionally, a univariable model is tested using historic evapotranspiration values as input, offering a comparable performance with a considerable reduction of computational time.}
}
@article{FARISCO2024106714,
title = {Is artificial consciousness achievable? Lessons from the human brain},
journal = {Neural Networks},
volume = {180},
pages = {106714},
year = {2024},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2024.106714},
url = {https://www.sciencedirect.com/science/article/pii/S0893608024006385},
author = {Michele Farisco and Kathinka Evers and Jean-Pierre Changeux},
keywords = {Brain, Consciousness, Artificial intelligence, Neuromorphic computing, Robotics, Cognition, Neuroscience},
abstract = {We here analyse the question of developing artificial consciousness from an evolutionary perspective, taking the evolution of the human brain and its relation with consciousness as a reference model or as a benchmark. This kind of analysis reveals several structural and functional features of the human brain that appear to be key for reaching human-like complex conscious experience and that current research on Artificial Intelligence (AI) should take into account in its attempt to develop systems capable of human-like conscious processing. We argue that, even if AI is limited in its ability to emulate human consciousness for both intrinsic (i.e., structural and architectural) and extrinsic (i.e., related to the current stage of scientific and technological knowledge) reasons, taking inspiration from those characteristics of the brain that make human-like conscious processing possible and/or modulate it, is a potentially promising strategy towards developing conscious AI. Also, it cannot be theoretically excluded that AI research can develop partial or potentially alternative forms of consciousness that are qualitatively different from the human form, and that may be either more or less sophisticated depending on the perspectives. Therefore, we recommend neuroscience-inspired caution in talking about artificial consciousness: since the use of the same word “consciousness” for humans and AI becomes ambiguous and potentially misleading, we propose to clearly specify which level and/or type of consciousness AI research aims to develop, as well as what would be common versus differ in AI conscious processing compared to human conscious experience.}
}
@article{MOTSA2023116912,
title = {A data-driven, machine learning scheme used to predict the structural response of masonry arches},
journal = {Engineering Structures},
volume = {296},
pages = {116912},
year = {2023},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2023.116912},
url = {https://www.sciencedirect.com/science/article/pii/S0141029623013275},
author = {Siphesihle Mpho Motsa and Georgios Ε. Stavroulakis and Georgios Α. Drosopoulos},
keywords = {FEM, Machine Learning, Artificial Neural Network, Multi-hinge failure, Damage Prediction, Masonry Arches, Data-driven Mechanics, Digital Twin},
abstract = {A data-driven methodology is proposed, for the investigation of the ultimate response of masonry arches. Aiming to evaluate their structural response in a computationally efficient framework, machine learning metamodels, in the form of artificial neural networks, are adopted. Datasets are numerically built, integrating Matlab, Python and commercial finite element software. Heyman’s assumptions are adopted within non-linear finite element analysis, incorporating contact-friction laws between adjacent stones, to capture failure in the arch. The artificial neural networks are trained, validated, and tested using the least square minimization technique. It is shown that the proposed scheme can be used to provide a fast and accurate prediction of the deformed geometry, the collapse mechanism and the ultimate load. Cases studies demonstrate the efficiency of the method in random, new arch geometries. Relevant Matlab/Python scripts and datasets are provided. The method can be extended towards structural health monitoring and the concept of digital twin.}
}
@article{DISESSA198067,
title = {Computation as a physical and intellectual environment for learning physics},
journal = {Computers & Education},
volume = {4},
number = {1},
pages = {67-75},
year = {1980},
issn = {0360-1315},
doi = {https://doi.org/10.1016/0360-1315(80)90009-3},
url = {https://www.sciencedirect.com/science/article/pii/0360131580900093},
author = {A.A. DiSessa}
}
@article{LOU2023102236,
title = {A function-behavior mapping approach for product conceptual design inspired by memory mechanism},
journal = {Advanced Engineering Informatics},
volume = {58},
pages = {102236},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.102236},
url = {https://www.sciencedirect.com/science/article/pii/S1474034623003646},
author = {Shanhe Lou and Yixiong Feng and Yicong Gao and Hao Zheng and Tao Peng and Jianrong Tan},
keywords = {Conceptual design, Situated function-behavior-structure, Memory mechanism, Reinforcement learning},
abstract = {Conceptual design is a pivotal stage for new product development that relies more on designers to solve open-ended and ill-defined problems. Situated function-behavior-structure ontology is an acknowledged method to facilitate conceptual design in a goal-oriented way. However, it depends on the subjective cognition abilities of designers, which are influenced by limited memory and reasoning capacities. Developing computer-aided methods grounded in this ontology holds significant promise in enhancing designers' cognitive abilities. This study delves into the function-behavior (F-B) mapping process. It explores the effect of working memory and long-term memory on design cognition and introduces a memory-inspired reinforcement learning framework for F-B mapping. The Markov decision process is then adopted to formalize F-B mapping while motivation-driven Q learning is employed by the design agent to learn knowledge from historical design cases. The learned state-action value matrix can be applied to guide the designer in selecting feasible behaviors for the specific function requirement. The proposed approach empowers design agents with self-learning and self-evolving capacities. A case study on the F-B mapping of a traction system is conducted to illustrate the feasibility and practicability of the proposed approach.}
}
@article{HALL1996115,
title = {The role of creativity within best practice manufacturing},
journal = {Technovation},
volume = {16},
number = {3},
pages = {115-121},
year = {1996},
issn = {0166-4972},
doi = {https://doi.org/10.1016/0166-4972(95)00050-X},
url = {https://www.sciencedirect.com/science/article/pii/016649729500050X},
author = {David J. Hall},
abstract = {‘Best practice’ manufacturing is linked directly to aspects of creativity, through an appreciation of the operation of the practitioner's brain. It is suggested that the introduction of techniques such as benchmarking or business process re-engineering cannot succeed in the long term, unless the correct understanding is developed within the management team. Concepts of mind set and lateral thinking are related to the top-down introduction of step change, whilst ‘total quality’ programmes develop the culture necessary for bottom-up continuous improvement. Successful companies will run the two approaches in parallel.}
}
@article{POLHEMUS2020,
title = {Human-Centered Design Strategies for Device Selection in mHealth Programs: Development of a Novel Framework and Case Study},
journal = {JMIR mHealth and uHealth},
volume = {8},
number = {5},
year = {2020},
issn = {2291-5222},
doi = {https://doi.org/10.2196/16043},
url = {https://www.sciencedirect.com/science/article/pii/S2291522220003046},
author = {Ashley Marie Polhemus and Jan Novák and Jose Ferrao and Sara Simblett and Marta Radaelli and Patrick Locatelli and Faith Matcham and Maximilian Kerz and Janice Weyer and Patrick Burke and Vincy Huang and Marissa Fallon Dockendorf and Gergely Temesi and Til Wykes and Giancarlo Comi and Inez Myin-Germeys and Amos Folarin and Richard Dobson and Nikolay V Manyakov and Vaibhav A Narayan and Matthew Hotopf},
keywords = {human-centric design, design thinking, patient centricity, device selection, technology selection, remote patient monitoring, remote measurement technologies},
abstract = {Background
Despite the increasing use of remote measurement technologies (RMT) such as wearables or biosensors in health care programs, challenges associated with selecting and implementing these technologies persist. Many health care programs that use RMT rely on commercially available, “off-the-shelf” devices to collect patient data. However, validation of these devices is sparse, the technology landscape is constantly changing, relative benefits between device options are often unclear, and research on patient and health care provider preferences is often lacking.
Objective
To address these common challenges, we propose a novel device selection framework extrapolated from human-centered design principles, which are commonly used in de novo digital health product design. We then present a case study in which we used the framework to identify, test, select, and implement off-the-shelf devices for the Remote Assessment of Disease and Relapse-Central Nervous System (RADAR-CNS) consortium, a research program using RMT to study central nervous system disease progression.
Methods
The RADAR-CNS device selection framework describes a human-centered approach to device selection for mobile health programs. The framework guides study designers through stakeholder engagement, technology landscaping, rapid proof of concept testing, and creative problem solving to develop device selection criteria and a robust implementation strategy. It also describes a method for considering compromises when tensions between stakeholder needs occur.
Results
The framework successfully guided device selection for the RADAR-CNS study on relapse in multiple sclerosis. In the initial stage, we engaged a multidisciplinary team of patients, health care professionals, researchers, and technologists to identify our primary device-related goals. We desired regular home-based measurements of gait, balance, fatigue, heart rate, and sleep over the course of the study. However, devices and measurement methods had to be user friendly, secure, and able to produce high quality data. In the second stage, we iteratively refined our strategy and selected devices based on technological and regulatory constraints, user feedback, and research goals. At several points, we used this method to devise compromises that addressed conflicting stakeholder needs. We then implemented a feedback mechanism into the study to gather lessons about devices to improve future versions of the RADAR-CNS program.
Conclusions
The RADAR device selection framework provides a structured yet flexible approach to device selection for health care programs and can be used to systematically approach complex decisions that require teams to consider patient experiences alongside scientific priorities and logistical, technical, or regulatory constraints.}
}
@incollection{BAREISS1993157,
title = {The Evolution of a Case-Based Computational Approach to Knowledge Representation, Classification, and Learning},
editor = {Glenn V. Nakamura and Roman Taraban and Douglas L. Medin},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {29},
pages = {157-186},
year = {1993},
booktitle = {Categorization by Humans and Machines},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(08)60139-5},
url = {https://www.sciencedirect.com/science/article/pii/S0079742108601395},
author = {Ray Bareiss and Brian M. Slator}
}
@article{FENG2022127434,
title = {Parallel cooperation search algorithm and artificial intelligence method for streamflow time series forecasting},
journal = {Journal of Hydrology},
volume = {606},
pages = {127434},
year = {2022},
issn = {0022-1694},
doi = {https://doi.org/10.1016/j.jhydrol.2022.127434},
url = {https://www.sciencedirect.com/science/article/pii/S0022169422000099},
author = {Zhong-kai Feng and Peng-fei Shi and Tao Yang and Wen-jing Niu and Jian-zhong Zhou and Chun-tian Cheng},
keywords = {Hydrological time series forecasting, Artificial intelligence, Evolutionary computation, Parallel computing},
abstract = {Reliable streamflow prediction is an important productive information in the hydrology and water resources management fields. As used to forecast the nonlinear streamflow time series, the conventional artificial intelligence model may suffer from local convergence defect and fail to track the dynamic changes of the hydrological process when the model parameters and network structure are not well identified. Thus, this research develops a practical hydrological forecasting model based on parallel cooperation search algorithm (PCSA) and extreme learning machine (ELM), where the standard ELM method is chosen as the basic forecasting model, and then the PCSA method using several smaller and independent subswarms for parallel computation is used to determine satisfying input-hidden weights and hidden biases of the ELM model. The proposed model is used to forecast the nonlinear streamflow time series of several real-world hydrological stations in China. The results demonstrate that the proposed model outperforms the standard ELM model in various evaluation indicators. Thus, the key contributions of this study lie in two aspects: (1) for the first time, the parallel computing technique is developed to improve the global search ability and resources utilization efficiency of the emerging cooperation search algorithm; (2) an artificial intelligence model coupled with parallel evolutionary optimizer is proposed to improve the prediction accuracy of hydrological time series.}
}
@article{BAYER202380,
title = {The SPEAK study rationale and design: A linguistic corpus-based approach to understanding thought disorder},
journal = {Schizophrenia Research},
volume = {259},
pages = {80-87},
year = {2023},
note = {Language and Speech Analysis in Schizophrenia and Related Psychoses},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2022.12.048},
url = {https://www.sciencedirect.com/science/article/pii/S0920996422004959},
author = {J.M.M. Bayer and J. Spark and M. Krcmar and M. Formica and K. Gwyther and A. Srivastava and A. Selloni and M. Cotter and J. Hartmann and A. Polari and Z.R. Bilgrami and C. Sarac and A. Lu and Alison R. Yung and A. McGowan and P. McGorry and J.L. Shah and G.A. Cecchi and R. Mizrahi and B. Nelson and C.M. Corcoran},
keywords = {Thought disorder, Ultra/clinical high risk, Natural language processing, Psychosis, Latent semantic analysis, Part-of-speech-tagging},
abstract = {Aim
Psychotic symptoms are typically measured using clinical ratings, but more objective and sensitive metrics are needed. Hence, we will assess thought disorder using the Research Domain Criteria (RDoC) heuristic for language production, and its recommended paradigm of “linguistic corpus-based analyses of language output”. Positive thought disorder (e.g., tangentiality and derailment) can be assessed using word-embedding approaches that assess semantic coherence, whereas negative thought disorder (e.g., concreteness, poverty of speech) can be assessed using part-of-speech (POS) tagging to assess syntactic complexity. We aim to establish convergent validity of automated linguistic metrics with clinical ratings, assess normative demographic variance, determine cognitive and functional correlates, and replicate their predictive power for psychosis transition among at-risk youths.
Methods
This study will assess language production in 450 English-speaking individuals in Australia and Canada, who have recent onset psychosis, are at clinical high risk (CHR) for psychosis, or who are healthy volunteers, all well-characterized for cognition, function and symptoms. Speech will be elicited using open-ended interviews. Audio files will be transcribed and preprocessed for automated natural language processing (NLP) analyses of coherence and complexity. Data analyses include canonical correlation, multivariate linear regression with regularization, and machine-learning classification of group status and psychosis outcome.
Conclusions
This prospective study aims to characterize language disturbance across stages of psychosis using computational approaches, including psychometric properties, normative variance and clinical correlates, important for biomarker development. SPEAK will create a large archive of language data available to other investigators, a rich resource for the field.}
}
@article{LI2025103353,
title = {A multi-task engineering design intention recognition approach based on Vision Transformer and EEG data},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103353},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103353},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625002460},
author = {Mingrui Li and Zuoxu Wang and Fan Li and Jihong Liu},
keywords = {Design intention recognition, Engineering design, EEG, Vision Transformer},
abstract = {Engineering product design involves a variety of tasks and scenarios, including design modeling, design calculation, process planning, etc. When performing these design tasks, designers generate constantly shifting design intentions. Accurately recognizing these design intentions allows for a more thorough exploration of design processes from the perspective of cognition, facilitating the advancement of intelligent engineering design. Electroencephalogram (EEG) technology has emerged as an effective tool in recent years, which can provide direct insight into designers’ cognitive processes and intentions. However, the current application of EEG technology in engineering design faces difficulties in adapting to multi-task scenarios and rarely targets the design process directly. This study proposed a design intention recognition approach based on Vision Transformer (ViT) and EEG data applicable to multiple engineering design tasks. An image-like representation matrix is introduced to organize designers’ EEG data with the retention of its spatial and frequency features. Then, standard EEG data under different design intentions as well as the EEG data from real design processes is utilized to train and fine-tune a ViT-based design intention recognition model. An experiment workflow for collecting the two types of EEG data is also presented, along with detailed examples of three design tasks. The comparative experiment results and the case study demonstrates the feasibility of the proposed design intention recognition approach.}
}
@article{MOHAN2020771,
title = {Spread Spectrum Hop Count analyzing technique based code-division multiple access for data frequencies examining in wireless network},
journal = {Computer Communications},
volume = {150},
pages = {771-776},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2019.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419304980},
author = {N. Mohan},
keywords = {Data transfer, Quality improvement, Spread Spectrum, Hop Count, CDMA},
abstract = {Code-division multiple access (CDMA) is a bandwidth access technique used by different radio waves and signal advancements. CDMA is a way of providing multiple access, where transmitters can send data at the same time, where a single clock channel can be completed. It enables us to share data frequencies with a few systems (refer to the data transfer and capacity). From the multiple backward spaces allow this CDMA uses a wide range of novelty innovation and exceptional coding scheme (where each transmitter is allocated code). In this work, the different application of the Spread Spectrum Hop Count Analyzing Technique (SSHCA–CDMA) is presented which organizes information testing techniques to create accessible assessment data, with the ultimate goal of providing the most efficient techniques for execution improvement thinking. The underlying area of eligibility testing and the evaluation of metadata inquiry are the expectation space, the data that select the most effective regulatory function. Similarly, in this work, master-based techniques have been demonstrated to validate and analyze SSHCA cells. Long, most recent developments have retained a perspective and the nature of customer correspondence management.}
}
@article{ASH1984412,
title = {Computations of cuspidal cohomology of congruence subgroups of SL(3, Z)},
journal = {Journal of Number Theory},
volume = {19},
number = {3},
pages = {412-436},
year = {1984},
issn = {0022-314X},
doi = {https://doi.org/10.1016/0022-314X(84)90081-7},
url = {https://www.sciencedirect.com/science/article/pii/0022314X84900817},
author = {Avner Ash and Daniel Grayson and Philip Green},
abstract = {Algorithms are presented which find a basis of the vector space of cuspidal cohomology of certain congruence subgroups of SL(3, Z) and which determine the action of the Hecke operators on this space. These algorithms were implemented on a computer. Four pairs of cuspidal classes were found with prime level less than 100. Tables are given of the eigenvalues of the first few Hecke operators on these classes.}
}
@article{ALIJAH2007193,
title = {On the N3O2- paradigm},
journal = {Journal of Molecular Structure},
volume = {844-845},
pages = {193-199},
year = {2007},
note = {STUDIES IN HYDROGEN-BONDED SYSTEMS – A collection of Invited Papers in honour of Professor Lucjan Sobcyk, on the occasion of his 80th Birthday},
issn = {0022-2860},
doi = {https://doi.org/10.1016/j.molstruc.2007.04.024},
url = {https://www.sciencedirect.com/science/article/pii/S0022286007003316},
author = {Alexander Alijah and Eugene S. Kryachko},
keywords = {, Theoretical calculations, Isomers, Electron detachment},
abstract = {A survey of the existing experimental and theoretical data on the trinitrogen dioxide anion N3O2- that manifests a controversy as to the number of isomers and their chemical structures is presented. To resolve the controversy, new computational studies are performed at the MP2/aug-cc-pVTZ computational level. Two hitherto unknown isomers are predicted, one with singlet and one with triplet spin multiplicity. The singlet isomer, structurally characterized as N2·[ONO]−, is the most stable among all known isomers and accounts for fragmentation patterns observed in the recent dissociative photodetachment experiments.}
}
@article{ONKAL2013772,
title = {Scenarios as channels of forecast advice},
journal = {Technological Forecasting and Social Change},
volume = {80},
number = {4},
pages = {772-788},
year = {2013},
note = {Scenario Method: Current developments in theory and practice},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2012.08.015},
url = {https://www.sciencedirect.com/science/article/pii/S0040162512002090},
author = {Dilek Önkal and Kadire Zeynep Sayım and Mustafa Sinan Gönül},
keywords = {Forecast, Scenario, Group, Judgment, Advice taking},
abstract = {Today's business environment provides tougher competition than ever before, stressing the important role played by information and forecasts in decision-making. The scenario method has been popular for focused organizational learning, decision making and strategic thinking in business contexts, and yet, its use in communicating forecast information and advice has received little research attention. This is surprising since scenarios may provide valuable tools for communication between forecast providers and users in organizations, offering efficient platforms for information exchange via structured storylines of plausible futures. In this paper, we aim to explore the effectiveness of using scenarios as channels of forecast advice. An experimental study is designed to investigate the effects of providing scenarios as forecast advice on individual and group-based judgmental predictions. Participants are given time series information and model forecasts, along with (i) best-case, (ii) worst-case, (iii) both, or (iv) no scenarios. Different forecasting formats are used (i.e., point forecast, best-case forecast, worst-case forecast, and surprise probability), and both individual predictions and consensus forecasts are requested. Forecasts made with and without scenarios are compared for each of these formats to explore the potential effects of providing scenarios as forecast advice. In addition, group effects are investigated via comparisons of composite versus consensus predictions. The paper concludes with a discussion of results and implications for future research on scenario use in forecasting.}
}
@incollection{BERNINGER2002273,
title = {Chapter 10 - Building a Computing Brain Pedagogically},
editor = {Virginia W. Berninger and Todd L. Richards},
booktitle = {Brain Literacy for Educators and Psychologists},
publisher = {Academic Press},
address = {San Diego},
pages = {273-294},
year = {2002},
series = {Practical Resources for the Mental Health Professional},
issn = {18730450},
doi = {https://doi.org/10.1016/B978-012092871-2/50011-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780120928712500115},
author = {Virginia W. Berninger and Todd L. Richards},
abstract = {Publisher Summary
This chapter focuses on building a computing brain pedagogically. The brain, as it interacts with the world, constructs more concepts that are represented as mental models in distributed neural networks. Both the quantitative dimension and logical structures contribute to how these mental models are constructed and represented in the brain. The brain draws on both inductive thinking. During the construction process, the brain also uses multiple codes to represent and understand this emerging conceptual domain. The hand that is instrumental in development of the written language system also plays a major role in development of conceptual representations of the world. Working memory also plays an important role in conceptual development in the math domain. Like the writing brain, the computing brain also develops from both play and conscious work. The chapters conclude that development of the computing brain requires guided assistance in translating implicit knowledge based on experience into explicit knowledge that can be used for the hard work of math problem solving, which is conducted in resource-limited, temporally constrained working memory.}
}
@article{BAUM1997195,
title = {A Bayesian approach to relevance in game playing},
journal = {Artificial Intelligence},
volume = {97},
number = {1},
pages = {195-242},
year = {1997},
note = {Relevance},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(97)00059-3},
url = {https://www.sciencedirect.com/science/article/pii/S0004370297000593},
author = {Eric B. Baum and Warren D. Smith},
keywords = {Relevance, Game tree search, Game theory, Computer game playing, Directed search, Utility guided search, Metareasoning, Computer chess, Computer Othello, Game trees, Graphical model, Decision theory, Utility, Bayesian model, Evaluation function, Rational search},
abstract = {The point of game tree search is to insulate oneself from errors in the evaluation function. The standard approach is to grow a full width tree as deep as time allows, and then value the tree as if the leaf evaluations were exact. The alpha-beta algorithm implements this with great computational efficiency. This approach has been effective in many games. Our approach is to form a Bayesian model of our uncertainty. We adopt an evaluation function that returns a probability distribution estimating the probability of various errors in valuing each position. These estimates are obtained by training from data. We thus use additional information at each leaf not available to the standard approach. We utilize this information in three ways: to evaluate which move is best after we are done expanding, to allocate additional thinking time to moves where additional time is most relevant to game outcome, and, perhaps most importantly, to expand the tree along the most relevant lines. Our measure of the relevance of expanding a given leaf provably approximates a measure of the impact of expanding the leaf on expected payoff, including the impact of the outcome of the leaf expansion on later expansion decisions. Our algorithms run (under reasonable assumptions) in time linear in the size of the final tree and hence except for a small constant factor, are as time efficient as alpha-beta. Our algorithm focuses on relevant lines, on which it can in principle grow a tree several times as deep as alpha-beta in a given amount of time. We have tested our approach on a variety of games, including Othello, Kalah, Warri, and others. Our probability independence approximations are seen to be significantly violated, but nonetheless our tree valuation scheme was found to play significantly better than minimax or the Probability Product rule when both competitors search the same tree. Our full search algorithm was found to outplay a highly ranked, directly comparable alpha-beta Othello program even when the alpha-beta program was given sizeable time odds, and also performed well against the three top Othello programs on the Internet Othello Server.}
}
@article{MWAPE2025343,
title = {Life cycle sustainability assessment of staple food processing: A double and dynamic materiality approach},
journal = {Sustainable Production and Consumption},
volume = {56},
pages = {343-363},
year = {2025},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2025.04.002},
url = {https://www.sciencedirect.com/science/article/pii/S2352550925000764},
author = {Mwewa Chikonkolo Mwape and Aditya Parmar and Franz Roman and Naushad M. Emmambux and Yaovi Ouézou Azouma and Oliver Hensel},
keywords = {LCSA, Staple foods, Double materiality, Dynamic materiality, MEFA, Global warming potential (GWP), Python modeling, ESG},
abstract = {Globally, 70 % of people are fed through peasant food systems that are responsible for growing 50 % of the world's food calories on 30 % of the land. In the global south, particularly in Sub-Saharan Africa, small-scale farming serves as a crucial lifeline for the food and income needs of local populations. Yet, it remains underfunded and under-researched in the context of sustainable development. Even if the traditional Life Cycle Sustainability Assessment offers a holistic approach to evaluating the impacts of staple food processing across environmental, economic, and social dimensions, its inability to track dynamic materiality limits its application in evaluating future impacts. Therefore, this study aimed to provide a comprehensive Life Cycle Sustainability Assessment framework for staple food processing, using cassava to produce gari, a staple food for more than 300 million West Africans, as a case study. This framework integrates Material and Energy Flow Analysis techniques to trace resource use and emissions. The research incorporated Environmental, Social and Governance pillars; double materiality, evaluating both the direct and indirect impacts of processing activities, alongside dynamic materiality to capture evolving environmental, financial, and social factors through scenarios. Python computational modeling was used to perform these complex analyses, ensuring accuracy and adaptability. The findings highlight significant energy inefficiencies (6.67 kWh kg-1) coupled with a high Global Warming Potential (GWP) of 9.02 kgCO2eq kg-1 and production costs of $0.56 kg-1. The most significant opportunities for improvement were identified in optimizing energy consumption and transforming waste into biogas. The dynamic model revealed that integrating renewable energy sources could substantially reduce environmental impacts and increase the Net Profit Margin from 34.43 to 52.52 %, as proposed in the energy transition from woodfuel and gasoline to a Hybrid Solar and Biogas energy system. This study contributes to the growing body of literature on Life Cycle Sustainability Assessment by applying a comprehensive framework to staple food processing. The findings offer valuable insights into the environmental, social, and economic trade-offs in food processing systems, providing practical recommendations for improving sustainability throughout the food supply chain. Extended studies using these methods on other staples are highly recommended.}
}
@incollection{NI2016239,
title = {Chapter 17 - More Intelligent Models},
editor = {Daiheng Ni},
booktitle = {Traffic Flow Theory},
publisher = {Butterworth-Heinemann},
pages = {239-251},
year = {2016},
isbn = {978-0-12-804134-5},
doi = {https://doi.org/10.1016/B978-0-12-804134-5.00017-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128041345000179},
author = {Daiheng Ni},
keywords = {Car-following models, Psycho-physical model, Carsim model, Rule-based model, Neural network model},
abstract = {Along the lines of car-following models, single-regime models stand at one end and use one equation to handle all driving situations. Models can become increasingly intelligent if they include more and more equations to represent different regimes, such as start-up, speedup, free-flow, approaching, following, and stopping. Even more intelligent models can mimic the way of human thinking—for example, using rules and reasoning based on neural networks.}
}
@article{FANG2025101300,
title = {Generative AI-enhanced human-AI collaborative conceptual design: A systematic literature review},
journal = {Design Studies},
volume = {97},
pages = {101300},
year = {2025},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2025.101300},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X25000122},
author = {Cong Fang and Yujie Zhu and Le Fang and Yonghao Long and Huan Lin and Yangfan Cong and Stephen Jia Wang},
keywords = {Human-AI collaboration, AI-enhanced design, Design methodology, Design process, Conceptual design},
abstract = {Generative Artificial Intelligence (GenAI) has gained increasing attention, enhancing design productivity by elevating creativity within the conceptual design process. Despite these advancements, how GenAI will influence the conceptual design process and methods remains ambiguous, hindering its full potential. This study introduces a systematic literature review to explore GenAI's role in the conceptual design process, emphasizing the GenAI-human interactions and collaborations. We offer a critical evaluation of the current state of GenAI-human collaboration, identifying challenges, opportunities, and future research directions to leverage GenAI's design potential for enhancing creativity in conceptual design practice. Finally, a Generative AI Enhanced Conceptual Design framework was further proposed to clarify the potential collaborative design process, which can serve as a guideline for effective human-AI collaboration in the conceptual design process.}
}
@article{MUTH1992278,
title = {Extraneous information and extra steps in arithmetic word problems},
journal = {Contemporary Educational Psychology},
volume = {17},
number = {3},
pages = {278-285},
year = {1992},
issn = {0361-476X},
doi = {https://doi.org/10.1016/0361-476X(92)90066-8},
url = {https://www.sciencedirect.com/science/article/pii/0361476X92900668},
author = {K.Denise Muth},
abstract = {To determine how middle school students cope with some of the demands imposed on them by arithmetic word problems, 140 eighth graders were asked to solve word problems modeled after those used by the National Assessment of Educational Progress. Processing demands were imposed on the students by adding extraneous information and extra steps to the problems. Results indicated that the presence of extraneous information and extra steps reduced the accuracy of students' solutions. Thinking-out-loud protocols also revealed several misconceptions that students have about solving word problems.}
}
@article{KHARE2022105028,
title = {A hybrid decision support system for automatic detection of Schizophrenia using EEG signals},
journal = {Computers in Biology and Medicine},
volume = {141},
pages = {105028},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.105028},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521008222},
author = {Smith K. Khare and Varun Bajaj},
keywords = {Schizophrenia, Electroencephalography, Optimization, Robust variational mode decomposition, Optimized extreme learning machine classifier},
abstract = {Background
Schizophrenia (SCZ) is a serious neurological condition in which people suffer with distorted perception of reality. SCZ may result in a combination of delusions, hallucinations, disordered thinking, and behavior. This causes permanent disability and hampers routine functioning. Trained neurologists use interviewing and visual inspection techniques for the detection and diagnosis of SCZ. These techniques are manual, time-consuming, subjective, and error-prone. Therefore, there is a need to develop an automatic model for SCZ classification. The aim of this study is to develop an automated SCZ classification model using electroencephalogram (EEG) signals. The EEG signals can capture the changes in neural dynamics of human cognition during SCZ.
Method
Based on the nature of the SCZ condition, the EEG signals must be examined. For accurate interpretation of EEG signals during SCZ, an automated model integrating a robust variational mode decomposition (RVMD) and an optimized extreme learning machine (OELM) classifier is developed. Traditional VMD suffers from noisy mode generation, mode duplication, under segmentation, and mode discarding. These problems are suppressed in RVMD by automating the selection of quadratic penalty factor (α) and a number of modes (L). The hyperparameters (HPM) of the OELM classifier are automatically selected to ensure maximum accuracy for each mode without overfitting or underfitting. For the selection of α and L in RVMD and HPM in the OELM classifier, a whale optimization algorithm is used. The root mean square error is minimized for RVMD and classification accuracy of each mode is maximized for the OELM classifier. The EEG signals of three conditions performing basic sensory tasks have been analyzed to detect SCZ.
Results
The Kruskal Wallis test is used to select different features extracted from the modes produced by RVMD. An OELM classifier is tested using a ten-fold cross-validation technique. An accuracy, precision, specificity, F-1 measure, sensitivity, and Cohen's Kappa of 92.93%, 93.94%, 91.06% 94.07%, 97.15%, and 85.32% are obtained.
Conclusion
The third mode's chaotic features helped to capture the significant changes that occurred during the SCZ state. The findings of the RVMD-OELM-based hybrid decision support system can help neuro-experts for the accurate identification of SCZ in real-time scenarios.}
}
@article{URSINO2015234,
title = {A neural network for learning the meaning of objects and words from a featural representation},
journal = {Neural Networks},
volume = {63},
pages = {234-253},
year = {2015},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2014.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0893608014002639},
author = {Mauro Ursino and Cristiano Cuppini and Elisa Magosso},
keywords = {Semantic memory, Lexical memory, Conceptual representation, Hebb rule, Dominant features, Category formation},
abstract = {The present work investigates how complex semantics can be extracted from the statistics of input features, using an attractor neural network. The study is focused on how feature dominance and feature distinctiveness can be naturally coded using Hebbian training, and how similarity among objects can be managed. The model includes a lexical network (which represents word-forms) and a semantic network composed of several areas: each area is topologically organized (similarity) and codes for a different feature. Synapses in the model are created using Hebb rules with different values for pre-synaptic and post-synaptic thresholds, producing patterns of asymmetrical synapses. This work uses a simple taxonomy of schematic objects (i.e., a vector of features), with shared features (to realize categories) and distinctive features (to have individual members) with different frequency of occurrence. The trained network can solve simple object recognition tasks and object naming tasks by maintaining a distinction between categories and their members, and providing a different role for dominant features vs. marginal features. Marginal features are not evoked in memory when thinking of objects, but they facilitate the reconstruction of objects when provided as input. Finally, the topological organization of features allows the recognition of objects with some modified features.}
}
@article{ZHENG2018214,
title = {An improved genetic approach for composing optimal collaborative learning groups},
journal = {Knowledge-Based Systems},
volume = {139},
pages = {214-225},
year = {2018},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2017.10.022},
url = {https://www.sciencedirect.com/science/article/pii/S0950705117304914},
author = {Yaqian Zheng and Chunrong Li and Shiyu Liu and Weigang Lu},
keywords = {Collaborative learning, Learner group formation, Genetic algorithm, Optimal solution},
abstract = {Collaborative learning is an effective strategy for promoting learning in both traditional face-to-face and online environments. When applying it, students should be assigned to best collaborative groups at the first step, which is called the learner group formation task. In previous studies, various approaches have been proposed to solve this problem. However, they failed to meet all the problem requirements. To address this problem, a generic group formation method that covers all aspects of the problem is proposed in this study. In this method, all requirements of the learner group formation problem are formulated into an integrated mathematical model and an improved genetic algorithm is proposed to solve the model and obtain optimal learning groups to meet various grouping requirements for different educational contexts. To analyse the performance of the proposed approach from a computational perspective, a series of computational experiments are conducted based on eight simulation datasets with different levels of complexity. The simulation results indicate that the proposed method is effective and stable for solving the learner group formation problem. An empirical study is also carried out to validate the proposed approach from a pedagogical view by comparing it with two traditional group formation strategies. The results show that groups formed through the proposed method produce better outcomes than others in terms of group grades, individual grades and student satisfaction.}
}
@article{SECCHI2024105891,
title = {Modeling and theorizing with agent-based sustainable development},
journal = {Environmental Modelling & Software},
volume = {171},
pages = {105891},
year = {2024},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2023.105891},
url = {https://www.sciencedirect.com/science/article/pii/S1364815223002773},
author = {D. Secchi and V. Grimm and D.B. Herath and F. Homberg},
keywords = {Sustainable development, Theory development, Human–environment interaction, Agent-based modeling, Common language, ODD protocol},
abstract = {Sustainable development is an expression that permeates large areas of knowledge. For it to be meaningful, environmental aspects must be considered as intertwined with economic and social aspects. This is a multidisciplinary effort that is made challenging by the task of synthesizing the many emerging contributions. This has limited theory development where the definition of mechanisms, assumptions, dynamics and the determination of the entities involved are largely left to the reader’s imagination. We suggest to engage with the rationale of agent-based modeling to better define the assumptions, mechanisms, and boundaries of sustainable development. For this, the O-part of the widely used ODD protocol for describing agent-based models (ABM) provides a standardized structure, which we here augment to OsDD to specifically take into account sustainability issues. Even without formulating and implementing the full ABM, using OsDD requires to be explicit about the mechanisms, assumptions, dynamics and the entities involved and thereby provides a common language for theory development.}
}
@article{GREENWOOD2021106597,
title = {Exploring a causal model in observational cohort data: The role of parents and peers in shaping substance use trajectories},
journal = {Addictive Behaviors},
volume = {112},
pages = {106597},
year = {2021},
issn = {0306-4603},
doi = {https://doi.org/10.1016/j.addbeh.2020.106597},
url = {https://www.sciencedirect.com/science/article/pii/S0306460320307279},
author = {C.J. Greenwood and G.J. Youssef and P. Letcher and E.A. Spry and K.C. Thomson and L.J. Hagg and D.M. Hutchinson and J.A. Macdonald and J. McIntosh and A. Sanson and J.W. Toumbourou and C.A. Olsson},
keywords = {Causal modeling, substance use, Adolescence, Young adulthood, Trajectory, Parents, Peers},
abstract = {Aims
To explore the process of applying counterfactual thinking in examining causal determinants of substance use trajectories in observational cohort data. Specifically, we examine the extent to which quality of the parent-adolescent relationship and affiliations with deviant peers are causally related to trajectories of alcohol, tobacco, and cannabis use across adolescence and into young adulthood.
Methods
Data were drawn from the Australian Temperament Project, a population-based cohort study that has followed a sample of young Australians from infancy to adulthood since 1983. Parent-adolescent relationship quality and deviant peer affiliations were assessed at age 13–14 years. Latent curve models were fitted for past month alcohol, tobacco, and cannabis use (n = 1590) from age 15–16 to 27–28 years (5 waves). Confounding factors were selected in line with the counterfactual framework.
Results
Following confounder adjustment, higher quality parent-adolescent relationships were associated with lower baseline cannabis use, but not alcohol or tobacco use trajectories. In contrast, affiliations with deviant peers were associated with higher baseline binge drinking, tobacco, and cannabis use, and an earlier peak in the cannabis use trajectory.
Conclusions
Despite careful application of the counterfactual framework, interpretation of associations as causal is not without limitations. Nevertheless, findings suggested causal effects of both parent-adolescent relationships and deviant peer affiliations on the trajectory of substance use. Causal effects were more pervasive (i.e., more substance types) and protracted for deviant peer affiliations. The exploration of causal relationships in observational cohort data is encouraged, when relevant limitations are transparently acknowledged.}
}
@article{LEVINSON2012167,
title = {Tools from evolutionary biology shed new light on the diversification of languages},
journal = {Trends in Cognitive Sciences},
volume = {16},
number = {3},
pages = {167-173},
year = {2012},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2012.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364661312000290},
author = {Stephen C. Levinson and Russell D. Gray},
abstract = {Computational methods have revolutionized evolutionary biology. In this paper we explore the impact these methods are now having on our understanding of the forces that both affect the diversification of human languages and shape human cognition. We show how these methods can illuminate problems ranging from the nature of constraints on linguistic variation to the role that social processes play in determining the rate of linguistic change. Throughout the paper we argue that the cognitive sciences should move away from an idealized model of human cognition, to a more biologically realistic model where variation is central.}
}
@incollection{SEN201629,
title = {3 - History of zero including its representation and role},
editor = {Syamal K. Sen and Ravi P. Agarwal},
booktitle = {Zero},
publisher = {Academic Press},
pages = {29-75},
year = {2016},
isbn = {978-0-08-100774-7},
doi = {https://doi.org/10.1016/B978-0-08-100774-7.00003-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008100774700003X},
author = {Syamal K. Sen and Ravi P. Agarwal},
keywords = {Algorithms for arithmetic operations, alphabetical positional number system, assumption versus axioms, avoidance of subtraction, Brahmagupta’s rule to compute with zero, building block of matter, direction separator, driver of calculus, dwarf and machine epsilon, exponential growth of computing power, Godel’s incompleteness theorem, Gregorian calendar, history of zero, image of the earth, infinite versus finite precisions, infinitive universe, Maya numbers and long count, mean value theorem, Mohanjodaro and Harappa civilization, most pervasive global symbol, object of zero dimension, Quipu, representation of nothingness, Rolle’s theorem, sexagesimal (base 60) positional number system, stone/copper plate inscription, Vedas and Puranas, violation of a law of nature, Zeno’s paradoxes, zero as a number, zero as a vacant position, zero-free system, zero with its eternal spiritual significance},
abstract = {The chronological development of the history of zero over the centuries is a tough job due to both poor man to man communication and also poor publication machinery. However, the time period 7000 BC–2015 AD is broadly divided into four parts based on the landmark innovations in each part. During 7000–2000 BC, the most important contribution, that is, the modern decimal based place value system with 0 as a number due to Aryabhatta was developed and used. The Maya numbers and Long Count days that were tallied in a modified radix-20 number system are notable. Zero with representation and arithmetic operations was fully developed during 2000 BC–1000 AD. Brahmagupta’s rules for arithmetic operations were developed. The Romans and the Greeks had no zero then and their system was order-valued. Egyptian numerals were base-10 while Babylonian mathematics had a base-60 positional number system. With better understanding of zero, calculus was born. Arab and Persian mathematicians were active and became an important interface between the east and the west in promoting number systems with arithmetic. The period 1000–1900 AD saw the introduction of the Hindu–Arabic numeral system in Europe. The link between the system and European mathematics is the Italian mathematician Fibonacci. During the late eleventh century AD, Shen Gua introduced infinitesimal and exhaustion. He described piling up very small things. During 1900–2015 AD, increasingly high-speed modern digital computing made its presence felt very intensely globally by one and all. Specifically due to its finite precision, unlike the infinite precision which the regular and natural mathematics have, the advent of numerical zero, as opposed to the exact zero, changed the face of all real-world computation. Understanding natural, regular, and computational mathematics with calculus, and specifically the role of zero, became extraordinarily important in all engineering computations. The computational error (implying quality of solution) and computational complexity (cost of computation) due to the presence of numerical zero became integral parts of any algorithm to justify the acceptability of a solution. During the early twentieth century, Ramanujan, to whom each number is a living being and his personal friend carrying an important distinct message, felt intensely the eternal spiritual significance of zero and its inverse infinity. He built a theory of reality around zero and infinity.}
}
@article{ARNOLD2018581,
title = {Combining conscious and unconscious knowledge within human-machine-interfaces to foster sustainability with decision-making concerning production processes},
journal = {Journal of Cleaner Production},
volume = {179},
pages = {581-592},
year = {2018},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2018.01.070},
url = {https://www.sciencedirect.com/science/article/pii/S0959652618300787},
author = {Marlen Gabriele Arnold},
keywords = {Exploratory design, Structural systemic constellations, Cognitive human biases, HMI, Sustainable production contexts},
abstract = {At present, sustainability science is mainly based on conscious information and strongly focused on analytical tools or strategies. Neuroscience has made obvious that human decisions are prepared by the unconsciousness. Intuition plays an important role in early and late stages of learning processes and has a crucial impact on decision-making. Thus, intuitive and unconscious thinking is crucial for management processes in general and production planning processes in the main. However, unconscious knowledge and human behaviour is predominantly neglected in production research. Especially the addressing of human machine interfaces (HMI), human cognitive biases have a crucial impact on decision making processes. Constellation work is based on unconscious knowledge and intuition. Thus, systemic structural constellations are an innovative tool to integrate unconscious knowledge in a research context. In systemic structural constellations specific foci of complex systems, such as a production system, can be simulated and represented through spatial arrangements of persons or symbols. So, the method was used to reveal relevant patterns of relationships, structures, interaction, implicit knowledge, including hidden or underlying dynamics and influences that are relevant to and within a production system to understand how the raised problems in HMI can be better solved. The guiding research question is: How can the use of structural systemic constellations improve decision-making processes in HMI contexts in production environments in order to increase sustainability? Results show sustainability seems to be a matter of consciousness and is closely linked to the bias group not enough meaning. Sustainability and complexity resemble more than being linked by trade-offs. The recognition of human biases can be trained to improve human-machine-interfaces and sustainability. Constellation work contributes to decision theory by supporting effectuation.}
}
@article{LOPEZPERSEM2023273,
title = {Conceptual promises and mechanistic challenges of the creative metacognition framework: Comment on “A systematic framework of creative metacognition” by Izabela Lebuda and Mathias Benedek},
journal = {Physics of Life Reviews},
volume = {47},
pages = {273-275},
year = {2023},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2023.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S1571064523001860},
author = {Alizée Lopez-Persem and Marion Rouault and Emmanuelle Volle}
}
@article{LO2022111357,
title = {Architectural patterns for the design of federated learning systems},
journal = {Journal of Systems and Software},
volume = {191},
pages = {111357},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111357},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222000899},
author = {Sin Kit Lo and Qinghua Lu and Liming Zhu and Hye-Young Paik and Xiwei Xu and Chen Wang},
keywords = {Federated learning, Pattern, Software architecture, Machine learning, Artificial intelligence},
abstract = {Federated learning has received fast-growing interests from academia and industry to tackle the challenges of data hungriness and privacy in machine learning. A federated learning system can be viewed as a large-scale distributed system with different components and stakeholders as numerous client devices participate in federated learning. Designing a federated learning system requires software system design thinking apart from the machine learning knowledge. Although much effort has been put into federated learning from the machine learning technique aspects, the software architecture design concerns in building federated learning systems have been largely ignored. Therefore, in this paper, we present a collection of architectural patterns to deal with the design challenges of federated learning systems. Architectural patterns present reusable solutions to a commonly occurring problem within a given context during software architecture design. The presented patterns are based on the results of a systematic literature review and include three client management patterns, four model management patterns, three model training patterns, four model aggregation patterns, and one configuration pattern. The patterns are associated to the particular state transitions in a federated learning model lifecycle, serving as a guidance for effective use of the patterns in the design of federated learning systems.}
}
@incollection{GOLDSCHMIDT201146,
title = {Architecture},
editor = {Mark A. Runco and Steven R. Pritzker},
booktitle = {Encyclopedia of Creativity (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {46-51},
year = {2011},
isbn = {978-0-12-375038-9},
doi = {https://doi.org/10.1016/B978-0-12-375038-9.00010-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780123750389000108},
author = {G. Goldschmidt},
keywords = {Architectural design, Architectural education, Culture, Digital design, Form, Function, Ideas, Leading idea, Starchitect, Style},
abstract = {Architecture is a cultural arena based on ideas, which communally produce styles and individually, at their best, generate outstanding buildings. Every building tackles form and function. In our era architecture is expected to innovate in its forms, while ensuring perfect functionality. Form and function handling are rough correlates of originality and practicality, by which we measure design creativity. Architecture is also a product of the technological state of its time. At present we experience computational advances that promise to fundamentally change buildings and the manner in which they are designed. Architectural education is groping to adjust to the changes.}
}
@incollection{WOOLLISCROFT2020153,
title = {Chapter 12 - Precision medicine},
editor = {James O. Woolliscroft},
booktitle = {Implementing Biomedical Innovations into Health, Education, and Practice},
publisher = {Academic Press},
pages = {153-167},
year = {2020},
isbn = {978-0-12-819620-5},
doi = {https://doi.org/10.1016/B978-0-12-819620-5.00012-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128196205000126},
author = {James O. Woolliscroft},
keywords = {Precision medicine, Environment, Behavior, Microbiome, Genome, Pharmacogenomics},
abstract = {The convergence of computational, technologic and biomedical advances has enabled the development of precision medicine. Growing out of an understanding that there is a need for a new taxonomy of disease, the vision for precision medicine is to better understand the complex relationships in health and disease through the assemblage of massive databases that include individuals’ genomes, microbiomes, exposomes (a subsection of the environment), epigenomes, physiologic data, signs and symptoms, and other relevant information. Through the development of a holistic picture of genomic, microbiota, environmental and behavioral factors leading to disease, the intent is to intervene before disease becomes manifest to maintain or restore to health. Precision medicine will drive not only disruptive changes in the practice of clinical medicine, but also changes in our very conceptualization of health and disease.}
}
@article{PEREZESCOBAR202423,
title = {Minimal logical teleology in artifacts and biology connects the two domains and frames mechanisms via epistemic circularity},
journal = {Studies in History and Philosophy of Science},
volume = {104},
pages = {23-37},
year = {2024},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2024.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0039368124000104},
author = {José Antonio Pérez-Escobar},
keywords = {Minimal logical teleology, Analogies, Scientific explanation, Epistemic circularity, Scientific modelling, Cognitive neuroscience},
abstract = {The understanding of artifacts and biological phenomena has often influenced each other. This work argues that at the core of these epistemic bridges there are shared teleological notions and explanations manifested in analogies between artifacts and biological phenomena. To this end, I first propose a focus on the logical structure of minimal teleological explanations, which renders said epistemic bridges more evident than an ontological or metaphysical approach to teleology, and which can be used to describe scientific practices in different areas by virtue of formal generality and minimalism (section 2). Second, I show how this approach highlights some epistemic features shared by the understanding of artifacts and biological phenomena, like a specific kind of epistemic circularity, and how functional analogies between artifacts and biological phenomena translate such epistemic circularity from one domain to the other (section 3). Third, I conduct a case study on the scientific practice around the brain's “compass”, showing how the understanding of artifacts influences purpose ascription and measurement, and frames mechanisms in biology, especially in areas where purpose ascription is most difficult, like cognitive neuroscience (sections 4 and 5).}
}
@article{CAPONNETTO2021104823,
title = {Examining nursing student academic outcomes: A forty-year systematic review and meta-analysis},
journal = {Nurse Education Today},
volume = {100},
pages = {104823},
year = {2021},
issn = {0260-6917},
doi = {https://doi.org/10.1016/j.nedt.2021.104823},
url = {https://www.sciencedirect.com/science/article/pii/S0260691721000800},
author = {Valeria Caponnetto and Angelo Dante and Vittorio Masotta and Carmen {La Cerra} and Cristina Petrucci and Celeste Marie Alfes and Loreto Lancia},
keywords = {Academic failure, Academic success, Attrition, Bachelor's degree, Determinants, Factors, Nursing student},
abstract = {Objectives
To synthesize the definitions of nursing students' academic outcomes and provide a quantitative synthesis of their associated and predictive factors.
Design
Systematic review and meta-analysis.
Data sources
Four scientific databases were searched until January 2020.
Review methods
Observational studies describing undergraduate nursing students' academic outcomes were included. Studies were analytically synthesized and meta-analyses were performed utilizing the Odds Ratio or Cohen's d as effect sizes.
Results
Eighteen studies, published from 1979 to 2018, were included in the review, nine were meta-analyzed. Studies involved 10,024 undergraduate nursing students and were mostly retrospective cohort (55.6%). Students were mostly female (75.4%) with a mean age ranging from 21.3 to 27.0 years. Meta-analysis revealed that being female (OR = 1.65, 95% CI = 1.26 to 2.12), having attended a Classical, Scientific or Academic high school (OR = 1.30, 95% IC = 1.16 to 1.46), and having reported higher final grades at the upper-secondary high school (Cohen's d = 0.42, 95% CI = 0.18 to 0.65) was significantly associated with student's ability to graduate within the regular duration of the program. Sensitivity analyses confirmed meta-analytic results and meta-analyses heterogeneity depended on study design. Contrasting and limited evidence were found for other investigated factors, and for academic outcomes different from graduation within the regular duration of the program.
Conclusions
Despite meta-analytic results, gender and upper-secondary school would be unethical students' entry selection criteria. Final upper-secondary school grades should be considered for this scope and purpose. Conflicting and limited evidence found for other factors, such as students' background, suggested the influence of local contexts on the phenomenon and its investigation. Investigating the role of modifiable individual variables, such as empathy and critical thinking, could contribute to the open debate about students' entry selection strategies. An improvement in methodological quality of future studies is recommended and expected.}
}
@incollection{MORGERA1986389,
title = {COMPUTATIONAL COMPLEXITY AND VLSI IMPLEMENTATION OF AN OPTIMAL FEATURE SELECTION STRATEGY††Work supported by Canada NSERC Grant AO912.},
editor = {Edzard S. GELSEMA and Laveen N. KANAL},
booktitle = {Pattern Recognition in Practice},
publisher = {Elsevier},
address = {Amsterdam},
pages = {389-400},
year = {1986},
isbn = {978-0-444-87877-9},
doi = {https://doi.org/10.1016/B978-0-444-87877-9.50036-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780444878779500364},
author = {Salvatore D. Morgera}
}
@article{LIETO20161,
title = {From human to artificial cognition and back: New perspectives on cognitively inspired AI systems},
journal = {Cognitive Systems Research},
volume = {39},
pages = {1-3},
year = {2016},
note = {From human to artificial cognition (and back): new perspectives of cognitively inspired AI systems},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2016.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S1389041716300183},
author = {Antonio Lieto and Daniele P. Radicioni},
keywords = {Cognitive systems, Artificial intelligence, Computational models of cognition, Epistemology of the artificial},
abstract = {We overview the main historical and technological elements characterising the rise, the fall and the recent renaissance of the cognitive approaches to Artificial Intelligence and provide some insights and suggestions about the future directions and challenges that, in our opinion, this discipline needs to face in the next years.}
}
@article{CHAKRABORTY2013180,
title = {Secret image sharing using grayscale payload decomposition and irreversible image steganography},
journal = {Journal of Information Security and Applications},
volume = {18},
number = {4},
pages = {180-192},
year = {2013},
issn = {2214-2126},
doi = {https://doi.org/10.1016/j.istr.2013.02.006},
url = {https://www.sciencedirect.com/science/article/pii/S1363412713000162},
author = {Soumendu Chakraborty and Anand Singh Jalal and Charul Bhatnagar},
keywords = {DSF matrix, Error matrix, Sign matrix, Bit plane},
abstract = {To provide an added security level most of the existing reversible as well as irreversible image steganography schemes emphasize on encrypting the secret image (payload) before embedding it to the cover image. The complexity of encryption for a large payload where the embedding algorithm itself is complex may adversely affect the steganographic system. Schemes that can induce same level of distortion, as any standard encryption technique with lower computational complexity, can improve the performance of stego systems. In this paper, we propose a secure secret image sharing scheme, which bears minimal computational complexity. The proposed scheme, as a replacement for encryption, diversifies the payload into different matrices which are embedded into carrier image (cover image) using bit X-OR operation. A payload is a grayscale image which is divided into frequency matrix, error matrix, and sign matrix. The frequency matrix is scaled down using a mapping algorithm to produce Down Scaled Frequency (DSF) matrix. The DSF matrix, error matrix, and sign matrix are then embedded in different cover images using bit X-OR operation between the bit planes of the matrices and respective cover images. Analysis of the proposed scheme shows that it effectively camouflages the payload with minimum computation time.}
}
@article{AIBINU2023100590,
title = {Solutions of fractional differential equations by using a blend of variational iteration method with Sumudu transform and application to price adjustment equations},
journal = {Partial Differential Equations in Applied Mathematics},
volume = {8},
pages = {100590},
year = {2023},
issn = {2666-8181},
doi = {https://doi.org/10.1016/j.padiff.2023.100590},
url = {https://www.sciencedirect.com/science/article/pii/S2666818123001031},
author = {M.O. Aibinu and S. Moyo},
keywords = {Sumudu transform, Caputo fractional derivative, Price adjustment, Model, Market equilibrium},
abstract = {The presence of delays in a mathematical model can improve its vitality and suitability in describing several phenomena. However, in the presence of delays, nonlinear fractional differential equations are more difficult to study. This paper presents the use of a blend of variational iteration method with Sumudu transform for solving delay differential equations with Caputo derivatives of fractional variable order. Moreover, the paper introduces delays into the price adjustment equations to propose new price adjustment models with more potential for vitality and suitability. The paper assigns suitable real values to the parameters for the graphical display and comparison of the obtained solutions. The paper presents the interactions that exist among the price, demand, supply and dependence of supply and demand on the price, which can be applied to estimate the equilibrium price.}
}
@article{JIANG2020556,
title = {Energy aware edge computing: A survey},
journal = {Computer Communications},
volume = {151},
pages = {556-580},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S014036641930831X},
author = {Congfeng Jiang and Tiantian Fan and Honghao Gao and Weisong Shi and Liangkai Liu and Christophe Cérin and Jian Wan},
keywords = {Edge computing, Energy efficiency, Computing offloading, Benchmarking, Computation partitioning},
abstract = {Edge computing is an emerging paradigm for the increasing computing and networking demands from end devices to smart things. Edge computing allows the computation to be offloaded from the cloud data centers to the network edge and edge nodes for lower latency, security and privacy preservation. Although energy efficiency in cloud data centers has been broadly investigated, energy efficiency in edge computing is largely left uninvestigated due to the complicated interactions between edge devices, edge servers, and cloud data centers. In order to achieve energy efficiency in edge computing, a systematic review on energy efficiency of edge devices, edge servers, and cloud data centers is required. In this paper, we survey the state-of-the-art research work on energy-aware edge computing, and identify related research challenges and directions, including architecture, operating system, middleware, applications services, and computation offloading.}
}
@article{LEE201618,
title = {Affective Computing as Complex Systems Science},
journal = {Procedia Computer Science},
volume = {95},
pages = {18-23},
year = {2016},
note = {Complex Adaptive Systems Los Angeles, CA November 2-4, 2016},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.09.288},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916324607},
author = {William Lee and Michael D. Norman},
keywords = {Affective Computing, Computational Models, Complexity, Emotion, Apprasial},
abstract = {Pioneered in the early ‘90s by Rosalind Picard, a professor and IEEE Fellow of the MIT Media Lab, Affective Computing – rooted originally in artificial intelligence – now branches into wearable computing, big data, psychology, neuroscience, and modeling in order to advance the knowledge, understanding, and development of systems for sensing, recognizing, categorizing, and reacting to human emotion. Yet, the challenges of sensing multiple modalities simultaneously, disambiguating complex emotional states non-linearly, and modeling multiple individuals’ emotional states dynamically have continued to ring true, despite dramatic advances in affective computing. This paper seeks to serve two objectives. The first objective is to discuss how these three challenges are related to the three characteristics of complex systems – namely multiple components, non-linearity, and emergent behaviors. The second objective is to identify opportunities from the complex systems domain to address these challenges in novel and comprehensive ways. Recent advances in the utilization of Dynamical Systems Theory (an applied complexity science methodology) have shown that complex human interaction can be rigorously studied and modeled. Coupling the technological advances that cloud-based affective computing have brought with the emerging complex systems science-perspective may well catalyze a new era of human-machine and human-human collaboration.}
}
@article{MILLER2016102,
title = {Provision for income tax expense ASC 740: A teaching note},
journal = {Journal of Accounting Education},
volume = {35},
pages = {102-126},
year = {2016},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2015.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0748575115000858},
author = {Tad Miller and Lindsay Miller and Jeffrey Tolin},
keywords = {Provision for income tax expense, Accounting Standards Codification, Deferred tax assets, Deferred tax liabilities, Deductible temporary differences, Taxable temporary differences},
abstract = {This project requires students to think critically to synthesize concepts they learned in their financial reporting and tax classes. They will use and interpret accounting standards to prepare tax provisions, comparative financial statements and the appropriate footnote disclosures. Even a simple tax provision results in a challenging project.}
}
@article{LI202514,
title = {Paradigm shifts from data-intensive science to robot scientists},
journal = {Science Bulletin},
volume = {70},
number = {1},
pages = {14-18},
year = {2025},
issn = {2095-9273},
doi = {https://doi.org/10.1016/j.scib.2024.09.029},
url = {https://www.sciencedirect.com/science/article/pii/S2095927324006807},
author = {Xin Li and Yanlong Guo}
}
@article{RAVALI2022100045,
title = {A systematic review of artificial intelligence for pediatric physiotherapy practice: Past, present, and future},
journal = {Neuroscience Informatics},
volume = {2},
number = {4},
pages = {100045},
year = {2022},
issn = {2772-5286},
doi = {https://doi.org/10.1016/j.neuri.2022.100045},
url = {https://www.sciencedirect.com/science/article/pii/S2772528622000073},
author = {Ravula Sahithya Ravali and Thangavel Mahalingam Vijayakumar and Karunanidhi {Santhana Lakshmi} and Dinesh Mavaluru and Lingala Viswanath Reddy and Mervin Retnadhas and Tintu Thomas},
keywords = {Artificial intelligence, Systematic review, Pediatric physical therapy, Physiotherapy education},
abstract = {Background: Artificial intelligence (AI) is one of the active research fields to develop systems that mimic human intelligence and is helpful in many fields, particularly in medicine. (“Role of Artificial Intelligence Techniques ... - PubMed”) Physiotherapy is mainly involving in curing bone-related pain and injuries. The recent emergence of artificially intelligent machines has seen human cognitive capacity enhanced by computational agents that can recognize previously hidden patterns within massive data sets. (“(PDF) Artificial intelligence in clinical practice ...”) In this context, artificial intelligence in pediatric physiotherapy could be one of the most important modalities in delivering better medical and healthcare services to needy people. It is an attempt to identify the types, as well as to assess the effectiveness of interventions provided by artificial intelligence on pediatric physical therapy optimization-related outcomes. Methods: Data acquisition was carried out by systematic searches from various academic and research databases i.e., google scholar, PubMed, and IEEE from March 2011 to March 2021. Besides, numerous trial registries and grey literature resources were also explored. A total of 187 titles/abstracts were screened, and forty-eight full-text articles were assessed for eligibility. Conclusions: This research describes some of the possible influences of artificial intelligence technologies on pediatric physiotherapy practice, and the subsequent ways in which physiotherapy education will need to change to graduate professionals who are fit for practice in the 21st century health system for promoting safe and effective use of artificial intelligence and the delivery of Pediatric Physical Therapy care to people.}
}
@article{PROKOPENKO2019134,
title = {Self-referential basis of undecidable dynamics: From the Liar paradox and the halting problem to the edge of chaos},
journal = {Physics of Life Reviews},
volume = {31},
pages = {134-156},
year = {2019},
note = {Physics of Mind},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2018.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S1571064519300077},
author = {Mikhail Prokopenko and Michael Harré and Joseph Lizier and Fabio Boschetti and Pavlos Peppas and Stuart Kauffman},
keywords = {Self-reference, Diagonalization, Undecidability, Incomputability, Program-data duality, Complexity},
abstract = {In this paper we explore several fundamental relations between formal systems, algorithms, and dynamical systems, focussing on the roles of undecidability, universality, diagonalization, and self-reference in each of these computational frameworks. Some of these interconnections are well-known, while some are clarified in this study as a result of a fine-grained comparison between recursive formal systems, Turing machines, and Cellular Automata (CAs). In particular, we elaborate on the diagonalization argument applied to distributed computation carried out by CAs, illustrating the key elements of Gödel's proof for CAs. The comparative analysis emphasizes three factors which underlie the capacity to generate undecidable dynamics within the examined computational frameworks: (i) the program-data duality; (ii) the potential to access an infinite computational medium; and (iii) the ability to implement negation. The considered adaptations of Gödel's proof distinguish between computational universality and undecidability, and show how the diagonalization argument exploits, on several levels, the self-referential basis of undecidability.}
}
@article{WANG2023109577,
title = {Study of the flow field of a new fishtail-type stirring impeller in a stirred tank},
journal = {Chemical Engineering and Processing - Process Intensification},
volume = {194},
pages = {109577},
year = {2023},
issn = {0255-2701},
doi = {https://doi.org/10.1016/j.cep.2023.109577},
url = {https://www.sciencedirect.com/science/article/pii/S0255270123003148},
author = {Zhaohui Wang and Deli Li and Quanjie Gao and Qianwen Yang and Xiao Xiong and Changzhi Jiang and Feng Zhang},
keywords = {Computational fluid dynamics, Power consumption, Particle image velocimetry, Impeller design, Blade inclination},
abstract = {Abstracts
In this study, a new fishtail impeller was introduced to improve the mixing of fluids in the stirred tank. The validity of the numerical model was first demonstrated by PIV experiments. Secondly, the CFD technique was used to analyze and predict the flow field characteristics in the stirred tank. Also, the effect of blade inclination on the mixing effect is analyzed. Finally, a comparative Analysis with the whale tail impeller is carried out to demonstrate the superiority of this research work. The results show that The results of the study showed that the power number of the fishtail impeller was reduced by 16.4 % compared to the RT impeller. The pumping efficiency of the fishtail impeller was increased by 25.52 %. The results also show that increasing the blade inclination increases the turbulent kinetic energy in the stirred tank. Comparative analysis with the whale-tail impeller reveals that the power number of the fish-tail impeller is reduced by 17.2 % and the pumping efficiency is increased by 19.97 %.}
}
@article{ZHANG2024127373,
title = {Adaptive emotion neural network based on ITCSO and grey correlation contribution},
journal = {Neurocomputing},
volume = {577},
pages = {127373},
year = {2024},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.127373},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224001449},
author = {Wei Zhang and Wanfeng Wei},
keywords = {Emotion neural network, Hormone regulation, Competitive swarm optimization, Grey correlation contribution, Convergence analysis},
abstract = {In order to further improve the performance of emotion neural network (ENN), a novel adaptive hormone regulation emotion neural network (HRENN) is proposed, which is based on the improved triple competitive swarm optimization (ITCSO) algorithm and grey correlation contribution. Firstly, the structure of HRENN is designed that is inspired by the biological mechanism of hormone regulation. The fast response characteristic of emotion processing and the feedback effect of hormone regulation can effectively improve the learning ability of HRENN. Secondly, the ITCSO algorithm is proposed for optimizing the parameters of HRENN. In order to strike a well balance between exploration and exploitation, triple competition mechanism is adopted. Two-thirds of particles participate in the optimization and different learning strategies for different particles are provided. These operations can greatly improve the optimization efficiency and the convergence accuracy. Moreover, grey correlation contribution is used to add or delete the dimension of particles. It means that the structure and parameters of HRENN can be adjusted simultaneously and the compact structure can be obtained. Finally, the stability and the convergence of ITCSO are proved using the Banach space and the principle of compression mapping. Experiment results show that the proposed ITCSO-HRENN has good self-organization ability, compact network structure, high convergence accuracy and superior computation efficiency compared with other methods.}
}
@article{LEAHY2019102422,
title = {The digital frontier: Envisioning future technologies impact on the classroom},
journal = {Futures},
volume = {113},
pages = {102422},
year = {2019},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2019.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S0016328718304166},
author = {Sean M. Leahy and Charlotte Holland and Francis Ward},
keywords = {Artificial intelligence, Augmented reality, Smart materials, Educational technology, Education futures, Education},
abstract = {Global advances in technology and information are purportedly propelling transformations and disruptions across many sectors, including education. However, historical reviews of technology integration in education mainly reveal weak or ineffectual impacts on learning, and only minor reforms to date within the education system. This study adopted a futures studies methodological approach to explore how K-12 educational spaces and experiences might be shaped by emerging and emergent technologies. In this regard, a series of vignettes are presented which critically examine the potential of augmented reality technologies, artificial intelligence, and smart materials technologies to transform future learning experiences and learning environments across K-12 education contexts, while also challenging assumptions about, and considering influences on, these futures. The focus of the study was not to predict a single or desired future for education, but rather to critically consider a range of possible education futures informed by the articulation of these three vignettes. The paper concludes with discourse on an emergent pedagogic approach that has the potential to prepare teachers and learners to interact and flourish within radically re-configured learning spaces that lean on the aforementioned technologies to support transitions within and beyond the school and its connected communities.}
}
@article{GIOVANNINI20143280,
title = {Approach for the rationalisation of product lines variety},
journal = {IFAC Proceedings Volumes},
volume = {47},
number = {3},
pages = {3280-3291},
year = {2014},
note = {19th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20140824-6-ZA-1003.02226},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016421130},
author = {A. Giovannini and A. Aubry and H. Panetto and H. El Haouzi and L. Pierrel and M. Dassisti},
keywords = {Mass customization, Product variety, Knowledge representation, Knowledge-based system},
abstract = {The product variety management is a key process to deal with the flexibility requested by the mass customisation. In this paper we show that current variety-modelling methods miss a customer representation: without a proper assessment of the customers is not possible to define the product variety that has to be developed to meet the requirements of a customer segment. Here we present an innovative approach to rationalise the product variety, i.e. to link each product variant to the customer profile who needs it. The aim is to optimise the product variety avoiding excesses (variants not related to a customer), lacks (customers not related to a variant) or redundancies (two or more variants proposed to a customer). An overview of customer modelling approaches in the classic product design (non-customisable) is presented. The innovative approach is here developed using system-thinking concepts. A knowledge-based system that uses this approach is designed. Finally the approach is explained using a real industrial case of a quasi-real coil design process.}
}
@article{NI2011100,
title = {Influence of curriculum reform: An analysis of student mathematics achievement in Mainland China},
journal = {International Journal of Educational Research},
volume = {50},
number = {2},
pages = {100-116},
year = {2011},
note = {Curricular effect on the teaching and learning of mathematics: Findings from two longitudinal studies in China and the United States},
issn = {0883-0355},
doi = {https://doi.org/10.1016/j.ijer.2011.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S0883035511000413},
author = {Yujing Ni and Qiong Li and Xiaoqing Li and Zhong-Hua Zhang},
keywords = {Curriculum reform, Primary mathematics, Curriculum evaluation, Student mathematics achievement, Cognitive, Affective},
abstract = {This study investigated curriculum influences on student mathematics achievement by following two groups of students from fifth to sixth grade that were taught either the reformed curriculum or the conventional curriculum. Analyses with three-level modeling were conducted to examine learning outcomes of the students who were assessed three times over a period of 18 months. Achievement was measured with regard to computation, routine problem solving, and complex problem solving. Affective aspects included self-reported interest in learning mathematics, classroom participation, views of the nature of mathematics, and views of learning mathematics. The results showed overall improved performance among all the students over the time on computation, routine problem solving, and complex problem solving but not on the affective measures. There were differentiated patterns of performance between the groups. On the initial assessment, the reform group performed better than the non-reform group on calculation, complex problem solving, and indicated higher interest in learning mathematics. The two groups did not differ on the other achievement and affective measures at the first time of assessment. There was no significant difference in growth rate between the groups on the cognitive and affective measures except that the non-reform group progressed at a faster pace on calculation. Therefore, the non-reform group outperformed the reform group on computation at the third (last) assessment. These results are discussed with respect to the possible influence of the curriculum on student learning.}
}
@article{SURYANARAYANA2024100495,
title = {Artificial Intelligence Enhanced Digital Learning for the Sustainability of Education Management System},
journal = {The Journal of High Technology Management Research},
volume = {35},
number = {2},
pages = {100495},
year = {2024},
issn = {1047-8310},
doi = {https://doi.org/10.1016/j.hitech.2024.100495},
url = {https://www.sciencedirect.com/science/article/pii/S104783102400004X},
author = {K.S. Suryanarayana and V.S. Prasad Kandi and G. Pavani and Akuthota Sankar Rao and Sandeep Rout and T. {Siva Rama Krishna}},
keywords = {Artificial intelligence, Digital education, Sustainability, Role of AI in education, Educational management},
abstract = {Maintenance schedules are scheduled ahead of time and automatically based on the continuous monitoring of the equipment by statistical methods, thanks to artificial intelligence-enabled digital transformation and the best fit model based on Machine Management Index in a pedagogical system. One of the most important aspects of universities is the widespread use of machine learning methods to evaluate students' progress. Machine learning approaches are designed to speed up the learning process without sacrificing accuracy. The dynamics of teaching and learning have shifted since the introduction of modern technological tools. The educational system as a whole has changed and developed over time. These days, people can get an education outside of the classroom as well, thanks to the proliferation of online courses and resources. Everyone's professional life begins with their education. By analyzing past data, artificial intelligence methods can resolve existing problems. When applied properly, artificial intelligence can be a highly efficient method for solving problems with a predictable and repeatable solution space. The learner's personality can be predicted based on a number of factors using machine learning approaches. This article examines how AI may improve digital learning in education management systems to sustain the education ecosystem. AI in education improves student results, learning experiences, and administrative processes. This study discusses AI applications in education management systems and associated problems and opportunities. We also explore ethical issues and the roadmap for using AI to improve education. Educational institutions can provide individualized curriculum for students based on their unique personalities and areas of interest. Institutions of higher learning can benefit greatly from this instrument for personality prediction by recommending a course of study that will better prepare students to enter the field of their choice and achieve professional success.}
}
@article{NAKHAEI2022116422,
title = {A novel framework for technical performance evaluation of water distribution networks based on the water-energy nexus concept},
journal = {Energy Conversion and Management},
volume = {273},
pages = {116422},
year = {2022},
issn = {0196-8904},
doi = {https://doi.org/10.1016/j.enconman.2022.116422},
url = {https://www.sciencedirect.com/science/article/pii/S0196890422012006},
author = {Mahdi Nakhaei and Mehran Akrami and Mohammad Gheibi and Pedro {Daniel Urbina Coronado} and Mostafa Hajiaghaei-Keshteli and Jürgen Mahlknecht},
keywords = {Water distribution network, Water energy nexus, EPANET, Design of experiments, Machine learning},
abstract = {Today energy recovery using Micro-Hydropowers (MHPs) in Water Distribution Networks (WDN) is a well-known approach for recycling the wasted energy in infrastructures as a sample of circular economy. Likewise, in this study for the first time a framework for evaluation of WDN for energy harvesting have been designed with the application of statistical optimization, simulation, and artificial intelligence concepts. In this study, after modelling a WDN in Mashhad, Iran, with Environmental Protection Agency Network Evaluation Tool (EPANET) software, the potential of energy recovery using MHP technology was optimized with the application of Design of Experiment (DOE) methods, including Taguchi and Response Surface Methodology (RSM) and then the model prediction ability was improved by Artificial Neural Network (ANN) technique. Results of this investigation revealed that the combination of Taguchi and RSM methods could successfully optimize the energy recovery potential with consideration of improving the hydraulic parameters of WDN. With the application of RSM and Taguchi, high potential positions for MHP placement are detected and analyzed based on a high-performance operational decision-making methodology. According to Artificial Intelligence (AI) computations, energy harvesting and hydraulic responses can be estimated with more than a 99 % correlation coefficient. Also, it shows that the soft-operator can be executed to control the features of MHPs in WDNs. The outputs of this research demonstrated that MHP harvested energy is more than 400KW for the run time of this study with consideration of hydraulic parameters.}
}
@incollection{SANCHEZSILVA2013437,
title = {17 - Risk assessment and management of civil infrastructure networks: a systems approach},
editor = {S. Tesfamariam and K. Goda},
booktitle = {Handbook of Seismic Risk Analysis and Management of Civil Infrastructure Systems},
publisher = {Woodhead Publishing},
pages = {437-464},
year = {2013},
series = {Woodhead Publishing Series in Civil and Structural Engineering},
isbn = {978-0-85709-268-7},
doi = {https://doi.org/10.1533/9780857098986.4.437},
url = {https://www.sciencedirect.com/science/article/pii/B9780857092687500177},
author = {M. Sánchez-Silva and C. Gómez},
keywords = {infrastructure, transportation networks, systems thinking, risk assessment, decision-making, optimization},
abstract = {Abstract:
Infrastructure networks are complex systems due to the large number of components that interact in a nonlinear way. Detecting and understanding the properties of such systems is of paramount importance to make effective decisions about risk management and sustainable development. This chapter presents a systems approach to risk management and risk-based decision making in infrastructure networks. In the proposed approach, the internal structure of a network is detected via pattern recognition (clustering), and structured information is used to enhance conceptual and computational analyses of reliability, vulnerability, damage propagation, and resource allocation. The approach can be applied to network analysis of complex infrastructure systems subjected to extreme events, such as earthquakes.}
}
@article{FARHADINIA2016135,
title = {Multiple criteria decision-making methods with completely unknown weights in hesitant fuzzy linguistic term setting},
journal = {Knowledge-Based Systems},
volume = {93},
pages = {135-144},
year = {2016},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2015.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0950705115004359},
author = {B. Farhadinia},
keywords = {Multi-criteria decision making, Hesitant fuzzy linguistic term set, Entropy measure, Similarity measure, Distance measure},
abstract = {As for multi-criteria decision making problems with hesitant fuzzy linguistic information, it is common that the criteria involved in the problems are associated with the predetermined weights, whereas the information about criteria weights is generally incomplete. This is because of the complexity and the inherent subjective nature of human thinking. In this circumstance, the weights of criteria can be derived by means of information entropy from the evaluation values of criteria for alternatives. To the best of our knowledge, up to now, there is no work having introduced the concept of entropy measure for hesitant fuzzy linguistic term sets (HFLTSs). Hence, in this paper, we are going to fill in this gap by developing information about how entropy measures of HFLTSs can be designed.}
}