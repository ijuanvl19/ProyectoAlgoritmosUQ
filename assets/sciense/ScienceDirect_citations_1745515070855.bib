@article{MOHAMED2022152376,
title = {The Search for Efficient and Stable Metal-Organic Frameworks for Photocatalysis: Atmospheric Fixation of Nitrogen},
journal = {Applied Surface Science},
volume = {583},
pages = {152376},
year = {2022},
issn = {0169-4332},
doi = {https://doi.org/10.1016/j.apsusc.2021.152376},
url = {https://www.sciencedirect.com/science/article/pii/S0169433221033948},
author = {Amro M.O. Mohamed and Yusuf Bicer},
keywords = {Computational screening, Electronic properties, Green ammonia, Life cycle assessment, Molecular simulation, Photoactivity, Solar energy},
abstract = {Recent research targets the low-pressure synthesis of ammonia via a light-initiated catalytic process. Despite the importance of materials selection for photocatalysis, computational efforts to guide candidate materials’ nomination ahead of experiments are lacking. The purpose of this study is to employ computational screening, using density functional theory and molecular simulations, to select and evaluate metal–organic frameworks (MOFs) as nitrogen fixation photocatalysts and further deduce correlations for the prediction of MOFs’ electronic properties. First, MOFs with appropriate electronic and structural properties are identified. The top candidates have been examined from the perspective of adsorption, diffusion, and mechanical and chemical stability properties. Four MOFs, Fe2Cl2(BBTA), Fe2(mDOBDC), Zn2(mDOBDC), and Ni-BTP, have been selected based on their band edges, while only Fe2Cl2(BBTA) MOF exhibited a bandgap less than 3 eV. Fe2(mDOBDC) exhibited the highest shear modulus of approximately 31 GPa. In addition, a life cycle assessment of the four MOFs showed that Ni-BTP has the lowest environmental impact. A set of 48 MOFs’ combinations are proposed for heterojunction application to enhance charge carriers’ separation. Intriguingly, we demonstrated the predictability of MOF’s bandgap and edges from MOF’s organic linker bandgap and metal node type (oxidation state and corresponding electronic configuration) for MOF families.}
}
@article{FU2015159,
title = {An iterative method for discovering feasible management interventions and targets conjointly using uncertainty visualizations},
journal = {Environmental Modelling & Software},
volume = {71},
pages = {159-173},
year = {2015},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2015.05.017},
url = {https://www.sciencedirect.com/science/article/pii/S1364815215001656},
author = {Baihua Fu and Joseph H.A. Guillaume and Anthony J. Jakeman},
keywords = {Uncertainty, Environmental flows, Visualization, Wetlands, Decision making, Environmental management, Systems},
abstract = {This paper presents a generic method, referred to as Iterative Discovery, to guide deliberation with analysis where the aim is to plan refinements to management interventions with difficult-to-define objectives, often due to system uncertainties and diverse stakeholder positions. The method can be initiated by evaluating a scenario describing the current-best intervention. This provides the starting point for three evaluation cycles, focusing on model assumptions, alternative interventions and management targets. The outcome of this method is a list of management targets that can and cannot be achieved, the potential interventions that correspond to these targets, and the assumptions and uncertainties associated with these interventions. It was applied to a case study for environmental flow management in the Macquarie Marshes, Australia. We identified feasible management targets based on ecological outcomes in flood suitability across different locations, climate conditions and species, and the suitable environmental flow volumes that correspond to these targets.}
}
@article{FUJINO201760,
title = {Role of Spontaneous Brain Activity in Explicit and Implicit Aspects of Cognitive Flexibility under Socially Conflicting Situations: A Resting-state fMRI Study using Fractional Amplitude of Low-frequency Fluctuations},
journal = {Neuroscience},
volume = {367},
pages = {60-71},
year = {2017},
issn = {0306-4522},
doi = {https://doi.org/10.1016/j.neuroscience.2017.10.025},
url = {https://www.sciencedirect.com/science/article/pii/S0306452217307534},
author = {Junya Fujino and Shisei Tei and Kathryn F. Jankowski and Ryosaku Kawada and Toshiya Murai and Hidehiko Takahashi},
keywords = {decision-making, cerebellum, prefrontal cortex, rationalism, experientialism},
abstract = {We are constantly exposed to socially conflicting situations in everyday life, and cognitive flexibility is essential for adaptively coping with such difficulties. Flexible goal choice and pursuit are not exclusively conscious, and therefore cognitive flexibility involves both explicit and implicit forms of processing. However, it is unclear how individual differences in explicit and implicit aspects of flexibility are associated with neural activity in a resting state. Here, we measured intrinsic fractional amplitude of low-frequency fluctuations (fALFF) by resting-state functional magnetic resonance imaging (RS-fMRI) as an indicator of regional brain spontaneous activity, together with explicit and implicit aspects of cognitive flexibility using the Cognitive Flexibility Scale (CFS) and Implicit Association Test (IAT). Consistent with the dual processing theory, there was a strong association between explicit aspects of flexibility (CFS score) and “rationalism” thinking style and between implicit aspects (IAT effect) and “experientialism.” The level of explicit flexibility was also correlated with fALFF values in the left lateral prefrontal cortex, whereas the level of implicit flexibility was correlated with fALFF values in the right cerebellum. Furthermore, the fALFF values in both regions predicted individual preference for flexible decision-making strategy in a vignettes simulation task. These results add to our understanding of the neural mechanisms underlying flexible decision-making for solving social conflicts. More generally, our findings highlight the utility of RS-fMRI combined with both explicit and implicit psychometric measures for better understanding individual differences in social cognition.}
}
@article{BRIANTHOROMAN2020104859,
title = {An integrated approach to near miss analysis combining AcciMap and Network Analysis},
journal = {Safety Science},
volume = {130},
pages = {104859},
year = {2020},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2020.104859},
url = {https://www.sciencedirect.com/science/article/pii/S0925753520302563},
author = {M.S. {Brian Thoroman} and Paul Salmon},
keywords = {Near miss, Systems thinking, AcciMap, Network analysis, Incident analysis},
abstract = {Contemporary safety philosophies, such as Safety II, promote the importance of understanding effective work practices as well as those leading to adverse events. Despite this, little safety research has focussed on identifying and representing these protective practices in incident analysis. This study combined AcciMap and network analysis methods to identify and evaluate the system-wide protective practices occurring within a set of led outdoor activity domain near miss incidents. The analysis was based on subject matter expert surveys and interviews regarding near miss incidents. The findings revealed a large set of interrelated protective factors. These factors were about communication, policy and procedure, individual and organisational behavioural influences, and environmental conditions across the sociotechnical system. It is argued that network analysis should be combined with AcciMap in future incident analyses.}
}
@article{LEBERRE2022103122,
title = {Systemic vulnerability of coastal territories to erosion and marine flooding: A conceptual and methodological approach applied to Brittany (France)},
journal = {International Journal of Disaster Risk Reduction},
volume = {78},
pages = {103122},
year = {2022},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2022.103122},
url = {https://www.sciencedirect.com/science/article/pii/S2212420922003417},
author = {Iwan {Le Berre} and Catherine Meur-Ferec and Véronique Cuq and Elisabeth Guillou and Thibaud Lami and Nicolas {Le Dantec} and Pauline Letortu and Caroline Lummert and Manuelle Philippe and Mathias Rouan and Camille Noûs and Alain Hénaff},
keywords = {Coastal risks, Erosion, Flooding, Vulnerability, Web-GIS interface},
abstract = {The attractiveness of the coasts tends to increase their exposure to erosion and marine flooding risks. This exposure is exacerbated by the effects of climate change, in particular sea level rise. To contribute to strategic thinking on the vulnerability of coastal areas, it is essential to develop, share and collectively maintain relevant knowledge on risks. This article will present the thinking behind the setting up of a coastal risks observatory in Brittany, a region located in north-western France. It relies on a conceptual approach to systemic vulnerability based on four components: hazards, assets, management, and social representations. Hazards and assets underpin the notion of risk and tend to increase the vulnerability, management tends to mitigate it, and representations can play a part in increasing or decreasing it depending on the context. To understand and analyse this system of vulnerability, our approach is based on the generation of a set of 62 indicators combined into different types of indices. A web-GIS interface was developed to navigate through and map this system of vulnerability. The difficulties associated with this type of synthetic approach will be discussed, whether they are related to data availability, to the links between scientific research and operational territorial management requirements, or to an understanding of the dynamics of all of the vulnerability components and their interactions. Ultimately, the approach developed has been successful in mobilising scientific and operational stakeholders around the co-construction of a diagnosis of territories with regard to their vulnerability to coastal risks.}
}
@article{RICHARDS2013113,
title = {Bayesian belief modeling of climate change impacts for informing regional adaptation options},
journal = {Environmental Modelling & Software},
volume = {44},
pages = {113-121},
year = {2013},
note = {Thematic Issue on Innovative Approaches to Global Change Modelling},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2012.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S136481521200206X},
author = {R. Richards and M. Sanó and A. Roiko and R.W. Carter and M. Bussey and J. Matthews and T.F. Smith},
keywords = {Bayesian Belief Networks, Climate change, Adaptation, Group-model building, Stakeholder beliefs},
abstract = {A sequential approach to combining two established modeling techniques (systems thinking and Bayesian Belief Networks; BBNs) was developed and applied to climate change adaptation research within the South East Queensland Climate Adaptation Research Initiative (SEQ-CARI). Six participatory workshops involving 66 stakeholders based within SEQ produced six system conceptualizations and 22 alpha-level BBNs. The outcomes of the initial systems modeling exercise successfully allowed the selection of critical determinants of key response variables for in depth analysis within more homogeneous, sector-based groups of participants. Using two cases, this article focuses on the processes and methodological issues relating to the use of the BBN modeling technique when the data are based on expert opinion. The study expected to find both generic and specific determinants of adaptive capacity based on the perceptions of the stakeholders involved. While generic determinants were found (e.g. funding and awareness levels), sensitivity analysis identified the importance of pragmatic, context-based determinants, which also had methodological implications. The article raises questions about the most appropriate scale at which the methodology applied can be used to identify useful generic determinants of adaptive capacity when, at the scale used, the most useful determinants were sector-specific. Comparisons between individual BBN conditional probabilities identified diverging and converging beliefs, and that the sensitivity of response variables to direct descendant nodes was not always perceived consistently. It was often the accompanying narrative that provided important contextual information that explained observed differences, highlighting the benefits of using critical narrative with modeling tools.}
}
@article{LANG20151369,
title = {Beyond the Golden Era of public health: charting a path from sanitarianism to ecological public health},
journal = {Public Health},
volume = {129},
number = {10},
pages = {1369-1382},
year = {2015},
issn = {0033-3506},
doi = {https://doi.org/10.1016/j.puhe.2015.07.042},
url = {https://www.sciencedirect.com/science/article/pii/S0033350615003029},
author = {Tim Lang and Geof Rayner},
keywords = {Public health, Prosperity, Economic growth, Human progress, Societal & ecosystem costs, Public policy, 21st century challenges, Ecological public health, Institutional reform},
abstract = {The paper considers the long-term trajectory of public health and whether a ‘Golden Era’ in Public Health might be coming to an end. While successful elements of the 20th century policy approach need still to be applied in the developing world, two significant flaws are now apparent within its core thinking. It assumes that continuing economic growth will generate sufficient wealth to pay for the public health infrastructure and improvement needed in the 21st century when, in reality, externalised costs are spiralling. Secondly, there is evidence of growing mismatch between ecosystems and human progress. While 20th century development has undeniably improved public health, it has also undermined the capacity to maintain life on a sustainable basis and has generated other more negative health consequences. For these and other reasons a rethink about the role, purpose and direction of public health is needed. While health has to be at the heart of any viable notion of progress the dominant policy path offers new versions of the ‘health follows wealth’ position. The paper posits ecological public health as a radical project to reshape the conditions of existence. Both of these broad paths require different functions and purposes from their institutions, professions and politicians. The paper suggests that eco-systems pressures, including climate change, are already adding to pressure for a change of course.}
}
@article{TUTHILL2019259,
title = {Decision Support to Enhance Automated Laboratory Testing by Leveraging Analytical Capabilities},
journal = {Clinics in Laboratory Medicine},
volume = {39},
number = {2},
pages = {259-267},
year = {2019},
note = {Clinical Decision Support: Tools, Strategies, and Emerging Technologies},
issn = {0272-2712},
doi = {https://doi.org/10.1016/j.cll.2019.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0272271219300058},
author = {J. Mark Tuthill},
keywords = {Business analytics, Clinical decision support, Laboratory automation, Dashboards, Artificial intelligence, Learning health systems}
}
@article{STEVENS2022108887,
title = {Hyper-differential sensitivity analysis for inverse problems governed by ODEs with application to COVID-19 modeling},
journal = {Mathematical Biosciences},
volume = {351},
pages = {108887},
year = {2022},
issn = {0025-5564},
doi = {https://doi.org/10.1016/j.mbs.2022.108887},
url = {https://www.sciencedirect.com/science/article/pii/S0025556422000827},
author = {Mason Stevens and Isaac Sunseri and Alen Alexanderian},
keywords = {Inverse problems, Sensitivity analysis, Uncertainty quantification, Design of experiments, Computational epidemiology},
abstract = {We consider inverse problems governed by systems of ordinary differential equations (ODEs) that contain uncertain parameters in addition to the parameters being estimated. In such problems, which are common in applications, it is important to understand the sensitivity of the solution of the inverse problem to the uncertain model parameters. It is also of interest to understand the sensitivity of the inverse problem solution to different types of measurements or parameters describing the experimental setup. Hyper-differential sensitivity analysis (HDSA) is a sensitivity analysis approach that provides tools for such tasks. We extend existing HDSA methods by developing methods for quantifying the uncertainty in the estimated parameters. Specifically, we propose a linear approximation to the solution of the inverse problem that allows efficiently approximating the statistical properties of the estimated parameters. We also explore the use of this linear model for approximate global sensitivity analysis. As a driving application, we consider an inverse problem governed by a COVID–19 model. We present comprehensive computational studies that examine the sensitivity of this inverse problem to several uncertain model parameters and different types of measurement data. Our results also demonstrate the effectiveness of the linear approximation model for uncertainty quantification in inverse problems and for parameter screening.}
}
@article{DEMAZIERE2024113028,
title = {Enhancing higher education through hybrid and flipped learning: Experiences from the GRE@T-PIONEeR project},
journal = {Nuclear Engineering and Design},
volume = {421},
pages = {113028},
year = {2024},
issn = {0029-5493},
doi = {https://doi.org/10.1016/j.nucengdes.2024.113028},
url = {https://www.sciencedirect.com/science/article/pii/S0029549324001286},
author = {C. Demazière and C. Stöhr and Y. Zhang and O. Cabellos and S. Dulla and N. Garcia-Herranz and R. Miró and R. Macian and M. Szieberth and C. Lange and M. Hursin and S. Strola},
keywords = {Nuclear education and training, Computational reactor physics, Experimental reactor physics, Flipped classroom, Active learning, Hybrid teaching, Online learning},
abstract = {GRE@T-PIONEeR is a Horizon 2020 project coordinated by Chalmers University of Technology, running over the period 2020–2024. 18 university teachers from 8 different universities located in 6 different countries gathered forces to develop and offer advanced courses in computational and experimental nuclear reactor physics and safety. All courses are flipped hybrid courses, i.e., students work on online preparatory activities at their own pace before attending a set of interactive sessions organized on five consecutive days. Those sessions can be attended either onsite or remotely. During the academic year 2022/2023, 8 different courses were offered, and 185 students successfully completed the courses, with a success rate of 87.7% for the students taking at least one activity during the interactive sessions. Student behaviour and performance were monitored via the Learning Management System (LMS) used in all courses. This paper presents an analysis of various metrics from the LMS and demonstrates a high level of engagement of the students committed to the courses and a high success rate for those students. Whereas all students are equally engaged in the online preparatory work and perform equally well, significant differences exist during the interactive sessions between the students who opted for onsite participation and those who attended the sessions online, with the onsite students outperforming the online students.}
}
@article{GARDNER2013167,
title = {Dark matter studies entrain nuclear physics},
journal = {Progress in Particle and Nuclear Physics},
volume = {71},
pages = {167-184},
year = {2013},
note = {Fundamental Symmetries in the Era of the LHC},
issn = {0146-6410},
doi = {https://doi.org/10.1016/j.ppnp.2013.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0146641013000203},
author = {Susan Gardner and George M. Fuller},
keywords = {Dark matter, Nuclear astrophysics, Neutrinos},
abstract = {We review theoretically well-motivated dark-matter candidates, and pathways to their discovery, in the light of recent results from collider physics, astrophysics, and cosmology. Taken in aggregate, these encourage broader thinking in regards to possible dark-matter candidates — dark-matter need not be made of “WIMPs”, i.e., elementary particles with weak-scale masses and interactions. Facilities dedicated to nuclear physics are well-poised to investigate certain non-WIMP models. In parallel to this, developments in observational cosmology permit probes of the relativistic energy density at early epochs and thus provide new ways to constrain dark-matter models, provided nuclear physics inputs are sufficiently well-known. The emerging confluence of accelerator, astrophysical, and cosmological constraints permit searches for dark-matter candidates in a greater range of masses and interaction strengths than heretofore possible.}
}
@article{MOALLEMI2018205,
title = {A participatory exploratory modelling approach for long-term planning in energy transitions},
journal = {Energy Research & Social Science},
volume = {35},
pages = {205-216},
year = {2018},
note = {Energy and the Future},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2017.10.022},
url = {https://www.sciencedirect.com/science/article/pii/S221462961730350X},
author = {Enayat A. Moallemi and Shirin Malekpour},
keywords = {Exploratory modelling, Policy analysis, Sustainability transitions, Energy policy, Uncertainty},
abstract = {Energy transitions are complex transformation processes, which involve different actors and unfold in a deeply uncertain future. These features make the long-term planning of energy transitions a wicked problem. Traditional strategic planning approaches fail to address this wickedness as they have a predictive, deterministic, and reactive standpoint to future issues. Modelling approaches that are used within conventional contexts are perceived to be inadequate too. They often simplify the qualitative characteristics of transitions and cannot cope with deeply uncertain futures. More recently, new ways of qualitative participatory planning, as well as new approaches to quantitative modelling have emerged to enable policy analysis under deep uncertainty. We argue that qualitative participatory and quantitative modelling approaches can be complementary to each other in different ways. We operationalise their coupling in the form of a practical approach to be used for long-term planning of energy transitions. The suggested approach enables energy decision makers to test various policy interventions under numerous possibilities with a computational model and in a participatory process. We explain our approach with illustrative examples mostly from transitions in electricity sectors. However, our approach is applicable to different forms of energy transitions, and to the broader context of transition in any societal system, such as water and transportation.}
}
@article{HUBALOVSKY2019691,
title = {Assessment of the influence of adaptive E-learning on learning effectiveness of primary school pupils},
journal = {Computers in Human Behavior},
volume = {92},
pages = {691-705},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2018.05.033},
url = {https://www.sciencedirect.com/science/article/pii/S0747563218302590},
author = {S. Hubalovsky and M. Hubalovska and M. Musilek},
keywords = {Learning analytics, Cognitive computing, Adaptive e-learning, Primary education, Learning effectiveness, Bloom's taxonomy},
abstract = {The paper deals with assessment of the influence of adaptive e-learning as a part of learning analytics on learning effectiveness of primary school pupils. E-learning exercises containing implemented adaptive elements were created in accordance with the Bloom's Taxonomy. Within the pilot study the authors detected high percentage success rate during e-learning exercise completion. This leads to formulation of the question „Can any e-learning exercise of lower cognitive levels of Bloom's taxonomy be skipped without affecting the cognitive thinking for solution of the e-learning exercises on upper cognitive levels of Bloom's taxonomy?” To answer the question, the algorithm of adaptive e-learning was defined and hypotheses were established. The research was carried out as pedagogical experiment comparing the results of both experimental and control groups of pupils. The research hypotheses were confirmed by statistical analysis of the research data. The results confirm that adaptive features of e-learning can be implemented in the primary education. The research results confirm the fact that educational objectives can be achieved with some pupils more effectively. Consequently, the implementation of adaptive elements into e-learning at the primary school supports an individual approach when completing e-learning exercises according to the principle of cognitive computing.}
}
@article{JI2023119326,
title = {Fast Progressive Differentiable Architecture Search based on adaptive task granularity reorganization},
journal = {Information Sciences},
volume = {645},
pages = {119326},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.119326},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523009118},
author = {Junzhong Ji and Xingyu Wang},
keywords = {Neural Architecture Search, Differentiable Architecture Search, Shrink search space, Granular Computing, Cluster candidate},
abstract = {Shrinkage methods reduce the search space of a Differentiable Architecture Search (DARTS) by progressively discarding candidates, which accelerates the search speed. However, their shrinkage strategy suffers from the vulnerability of too fine task granularity. In other words, they drop only one of the least promising candidates per round of shrinkage, which is suboptimal in terms of performance and efficiency. In this study, we introduce the concept of Granular Computing (GrC) into the shrinkage method and present a Fast Progressive Differentiable Architecture Search (FP-DARTS) method. This method effectively reduces the computational complexity of each round of shrinkage, thereby improving the efficiency and performance of the algorithm. FP-DARTS can be divided into three stages: adaptive granularity division and selection, granular-channel performance evaluation, and progressive shrinkage. In the first stage, to reorganize the task granularity, we cluster the candidate operations into granular-channels and adaptively select the appropriate task granularity. We also propose a dynamic clustering strategy to avoid introducing additional computation. In the second stage, we train the architecture parameters to measure the potential of the granular-channels. In the third stage, to improve the stability of the shrinkage results, we introduce a channel annealing mechanism to smoothly discard unpromising granular-channels. We conducted systematic experiments on CIFAR-10 and ImageNet and achieved a test accuracy of 97.56% on CIFAR-10 with 0.04 GPU-days, and a test accuracy of 75.5% on ImageNet with 1.2 GPU-days. We also conducted experiments on the search space of NAS-Bench-201, and obtained test accuracies of 94.22, 73.07, and 46.23% for CIFAR-10, CIFAR-100 and ImageNet16-120, respectively. The above experimental results demonstrate that FP-DARTS achieves higher search speed and competitive performance compared to other state-of-the-art shrinkage methods and non-shrinkage methods.}
}
@article{ZHANG2024111498,
title = {Modular reverse design of acoustic metamaterial and sound barrier engineering applications: High ventilation and broadband sound insulation},
journal = {Thin-Walled Structures},
volume = {196},
pages = {111498},
year = {2024},
issn = {0263-8231},
doi = {https://doi.org/10.1016/j.tws.2023.111498},
url = {https://www.sciencedirect.com/science/article/pii/S0263823123009758},
author = {Xinhao Zhang and Qi Yu and Caiyou Zhao and Duojia Shi and Mingjing Geng and Junyuan Zheng and Tao Lu and Ping Wang},
keywords = {Novel acoustic metamaterials, Modular inverse design, High ventilation broadband acoustic insulation, PSO-DNN algorithm, Impedance tube test, OMCAM sound barrier},
abstract = {A multi-gradient cavity acoustic metamaterial (MCAM) structure and a modular reverse design method (MRDM) that can realize high ventilation and broadband acoustic isolation are proposed. The method controls the deep neural network model of acoustic metamaterials through a particle swarm algorithm, and the optimized multi-gradient cavity acoustic metamaterial structure (OMCAM) can be reverse-designed by inputting only the constraints and the objective function such as the amount of noise reduction. Compared with the finite element method, the computational efficiency can be improved by about 500 times to achieve an optimized design. The acoustic simulation results show that the average noise reduction of the structure is 23.5 dB in the range of 0∼4000 Hz, and a broadband sound attenuation with 38 dB noise reduction is formed in the target frequency band of 500Hz∼2000 Hz. The acoustic experimental results of the 3D-printed structure are in agreement with the simulation results. Compared with the two existing ventilated acoustic metamaterials, the average noise reduction of OMCAM under equal ventilation capacity is improved by 10.6 dB and 17.4 dB, respectively. The sound barrier based on the proposed OMCAM design is implemented on an elevated rail transit line, showing an improvement of 9.4 dB of average noise reduction compared with existing upright railroad sound barriers. The noise reduction mechanism of the OMCAM structure was finally revealed by the sound field distribution in different modes.}
}
@article{DEVGUN2022143,
title = {Pre-cath Laboratory Planning for Left Atrial Appendage Occlusion – Optional or Essential?},
journal = {Interventional Cardiology Clinics},
volume = {11},
number = {2},
pages = {143-152},
year = {2022},
note = {Left Atrial Appendage Occlusion},
issn = {2211-7458},
doi = {https://doi.org/10.1016/j.iccl.2021.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S2211745821001073},
author = {Jasneet Devgun and Tom {De Potter} and Davide Fabbricatore and Dee Dee Wang},
keywords = {Left atrial appendage occlusion, Left atrial appendage, Atrial fibrillation, Cardiac CT, 3D printing, Imaging, Structural heart disease}
}
@incollection{MCKAY2017,
title = {Behavior Therapy: Theoretical Bases☆},
booktitle = {Reference Module in Neuroscience and Biobehavioral Psychology},
publisher = {Elsevier},
year = {2017},
isbn = {978-0-12-809324-5},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.05242-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093245052421},
author = {D. McKay and W.W. Tryon},
keywords = {Acceptance and Commitment Therapy, Applied behavior analysis, Behavior therapy, Behavioral contextualism, Cognitive therapy, Computational neuropsychology, Conditioning, Connectionist neural networks, Dialectical behavior therapy, Mindfulness based cognitive therapy, Rational emotive behavior therapy},
abstract = {Behavior therapy has become a dominant approach to treating psychological conditions. The theories underlying this approach have been broadly defined by three conceptual frameworks, or ‘waves’. The first wave is based on classical and operant conditioning. The second wave is based on targeting cognitions, or dysfunctional beliefs, that are assumed to contribute to distressing emotional experiences and problematic behaviors. The third wave is based, broadly, on acceptance and mindfulness. Collectively, all three waves may be understood from a computational neuropsychology perspective that is based on connectionist neural network models.}
}
@article{LI2025115048,
title = {Linking firm performance with innovation culture: An algorithmic approach towards theory building},
journal = {Journal of Business Research},
volume = {187},
pages = {115048},
year = {2025},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2024.115048},
url = {https://www.sciencedirect.com/science/article/pii/S0148296324005526},
author = {Wanqing Li and Jiang Yu and Feng Chen},
keywords = {Data-driven analysis, Theory building, Firm performance management, Innovation culture},
abstract = {To address the growing demand for theory development in computational social science research, this study employs an algorithm-driven approach to formulate a comprehensive six-step theory generation process. By applying this original research method, a new theoretical model—the “Dynamic Resource-Culture Synergy Theory” is proposed which enhances the explanatory power regarding how firms maintain competitiveness in rapidly changing environments by emphasizing the pivotal role of culture in resource integration and innovation processes. Drawing on empirical data from 887 Chinese high-tech manufacturing firms, our analysis identifies key drivers of organizational performance, with a particular focus on the role of organizational culture, especially innovation culture, as a mediating force. Utilizing the GWO-SVM technique, we gain a nuanced understanding of how different cultural traits interact with innovation and leverage, uncovering how the initial enhancement of innovation culture positively impacts performance metrics such as ROA. The findings confirm that innovation, facilitated by organizational culture, significantly enhances performance outcomes. Furthermore, this study considers factors such as leverage and the proportion of technical personnel, investigating their moderating effects on the relationship between innovation culture and firm performance. This study not only deepens the understanding of how innovation and culture interact to influence firm performance but also provides significant theoretical and practical contributions to the study of the dynamics of high-tech manufacturing enterprises.}
}
@article{OBYRNE2022820,
title = {How critical is brain criticality?},
journal = {Trends in Neurosciences},
volume = {45},
number = {11},
pages = {820-837},
year = {2022},
issn = {0166-2236},
doi = {https://doi.org/10.1016/j.tins.2022.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0166223622001643},
author = {Jordan O’Byrne and Karim Jerbi},
keywords = {phase transition, edge of chaos, neuronal avalanche, cognition, scale-free, complexity},
abstract = {Criticality is the singular state of complex systems poised at the brink of a phase transition between order and randomness. Such systems display remarkable information-processing capabilities, evoking the compelling hypothesis that the brain may itself be critical. This foundational idea is now drawing renewed interest thanks to high-density data and converging cross-disciplinary knowledge. Together, these lines of inquiry have shed light on the intimate link between criticality, computation, and cognition. Here, we review these emerging trends in criticality neuroscience, highlighting new data pertaining to the edge of chaos and near-criticality, and making a case for the distance to criticality as a useful metric for probing cognitive states and mental illness. This unfolding progress in the field contributes to establishing criticality theory as a powerful mechanistic framework for studying emergent function and its efficiency in both biological and artificial neural networks.}
}
@article{FOELLMI2019103136,
title = {Loss aversion at the aggregate level across countries and its relation to economic fundamentals},
journal = {Journal of Macroeconomics},
volume = {61},
pages = {103136},
year = {2019},
issn = {0164-0704},
doi = {https://doi.org/10.1016/j.jmacro.2019.103136},
url = {https://www.sciencedirect.com/science/article/pii/S0164070419301028},
author = {Reto Foellmi and Adrian Jaeggi and Rina Rosenblatt-Wisch},
abstract = {Preferences are important when thinking about macroeconomic problems and questions. Differences in preferences might, for example, explain cross-country variations in economic fundamentals. In recent years, differences in preferences across countries and cultures have been studied more frequently, usually concentrating on micro evidence. However, it is an open question as to how differences in average preferences affect the aggregate economy. Coming from a macroeconomic perspective, we test whether preferences stated in Kahneman and Tversky’s prospect theory, namely, reference point dependence and loss aversion, prevail on the aggregate and whether the average degree of loss aversion differs across countries. We find evidence of loss aversion for a broad set of OECD countries, while the average loss aversion clearly differs across these countries. We find little evidence that these differences could be linked to micro evidence. Furthermore, we analyse whether the different degrees of loss aversion correlate with economic fundamentals such as the level of GDP and consumption per capita. We find that indeed loss aversion is negatively correlated with GDP and consumption per capita and positively correlated with consumption smoothing.}
}
@incollection{SCHEINER2005831,
title = {Chapter 29 - The CH···O hydrogen bond: A historical account},
editor = {Clifford E. Dykstra and Gernot Frenking and Kwang S. Kim and Gustavo E. Scuseria},
booktitle = {Theory and Applications of Computational Chemistry},
publisher = {Elsevier},
address = {Amsterdam},
pages = {831-857},
year = {2005},
isbn = {978-0-444-51719-7},
doi = {https://doi.org/10.1016/B978-044451719-7/50072-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044451719750072X},
author = {Steve Scheiner},
abstract = {Publisher Summary
This chapter presents a history of the problem, how the CH· · ·O H-bond went from a nonissue to one of recognized importance. The history also includes a discussion on the contributions made by computational chemistry along the way, at each stage. The chapter discusses a unique and surprising property of the CH· · ·O bond that led some to initially deny its characterization as a H-bond at all, and others to go so far as to dub it an “anti-H-bond”. This property has been analyzed and placed into proper perspective through the power of modern quantum chemistry. The early definition of a H-bond paired a proton donor group, typically OH or NH, with an acceptor that contained a nonbonded electron pair. It was the OH· · ·O, OH· · ·N, and NH· · ·O sorts of H-bonds that dominated most thinking about H-bonds. Nonetheless, the weaker sorts of H-bonds were not completely ignored: a smaller number of studies considered the H-bonding abilities of F, Cl, S, and so on.}
}
@article{GUO2023100246,
title = {Function approximation reinforcement learning of energy management with the fuzzy REINFORCE for fuel cell hybrid electric vehicles},
journal = {Energy and AI},
volume = {13},
pages = {100246},
year = {2023},
issn = {2666-5468},
doi = {https://doi.org/10.1016/j.egyai.2023.100246},
url = {https://www.sciencedirect.com/science/article/pii/S2666546823000186},
author = {Liang Guo and Zhongliang Li and Rachid Outbib and Fei Gao},
keywords = {Energy management strategy, Fuel cell hybrid electric vehicle, Reinforcement learning, Fuzzy inference system, Fuzzy policy gradient, Hardware-in-loop},
abstract = {In the paper, a novel self-learning energy management strategy (EMS) is proposed for fuel cell hybrid electric vehicles (FCHEV) to achieve the hydrogen saving and maintain the battery operation. In the EMS, it is proposed to approximate the EMS policy function with fuzzy inference system (FIS) and learn the policy parameters through policy gradient reinforcement learning (PGRL). Thus, a so-called Fuzzy REINFORCE algorithm is first proposed and studied for EMS problem in the paper. Fuzzy REINFORCE is a model-free method that the EMS agent can learn itself through interactions with environment, which makes it independent of model accuracy, prior knowledge, and expert experience. Meanwhile, to stabilize the training process, a fuzzy baseline function is adopted to approximate the value function based on FIS without affecting the policy gradient direction. Moreover, the drawbacks of traditional reinforcement learning such as high computation burden, long convergence time, can also be overcome. The effectiveness of the proposed methods were verified by Hardware-in-Loop experiments. The adaptability of the proposed method to the changes of driving conditions and system states is also verified.}
}
@article{PALITTA2023112068,
title = {Stein-based preconditioners for weak-constraint 4D-var},
journal = {Journal of Computational Physics},
volume = {482},
pages = {112068},
year = {2023},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2023.112068},
url = {https://www.sciencedirect.com/science/article/pii/S0021999123001638},
author = {Davide Palitta and Jemima M. Tabeart},
keywords = {4D-var, Data assimilation, Preconditioning, Stein equations},
abstract = {Algorithms for data assimilation try to predict the most likely state of a dynamical system by combining information from observations and prior models. Variational approaches, such as the weak-constraint four-dimensional variational data assimilation formulation considered in this paper, can ultimately be interpreted as a minimization problem. One of the main challenges of such a formulation is the solution of large linear systems of equations which arise within the inner linear step of the adopted nonlinear solver. Depending on the selected approach, these linear algebraic problems amount to either a saddle point linear system or a symmetric positive definite (SPD) one. Both formulations can be solved by means of a Krylov method, like GMRES or CG, that needs to be preconditioned to ensure fast convergence in terms of the number of iterations. In this paper we illustrate novel, efficient preconditioning operators which involve the solution of certain Stein matrix equations. In addition to achieving better computational performance, the latter machinery allows us to derive tighter bounds for the eigenvalue distribution of the preconditioned linear system for certain problem settings. A panel of diverse numerical results displays the effectiveness of the proposed methodology compared to current state-of-the-art approaches.}
}
@article{QIN2025100117,
title = {LingoTrip: Spatiotemporal context prompt driven large language model for individual trip prediction},
journal = {Journal of Public Transportation},
volume = {27},
pages = {100117},
year = {2025},
issn = {1077-291X},
doi = {https://doi.org/10.1016/j.jpubtr.2025.100117},
url = {https://www.sciencedirect.com/science/article/pii/S1077291X25000025},
author = {Zhenlin Qin and Pengfei Zhang and Leizhen Wang and Zhenliang Ma},
keywords = {Large Language Models, Individual Mobility, Spatiotemporal Context Prompt, Personalied Infromation, Public Transport},
abstract = {Large language models (LLMs) showed superior performance in many language-related tasks. It is promising to model the individual mobility prediction problem as a language model and use pretrained LLMs to predict the individual next trip information (e.g., time and location) for personalized travel recommendations. Theoretically, it is expected to overcome the common limitations of data-driven prediction models in zero/few shot learning, generalization, and interpretability. The paper proposes a LingoTrip model for predicting individual next trip location by designing the spatiotemporal context prompts for LLMs. The designed prompting strategies enable LLMs to capture implicit land use information (trip purposes), spatiotemporal mobility patterns (choice preferences), and geographical dependencies of the stations used (choice variability). The lingoTrip is validated using Hong Kong Mass Transit Railway trip data by comparing it with the state-of-the-art data-driven mobility prediction models under different training data sizes. Sensitivity analyses are performed for model hyperparameters and their tuning methods to adapt for other datasets. The results show that LingoTrip outperforms data-driven models in terms of prediction accuracy, transferability (between individuals), zero/few shot learning (limited training sample size) and interpretability of predictions. The LingoTrip model can facilitate the effective provision of personalized information for system crowding and disruption contexts (i.e., proactively providing information to targeted individuals).}
}
@article{CHEN2006103,
title = {Toward development of activity coefficient models for process and product design of complex chemical systems},
journal = {Fluid Phase Equilibria},
volume = {241},
number = {1},
pages = {103-112},
year = {2006},
note = {A Festschrift in Honor of John M. Prausnitz},
issn = {0378-3812},
doi = {https://doi.org/10.1016/j.fluid.2006.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0378381206000331},
author = {Chau-Chyun Chen},
keywords = {Complex chemical systems, Electrolytes, Nonelectrolytes, Polymers, Pharmaceuticals, Activity coefficient models, Excess Gibbs energy},
abstract = {Molecular thermodynamics, an engineering science for quantitative representation of thermophysical properties and phase behavior for mixtures, has served as a core scientific foundation for process modeling and process and product design in the industries. This paper presents a personal adventure through molecular thermodynamics that follows the footprints of John Prausnitz and leads toward the development of activity coefficient models for process modeling and process and product design of complex chemical systems. In this scientific expedition, passion and endurance, industrial applications, molecular insights, and out-of-the-box thinking all play key roles. We venerate past accomplishments that serve industrial needs and cherish new opportunities that await future exploration by adventurous souls.}
}
@article{CHAUDHARI201687,
title = {Traffic and mobility aware resource prediction using cognitive agent in mobile ad hoc networks},
journal = {Journal of Network and Computer Applications},
volume = {72},
pages = {87-103},
year = {2016},
issn = {1084-8045},
doi = {https://doi.org/10.1016/j.jnca.2016.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S108480451630131X},
author = {Shilpa Shashikant Chaudhari and Rajashekhar C. Biradar},
keywords = {Mobile ad hoc network, Wavelet neural network, Resource prediction, Cognitive agent, Belief desire intention architecture},
abstract = {Mobile Ad hoc NETwork (MANET) characteristics such as limited resources, shared channel, unpredictable mobility, improper load balancing, and variation in signal strength affect the routing of real-time multimedia data that requires Quality of Service (QoS) provisioning. Accurate prediction of the resource availability assists efficient resource allocation before the routing of such data. Most of the published work on resource prediction in MANET focuses on either bandwidth or energy without considering mobility effects. Adoption of intelligent software agent such as Cognitive Agent (CA) for the accurate resource prediction has a significant potential to solve the challenges of resource prediction in MANET. The intelligence provided in CA is similar to the logical thinking like a human for decision-making. The predominant CA architecture is the Belief-Desire-Intention (BDI) model, which performs the various tasks on behalf of the human user as an assistant. In this paper, we propose a CA-based Resource Prediction mechanism considering Mobility (CA-RPM) that predicts the resources using agents through the resource prediction agency consisting of one static agent, one cognitive agent and two mobile agents. Agents predict the traffic, mobility, buffer space, energy, and bandwidth effectively that is necessary for efficient resource allocation to support real-time and multimedia communications. The mobile agents collect and distribute network traffic statistics over MANET whereas a static agent collects the local statistics. CA creates static/mobile agent during the process of resource prediction. Initially, the designed time-series Wavelet Neural Networks (WNNs) predict traffic and mobility. Buffer space, energy, and bandwidth prediction use the predicted mobility and traffic. Simulation results show that the predicted resources closely match with the real values at the cost of little overheads due to the usage of agents. Simulation analysis of predicted traffic and mobility also shows the improvement compared to recurrent WNN in terms of mean square error, covariance, memory overhead, agent overhead and computation overhead. We plan to use these predicted resources for its efficient utilization in QoS routing is our future work.}
}
@article{PICCIONI2024114222,
title = {From layer to building: Multiscale modeling of thermo-optical properties in 3D-printed facades},
journal = {Energy and Buildings},
volume = {314},
pages = {114222},
year = {2024},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.114222},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824003384},
author = {Valeria Piccioni and Lars O. Grobe and Illias Hischier and Arno Schlueter},
keywords = {3D-printed facades, Thermo-optical properties, Multi-scale modeling, Experimental validation},
abstract = {The challenge of building sector decarbonization has driven an integral rethinking of the way we design and build facades. Recently, large scale 3D-printing has emerged as an alternative manufacturing technique for novel facade components aiming at high operational efficiency and low environmental impact. Focusing on translucent polymer 3DPFs, this study tackles the challenges of modeling thermal and optical effects in geometrically complex components where interactions across multiple domains and scales occur. In particular, we introduce a novel method for modeling the irregular thermo-optical properties of 3DPFs, capable of capturing relevant effects often out of the scope of traditional modeling approaches. Our model accounts for geometry-dependent physical effects ranging from millimeter-scale fabrication details that impact optical behavior to centimeter-scale geometric features influencing heat and radiation transfer, extending up to the meter-scale implications for the building application. By employing computational techniques such as ray-tracing, computational fluid dynamics, and finite element analysis, we establish a model that offers detailed thermal and optical analysis to support performance-driven design iterations. Finally, demonstrating this approach in an office building context, we show that 3DPFs can match the performance of double glazing with dynamic shading, providing effective solar and thermal management over the year. This is achieved in a single, mono-material component with no active control, suggesting 3DPFs are a promising direction for low-environmental impact facade design.}
}
@article{MOLINS2022113953,
title = {Stressed individuals exhibit pessimistic bursting beliefs and a lower risk preference in the balloon analogue risk task},
journal = {Physiology & Behavior},
volume = {256},
pages = {113953},
year = {2022},
issn = {0031-9384},
doi = {https://doi.org/10.1016/j.physbeh.2022.113953},
url = {https://www.sciencedirect.com/science/article/pii/S0031938422002591},
author = {Francisco Molins and Mónica Paz and Liza Rozman and Nour {Ben Hassen} and Miguel Ángel Serrano},
keywords = {Decision-making, Balloon analogue risk task, Computational modelling, Stress, Trier social stress test},
abstract = {Stress alters decision-making by usually promoting risk-taking and reward-seeking, which could be advantageous in a context where risk is rewarded, such as the Balloon Analogue Risk Task (BART). However, previous studies addressing this issue showed inconsistencies which could emerge from assessing decision-making as a single dimension. Our aim is to study through computational modelling how stress influences cognitive subprocesses of the decision-making during the BART. For this purpose, 94 healthy participants were submitted to BART, but only half were exposed to the virtual Trier Social Stress Test (TSST-VR). The Experimental-Weight Mean-Variance (EWMV) model was used to gain insight into the subprocesses involved in risk-taking during BART. Rather than reward-seeking, our results showed a pessimistic prior belief about the balloons bursting likelihood, and a lower risk preference in the stressed participants. This cautious attitude could be attributable to an alertness state promoted by stress. Yet, since risk is rewarded in BART, it could also evidence a maladaptive decision-making derived from learning difficulties and altered feedback-processing under stress.}
}
@article{VASQUEZ2019306,
title = {Curriculum change for graduate-level control engineering education at the Universidad Pontificia Bolivariana},
journal = {IFAC-PapersOnLine},
volume = {52},
number = {9},
pages = {306-311},
year = {2019},
note = {12th IFAC Symposium on Advances in Control Education ACE 2019},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2019.08.225},
url = {https://www.sciencedirect.com/science/article/pii/S2405896319305622},
author = {Rafael E. Vásquez and Fabio Castrillón and Santiago Rúa and Norha L. Posada and Carlos A. Zuluaga},
keywords = {Control engineering, control education, active learning, curriculum change},
abstract = {This paper addresses the graduate-level control engineering curriculum change performed at the Universidad Pontificia Bolivariana (UPB), Medellin, Colombia. New proposed methodologies include active learning activities using a new multipurpose experimental test bed that was developed with industrial components. The renovated graduate-level control engineering related courses include: Continuous Processes, Discrete Processes, Fuzzy Logic, Neural Networks and Genetic Algorithms, Linear Control, Nonlinear Control, and Optimal Estimation. The new experimental station was developed for teaching, research, and industrial training activities for the School of Engineering at the UPB. In this work, we report the use of the station in an Optimal Estimation course to replace a traditional homework/exams evaluation approach with an applied work that required independent study, the implementation of different observers in a real lab-scale industrial plant, and a paper-style written report. Increasing independent study activities resulted in academic discussions that are valuable for the learning process of the student. The use of the experimental station and the real comparison of estimation algorithms, implemented by using industrial controllers and high-level programming environments, provided the student skills that cannot be acquired by using only simulations in which real implementation restrictions/challenges do not appear. This work represents one of the first approaches for the implementation of the new curriculum model at the UPB for graduate education. The methodology used in the Optimal Estimation class promoted independent learning, critical thinking and writing skills through significant learning activities.}
}
@article{KIRCHER2018515,
title = {Formal thought disorders: from phenomenology to neurobiology},
journal = {The Lancet Psychiatry},
volume = {5},
number = {6},
pages = {515-526},
year = {2018},
issn = {2215-0366},
doi = {https://doi.org/10.1016/S2215-0366(18)30059-2},
url = {https://www.sciencedirect.com/science/article/pii/S2215036618300592},
author = {Tilo Kircher and Henrike Bröhl and Felicitas Meier and Jennifer Engelen},
abstract = {Summary
Formal thought disorder (FTD) is present in most psychiatric disorders and in some healthy individuals. In this Review, we present a comprehensive, integrative, and multilevel account of what is known about FTD, covering genetic, cellular, and neurotransmitter effects, environmental influences, experimental psychology and neuropsychology, brain imaging, phenomenology, linguistics, and treatment. FTD is a dimensional, phenomenologically defined construct, which can be clinically subdivided into positive versus negative and objective versus subjective symptom clusters. Because FTDs have been traditionally linked to schizophrenia, studies in other diagnoses are scarce. Aetiologically, FTD is the only symptom under genetic influence in schizophrenia as shown in linkage studies, but familial communication patterns (allusive thinking) have also been associated with the condition. Positive FTDs are related to synaptic rarefication in the glutamate system of the superior and middle lateral temporal cortices. Cortical volume of the left superior temporal gyrus is decreased in patients with schizophrenia who have positive FTD in structural MRI studies and shows reversed hemispheric (right more than left) activation in functional MRI experiments during speech production. Semantic network dysfunction in positive FTD has been demonstrated in experiments of indirect semantic hyperpriming (reaction time). In acute positive FTD, antipsychotics are effective, but a subgroup of patients have treatment-resistant, chronic, positive or negative FTD. Specific psychotherapy as treatment for FTD has not yet been developed. With this solid data on the pathogenesis of FTD, we can now implement clinical studies to treat this condition.}
}
@article{SOLARI2008106,
title = {Confabulation Theory},
journal = {Physics of Life Reviews},
volume = {5},
number = {2},
pages = {106-120},
year = {2008},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2008.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S1571064508000122},
author = {Soren Solari and Andrew Smith and Rupert Minnett and Robert Hecht-Nielsen},
keywords = {Confabulation Theory},
abstract = {Confabulation Theory [Hecht-Nielsen R. Confabulation theory. Springer-Verlag; 2007] is the first comprehensive theory of human and animal cognition. Here, we briefly describe Confabulation Theory and discuss experimental results that suggest the theory is correct. Simply put, Confabulation Theory proposes that thinking is like moving. In humans, the theory postulates that there are roughly 4000 thalamocortical modules, the “muscles of thought”. Each module performs an internal competition (confabulation) between its symbols, influenced by inputs delivered via learned axonal associations with symbols in other modules. In each module, this competition is controlled, as in an individual muscle, by a single graded (i.e., analog) thought control signal. The final result of this confabulation process is a single active symbol, the expression of which also results in launching of action commands that trigger and control subsequent movements and/or thought processes. Modules are manipulated in groups under coordinated, event-contingent control, in a similar manner to our 700 muscles. Confabulation Theory hypothesizes that the control of thinking is a direct evolutionary outgrowth of the control of movement. Establishing a complete understanding of Confabulation Theory will require launching and sustaining a massive new phalanx of confabulation neuroscience research.}
}
@article{FENG2017150,
title = {Parallel programming with pictures is a Snap!},
journal = {Journal of Parallel and Distributed Computing},
volume = {105},
pages = {150-162},
year = {2017},
note = {Keeping up with Technology: Teaching Parallel, Distributed and High-Performance Computing},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2017.01.018},
url = {https://www.sciencedirect.com/science/article/pii/S0743731517300242},
author = {Annette Feng and Mark Gardner and Wu-chun Feng},
keywords = {Explicit parallel computing, Computer science education, Block-based programming, Visual programming, Parallel computational patterns, Pedagogical tools, Programming environments, Languages for PDC and HPC},
abstract = {For decades, computing speeds seemingly doubled every 24 months by increasing the processor clock speed, thus giving software a “free ride” to better performance. This free ride, however, effectively ended by the mid-2000s. With clock speeds having plateaued and computational horsepower instead increasing due to increasing the number of cores per processor, the vision for parallel computing, which started more than 40 years ago, is a revolution that has now (ubiquitously) arrived. In addition to traditional supercomputing clusters, parallel computing with multiple cores can be found in desktops, laptops, and even mobile smartphones. This ubiquitous parallelism in hardware presents a major challenge: the difficulty in easily extracting parallel performance via current software abstractions. Consequently, this paper presents an approach that reduces the learning curve to parallel programming by introducing such concepts into a visual (but currently sequential) programming language called Snap!, which was inspired by MIT’s Scratch project. Furthermore, our proposed visual abstractions can automatically generate parallel code for the end user to run in parallel on a variety of platforms from personal computing devices to supercomputers. Ultimately, this work seeks to increase parallel programming literacy so that users, whether novice or experienced, may leverage a world of ubiquitous parallelism to enhance productivity in all walks of life, including the sciences, engineering, commerce, and liberal arts.}
}
@article{DEGRAAF2006181,
title = {Fall and rise of behavioural pharmacology},
journal = {Drug Discovery Today: Technologies},
volume = {3},
number = {2},
pages = {181-185},
year = {2006},
issn = {1740-6749},
doi = {https://doi.org/10.1016/j.ddtec.2006.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S174067490600031X},
author = {Joop S. {de Graaf}},
abstract = {Since the 1970s, a fortunate ensemble of technological and scientific developments has radically changed pharmacology, both in practice and imaginative thinking, towards a predominantly molecular science. Economic and political forces contributed to the undervaluation of in vivo experiments. The present generation of bioscientists, undertrained in whole animal, particularly behavioural pharmacology, now faces the challenge to interpret and translate an interminable hoard of molecular data into understandable and applicable medicine. The article provides a retrospection in four decades of progress.}
}
@article{BOSCHETTI201671,
title = {Modelling and attitudes towards the future},
journal = {Ecological Modelling},
volume = {322},
pages = {71-81},
year = {2016},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2015.11.009},
url = {https://www.sciencedirect.com/science/article/pii/S0304380015005311},
author = {Fabio Boschetti and Iain Walker and Jennifer Price},
keywords = {Forecasting, Futures Studies, Ecological modelling, Natural resource management, Cognitive science},
abstract = {The outputs of ecological models often need to be projected several years, or decades, into the future. The psychological literature tells us that stakeholders rarely think of such a distant future and when they do, they employ cognitive styles different from the ones commonly used for planning and decision making, which the ecological models are designed to facilitate. This may affect the reception of modelling efforts in several ways. Stakeholders may question the very purpose of trying to say anything meaningful about such a distant future; may consider model outputs as irrelevant to planning; or may provide emotional, often unconscious, responses motivated by deeply held fears and aspirations. Modellers too may display some of these behaviours. Here, we review the relevant literature and describe a questionnaire a modeller could use to explore these issues within a stakeholder group. We also report an experiment which shows how the very act of answering the questionnaire can significantly change the perception of future time horizon and future concerns and discuss the possible implications for modelling projects.}
}
@article{SOYEL20131312,
title = {Towards an affect sensitive interactive companion},
journal = {Computers & Electrical Engineering},
volume = {39},
number = {4},
pages = {1312-1319},
year = {2013},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2013.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0045790613000712},
author = {Hamit Soyel and Peter W. McOwan},
abstract = {As robots are increasingly being viewed as social entities to be integrated in our daily lives, social perceptive abilities seem a necessary requirement for enabling more natural interaction with human users. In this paper, we present an interaction scenario where user play chess with an iCat robot and propose an affect recognition system that uses computational models to automatically extract visual features allowing the detection of the level of engagement with a social robot that acts as a game companion. Experimental results show that the multimodal integration of head direction information with facial expressions displayed by the user improves the recognition of the user’s affective states.}
}
@article{CAI2021106,
title = {The Neural Instantiation of an Abstract Cognitive Map for Economic Choice},
journal = {Neuroscience},
volume = {477},
pages = {106-114},
year = {2021},
issn = {0306-4522},
doi = {https://doi.org/10.1016/j.neuroscience.2021.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0306452221004784},
author = {Xinying Cai},
keywords = {cognitive map, economic choice, orbitofrontal cortex, subjective value},
abstract = {Since the discovery of cognitive maps in rodent hippocampus (HC), the cognitive map has evolved from originally referring to spatial representations encoding locations and objects in Euclidean spaces to a general low-dimensional organization of information along selected feature dimensions. A cognitive map includes hypothetical constructs that bridge between environmental stimuli and the final overt behavior. To neuroeconomists, utility and utility functions are such constructs with neurobiological basis that drive choice behavior. Emergence of distinct functional neuron groups in the primate orbitofrontal cortex (OFC) during simple economic choice indicates the formation of an abstract cognitive map for organizing information of goods for value computation. Experimental evidence suggests that organization of neuronal activity in such cognitive map reflects the abstraction of core task features. Thus, such map can be adapted to accommodate economic choices under various task contexts.}
}
@article{IVANOV201952,
title = {Infinite lattice models by an expansion with a non-Gaussian initial approximation},
journal = {Physics Letters B},
volume = {796},
pages = {52-58},
year = {2019},
issn = {0370-2693},
doi = {https://doi.org/10.1016/j.physletb.2019.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0370269319304460},
author = {Aleksandr Ivanov and Vasily Sazonov},
abstract = {Recently, a convergent series employing a non-Gaussian initial approximation was constructed and shown to be an effective computational tool for the finite size lattice models with a polynomial interaction. Here we show that the Borel summability is a sufficient condition for the correctness of the convergent series applied to infinite lattice models. We test the numerical workability of the convergent series method by examining one- and two-dimensional ϕ4-infinite lattice models. The comparison of the convergent series computations and the infinite lattice extrapolations of the Monte Carlo simulations reveals an agreement between two approaches.}
}
@article{BOLANOS201926,
title = {Energy, uncertainty, and entrepreneurship: John D Rockefeller’s sequential approach to transaction costs management in the early oil industry},
journal = {Energy Research & Social Science},
volume = {55},
pages = {26-34},
year = {2019},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2019.04.020},
url = {https://www.sciencedirect.com/science/article/pii/S2214629618304444},
author = {Jose A. Bolanos},
keywords = {Uncertainty, Rockefeller, Standard Oil Company, Entrepreneurship, Transaction costs},
abstract = {This article delves into the challenge of successful entrepreneurship in the energy industry under conditions of uncertainty by examining the case of John D Rockefeller’s Standard Oil Company, which rapidly seized control of an initially-uncertain industry. It finds that Rockefeller cemented control through a willingness to internalise contextual uncertainty (related to the nature of the energy business) as a stepping stone to managing contractual uncertainty (related to transactions with other parties). This finding suggests that thinking sequentially about the management of contextual and contractual uncertainty aids entrepreneurial success in the field of energy. This suggestion accords with standing calls in the transaction costs literature, which means that findings may generalise to some extent. However, the exploratory nature of the analysis implies the need for further research about the argument’s compatibility with modern energy practices and its generalisability.}
}
@incollection{BERNARD20221,
title = {Chapter One - Understanding cerebellar function through network perspectives: A review of resting-state connectivity of the cerebellum},
editor = {Kara D. Federmeier},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {76},
pages = {1-49},
year = {2022},
issn = {0079-7421},
doi = {https://doi.org/10.1016/bs.plm.2022.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0079742122000019},
author = {Jessica A. Bernard},
keywords = {Cerebellum, Resting-state connectivity, Cognition, Internal models},
abstract = {The human cerebellum, though relatively small in total volume, makes up for it in its neuronal density and immense computational power. As we seek to understand complex higher order human behavior, it is critical to consider how this structure may contribute to these domains. While historically conceptualized as a motor structure, likely in large part due to the overt motor deficits often experienced by those with cerebellar damage, it is now known to play a critical role in cognition. In the last decade in particular there has been a great deal of growth in the literature in this regard (though these ideas have been percolating since the 1980s). The development of resting-state functional connectivity magnetic resonance imaging (fcMRI) also resulted in a boom of literature seeking to clarify cerebellar interactions with the cortex. In this chapter, cerebellar anatomy and function are reviewed, with a particular focus on how fcMRI has impacted our understanding of the human cerebellum and what this has meant for our accounts of cerebellar processing (that is, the underlying computations). This work has broadened our appreciation of the cerebellum's networks linked to higher order processing, and resulted in thought provoking findings with respect to the functional organization of the cerebellum that may in the future impact our understanding of how the “little brain” helps the cortex produce nuanced and complicated human behavior.}
}
@incollection{BLOCKLEY2013229,
title = {9 - Earthquake risk management of civil infrastructure: integrating soft and hard risks},
editor = {S. Tesfamariam and K. Goda},
booktitle = {Handbook of Seismic Risk Analysis and Management of Civil Infrastructure Systems},
publisher = {Woodhead Publishing},
pages = {229-254},
year = {2013},
series = {Woodhead Publishing Series in Civil and Structural Engineering},
isbn = {978-0-85709-268-7},
doi = {https://doi.org/10.1533/9780857098986.2.229},
url = {https://www.sciencedirect.com/science/article/pii/B9780857092687500098},
author = {D. Blockley},
keywords = {seismic risk, uncertainty, safety, systems thinking, integration, hard and soft systems},
abstract = {Abstract:
Risk is an inevitable part of all human activity. Similarly sized earthquakes can have very different impacts in different countries depending on the degree of engineering input into the design and construction of the facilities. In this chapter we will propose an approach based on systems thinking and new systems boundaries. We will identify and characterise three different sources of uncertainty: hard physical system parameter uncertainty, hard system model uncertainty and soft system human uncertainty. We will explore ways in which evidence from previously disparate sources can be managed in an integrated way.}
}
@article{SALAMATI2014283,
title = {Personal Wellness: Complex and Elusive Product and Distributed Self-services},
journal = {Procedia CIRP},
volume = {16},
pages = {283-288},
year = {2014},
note = {Product Services Systems and Value Creation. Proceedings of the 6th CIRP Conference on Industrial Product-Service Systems},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2014.02.016},
url = {https://www.sciencedirect.com/science/article/pii/S2212827114001310},
author = {Farzaneh Salamati and Zbigniew J. Pasek},
keywords = {Consumer-personalized medicine, Quantified self-tracking, Health social networks},
abstract = {In many countries across the world a universal issue of growing concern is increasing demand for health services and corresponding escalating costs. While there are many reasons for these two trends, reasonable solutions are nowhere in sight and a subject of heated debates. One potential source of relief for the health care systems is to shift some (if not majority – but in long term) of responsibilities to patients themselves. To do so effectively, however, better definition of personal well-being is needed, supported by medical knowledge transfer to the consumer and creation of some personal health management tools. Service engineering concepts, such as service package, are useful in decoupling all elements needed to develop an infrastructure in support of wellness as a core product and addressed by variety of limited-focus services. This paper reviews the emerging health care paradigms, in particular health care networks, consumer-personalized medicine and quantified self-tracking. With the Quantified Self movement on the rise for the past several years and a corresponding growth in offering of tools for variety of personal data collection (both hardware- and software-based), the obvious question arises how effective they are and what impact they actually have. The discussion also addresses the question whether it is possible to reframe the personal health issues by applying both design thinking and service engineering approaches aimed at individual's own well-being.}
}
@article{FORTESCUE197967,
title = {Why the ‘language of thought’ is not a language: Some inconsistencies of the computational analogy of thought},
journal = {Journal of Pragmatics},
volume = {3},
number = {1},
pages = {67-80},
year = {1979},
issn = {0378-2166},
doi = {https://doi.org/10.1016/0378-2166(79)90006-7},
url = {https://www.sciencedirect.com/science/article/pii/0378216679900067},
author = {Michael Fortescue}
}
@article{MIYATA2018370,
title = {Emergence of symbolic inference based on value-driven intuitive inference via associative memory},
journal = {Procedia Computer Science},
volume = {145},
pages = {370-375},
year = {2018},
note = {Postproceedings of the 9th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2018 (Ninth Annual Meeting of the BICA Society), held August 22-24, 2018 in Prague, Czech Republic},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.11.087},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918323755},
author = {Masahiro Miyata and Takashi Omori},
keywords = {Human inference, Associative memory, Model, Symbolic inference, Intuitive inference},
abstract = {Humans use two types of inferences: intuitive and logical. However, they are studied separately. A site for logical inference has not been found in the brain, but modeling it as a distributed neural network form is desirable. In this study, we propose an inference model of an intuitive search process in continuous and distributed associative memory (AM), and it switches to a symbolic mode, in which each step of association converges to a stable state of self-recollection, realizing step-by-step logic. Switching is evoked by biasing the associative gain upon finding a valued state during the intuitive inference. In this study, we show the computational model of symbolic inference via AM, and we verify its practicality by solving a maze task. We show the emergence of a tree search-like behavior with pruning.}
}
@incollection{VELINGKAR2025239,
title = {Chapter 16 - Smart computing in brain-computer interface and neuroscientific research: opportunities, methods, and challenges},
editor = {Bikesh Kumar Singh and G.R. Sinha},
booktitle = {Intelligent Computing Techniques in Biomedical Imaging},
publisher = {Academic Press},
pages = {239-249},
year = {2025},
isbn = {978-0-443-15999-2},
doi = {https://doi.org/10.1016/B978-0-443-15999-2.00005-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443159992000050},
author = {Harish Velingkar and Roopa R. Kulkarni and Prashant P. Patavardhan},
keywords = {BCI, BMI, AI algorithms, machine learning, neuroscientific, brain waves, EEG},
abstract = {Brain-machine interfacing (BMI), also known as brain-computer interfacing (BCI), is an innovative field of technology with a primary objective to establish a connection and foster seamless interaction between the human brain and machines. The underlying concept aims to facilitate direct communication between the brain and external devices, enabling a direct interface between the two, allowing individuals to control various applications, such as prosthetics, robotics, or computer software, with their thoughts alone. BCI is a multidisciplinary field that involves expertise from different areas, such as neuroscience, computer science, engineering, and psychology. BMI is also being extensively used in early prediction of neurophysiological abnormalities and brain disorders. This article will lay out various benefits of using computational intelligence, specifically machine learning, in mental health disciplines and will also explain some of the popular AI algorithms from a neuroscientific research point of view.}
}
@article{JIANG2015154,
title = {Defining least community as a homogeneous group in complex networks},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {428},
pages = {154-160},
year = {2015},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2015.02.029},
url = {https://www.sciencedirect.com/science/article/pii/S0378437115001326},
author = {Bin Jiang and Ding Ma},
keywords = {Head/tail breaks, ht-index, Scaling, k-means, Natural breaks, Classification},
abstract = {This paper introduces a new concept of least community that is as homogeneous as a random graph, and develops a new community detection algorithm from the perspective of homogeneity or heterogeneity. Based on this concept, we adopt head/tail breaks–a newly developed classification scheme for data with a heavy-tailed distribution–and rely on edge betweenness given its heavy-tailed distribution to iteratively partition a network into many heterogeneous and homogeneous communities. Surprisingly, the derived communities for any self-organized and/or self-evolved large networks demonstrate very striking power laws, implying that there are far more small communities than large ones. This notion of far more small things than large ones constitutes a new fundamental way of thinking for community detection.}
}
@article{MACK2000307,
title = {Long-term effects of building on informal knowledge in a complex content domain: the case of multiplication of fractions},
journal = {The Journal of Mathematical Behavior},
volume = {19},
number = {3},
pages = {307-332},
year = {2000},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(00)00050-X},
url = {https://www.sciencedirect.com/science/article/pii/S073231230000050X},
author = {Nancy K. Mack},
keywords = {Fractions, Informal knowledge, Long-term effects},
abstract = {Four students participated in a 2-year study (fifth and sixth grades) that examined the development of their understanding of multiplication of fractions. During both years, students received individualized instruction that encouraged them to build on their informal knowledge of partitioning to understand and solve problems involving multiplication of fractions. Students also received classroom instruction on algorithmic procedures for multiplication of fractions during the second year. In the long term, students consistently drew on their informal knowledge of partitioning to reconceptualize and partition units to solve problems involving multiplication of fractions in meaningful ways. At times, students' thinking was also dominated by their knowledge of algorithmic procedures for multiplication of fractions.}
}
@article{BONANI2025100709,
title = {A rapid and inclusive instrument for assessing children’s basic understanding of physical computing},
journal = {International Journal of Child-Computer Interaction},
volume = {43},
pages = {100709},
year = {2025},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100709},
url = {https://www.sciencedirect.com/science/article/pii/S2212868924000783},
author = {Andrea Bonani and Rosella Gennari and Alessandra Melonio and Pierpaolo Vittorini},
keywords = {Physical computing, Assessment, Understanding, Rapid questionnaire, Inclusive questionnaire, School, Teacher, Children},
abstract = {While there are many initiatives that strive to empower children with physical computing, there seems to be no validated questionnaire for rapidly measuring different children’s understanding of the basics of physical computing. This paper presents the design of such an instrument—PCBUQ. It is rapid in that it consists of few items. It is inclusive because designed for different young children. It is for the basics of physical computing in that it considers physical input and output devices, basic patterns and programs that use them for interacting with the physical world. Data gathered from experts, primary and middle schools were used to validate PCBUQ. The first items assess children’s capability of classifying physical devices as input (e.g., buttons), and output devices (e.g., LED, speaker). The other items evaluate whether children can interpret problematic scenarios and infer how to resolve them with adequate input and output devices, patterns and programs. PCBUQ was found to have adequate reliability. The reported statistical analyses highlight the items that strongly and weakly correlate with the construct under analysis, their difficulty and discrimination. Results are discussed to guide future physical computing initiatives for children and their assessment.}
}
@article{LUDGERIO2023e10,
title = {Pedagogical practices developed with children through hospital classes: An integrative literature review},
journal = {Journal of Pediatric Nursing},
volume = {72},
pages = {e10-e18},
year = {2023},
issn = {0882-5963},
doi = {https://doi.org/10.1016/j.pedn.2023.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S0882596323001227},
author = {Muanna Jéssica Batista Ludgério and Cleide Maria Pontes and Bárbara Letícia Cruz {dos Santos} and Eliza Cristina Macedo and Maria Wanderleya {de Lavor Coriolano Marinus} and Luciana Pedrosa Leal},
keywords = {Special education, Hospital education department, Hospitalized child, Child rearing, Teaching},
abstract = {Objective
To analyze the evidence available in the literature on the pedagogical practices developed with children through hospital classes.
Method
An integrative review was conducted on July 20, 2022, in Scopus, MEDLINE/PubMed, CINAHL, LILACS, Web of Science, ERIC, Educ@, and Scielo using the following descriptors in English, Portuguese, and Spanish, extracted from DECS/MeSH, CINAHL, Brased/INEP, and ERIC Thesaurus: “Child, Hospitalized”, “Education, Special”, “Education Department, Hospital”, “Hospital Classroom”, “Hospital Class”, “Child Rearing”, “Educational Practices”, “Early Childhood Education”, “Education”, “Hospital Pedagogy”, and “Hospital Special Class”. No time restriction was applied. The EndNot Web reference manager and the Rayyan software were used to select studies, and later, the methodological rigor and level of evidence were assessed.
Results
The 22 articles described pedagogical practices, including ludic activities, individualized work, working with regular school content, stimulation activities, pedagogical and dialogic listening, learning based on the exchange of knowledge, video games, computational robotics, and theatrical performance.
Conclusion
Although difficulties were identified in implementing pedagogical practices in the hospital, they were shown to allow educational continuity and clinical improvement of hospitalized children.
Implications for practice
Studies on the educational process within the hospital setting can contribute to the development of public policies and the guarantee of the right to education for hospitalized children.
Descriptors
Special education; Hospital education department; Hospitalized child; Child rearing; Teaching.}
}
@article{KEELEY2020194,
title = {Modeling statistical dependencies in multi-region spike train data},
journal = {Current Opinion in Neurobiology},
volume = {65},
pages = {194-202},
year = {2020},
note = {Whole-brain interactions between neural circuits},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2020.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0959438820301720},
author = {Stephen L Keeley and David M Zoltowski and Mikio C Aoi and Jonathan W Pillow},
abstract = {Neural computations underlying cognition and behavior rely on the coordination of neural activity across multiple brain areas. Understanding how brain areas interact to process information or generate behavior is thus a central question in neuroscience. Here we provide an overview of statistical approaches for characterizing statistical dependencies in multi-region spike train recordings. We focus on two classes of models in particular: regression-based models and shared latent variable models. Regression-based models describe interactions in terms of a directed transformation of information from one region to another. Shared latent variable models, on the other hand, seek to describe interactions in terms of sources that capture common fluctuations in spiking activity across regions. We discuss the advantages and limitations of each of these approaches and future directions for the field. We intend this review to be an introduction to the statistical methods in multi-region models for computational neuroscientists and experimentalists alike.}
}
@article{LAUTENBACHER2024129583,
title = {Petz recovery maps for qudit quantum channels},
journal = {Physics Letters A},
volume = {512},
pages = {129583},
year = {2024},
issn = {0375-9601},
doi = {https://doi.org/10.1016/j.physleta.2024.129583},
url = {https://www.sciencedirect.com/science/article/pii/S0375960124002779},
author = {Lea Lautenbacher and Vinayak Jagadish and Francesco Petruccione and Nadja K. Bernardes},
keywords = {Petz recovery map, Qudit channels, Non-unital maps},
abstract = {This study delves into the efficacy of the Petz recovery map within the context of two paradigmatic quantum channels: dephasing and amplitude-damping. While prior investigations have predominantly focused on qubits, our research extends this inquiry to higher-dimensional systems. We introduce a novel, state-independent framework based on the Choi-Jamiołkowski isomorphism to evaluate the performance of the Petz map. By analyzing different channels and the (non-)unital nature of these processes, we emphasize the pivotal role of the reference state selection in determining the map's effectiveness. Furthermore, our analysis underscores the considerable impact of suboptimal choices on performance, prompting a broader consideration of factors such as system dimensionality.}
}
@article{BLAU2020100722,
title = {How does the pedagogical design of a technology-enhanced collaborative academic course promote digital literacies, self-regulation, and perceived learning of students?},
journal = {The Internet and Higher Education},
volume = {45},
pages = {100722},
year = {2020},
issn = {1096-7516},
doi = {https://doi.org/10.1016/j.iheduc.2019.100722},
url = {https://www.sciencedirect.com/science/article/pii/S1096751619304403},
author = {Ina Blau and Tamar Shamir-Inbal and Orit Avdiel},
keywords = {Digital literacy skills, Pedagogical design, Online communication and collaboration, Psychological ownership, Self-regulated learning strategies, Cognitive, emotional and social perceived learning},
abstract = {The wide expansion of digital technologies in higher education has introduced the need for an examination of the added value of various technological tools for quality teaching and active individual and collaborative learning. The current study explored whether and how the pedagogical design of an academic course, which developed a variety of digital literacy competencies, supported students in regulating collaborative technology-enhanced learning and helped them cope with the sense of psychological ownership over collaborative learning outcomes. In addition, we examined how these issues were expressed in cognitive, emotional and social aspects of students' perceived learning (Caspi & Blau, 2011). During four semesters, we conducted a qualitative analysis on reflective learning diaries, written by 78 graduate students studying education (N = 1870 codes). The bottom-up analysis focused on learning processes that enabled the development of various digital literacies conceptualized by the Digital Literacy Framework (DLF; Eshet-Alkalai, 2012): photo-visual, information, reproduction, branching, social-emotional, and real-time thinking skills. Furthermore, findings highlighted the importance of self-regulation and learning new technologies as an integral part of digital literacies. In addition, social-emotional statements expressed the development of effective communication and collaboration that enable students to cope with a sense of ownership over learning outcomes, and present different levels of teamwork: sharing, cooperation, and collaboration. Qualitative coding provided a more granulated perspective on perceived learning by differentiating between positive and negative aspects of emotional and social retrospection during the learning process. The findings contribute to educational theory by extending DLF and by providing new insights to the literature on students' perceived learning. We discuss the implications for instructional design and adoption of innovative pedagogy in higher education.}
}
@article{LI20232,
title = {Adult acquired flatfoot deformity: an update in classification},
journal = {Orthopaedics and Trauma},
volume = {37},
number = {1},
pages = {2-10},
year = {2023},
issn = {1877-1327},
doi = {https://doi.org/10.1016/j.mporth.2022.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S187713272200121X},
author = {James Li and Chandra Pasapula and Vivekanandan Dhukaram},
keywords = {AAFD, adult acquired flatfoot deformity, classification, flatfoot},
abstract = {Adult acquired flatfoot deformity (AAFD) involves a complex spectrum of pathologies, arising primarily from failure of static restrains, leading to collapse of the medial longitudinal arch and further subsequent deformities in the foot. The landmark paper and classification by Johnson et al. proposed that pathology in the posterior tibial tendon (PTT) was key in the development of AAFD. Since then, the understanding of AAFD has evolved and advanced. Multiple structures aside from the PTT, such as the spring ligament, plantar fascia and deltoid ligament, have been identified to play a similarly key role in disease development and progression. Classification systems have also evolved to incorporate this new understanding. These include modifications to Johnson's classification (Myerson, Bluman) as well as new systems which aim to incorporate modern thinking, capture the wide spectrum of presentations or utilize modern advancements in imaging modalities. Current classification systems continue to aid understanding and management of AAFD, despite their increasing complexity. Future classifications should aim to provide a succinct way to describe and understand AAFD, as well as guiding prognosis and management.}
}
@article{BRIGHT2020187,
title = {Applying computer algebra systems with SAT solvers to the Williamson conjecture},
journal = {Journal of Symbolic Computation},
volume = {100},
pages = {187-209},
year = {2020},
note = {Symbolic Computation and Satisfiability Checking},
issn = {0747-7171},
doi = {https://doi.org/10.1016/j.jsc.2019.07.024},
url = {https://www.sciencedirect.com/science/article/pii/S0747717119300902},
author = {Curtis Bright and Ilias Kotsireas and Vijay Ganesh},
keywords = {Williamson matrices, Boolean satisfiability, SAT solvers, Exhaustive search, Autocorrelation},
abstract = {We employ tools from the fields of symbolic computation and satisfiability checking—namely, computer algebra systems and SAT solvers—to study the Williamson conjecture from combinatorial design theory and increase the bounds to which Williamson matrices have been enumerated. In particular, we completely enumerate all Williamson matrices of even order up to and including 70 which gives us deeper insight into the behaviour and distribution of Williamson matrices. We find that, in contrast to the case when the order is odd, Williamson matrices of even order are quite plentiful and exist in every even order up to and including 70. As a consequence of this and a new construction for 8-Williamson matrices we construct 8-Williamson matrices in all odd orders up to and including 35. We additionally enumerate all Williamson matrices whose orders are divisible by 3 and less than 70, finding one previously unknown set of Williamson matrices of order 63.}
}
@article{MCCARTHY2019152,
title = {Shaking Gordon Wilson Flats: early seismic engineering research in New Zealand},
journal = {Proceedings of the Institution of Civil Engineers - Engineering History and Heritage},
volume = {172},
number = {4},
pages = {152-163},
year = {2019},
issn = {1757-9430},
doi = {https://doi.org/10.1680/jenhh.18.00030},
url = {https://www.sciencedirect.com/science/article/pii/S1757943019000066},
author = {Christine McCarthy},
keywords = {buildings, structures & design, history, seismic engineering},
abstract = {In the late 1950s and early 1960s, Gordon Wilson Memorial Flats (Wellington, New Zealand, 19551959) was instrumented for seismic engineering research and subjected to vibration testing. The research was prompted by new thinking about architectural design in the mid-twentieth century (i.e. modernism) that had caused a mismatch between structural assumptions in building codes (which relied on significant amounts of uncalculated stiffness inherent in 1920s building design) and the structural characteristics of new buildings that had, for example, greater areas of glazing. This type of research led to the revision of New Zealand building codes in the 1960s and informed Japanese processes for permitting buildings higher than 100 ft (305m). This paper outlines the research conducted and provides the context for understanding its significance. It is particularly topical given current proposals to instrument 400 Wellington buildings, creating the highest density of seismic instrumentation in any city.}
}
@article{KNIGHT2020100624,
title = {Researching the future of purchasing and supply management: The purpose and potential of scenarios},
journal = {Journal of Purchasing and Supply Management},
volume = {26},
number = {3},
pages = {100624},
year = {2020},
issn = {1478-4092},
doi = {https://doi.org/10.1016/j.pursup.2020.100624},
url = {https://www.sciencedirect.com/science/article/pii/S1478409220300777},
author = {Louise Knight and Joanne Meehan and Efstathios Tapinos and Laura Menzies and Alexandra Pfeiffer},
keywords = {Scenario planning, Procurement, Future studies, Prescience, Critical management, Covid-19 coronavirus},
abstract = {Drawing on prior research, the value of scenario planning as a methodology for researching the future of purchasing and supply management (PSM) is explored. Using three criteria of research quality – rigour, originality and significance – it is shown how developing scenarios and analysing their implications present new, important research opportunities for PSM academics, practitioners, and leaders of the profession. Researching the future of PSM supports the identification of uncertainties and anticipates change across many units and levels of analysis of interest to PSM scholars and practitioners, such as the profession/discipline, markets/sectors, or organisations. Scenarios are particularly effective for: considering how the complex interaction of macro-environmental factors affects the PSM context; avoiding incremental thinking; surfacing assumptions and revealing significant blind spots. PSM research using scenarios aligns with Corley and Gioia's (2011) call for prescience-oriented research in which academics aim for more impactful research, enhancing sense-giving potential and theoretical relevance to practice, to better perform their adaptive role in society.}
}
@article{FENG2024117540,
title = {Large language models for biomolecular analysis: From methods to applications},
journal = {TrAC Trends in Analytical Chemistry},
volume = {171},
pages = {117540},
year = {2024},
issn = {0165-9936},
doi = {https://doi.org/10.1016/j.trac.2024.117540},
url = {https://www.sciencedirect.com/science/article/pii/S0165993624000220},
author = {Ruijun Feng and Chi Zhang and Yang Zhang},
keywords = {Large language model, Biomolecular analysis, Fine-tuning, Prompt engineering, Parameter-efficient fine-tuning, In-context learning, Protein structure analysis, Protein sequence generation, Gene sequence analysis, Molecular representation learning},
abstract = {Large language models (LLMs) are proving to be very useful in many fields, especially chemistry and biology, because of their amazing capabilities. Biomolecular data is often represented sequentially, much like textual data used to train LLMs. However, developing LLMs from scratch requires a substantial amount of data and computational resources, which may not be feasible for most researchers. A more workable solution to this problem is to change the inputs or parameters so that the previously trained general LLMs can pick up the specific knowledge needed for biomolecular analysis. These adaption strategies lower the amount of data and hardware needed, providing a more affordable option. This review provides the introduction of two popular LLM adaptation techniques: fine-tuning and prompt engineering, along with their uses in the analysis of molecules, proteins, and genes. A thorough overview of current common datasets and pre-trained models is also provided. This review outlines the possible advantages and difficulties of LLMs for biomolecular analysis, opening the door for chemists and biologists to effectively utilize LLMs in their future studies.}
}
@incollection{KHAN2025213,
title = {Chapter 13 - The metaverse beyond the hype: Interdisciplinary perspectives on applications, tools, techniques, opportunities, and challenges of the metaverse},
editor = {Deepika Koundal and Naveen Kumar},
booktitle = {Exploring the Metaverse},
publisher = {Academic Press},
pages = {213-223},
year = {2025},
isbn = {978-0-443-24132-1},
doi = {https://doi.org/10.1016/B978-0-443-24132-1.00013-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443241321000132},
author = {Yusera Farooq Khan and Bilal Mir and Deepika koundal},
keywords = {Artificial intelligence, Augmented reality, Edge computing, Metaverse, Virtual reality},
abstract = {The concept of the metaverse has surged to the forefront of technological discourse, fuelled by both visionary promises and speculative hype. This chapter aims explores the metaverse in depth, delving beneath the superficial attention to provide a multivariate analysis from various interdisciplinary perspectives. The future of the metaverse is intricately explored, emphasising the applications, tools, approaches, possibilities, and difficulties that characterise its trajectory. The chapter highlights the revolutionary effects of the metaverse in areas like entertainment, education, business, and social interaction by conducting a methodical dissection of its possible uses. It investigates the central role immersive experiences, augmented reality, virtual collaboration, and decentralised technologies play in redefining user engagement and interaction paradigms. Furthermore, the chapter reveals the intricate tools and techniques that serve as the foundation for the evolution of the metaverse. It investigates 3D modelling, spatial computation, haptic feedback systems, artificial intelligence-driven simulations, and other technological enablers that support the creation and maintenance of the metaverse ecosystem. The chapter dives deep into the prospects that the metaverse offers, such as different ways to express imaginative abilities, and better digital experiences. However, these opportunities are accompanied with a tangle of difficulties. The technological challenges of interoperability, scalability, and cross-platform synchronisation are analysed alongside ethical concerns such as data protection, identity preservation, and digital ownership. Through an interdisciplinary view, this book chapter equips a introductory outline for evaluating the potential, complexities, and implications of the imminent incorporation of the metaverse into our digital landscape by navigating its multifaceted dimensions.}
}
@article{ZHANG202044,
title = {From mathematical equivalence such as Ma equivalence to generalized Zhang equivalency including gradient equivalency},
journal = {Theoretical Computer Science},
volume = {817},
pages = {44-54},
year = {2020},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2019.07.027},
url = {https://www.sciencedirect.com/science/article/pii/S0304397519304621},
author = {Yunong Zhang and Min Yang and Binbin Qiu and Jian Li and Mingjie Zhu},
keywords = {Mathematical equivalence, Physical equivalency, Ma equivalence, Generalized Zhang equivalency, Gradient equivalency},
abstract = {The authors carried out time-varying problems solving (TVPS) including robot problems solving in 2001, and began to figure out the reasons for the problems solving via diverse layers. After eight years' thinking, i.e., in 2009, the authors began to manifest, put forward and carry out the thought of “physical equivalency”. By another eight years' practicing and experimenting, i.e., in 2017, the authors basically finished establishing the framework of Zhang equivalency. Now, it is the time to establish the complete theory in a brief manner. Therefore, concepts about mathematical equivalence simply termed equivalence are presented firstly including Ma equivalence (especially for robotics), and then concepts about physical equivalency simply termed equivalency are proposed. Meanwhile, concepts about Zhang equivalency as a kind of equivalency are further proposed, and concepts about gradient-dynamics equivalency simply termed gradient equivalency as a kind of equivalency are proposed as well. Furthermore, two specific applications are considered and investigated, which substantiate the efficacy of Zhang equivalency.}
}
@incollection{GOYAMARTINEZ2016171,
title = {Chapter 8 - The Emulation of Emotions in Artificial Intelligence: Another Step into Anthropomorphism},
editor = {Sharon Y. Tettegah and Safiya Umoja Noble},
booktitle = {Emotions, Technology, and Design},
publisher = {Academic Press},
address = {San Diego},
pages = {171-186},
year = {2016},
series = {Emotions and Technology},
isbn = {978-0-12-801872-9},
doi = {https://doi.org/10.1016/B978-0-12-801872-9.00008-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128018729000089},
author = {Mariana Goya-Martinez},
keywords = {Emotions, Artificial intelligence, Anthropomorphism, Human emulation},
abstract = {The conceptualization of emotions as unique to biological organisms and a flaw in human intelligence has been challenged in recent times. Artificial intelligence researchers defy the idea of emotions as solely biological and try to incorporate them in their agents' design. Emotions and feelings are now considered cognitive percepts that play an important role in decision making processes. Based on artificial intelligence discourses and inventions, this chapter explores how emotions are defined in the realm of artificial intelligence, what is their role in the agents' performance, and how and why they are being emulated. From improving human-machine interaction and achieving empathy, to providing machines with cognitive shortcuts for rational thinking, emotions could be a key element in building a coherent system of thought capable of organizing several kinds of knowledge. This could provide a way to finally pass the Turing test or to provide a smooth transformation of the human nature when we finally merge with the machines. The emulation of emotions is just another step in the objective of replicating the human, promoting a higher level of anthropomorphism in the field of artificial intelligence.}
}
@incollection{GANESAN2025411,
title = {Chapter 13 - Role of quantum computing in accelerating drug discovery process},
editor = {Shubham Mahajan and Amit Kant Pandit},
booktitle = {Innovations in Biomedical Engineering},
publisher = {Academic Press},
pages = {411-435},
year = {2025},
isbn = {978-0-443-30146-9},
doi = {https://doi.org/10.1016/B978-0-443-30146-9.00013-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780443301469000137},
author = {Nirmala Ganesan and R. Rahul and S. {Sibi Sidharth}},
keywords = {Quantum computing, Drug discovery, Quantum algorithms, Pharmaceutical research, Quantum mechanics},
abstract = {This chapter investigates the application of a variational quantum eigen solver and other quantum algorithms to drug development. The uses and constraints of quantum algorithms, such as the variational quantum eigen solver, are discusses. This article examines the usefulness of variational quantum eigen solver and other quantum algorithms to drug development, while also highlighting the limitations and application areas of quantum algorithms. A paradigm shift has occurred in the pharmaceutical business with the introduction of quantum computing, as formerly unimaginable computational powers to expedite the drug discovery process have become achievable. Examining how quantum computing is changing the field, this research shows how it can significantly increase the accuracy and effectiveness of drug discovery, virtual screening, and molecular simulations. Quantum computers use quantum physics techniques to perform calculations that are more complex than possible with their classical equivalents. These systems, by utilizing quantum parallelism and entanglement, may probe huge chemical areas much faster than classical approaches. This study investigates the application of quantum algorithms, particularly variational quantum eigen solvers, in drug development. Hybrid quantum-classical technologies present a viable solution to surmount the constraints of quantum technology and enhance the scalability of drug development processes. These technologies blend quantum computing with classical approaches. The report discusses the most recent advancements in this hybrid paradigm and highlights collaborative efforts between experts in quantum computing and academics researching drugs. As technology develops, quantum computing becomes more and more significant for drug discovery. The groundbreaking effects of quantum computing on drug development and its potential to drastically alter the pharmaceutical research landscape are highlighted in the conclusion of this chapter. Through the application of quantum computing, drug development processes stand to benefit greatly from a new age of efficacy and efficiency brought about by addressing current problems and embracing innovative ideas.}
}
@article{HAFRI2021475,
title = {The Perception of Relations},
journal = {Trends in Cognitive Sciences},
volume = {25},
number = {6},
pages = {475-492},
year = {2021},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2021.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S1364661321000085},
author = {Alon Hafri and Chaz Firestone},
keywords = {visual psychophysics, core cognition, intuitive physics, compositionality, structured representations, role-binding},
abstract = {The world contains not only objects and features (red apples, glass bowls, wooden tables), but also relations holding between them (apples contained in bowls, bowls supported by tables). Representations of these relations are often developmentally precocious and linguistically privileged; but how does the mind extract them in the first place? Although relations themselves cast no light onto our eyes, a growing body of work suggests that even very sophisticated relations display key signatures of automatic visual processing. Across physical, eventive, and social domains, relations such as support, fit, cause, chase, and even socially interact are extracted rapidly, are impossible to ignore, and influence other perceptual processes. Sophisticated and structured relations are not only judged and understood, but also seen — revealing surprisingly rich content in visual perception itself.}
}
@article{TANAKA201664,
title = {Modeling the motor cortex: Optimality, recurrent neural networks, and spatial dynamics},
journal = {Neuroscience Research},
volume = {104},
pages = {64-71},
year = {2016},
note = {Body representation in the brain},
issn = {0168-0102},
doi = {https://doi.org/10.1016/j.neures.2015.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0168010215002631},
author = {Hirokazu Tanaka},
keywords = {Body representation, Motor control, Computational modeling, Optimization, Neural computation},
abstract = {Specialization of motor function in the frontal lobe was first discovered in the seminal experiments by Fritsch and Hitzig and subsequently by Ferrier in the 19th century. It is, however, ironical that the functional and computational role of the motor cortex still remains unresolved. A computational understanding of the motor cortex equals to understanding what movement variables the motor neurons represent (movement representation problem) and how such movement variables are computed through the interaction with anatomically connected areas (neural computation problem). Electrophysiological experiments in the 20th century demonstrated that the neural activities in motor cortex correlated with a number of motor-related and cognitive variables, thereby igniting the controversy over movement representations in motor cortex. Despite substantial experimental efforts, the overwhelming complexity found in neural activities has impeded our understanding of how movements are represented in the motor cortex. Recent progresses in computational modeling have rekindled this controversy in the 21st century. Here, I review the recent developments in computational models of the motor cortex, with a focus on optimality models, recurrent neural network models and spatial dynamics models. Although individual models provide consistent pictures within their domains, our current understanding about functions of the motor cortex is still fragmented.}
}
@incollection{ZIEDCHAARI202565,
title = {Chapter 3 - Basics of automation},
editor = {Kishor Kumar Sadasivuni and Nebojsa Bacanin and Jaehwan Kim and Neha B Vashisht},
booktitle = {Harnessing Automation and Machine Learning for Resource Recovery and Value Creation},
publisher = {Elsevier},
pages = {65-90},
year = {2025},
isbn = {978-0-443-27374-2},
doi = {https://doi.org/10.1016/B978-0-443-27374-2.00003-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443273742000030},
author = {Mohamed {Zied Chaari} and Oleg Serov and Gilroy Pereira and Aghzout Otman},
keywords = {Artificial intelligence, automation engineering, machine learning, systems engineering, sustainable development},
abstract = {Automation uses different technologies to manufacture products or complete tasks with minimal human involvement. It is often used to perform repetitive tasks so that humans can prioritize vital and complex work. Automation engineering is the development, analysis, and implementation of such complex systems. An example that comes to most people’s minds when thinking about automation is robotic arms picking and placing objects onto conveyor belts. While this example certainly fits the definition of automation, it is a modern take on automation. Automation systems have existed since ancient times, such as the steam engine of ancient Greece, and shortly, artificial intelligence (AI)-powered automation will become an essential part of our daily lives. In this chapter, we will explore the concepts of automation and automation engineering, look at the evolution of automation from ancient times to the present, and examine two case studies that highlight the use of automation. The first case study uses automation to monitor and provide water to crops, while the second uses deep learning to sort waste. Lastly, we will explore how AI will change the field of automation.}
}
@incollection{QUEK20021831,
title = {49 - Tap: An Inquiry Teaching Shell Using Both Rule-Based and State-Space Approaches},
editor = {Cornelius T. Leondes},
booktitle = {Expert Systems},
publisher = {Academic Press},
address = {Burlington},
pages = {1831-1895},
year = {2002},
isbn = {978-0-12-443880-4},
doi = {https://doi.org/10.1016/B978-012443880-4/50093-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780124438804500934},
author = {C. Quek and L.H. Wong and C.K. Looi},
abstract = {Publisher Summary
Inquiry teaching is similar to a scientific discovery process. The essence of inquiry teaching lies in the “higher-level Thinking” or “scientific-systematic reasoning” that helps in coping with and making proper use of any causal, rule-based, and procedural knowledge. The core of the process of inquiry teaching is the loop consisting of “forming-debugging hypothesis” and “testing hypothesis.” Once a hypothesis is successfully verified, one exits the loop and proceeds to study the generalizations or the applications of the rule. Inquiry teaching can be divided into three inquiry levels characterized by some degree of expectation about the students' performance. At level 1, students are guided to go through the form-debug-test loop of inquiry teaching for learning each concept. The hypothesizing and testing procedures-experiments-methods are based on processes predefined by the teacher or the intelligent tutoring system (ITS). The students learn the “core loop” of systematic method. At level 2, students experience the core loop while learning a rule. On verifying the rule, they proceed to learn about how to “make novel predictions” based on the rule and/or “apply the rule in the real world.” The aim at this level is to familiarize the student with the complete systematic reasoning process. Inquiry level 3 is introduced as a hypothetical level, where students encounter all the steps involved at level 2 but there are no predefined procedures. Students are required to propose their own methods for hypothesizing and testing. The main challenge at this level is how to validate the proposed methods. A human teacher is able to carry out such a task, whereas, an ITS is not. Building a computer system that has the intelligence to understand, analyze, and critique a user-defined method is an important computational problem.}
}
@article{GIGERENZER2004587,
title = {Mindless statistics},
journal = {The Journal of Socio-Economics},
volume = {33},
number = {5},
pages = {587-606},
year = {2004},
note = {Statistical Significance},
issn = {1053-5357},
doi = {https://doi.org/10.1016/j.socec.2004.09.033},
url = {https://www.sciencedirect.com/science/article/pii/S1053535704000927},
author = {Gerd Gigerenzer},
keywords = {Rituals, Collective illusions, Statistical significance, Editors, Textbooks},
abstract = {Statistical rituals largely eliminate statistical thinking in the social sciences. Rituals are indispensable for identification with social groups, but they should be the subject rather than the procedure of science. What I call the “null ritual” consists of three steps: (1) set up a statistical null hypothesis, but do not specify your own hypothesis nor any alternative hypothesis, (2) use the 5% significance level for rejecting the null and accepting your hypothesis, and (3) always perform this procedure. I report evidence of the resulting collective confusion and fears about sanctions on the part of students and teachers, researchers and editors, as well as textbook writers.}
}
@article{HUMPHREYS2020101346,
title = {Explaining short-term memory phenomena with an integrated episodic/semantic framework of long-term memory},
journal = {Cognitive Psychology},
volume = {123},
pages = {101346},
year = {2020},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2020.101346},
url = {https://www.sciencedirect.com/science/article/pii/S001002852030075X},
author = {Michael S. Humphreys and Gerald Tehan and Oliver Baumann and Shayne Loft},
keywords = {Short-term memory, Long-term memory, Episodic memory, Semantic memory, Systems consolidation, Associative interference, Context},
abstract = {Current thinking about human memory is dominated by distinctions between episodic and semantic memory and between short-term memory (STM) and long-term memory (LTM). However, many memory phenomena seem to cut across these distinctions. This article attempts to set the groundwork for the issues that need to be resolved in generating an integrated model of long-term memory that incorporates semantic, episodic, and short-term memory. We contrast Nairne’s (2002, Annual Review of Psychology) consensus account of short-term memory with a relatively generic theory of an integrated episodic-semantic memory. The later consists primarily of a list of principles which we and others argue are necessary to include in any theory of long-term memory. We then add some more specific assumptions to outline a modern theory of forgetting. We then turn to the issue of much of the phenomena thought to necessitate a dedicated short-term memory can be explained by an integrated theory of episodic and semantic memory. Our conclusion is that an integrated theory of long-term memory must be augmented to explain a small number of outstanding memory phenomena. Finally, we ask whether the augmentation needs to involve a dedicated mnemonic system, or sensory or language-based systems, which also have mnemonic capabilities.}
}
@incollection{DUCHATEAU2023147,
title = {Chapter 7 - Machine learning and biophysical models: how to benefit each other?},
editor = {Francisco Chinesta and Elías Cueto and Yohan Payan and Jacques Ohayon},
booktitle = {Reduced Order Models for the Biomechanics of Living Organs},
publisher = {Academic Press},
pages = {147-164},
year = {2023},
series = {Biomechanics of Living Organs},
issn = {25890999},
doi = {https://doi.org/10.1016/B978-0-32-389967-3.00009-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323899673000093},
author = {Nicolas Duchateau and Oscar Camara},
keywords = {Machine learning, Simulations, Biophysical models, Synthetic data, Reduced-order models},
abstract = {Biophysical models and machine learning may be perceived as rather different entities, or on the contrary as very related forms of modelling. In this chapter, we precisely develop the latter idea to provide a didactic and up-to-date overview of some major research tracks where these two fields can collaborate and benefit each other. We specifically articulate contents around two complementary points-of-view on the potential benefits of one field to the other. For biophysical modelling, we focused on accelerating computations, estimating unobservable parameters and examining complex outputs; for machine learning, we laid stress on adding physiologically relevant knowledge and generating synthetic data for training and validation. Throughout this review, we detail specific questions of relevance with examples mostly in the context of computational cardiology, which is our field of interest, and encourage further interaction between these two areas of active research.}
}
@article{QUESTATORTEROLO2025102594,
title = {A case of teaching in multigrade classrooms in Uruguay: Challenges and opportunities for learning and teaching in inclusive environments},
journal = {International Journal of Educational Research},
volume = {131},
pages = {102594},
year = {2025},
issn = {0883-0355},
doi = {https://doi.org/10.1016/j.ijer.2025.102594},
url = {https://www.sciencedirect.com/science/article/pii/S0883035525000680},
author = {Mariela Questa-Torterolo and Claudia {Cabrera Borges} and Camila {Fajardo Puentes}},
keywords = {Multigraded classes, Teaching methods, Inclusion, Teacher education},
abstract = {This study aimed to investigate the pedagogical practices and perceptions of an experienced teacher in the context of multigrade preschool and primary education classrooms in Uruguay. The focus was on understanding how these practices contribute to inclusive education and promote equity. To achieve this objective, a qualitative case study methodology was employed, collecting data through three in-depth interviews with the teacher and a thorough analysis of twelve pedagogical documents, which were examined using thematic content analysis. The results reveal the inclusive teaching strategies implemented by the teacher, the curricular adaptations applied in the classroom, and the management practices developed in response to various emerging situations. Although based on a single case, these findings suggest that the multigrade classroom modality presents both challenges and opportunities for achieving quality teaching and meaningful learning for students through authentic inclusion.}
}
@article{SOEMER201912,
title = {Text difficulty, topic interest, and mind wandering during reading},
journal = {Learning and Instruction},
volume = {61},
pages = {12-22},
year = {2019},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2018.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S095947521830389X},
author = {Alexander Soemer and Ulrich Schiefele},
keywords = {Mind wandering, Reading comprehension, Interest, Text difficulty},
abstract = {The present article deals with the question of how the difficulty of a text affects a reader's tendency to engage in task-unrelated thinking (mind wandering) during reading, and the potential role of topic interest as a mediator of the relation between text difficulty and mind wandering. Two-hundred and sixteen participants read three texts with each text either being easy, moderate, or difficult in terms of readability and cohesion. From time to time during reading, participants were interrupted and required to indicate whether they were voluntarily or involuntarily engaging in mind wandering. After reading each text, they rated their interest in and familiarity with the topic, and subsequently answered a number of comprehension questions. The results revealed that reading difficult texts increased both voluntary and involuntary mind wandering and this increase partially explained the negative relation between text difficulty and comprehension. Furthermore, topic interest fully mediated the effect of text difficulty on both forms of mind wandering.}
}
@article{VALENTI2025978,
title = {Costs and benefits of item reduction: The abridgment of the Emotion Regulation Questionnaire (ERQ)},
journal = {Journal of Affective Disorders},
volume = {369},
pages = {978-985},
year = {2025},
issn = {0165-0327},
doi = {https://doi.org/10.1016/j.jad.2024.10.079},
url = {https://www.sciencedirect.com/science/article/pii/S0165032724017749},
author = {Giusy Danila Valenti and Palmira Faraci},
keywords = {Short-scale development, Item reduction, Psychometric properties, Emotion Regulation Questionnaire (ERQ)},
abstract = {Shortening existing instruments is a highly required procedure, as short scales may have several advantages over the long versions, especially in time and/or resources restrictions. However, abbreviated forms may be weaker than their parent versions from both content coverage and psychometric robustness. Also, the abridgment of instruments is often lacking in methodological strictness, and the potential drawbacks of the shortened scales are rarely reported. The current study aims to describe the whole process of scale shortening, emphasizing the potential costs and benefits, in terms of balance between time-resource savings and loss of validity and reliability. We shortened the Emotion Regulation Questionnaire (ERQ), involving a sample of 459 participants (53.2% males). Item reduction was driven by searching to preserve the content breadth of the construct and scale's psychometric quality. Our results supported the two-factor structure of the measure (Cognitive Reappraisal and Expressive Suppression), χ2(8) = 11.357 ns, CFI = 0.995, TLI = 0.990, RMSEA = 0.030 (0.000–0.067), SRMR = 0.031, and three items were selected for each subscale. The two intended factors showed good levels of reliability (α > 0.710). A latent variable model was performed to evaluate how the original ERQ and our proposed short version (ERQS) were related to depression, anxiety, and stress: A similar pattern of associations was found, with Cognitive Reappraisal (negatively) and Expressive Suppression (positively) reporting significant but weak associations. The ERQ-S can be beneficial over the original version, as it effectively assesses the two emotion regulation strategies with a trivial loss in reliability and predictive validity.}
}
@incollection{UTEVSKY2015231,
title = {Social Decision Making},
editor = {Arthur W. Toga},
booktitle = {Brain Mapping},
publisher = {Academic Press},
address = {Waltham},
pages = {231-234},
year = {2015},
isbn = {978-0-12-397316-0},
doi = {https://doi.org/10.1016/B978-0-12-397025-1.00185-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780123970251001858},
author = {A.V. Utevsky and S.A. Huettel},
keywords = {Altruism, Decision making, Frontal cortex, Mentalizing, Reward, Social, Temporoparietal junction, Theory of mind},
abstract = {Many decisions are made in a social context. These decisions may be interactive and involve cooperation or conflict, or they may be made individually but have consequences for others. The brain processes underlying social decisions are complex, requiring not only computations associated with valuation and comparison but also social-cognitive processes like inferring others' mental states and predicting their behaviors. Accordingly, progress in understanding social decision making comes from combinations of techniques, primarily from the fields of economics and neuroscience. A standard model has arisen that contends that interpreting social information relies on a network of brain regions – including the medial prefrontal cortex, the posterior cingulate cortex, and the temporoparietal junction – and that information in turn feeds into the brain's system for valuation. Here, we review different types of social decisions, as well as the neural regions underlying their components.}
}
@incollection{HERNANDEZLEMUS2017251,
title = {Chapter 14 - Handling Big Data in Precision Medicine},
editor = {Mukesh Verma and Debmalya Barh},
booktitle = {Progress and Challenges in Precision Medicine},
publisher = {Academic Press},
pages = {251-268},
year = {2017},
isbn = {978-0-12-809411-2},
doi = {https://doi.org/10.1016/B978-0-12-809411-2.00014-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128094112000143},
author = {E. Hernández-Lemus and J. Espinal-Enríquez and R. García-Herrera},
keywords = {Big data, Cloud computing, Data confidentiality, High-throughput data, Individualized therapy, Meta-data, Precision medicine},
abstract = {Precision medicine looks for the integration of vast information data sets on the molecular and environmental origins of disease for the development of individualized, context-dependent diagnostics and therapies. To build predictive models of complex disease compliant with individual variability on genetic and socially determined conditions, it is necessary to have computationally efficient methods to handle, visualize, and integrate large data sets of different origins, in a multitude of formats and subject to different levels of confidentiality into a single framework. This involves the ability to comply with the big data paradigm under demanding conditions of performance and subject to time, computational power, and bioethical constraints. This is still an open problem; however, we can devise some ways in which big data analytics may join forces with bioinformatics, medical informatics, and computational systems biology in a fast and effective way, motivated with the success of approaches such as the one given by translational bioinformatics.}
}
@incollection{VODOVOTZ201557,
title = {Chapter 3.1 - Towards Translational Systems Biology of Inflammation},
editor = {Yoram Vodovotz and Gary An},
booktitle = {Translational Systems Biology},
publisher = {Academic Press},
address = {Boston},
pages = {57-61},
year = {2015},
isbn = {978-0-12-397884-4},
doi = {https://doi.org/10.1016/B978-0-12-397884-4.00008-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780123978844000082},
author = {Yoram Vodovotz and Gary An},
keywords = {Translational medicine, translational research, systems biology, computational biology, bioinformatics, molecular biology, genomics, proteomics, metabolomics, clinical trials},
abstract = {Having introduced the historical, methodological, procedural, and societal antecedents that have contributed to the Translational Dilemma, in this chapter we propose our strategy to overcoming the challenges associated with the Dilemma, a research program we call Translational Systems Biology. This investigative strategy is predicated on the use of dynamic computational modeling and associated computational methods of data analysis and aggregation to accelerate the Scientific Cycle with an explicit target of generating clinically actionable knowledge. Translational Systems Biology is firmly grounded in the fundamental scientific principles discussed in earlier chapters, with its computational component specifically designed to overcome the numerous factors previously identified as contributing to the Translational Dilemma. These factors include the challenge of integrating and synthesizing mechanistic knowledge in systems with known nonlinear dynamics, utilizing and analyzing high-throughput data in a manner that facilitates the construction and use of dynamic computational models, the use of computational models as means of dynamic knowledge representation to test and falsify mechanistic hypotheses, and the use of computational modeling to bridge experimental biology to clinical application through the execution of in silico clinical trials.}
}
@article{TANG2025103900,
title = {A multi-task deep reinforcement learning approach to real-time railway train rescheduling},
journal = {Transportation Research Part E: Logistics and Transportation Review},
volume = {194},
pages = {103900},
year = {2025},
issn = {1366-5545},
doi = {https://doi.org/10.1016/j.tre.2024.103900},
url = {https://www.sciencedirect.com/science/article/pii/S1366554524004915},
author = {Tao Tang and Simin Chai and Wei Wu and Jiateng Yin and Andrea D’Ariano},
keywords = {Real-time train rescheduling, High-speed railway, Train delay time, Multi-task deep reinforcement learning, Quadratic assignment programming},
abstract = {In high-speed railway systems, unexpected disruptions can result in delays of trains, significantly affecting the quality of service for passengers. Train Timetable Rescheduling (TTR) is a crucial task in the daily operation of high-speed railways to maintain punctuality and efficiency in the face of such unforeseen disruptions. Most existing studies on TTR are based on integer programming (IP) techniques and are required to solve IP models repetitively in case of disruptions, which however may be very time-consuming and greatly limit their usefulness in practice. Our study first proposes a multi-task deep reinforcement learning (MDRL) approach for TTR. Our MDRL is constructed and trained offline with a large number of historical disruptive events, enabling to generate TTR decisions in real-time for different disruption cases. Specifically, we transform the TTR problem into a Markov decision process considering the retiming and rerouting of trains. Then, we construct the MDRL framework with the definition of state, action, transition, reward, and value function approximations with neural networks for each agent (i.e., rail train), by considering the information of different disruption events as tasks. To overcome the low training efficiency and huge memory usage in the training of MDRL, given a large number of disruptive events in the historical data, we develop a new and high-efficient training method based on a Quadratic assignment programming (QAP) model and a Frank-Wolfe-based algorithm. Our QAP model optimizes only a small number but most “representative” tasks from the historical data, while the Frank-Wolfe-based algorithm approximates the nonlinear terms in the value function of MDRL and updates the model parameters among different training tasks concurrently. Finally, based on the real-world data from the Beijing–Zhangjiakou high-speed railway systems, we evaluate the performance of our MDRL approach by benchmarking it against state-of-the-art approaches in the literature. Our computational results demonstrate that an offline-trained MDRL is able to generate near-optimal TTR solutions in real-time against different disruption scenarios, and it evidently outperforms state-of-art models regarding solution quality and computational time.}
}
@article{GULOTTA2022106698,
title = {Life Cycle Assessment and Life Cycle Costing of unitized regenerative fuel cell: A systematic review},
journal = {Environmental Impact Assessment Review},
volume = {92},
pages = {106698},
year = {2022},
issn = {0195-9255},
doi = {https://doi.org/10.1016/j.eiar.2021.106698},
url = {https://www.sciencedirect.com/science/article/pii/S0195925521001487},
author = {Teresa Maria Gulotta and Roberta Salomone and Francesco Lanuzza and Giuseppe Saija and Giovanni Mondello and Giuseppe Ioppolo},
keywords = {Unitized regenerative proton exchange membrane fuel cells, Environmental impacts, Economic impacts, Hydrogen technologies, PEM devices},
abstract = {Unitized Regenerative Fuel Cell (URFC) is considered a promising green hydrogen technology for producing clean energy, but further research is needed to make it attractive for a wide range of sectors and applications. In particular, the environmental and economic implications related to the life cycle of this electrochemical device play a fundamental role in determining its attractiveness and potential for improvement, and Life Cycle Thinking (LCT) assessment methods are considered to be the most effective means to improve knowledge about these implications. In this context, the present article provides a systematic and bibliometric literature review analysis of Life Cycle Assessment (LCA) and Life Cycle Costing (LCC) studies applied to URFCs using proton exchange membrane (PEM) devices. The aim is to evaluate the state-of-the-art of implementations of LCT methods to this electrochemical device in order to highlight good practices and critical issues, referred to both technical and methodological data. A reference sample of 44 scientific articles is extracted from the Web of Science (WoS), Scopus, and ScienceDirect databases and analysed using two computational tools: VOS viewer and Microsoft Excel. This group of publications helped establish the development over the last few decades of some key themes: LCC and LCA studies applied on PEM and URFC, also extending the search to its main components (such as fuel cell and electrolyser) and its original shape (i.e., regenerative fuel cell). The results of the analysis are presented quantitatively and qualitatively. Regarding the technical issues, there is significant variability in environmental and economic impacts, given by the selected system boundaries, the final users, and the fuel used by the systems. Regarding the methodological issues, no consensus emerges on how to model the LCT studies according to functional units, system boundaries, type of data selected, or model environmental externalities. The analysis also highlights the strong need for a higher level of transparency and harmonization of LCAs and LCCs applied on PEM technologies in order to improve the comparability of the results of these assessments.}
}
@article{HEMMATIAN202469,
title = {The utilitarian brain: Moving beyond the Free Energy Principle},
journal = {Cortex},
volume = {170},
pages = {69-79},
year = {2024},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2023.11.013},
url = {https://www.sciencedirect.com/science/article/pii/S0010945223003076},
author = {Babak Hemmatian and Lav R. Varshney and Frederick Pi and Aron K. Barbey},
keywords = {Free Energy Principle, Subjective utility, Extended cognition, Decision-making, Cognitive neuroscience, Bayesian Brain Hypothesis},
abstract = {The Free Energy Principle (FEP) is a normative computational framework for iterative reduction of prediction error and uncertainty through perception–intervention cycles that has been presented as a potential unifying theory of all brain functions (Friston, 2006). Any theory hoping to unify the brain sciences must be able to explain the mechanisms of decision-making, an important cognitive faculty, without the addition of independent, irreducible notions. This challenge has been accepted by several proponents of the FEP (Friston, 2010; Gershman, 2019). We evaluate attempts to reduce decision-making to the FEP, using Lucas' (2005) meta-theory of the brain's contextual constraints as a guidepost. We find reductive variants of the FEP for decision-making unable to explain behavior in certain types of diagnostic, predictive, and multi-armed bandit tasks. We trace the shortcomings to the core theory's lack of an adequate notion of subjective preference or “utility”, a concept central to decision-making and grounded in the brain's biological reality. We argue that any attempts to fully reduce utility to the FEP would require unrealistic assumptions, making the principle an unlikely candidate for unifying brain science. We suggest that researchers instead attempt to identify contexts in which either informational or independent reward constraints predominate, delimiting the FEP's area of applicability. To encourage this type of research, we propose a two-factor formal framework that can subsume any FEP model and allows experimenters to compare the contributions of informational versus reward constraints to behavior.}
}
@article{LIEBERMAN2020355,
title = {Comparison of intelligent transportation systems based on biocybernetic vehicle control systems},
journal = {Transportation Research Procedia},
volume = {50},
pages = {355-362},
year = {2020},
note = {XIV International Conference on Organization and Traffic Safety Management in Large Cities (OTS-2020)},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2020.10.042},
url = {https://www.sciencedirect.com/science/article/pii/S2352146520307900},
author = {Irina Lieberman and Pavel Klachek and Sergei Korjagin},
keywords = {intelligent transportation system, artificial intelligence vehicle, biocybernetic control system, sensor network, traffic safety, control module},
abstract = {Prompt thinking and quick reaction in complex dynamically changing traffic situations are determinant factors of road accidents. A key part of modern and promising intelligent transportation systems (ITSs) can be represented by VANET vehicular ad hoc network and its equivalents, whose nodes are represented by vehicles with installed special communication modules and new-generation intelligent on-board control systems. A concept of VANET network development is presented, which is based on biocybernetic systems of vehicle control and can serve as a starting point for building conceptually new ITSs and allow solving the primary ITS task at a totally new level, which lies in obtaining optimal prompt decisions (when driving a vehicle) in a short period of time, during which neither human no automated system can make a safe decision. We consider the architecture and basics of creating an intelligent on-board information-and-control module of a vehicle, based on the integration of a biocybernetic human-machine interface and elements of VANET vehicular ad hoc network. We also consider the basics of creating a bank of mathematical models as a central element of the intelligent on-board information-and-control module. Based on the analysis of completed experiments, we can conclude that the use of biocybernetic approaches based on sensor networks and corresponding applied vehicle control systems helps solving crucial traffic safety issues, namely: obtaining optimal prompt decisions (when driving a vehicle) in a short period of time, which significantly improves traffic safety and increases traffic intensity.}
}
@article{LI2012276,
title = {Predicting sRNAs and Their Targets in Bacteria},
journal = {Genomics, Proteomics & Bioinformatics},
volume = {10},
number = {5},
pages = {276-284},
year = {2012},
issn = {1672-0229},
doi = {https://doi.org/10.1016/j.gpb.2012.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S1672022912000745},
author = {Wuju Li and Xiaomin Ying and Qixuan Lu and Linxi Chen},
keywords = {Bacterial, sRNA, Target, Bioinformatics, Prediction},
abstract = {Bacterial small RNAs (sRNAs) are an emerging class of regulatory RNAs of about 40–500 nucleotides in length and, by binding to their target mRNAs or proteins, get involved in many biological processes such as sensing environmental changes and regulating gene expression. Thus, identification of bacterial sRNAs and their targets has become an important part of sRNA biology. Current strategies for discovery of sRNAs and their targets usually involve bioinformatics prediction followed by experimental validation, emphasizing a key role for bioinformatics prediction. Here, therefore, we provided an overview on prediction methods, focusing on the merits and limitations of each class of models. Finally, we will present our thinking on developing related bioinformatics models in future.}
}
@article{HOLSAPPLE2014130,
title = {A unified foundation for business analytics},
journal = {Decision Support Systems},
volume = {64},
pages = {130-141},
year = {2014},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2014.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0167923614001730},
author = {Clyde Holsapple and Anita Lee-Post and Ram Pakath},
keywords = {Analytics, Business analytics, Business intelligence, Decision making, Decision support, Evidence-based},
abstract = {Synthesizing prior research, this paper designs a relatively comprehensive and holistic characterization of business analytics – one that serves as a foundation on which researchers, practitioners, and educators can base their studies of business analytics. As such, it serves as an initial ontology for business analytics as a field of study. The foundation has three main parts dealing with the whence and whither of business analytics: identification of dimensions along which business analytics possibilities can be examined, derivation of a six-class taxonomy that covers business analytics perspectives in the literature, and design of an inclusive framework for the field of business analytics. In addition to unifying the literature, a major contribution of the designed framework is that it can stimulate thinking about the nature, roles, and future of business analytics initiatives. We show how this is done by deducing a host of unresolved issues for consideration by researchers, practitioners, and educators. We find that business analytics involves issues quite aside from data management, number crunching, technology use, systematic reasoning, and so forth.}
}
@article{HUNG20131,
title = {Conceptual Recombination: A method for producing exploratory and transformational creativity in creative works},
journal = {Knowledge-Based Systems},
volume = {53},
pages = {1-12},
year = {2013},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2013.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950705113002098},
author = {Edward C.K. Hung and Clifford S.T. Choy},
keywords = {Conceptual Recombination, Creative work ontology, Creative method, Computational creativity, Creativity Support Tools},
abstract = {Computational creativity researchers have long been searching for a reliable creative method of generating transformational creativity in Creativity Support Tools, in vain, especially when these systems are supposed to take in a user’s unfinished creative work and produce representational and creative outputs as continuations to the user input. In this paper we propose a new creative method called Conceptual Recombination to take up this challenge. We first define creative work for this study followed by creative work ontology to be the theoretical background of Conceptual Recombination. We further refer to application ontology and regard Conceptual Recombination as the task model for creative work ontology. In this task model there are three levels of prediction leading to the formations of output features, output structures, and their combinations as the final system outputs constrained by rules, biases, and homeomorphism. Furthermore, this new creative method allows the use of exploratory creativity on structures and transformational creativity on features to attain a balance between usefulness and novelty in system outputs. A 7-tuple computational model and the search mechanisms for exploratory and transformational creativity are also defined for it. Lastly, we evaluate Conceptual Recombination with our case study about producing a 2-dimensional asymmetrical shape with a given symmetrical shape to demonstrate its practicality and conclude that it not only offers a new reliable creative method for Creativity Support Tools, but also provides an objective evaluation method for transformational creativity.}
}
@article{KENETT201879,
title = {Driving the brain towards creativity and intelligence: A network control theory analysis},
journal = {Neuropsychologia},
volume = {118},
pages = {79-90},
year = {2018},
note = {The neural bases of creativity and intelligence: common grounds and differences},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2018.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0028393218300010},
author = {Yoed N. Kenett and John D. Medaglia and Roger E. Beaty and Qunlin Chen and Richard F. Betzel and Sharon L. Thompson-Schill and Jiang Qiu},
keywords = {Creativity, Intelligence, Network control theory, Cognitive control},
abstract = {High-level cognitive constructs, such as creativity and intelligence, entail complex and multiple processes, including cognitive control processes. Recent neurocognitive research on these constructs highlight the importance of dynamic interaction across neural network systems and the role of cognitive control processes in guiding such a dynamic interaction. How can we quantitatively examine the extent and ways in which cognitive control contributes to creativity and intelligence? To address this question, we apply a computational network control theory (NCT) approach to structural brain imaging data acquired via diffusion tensor imaging in a large sample of participants, to examine how NCT relates to individual differences in distinct measures of creative ability and intelligence. Recent application of this theory at the neural level is built on a model of brain dynamics, which mathematically models patterns of inter-region activity propagated along the structure of an underlying network. The strength of this approach is its ability to characterize the potential role of each brain region in regulating whole-brain network function based on its anatomical fingerprint and a simplified model of node dynamics. We find that intelligence is related to the ability to “drive” the brain system into easy to reach neural states by the right inferior parietal lobe and lower integration abilities in the left retrosplenial cortex. We also find that creativity is related to the ability to “drive” the brain system into difficult to reach states by the right dorsolateral prefrontal cortex (inferior frontal junction) and higher integration abilities in sensorimotor areas. Furthermore, we found that different facets of creativity—fluency, flexibility, and originality—relate to generally similar but not identical network controllability processes. We relate our findings to general theories on intelligence and creativity.}
}
@article{KASALICA20212157,
title = {APE in the Wild: Automated Exploration of Proteomics Workflows in the bio.tools Registry},
journal = {Journal of Proteome Research},
volume = {20},
number = {4},
pages = {2157-2165},
year = {2021},
issn = {1535-3907},
doi = {https://doi.org/10.1021/acs.jproteome.0c00983},
url = {https://www.sciencedirect.com/science/article/pii/S1535390721002031},
author = {Vedran Kasalica and Veit Schwämmle and Magnus Palmblad and Jon Ison and Anna-Lena Lamprecht},
keywords = {proteomics, scientific workflows, computational pipelines, workflow exploration, automated workflow composition, semantic tool annotation},
abstract = {The bio.tools registry is a main catalogue of computational tools in the life sciences. More than 17 000 tools have been registered by the international bioinformatics community. The bio.tools metadata schema includes semantic annotations of tool functions, that is, formal descriptions of tools’ data types, formats, and operations with terms from the EDAM bioinformatics ontology. Such annotations enable the automated composition of tools into multistep pipelines or workflows. In this Technical Note, we revisit a previous case study on the automated composition of proteomics workflows. We use the same four workflow scenarios but instead of using a small set of tools with carefully handcrafted annotations, we explore workflows directly on bio.tools. We use the Automated Pipeline Explorer (APE), a reimplementation and extension of the workflow composition method previously used. Moving “into the wild” opens up an unprecedented wealth of tools and a huge number of alternative workflows. Automated composition tools can be used to explore this space of possibilities systematically. Inevitably, the mixed quality of semantic annotations in bio.tools leads to unintended or erroneous tool combinations. However, our results also show that additional control mechanisms (tool filters, configuration options, and workflow constraints) can effectively guide the exploration toward smaller sets of more meaningful workflows.
}
}
@article{SCHULZ20197,
title = {The algorithmic architecture of exploration in the human brain},
journal = {Current Opinion in Neurobiology},
volume = {55},
pages = {7-14},
year = {2019},
note = {Machine Learning, Big Data, and Neuroscience},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2018.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0959438818300904},
author = {Eric Schulz and Samuel J. Gershman},
abstract = {Balancing exploration and exploitation is one of the central problems in reinforcement learning. We review recent studies that have identified multiple algorithmic strategies underlying exploration. In particular, humans use a combination of random and uncertainty-directed exploration strategies, which rely on different brain systems, have different developmental trajectories, and are sensitive to different task manipulations. Humans are also able to exploit sophisticated structural knowledge to aid their exploration, such as information about correlations between options. New computational models, drawing inspiration from machine learning, have begun to formalize these ideas and offer new ways to understand the neural basis of reinforcement learning.}
}
@article{MITTERAUER199899,
title = {An interdisciplinary approach towards a theory of consciousness},
journal = {Biosystems},
volume = {45},
number = {2},
pages = {99-121},
year = {1998},
issn = {0303-2647},
doi = {https://doi.org/10.1016/S0303-2647(97)00070-1},
url = {https://www.sciencedirect.com/science/article/pii/S0303264797000701},
author = {Bernhard Mitterauer},
keywords = {Reflection processes, Glial–neuronal interaction, Glial boundary-setting function, Tree of reflection, Self-systems},
abstract = {Instead of attacking the difficult problem of consciousness or self-consciousness directly, the theory is based on the more basic concept of reflection. A concept of reflection is suggested on four levels (recursion, reflective thinking, self-reflection, intersubjective reflection). We propose the glial–neuronal interaction as a neurobiological substrate for reflection processes. It is assumed that glia have a boundary-setting function (scaffolding, compartmentalization) in the spatio–temporal interaction with the neurons. This function could be a possible mechanism of `dividing' the brain into different self-systems each with their own capacity of self-organization. Although the brain's different self-systems are normally integrated, they may disintegrate and show themselves in special states of the brain (e.g. multiple personality disorder). A tree of reflection consisting of a number of places (ontological loci) on which reflection processes of varying complexity take place, is suggested as the formal model. Finally, the problem of self-conscious qualitative experience (Qualia) is discussed in terms of the reflection model.}
}
@article{MAGNANI2004439,
title = {Reasoning through doing. Epistemic mediators in scientific discovery},
journal = {Journal of Applied Logic},
volume = {2},
number = {4},
pages = {439-450},
year = {2004},
note = {CMSRA},
issn = {1570-8683},
doi = {https://doi.org/10.1016/j.jal.2004.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S1570868304000448},
author = {Lorenzo Magnani},
keywords = {Epistemic mediators, Abduction, Manipulative reasoning, Optical diagrams, Morphodynamics of discovery},
abstract = {The recent epistemological and cognitive studies concentrate on the concept of abduction, as a means to originate and refine new ideas. Traditional cognitive science and computational accounts concerning abduction aim at illustration discovery and creativity processes in terms of theoretical and “internal” aspects, by means of computational simulations and/or abstract cognitive models. I will illustrate in this paper that some typical internal abductive processes are involved in scientific reasoning and discovery (for example through radical innovations). Nevertheless, especially concrete manipulations of the external world constitute a fundamental passage in science: by a process of manipulative abduction it is possible to build prostheses (epistemic mediators) for human minds, by interacting with external objects and representations in a constructive way. In this manner it is possible to create implicit knowledge through doing and to produce various opportunity to find, for example, anomalies and fruitful new risky perspectives. This kind of embodied and unexpressed knowledge holds a key role in the subsequent processes of scientific comprehension and discovery.}
}
@article{CHECHURIN2016119,
title = {Understanding TRIZ through the review of top cited publications},
journal = {Computers in Industry},
volume = {82},
pages = {119-134},
year = {2016},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2016.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0166361516301129},
author = {Leonid Chechurin and Yuri Borgianni},
keywords = {TRIZ, Conceptual design, Industrial practice, Information processing, Computer-Aided Innovation},
abstract = {The development of the Theory of Inventive Problem Solving (TRIZ) has not followed the usual patterns of scientific validation required by engineering methods. Consequently, its outreach within engineering design is interpreted differently in the scholarly community. At the same time, the claimed powerful support in tackling technical problems of any degree of difficulty conflicts with TRIZ diffusion in industrial settings, which is relatively low according to insights into product development practices. The mismatch between ambitious goals and moderate spill-over benefits in the industry ranges among the various open issues concerning TRIZ, its way of thinking, its effectiveness, the usability of its tools. In order to provide a general overview of TRIZ in science, the authors have attempted to analyse reliable and influential sources from the literature. The performed survey includes the top 100 indexed publications concerning TRIZ, according to the number of received citations. Variegated and poorly interconnected research directions emerge in the abundant literature that tackles TRIZ-related topics. The outcomes of the investigation highlight the successful implementation of TRIZ within, among the others, biomimetics and information processing. The traditional borders of mechanical and industrial engineering have been frequently crossed, as the use of TRIZ is also witnessed in the domain of business and services. At the same time, computer-aided platforms represent diffused attempts to boost TRIZ diffusion and applicability.}
}
@article{HUANG2019592,
title = {Challenges, opportunities and paradigm of applying big data to production safety management: From a theoretical perspective},
journal = {Journal of Cleaner Production},
volume = {231},
pages = {592-599},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.05.245},
url = {https://www.sciencedirect.com/science/article/pii/S0959652619317810},
author = {Lang Huang and Chao Wu and Bing Wang},
keywords = {Big data, Production safety management, Big-data-driven, Challenges, Opportunities},
abstract = {Big data has caused the scientific community to re-examine the scientific research methodologies and has triggered a revolution in scientific thinking. As a branch of scientific research, production safety management is also exploring methods to take advantage of big data. This research aims to provide a theoretical basis for promoting the application of big data in production safety management. First, four different types of production safety management paradigms were identified, namely small-data-based, static-oriented, interpretation-based and causal-oriented paradigm, and the challenges to these paradigms in the presence of big data were introduced. Second, the opportunities of employing big data in production safety management were identified from four aspects, including better predict the future production safety phenomena, promote production safety management highlight relevance, achieve the balance between deductive and inductive approaches and promote the interdisciplinary development of production safety management. Third, the paradigm shifting trend of production safety management was concluded, and the discipline foundation of the new paradigm was considered as the integration of data science, production management and safety science. Fourth, a new big-data-driven production safety management paradigm was developed, which consists of the logical line of production safety management, the macro-meso-micro data spectrum, the key big data analytics, and the four-dimensional morphology. At last, the strengths (e.g., supporting better-informed safety description, safety inquisition, safety prediction) and future research direction (e.g., theory research focuses on safety-related data mining/capturing/cleansing) of the new paradigm were discussed. The research results not only can provide theoretical and practical basis for big-data-driven production safety management, but also can offer advice to managerial consideration and scholarly investigation.}
}
@article{ARBIB201883,
title = {From cybernetics to brain theory, and more: A memoir},
journal = {Cognitive Systems Research},
volume = {50},
pages = {83-145},
year = {2018},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2018.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041718301360},
author = {Michael A. Arbib},
keywords = {Action-oriented perception, Ape, Architecture, Artificial intelligence, Automata theory, Basal ganglia, Brain theory, Cerebellum, Cerebral cortex, Cognitive science, Computational neuroscience, Cybernetics, Frog, Hippocampus, Human, Language evolution, Linguistics, Monkey, Rat, Robotics, Schema theory, Social implications, Systems theory, Theological implications},
abstract = {While structured as an autobiography, this memoir exemplifies ways in which classic contributions to cybernetics (e.g., by Wiener, McCulloch & Pitts, and von Neumann) have fed into a diversity of current research areas, including the mathematical theory of systems and computation, artificial intelligence and robotics, computational neuroscience, linguistics, and cognitive science. The challenges of brain theory receive special emphasis. Action-oriented perception and schema theory complement neural network modeling in analyzing cerebral cortex, cerebellum, hippocampus, and basal ganglia. Comparative studies of frog, rat, monkey, ape and human not only deepen insights into the human brain but also ground an EvoDevoSocio view of “how the brain got language.” The rapprochement between neuroscience and architecture provides a recent challenge. The essay also assesses some of the social and theological implications of this broad perspective.}
}
@article{WYCKMANS2024100574,
title = {Impact of provoked stress on model-free and model-based reinforcement learning in individuals with alcohol use disorder},
journal = {Addictive Behaviors Reports},
volume = {20},
pages = {100574},
year = {2024},
issn = {2352-8532},
doi = {https://doi.org/10.1016/j.abrep.2024.100574},
url = {https://www.sciencedirect.com/science/article/pii/S2352853224000518},
author = {Florent Wyckmans and Armand Chatard and Charles Kornreich and Damien Gruson and Nemat Jaafari and Xavier Noël},
keywords = {Alcohol Use Disorder, Reinforcement Learning, Model-Based, Model-Free, Stress, Cortisol},
abstract = {Background
From both clinical and theoretical perspectives, understanding the functionality of evaluative reinforcement learning mechanisms (Model-Free, MF, and Model-Based, MB) under provoked stress, particularly in Alcohol Use Disorder (AUD), is crucial yet underexplored. This study aims to evaluate whether individuals with AUD who do not seek treatment show a greater tendency towards retrospective behaviors (MF) rather than prospective and deliberative simulations (MB) compared to controls. Additionally, it examines the impact of induced social stress on these decision-making processes.
Methods
A cohort comprising 117 participants, including 55 individuals with AUD and 62 controls, was examined. Acute social stress was induced through the socially evaluated cold pressor task (SECPT), followed by engagement in a Two-Step Markov task to assess MB and MF learning tendencies. We measured hypothalamic–pituitary–adrenal axis stress response using salivary cortisol levels.
Results
Both groups showed similar baseline cortisol levels and responses to the SECPT. Our findings indicate that participants with AUD exhibit a reduced reliance on MB strategies compared to those without AUD. Furthermore, stress decreases reliance on MB strategies in healthy participants, but this effect is not observed in those with AUD.
Conclusion
An atypical pattern of stress modulation impacting the balance between MB and MF reinforcement learning was identified in individuals with AUD who are not seeking treatment. Potential explanations for these findings and their clinical implications are explored.}
}
@article{DUCH201928,
title = {Mind as a shadow of neurodynamics},
journal = {Physics of Life Reviews},
volume = {31},
pages = {28-31},
year = {2019},
note = {Physics of Mind},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2019.01.023},
url = {https://www.sciencedirect.com/science/article/pii/S1571064519300971},
author = {Włodzisław Duch},
keywords = {Mind models, Neurodynamics, Physics of the mind, Mental spaces, Mental trajectories}
}
@article{MANFRE201612,
title = {Exploiting interactive genetic algorithms for creative humanoid dancing},
journal = {Biologically Inspired Cognitive Architectures},
volume = {17},
pages = {12-21},
year = {2016},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2016.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X16300378},
author = {Adriano Manfré and Agnese Augello and Giovanni Pilato and Filippo Vella and Ignazio Infantino},
keywords = {Robot, Dance, Computational creativity, Music perception, Co-creative tool},
abstract = {The paper discusses an approach aimed at endowing a cognitive architecture with artificial creativity capabilities in order to make a humanoid able to dance in a pleasant manner. The robot associates movements to music perception creating an aesthetically valuable dance by using a Hidden Markov Model with a nonclassical approach. Two matrices mainly influence the model: a Transition matrix TM, and an Emission Matrix EM. The TM matrix rules the transition between two subsequent movements. The EM matrix constitutes the link between a set of movements and the perceived music features. In order to compute the EM matrix, we exploit a genetic algorithm approach. The approach makes use of two kinds of fitness functions. The first one is an internal evaluation fitness that allows the robot to autonomously learn the association between music and movements. The second one depends on the interaction with a human teacher, leading to the determination of different dance styles, which constitute the robot repertoire. The experimental part discusses the effects on the creativity of different distances to compute fitness.}
}
@article{GUIMARAES2019242,
title = {Extension of Reward-Attention Circuit Model: Alcohol’s Influence on Attentional Focus and Consequences on Autism Spectrum Disorder},
journal = {Neurocomputing},
volume = {325},
pages = {242-253},
year = {2019},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2018.10.034},
url = {https://www.sciencedirect.com/science/article/pii/S0925231218312220},
author = {Karine Guimarães},
keywords = {Alcohol, Dopamine, Attention, Autism spectrum disorder, Computational neuroscience},
abstract = {Attention is a key element that allows us to enhance or decrease the cognitive processing of distinct stimuli, depending on their relevance. In this work we investigate the influence that alcohol exerts on attention focusing, modeling the coupling of reward and thalamocortical circuits. Computer simulations of the reward-attention circuit reflect the spiking behavior of each neuron in the network, under the presence or absence of alcohol. Each neuron in the neural networks that replicate such circuits is described by a carefully designed coupled system of nonlinear differential equations that details essential neurophysiological properties. The computational simulations highlight aspects of clinical inattention symptoms in the autism spectrum disorder. Our results indicate that alcohol may lead to distraction or lack of attentional focus. Also, the simulations suggest why people with ASD might relaxes enhanced attentional focus when exposed to alcohol.}
}
@article{LAPIDUS202183,
title = {The road less traveled in protein folding: evidence for multiple pathways},
journal = {Current Opinion in Structural Biology},
volume = {66},
pages = {83-88},
year = {2021},
note = {Centrosomal Organization and Assemblies ● Folding and Binding},
issn = {0959-440X},
doi = {https://doi.org/10.1016/j.sbi.2020.10.012},
url = {https://www.sciencedirect.com/science/article/pii/S0959440X20301809},
author = {Lisa J Lapidus},
abstract = {Free Energy Landscape theory of Protein Folding, introduced over 20 years ago, implies that a protein has many paths to the folded conformation with the lowest free energy. Despite the knowledge in principle, it has been remarkably hard to detect such pathways. The lack of such observations is primarily due to the fact that no one experimental technique can detect many parts of the protein simultaneously with the time resolution necessary to see such differences in paths. However, recent technical developments and employment of multiple experimental probes and folding prompts have illuminated multiple folding pathways in a number of proteins that had all previously been described with a single path.}
}
@article{CYSEWSKI201623,
title = {Efficacy of bi-component cocrystals and simple binary eutectics screening using heat of mixing estimated under super cooled conditions},
journal = {Journal of Molecular Graphics and Modelling},
volume = {68},
pages = {23-28},
year = {2016},
issn = {1093-3263},
doi = {https://doi.org/10.1016/j.jmgm.2016.06.003},
url = {https://www.sciencedirect.com/science/article/pii/S1093326316300870},
author = {Piotr Cysewski},
keywords = {Cocrystals, Simple binary eutectics, Theoretical screening, Heat of mixing, Confusion matrix, COSMO-RS, COSMOtherm},
abstract = {The values of excess heat characterizing sets of 493 simple binary eutectic mixtures and 965 cocrystals were estimated under super cooled liquid condition. The application of a confusion matrix as a predictive analytical tool was applied for distinguishing between the two subsets. Among seven considered levels of computations the BP-TZVPD-FINE approach was found to be the most precise in terms of the lowest percentage of misclassified positive cases. Also much less computationally demanding AM1 and PM7 semiempirical quantum chemistry methods are likewise worth considering for estimation of the heat of mixing values. Despite intrinsic limitations of the approach of modeling miscibility in the solid state, based on components affinities in liquids under super cooled conditions, it is possible to define adequate criterions for classification of coformers pairs as simple binary eutectics or cocrystals. The predicted precision has been found as 12.8% what is quite accepted, bearing in mind simplicity of the approach. However, tuning theoretical screening to such precision implies the exclusion of many positive cases and this wastage exceeds 31% of cocrystals classified as false negatives.}
}
@article{JI2023126734,
title = {Experimental and numerical investigation on a radiative cooling driving thermoelectric generator system},
journal = {Energy},
volume = {268},
pages = {126734},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2023.126734},
url = {https://www.sciencedirect.com/science/article/pii/S0360544223001287},
author = {Yishuang Ji and Song Lv},
keywords = {Radiative cooling, Thermoelectric generator, Hybrid system, Thermal-electrical properties, Numerical analysis},
abstract = {Thermoelectric (TE) technology and radiative sky cooling (RSC) technology have proven to be a promising and green way to harvest energy from the environment. Combining RSC technology with Thermoelectric generator (TEG) device for passive power generation at night is meaningful and remains a challenge. Here, a radiative sky cooling driving thermoelectric generator (RSC-TE) system integrated by a doped modified TiO2/PMMA radiative cooling film, a commercial TEG, and an aluminum heat sink is developed, with a simple structure, low cost and high efficiency. The thermal-electrical performance of the RSC-TE system was evaluated through a consecutive nighttime experiment. Experimental results show that the temperature of the cold side of the TEG in contact with the radiative cooler is 2.7–4.2 °C lower than the ambient temperature, and the temperature difference between the hot and cold sides of TEG is 2.3–3.2 °C. The temperature difference at 00:00 can reach 2.5 °C, which corresponds to an open circuit voltage of 87 mV. Furthermore, a 3D model has been established by COMSOL software to investigate the effects of different environmental parameters and component-related parameters on system performance, which has guiding significance for the improvement and optimization of the experimental setup. This study can provide a new thinking and some practical guidelines for the design and application of the RSC-TE system.}
}
@article{CARDONAVASQUEZ2024110267,
title = {Enhancing time series aggregation for power system optimization models: Incorporating network and ramping constraints},
journal = {Electric Power Systems Research},
volume = {230},
pages = {110267},
year = {2024},
issn = {0378-7796},
doi = {https://doi.org/10.1016/j.epsr.2024.110267},
url = {https://www.sciencedirect.com/science/article/pii/S037877962400155X},
author = {David Cardona-Vasquez and Thomas Klatzer and Bettina Klinz and Sonja Wogrin},
keywords = {Power systems optimization, Mathematical modeling, Dimensionality reduction, Renewable energy sources, Time series aggregation, Linear programming},
abstract = {In this paper, we extend a recently developed Basis-Oriented time series aggregation approach for aggregating input-data in power system optimization models which has proven to be exact in simple economic dispatch problems. We extend this methodology to include network and ramping constraints, for the latter, to handle temporal linking, we developed a heuristic that, in its current version, relies on the dual solution to find a partition of the input data, which is then aggregated. Our numerical results, for a 3-bus system, show that with network constraints only, we reduced the number of hours needed for an exact approximation by a factor of 1747, and a factor of 12 with network and ramping constraints. Moreover, our findings suggest that in the presence of temporal linking, aggregations of variable length must be employed to obtain an exact result (i.e., the same objective function value in the aggregated model) while maintaining the computational tractability. Our findings also imply that better performing aggregations do not necessarily correspond to commonly used lengths like days or weeks; additionally, we also prove that this input-data partition, based on the dual information, is always possible for these models independent of their size.}
}
@article{MAHY201468,
title = {How and where: Theory-of-mind in the brain},
journal = {Developmental Cognitive Neuroscience},
volume = {9},
pages = {68-81},
year = {2014},
issn = {1878-9293},
doi = {https://doi.org/10.1016/j.dcn.2014.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S1878929314000048},
author = {Caitlin E.V. Mahy and Louis J. Moses and Jennifer H. Pfeifer},
keywords = {Theory of mind, Neuroimaging, Modularity, Theory theory, Simulation, Executive functioning},
abstract = {Theory of mind (ToM) is a core topic in both social neuroscience and developmental psychology, yet theory and data from each field have only minimally constrained thinking in the other. The two fields might be fruitfully integrated, however, if social neuroscientists sought evidence directly relevant to current accounts of ToM development: modularity, simulation, executive, and theory theory accounts. Here we extend the distinct predictions made by each theory to the neural level, describe neuroimaging evidence that in principle would be relevant to testing each account, and discuss such evidence where it exists. We propose that it would be mutually beneficial for both fields if ToM neuroimaging studies focused more on integrating developmental accounts of ToM acquisition with neuroimaging approaches, and suggest ways this might be achieved.}
}
@article{DIMARTINO2023138293,
title = {A comprehensive classification of food–energy–water nexus optimization studies: State of the art},
journal = {Journal of Cleaner Production},
volume = {420},
pages = {138293},
year = {2023},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2023.138293},
url = {https://www.sciencedirect.com/science/article/pii/S0959652623024514},
author = {Marcello {Di Martino} and Patrick Linke and Efstratios N. Pistikopoulos},
keywords = {Food–energy–water nexus, Resource supply systems, Process systems engineering, Optimization, Sustainability},
abstract = {To tackle the globally increasing discrepancy between food, energy and water demands and resource availability sustainably, resource supply system models have to incorporate the inter-dependencies and -connectivities to other supply systems. This leads naturally to a food–energy–water nexus (FEWN) approach. The FEWN can be interpreted as the study of the connections between the food, energy and water resource systems, emphasizing how decision-making influences the synergies, conflicts and trade-offs among the various sectors. In recent years, modeling and optimization of FEWN systems has been receiving an increasing interest in the open literature, however, with limited emphasis on how decisions of the FEWN are derived. In this review, FEWN optimization studies are analyzed with focus on the employed objectives, optimization and solution strategies, as well as the selected sub-systems and their corresponding spatial and temporal scales to uncover in detail how decision-making is facilitated. More specifically, FEWN optimization studies are classified according to their modeling and solution strategies. Based on this classification it is uncovered that (i) the decision-making itself has not yet been investigated in detail in FEWN literature, (ii) the incorporation of all aspects of the FEWN is still a challenge, (iii) the interconnection between FEWN systems and society has to be further investigated, and (iv) the implications of uncertainty for the resiliency, robustness and security of process systems is not yet well defined. Additionally, a generic FEWN resource-task network formulation is introduced to illustrate the similarities across the various resource supply sectors. Special interest is placed on how synergies are identified and competition be avoided among resource systems. It is shown that the selected spatial scale as well as the utilized modeling and optimization strategies significantly influence the synergy level of obtained solutions. Furthermore, it is derived that the energy transition has to incorporate FEWN systems thinking for sustainable solution generation. Overall, this review summarizes the different applications and implications of process systems engineering concepts to FEWN systems.}
}
@article{LAU2017241,
title = {The many worlds hypothesis of dopamine prediction error: implications of a parallel circuit architecture in the basal ganglia},
journal = {Current Opinion in Neurobiology},
volume = {46},
pages = {241-247},
year = {2017},
note = {Computational Neuroscience},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2017.08.015},
url = {https://www.sciencedirect.com/science/article/pii/S0959438817301587},
author = {Brian Lau and Tiago Monteiro and Joseph J Paton},
abstract = {Computational models of reinforcement learning (RL) strive to produce behavior that maximises reward, and thus allow software or robots to behave adaptively [1]. At the core of RL models is a learned mapping between ‘states’—situations or contexts that an agent might encounter in the world—and actions. A wealth of physiological and anatomical data suggests that the basal ganglia (BG) is important for learning these mappings [2, 3]. However, the computations performed by specific circuits are unclear. In this brief review, we highlight recent work concerning the anatomy and physiology of BG circuits that suggest refinements in our understanding of computations performed by the basal ganglia. We focus on one important component of basal ganglia circuitry, midbrain dopamine neurons, drawing attention to data that has been cast as supporting or departing from the RL framework that has inspired experiments in basal ganglia research over the past two decades. We suggest that the parallel circuit architecture of the BG might be expected to produce variability in the response properties of different dopamine neurons, and that variability in response profile may not reflect variable functions, but rather different arguments that serve as inputs to a common function: the computation of prediction error.}
}
@article{WANG2025127400,
title = {Research on product design improvement method based on online review and improvement importance performance competitor analysis},
journal = {Expert Systems with Applications},
volume = {279},
pages = {127400},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127400},
url = {https://www.sciencedirect.com/science/article/pii/S095741742501022X},
author = {Zeng Wang and Shi-jie Hu and Shi-fan Niu and Si-yi Li and Wei-dong Liu and Ling-yu Huang},
keywords = {Online reviews, Product design improvement, Importance-performance competitor analysis, Deep learning, TRIZ},
abstract = {To mitigate the shortcomings observed in current online review-based Importance-Performance Competitor Analysis (IPCA) model and the limitations inherent in extant performance evaluation algorithms, this research proposes a novel method for product design improvement leveraging online review-based Textual Importance-Performance Competitor Analysis (TIPCA). Initially, product attributes and their respective sentiment polarities within online reviews are scrutinized using the GRU-CAP, a composite deep learning model. Subsequently, the TIPCA model facilitates the computation of performance metrics and significance, leading to the formulation of a three-dimensional analysis graph that delineates critical avenues for product enhancement. The application of TRIZ then guides the derivation of an optimized product design proposal. Empirical assessments utilizing a neck massager and a new energy vehicle demonstrate the method’s adeptness in accurately identifying and articulating pathways for product enhancement. This approach provides designers with a robust and innovative blueprint for design, enhancing creative development. The method facilitates a meticulous analysis and strategic refinement of product designs, anchored in a comprehensive theoretical framework and corroborated by practical implementation.}
}