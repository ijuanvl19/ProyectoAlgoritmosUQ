@article{TRICHKOVAKASHAMOVA2024123,
title = {Criteria and Approaches for Optimization of Innovative Methods for STEM Education},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {3},
pages = {123-128},
year = {2024},
note = {22nd IFAC Conference on Technology, Culture and International Stability TECIS 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.07.137},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324002209},
author = {E. Trichkova-Kashamova and E. Paunova-Hubenova and Y. Boneva and S. Dimitrov},
keywords = {STEM education, innovative educational methods, optimisation, data analyses, technology-based learning},
abstract = {The proposed research aims to evaluate the modern learning process in STEM subjects in a technology-rich environment. The study examines contemporary teaching methods and evaluates their application in different educational levels in Bulgaria. The aim is to provide information for developing a concept of a modern technology-based learning process and integrating innovative methods with appropriate technological tools.}
}
@article{SHAN201032,
title = {Study on large time-delay constant temperature control system based on TEC},
journal = {The Journal of China Universities of Posts and Telecommunications},
volume = {17},
pages = {32-35},
year = {2010},
issn = {1005-8885},
doi = {https://doi.org/10.1016/S1005-8885(09)60586-0},
url = {https://www.sciencedirect.com/science/article/pii/S1005888509605860},
author = {Jiang-dong SHAN and Ge WU and Xiao-jian TIAN},
keywords = {thermoelectric cooler, large time-delay control system, proportion integration differentiation (PID) control, constant temperature control},
abstract = {This paper designes a diminutive constant temperature control system based on thermoelectric cooler (TEC). Considering that the system is a large time-delay control system, the paper proposes a new method to determine the transfer function of the controlled system which gets the transfer function by doing nonlinear fitting of the step response of the controlled system. The characteristics of system model which is established by the method are basically same as the actual constant temperature control system. This method provides a new way of thinking to the design of large time-delay control system.}
}
@article{ELMAIZI2019126,
title = {A novel information gain based approach for classification and dimensionality reduction of hyperspectral images},
journal = {Procedia Computer Science},
volume = {148},
pages = {126-134},
year = {2019},
note = {THE SECOND INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTING IN DATA SCIENCES, ICDS2018},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.01.016},
url = {https://www.sciencedirect.com/science/article/pii/S187705091930016X},
author = {Asma Elmaizi and Hasna Nhaila and Elkebir Sarhrouni and Ahmed Hammouch and Chafik Nacir},
keywords = {Hyperspectral images, dimentionality reduction, information gain, classification accuracy},
abstract = {Recently, the hyperspectral sensors has improved our ability to monitor the earth surface with high spectral resolution. However, the high dimensionality of spectral data brings challenges for the image processing. Consequently, the dimensionality reduction is a necessary step in order to reduce the computational complexity and increase the classification accuracy. In this paper, we propose a new filter approach based on information gain for dimensionality reduction and classification of hyperspectral images. A special strategy based on hyperspectral bands selection is adopted to pick the most informative bands and discard the irrelevant and noisy ones. The algorithm evaluates the relevancy of the bands based on the information gain function with the support vector machine classifier. The proposed method is compared using two benchmark hyperspectral datasets (Indiana, Pavia) with three competing methods. The comparison results showed that the information gain filter approach outperforms the other methods on the tested datasets and could significantly reduce the computation cost while improving the classification accuracy.}
}
@article{FUSHING2023129227,
title = {Multiscale major factor selections for complex system data with structural dependency and heterogeneity},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {630},
pages = {129227},
year = {2023},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2023.129227},
url = {https://www.sciencedirect.com/science/article/pii/S0378437123007823},
author = {Hsieh Fushing and Elizabeth P. Chou and Ting-Li Chen},
keywords = {Broken symmetry, Conditional entropy, Contingency table, Major factor selection, Multiclass Classification, Pitching dynamics},
abstract = {The unknown multiscale structure hidden in large complex systems is explored bottom-up through discovered heterogeneity under structural dependency embedded within structured data sets. Via two real complex systems, we demonstrate computed hierarchical structures with broken symmetry constituting data’s information content. Through graphic displays, such information content indirectly, but efficiently resolves system-related scientific issues that are difficult to resolve directly. All bottom-up explorations and computations are based on conditional entropy and mutual information evaluated upon contingency table platforms after categorizing all quantitative features. Categorical Exploratory Data Analysis (CEDA) first extracts global major factors that share significant mutual information with the targeted response (Re) variable against many covariate (Co) features under the presence of structural dependency. Then each global major factor is taken as one perspective of heterogeneity to subdivide the entire data set according to its categories into sub-collections. This simple “de-associating” protocol significantly reduces structural dependency among the rest of the features such that another run of major factor selection performed on the sub-collection scale can precisely identify which feature sets could provide extra information beyond the global major factor. Finally, informative patterns collected from multiple perspectives of heterogeneity are displayed to explicitly resolve issues of prediction, classification, and detecting minute dynamic changes.}
}
@article{NEDIC2019224,
title = {OP0284 PARE THE INFLUENCE OF FUNCTIONAL TRAINING AND PSYCHO-SOCIAL SUPPORT},
journal = {Annals of the Rheumatic Diseases},
volume = {78},
pages = {224},
year = {2019},
issn = {0003-4967},
doi = {https://doi.org/10.1136/annrheumdis-2019-eular.4256},
url = {https://www.sciencedirect.com/science/article/pii/S0003496724042614},
author = {Nenad Nedić and Mirjana Lapčević},
abstract = {Background
Treatment of chronic noncontagious diseases in which we include RMD implies usage of medicines and non medicine (changing bad lifestyle habits like losing nutrition, physical activity, smoking…).1 Unwanted cardiovascular and cerebrovascular states are the most common cause of shortening of live of people with RMD.2 None of the chemical drugs can replace physical activity. Physical activity dosage is individual, and depends on aerobic capability and heart rate increase, taking in account age, type of noncontagious diseases, level of tissue and organs damage as well as the type of work the person is engaged in. Patients with RMD have common symptoms, such as stiffness, fatigue, poor mobility, joint pain and muscle pain, anxiety and depression, and lack of fitness. In addition to physical medicine and rehabilitation and balneoklimatology, various forms of physical activity are recommended, such as walking, swimming, functional training. Today, moderate physical activity is known to help reduce fatigue, strengthen muscles and bones, improve flexibility and endurance of the joints, and improve general health. It is necessary to find the best combination of rest, activities and exercise programs to prevent deformities of the joints, the development of disability, improve the quality of life, and the mental health of patients with RMD.3
Objectives
1. By practicing Cigong, an increase in the volume of movement in the joint, the strengthening of joint muscles and the improvement of general condition, pain relief, fatigue reduction is achieved and also, it helps patients look and feel better. 2. The goal of Yoga is to neutralize and remove all obstacles that stand in the way and disturb the function of the body and the mind and achieves inner peace. 3. Changing the psychological state during exercise also led to a positive way of thinking. 4. Psycho-physical support for the people with RMD
Methods
From 2011, we organize two times a week functional training of Cigong.4 Since January 2016, twice a week persons with RMD have been practicing Yoga.5 From 2015, one per week four psychologists volunteer hold workshops for psycho-social support for persons with RMD.6,7 Participants of the training and psycho-social sessions took a survey.
Results
1. Joint pain reduction – 50% of total number of participants 2. Joint mobility increase – 95% of total number of participants 3. General fitness improvement – 73% of total number of participants 4. Less pronounced negative emotions – 59% of total number of participants
Conclusion
Practicing cigong, yoga and psycho-social support workshops help patients look better and feel better. Changing the psychological state during exercise also led to a positive way of thinking. All of this increased the effectiveness of drug treatment and improved the quality of life of patients with RMD.
References
[1] Dragojević R. Vodič za zdrav život: preporuke medicine i pouke mudrosti, Beograd 2017. [2] Lapcević M., Vuković M. Dimitrijevic I. Et all Uticaj medikamentnog I nemedikamentnog lečenja na smanjenje faktora rizika za kardiovaskularne I cerebrovaskularne događaje u interventnoj studiji. Srp Arh Celok Lek 2007. [3] Lapčević M, Prvanov D, Đorđević S. Procena kvaliteta života obolelih od hroničnih reumatskih oboljenja. Opšta medicina 2010. [4] Ilinka Acimovic, “The Influence of Health Qigong on the Subjectively Expressed Psychophysical State of Patients with Rheumatoid Arthritis, Rheum, Osteoporosis, Osteopenia” CHINESE MEDICINE AND CULTURE [5] Swami Kriyananda, “Demystifying Patanjali: The Yoga Sutras - The Wisdom of Paramhansa Yogananda”. Crystal Clarity Publishers, Nevada City, CA, 2013. [6] Lapčević M, et al. Socioeconomic and therapy factor influence on self-reported fatigue, anxiety and depression in rheumatoid arthritis patients. Rev Bras Reumatol. 2017. [7] Milić V. Analiza ličnosti I uticaj optimizma na pozitivan ishod lečenja; 2018.
Disclosure of Interests
None declared}
}
@article{YANG2024106650,
title = {Integrating parcel delivery schedules with public transport networks in urban co-modality systems},
journal = {Computers & Operations Research},
volume = {167},
pages = {106650},
year = {2024},
issn = {0305-0548},
doi = {https://doi.org/10.1016/j.cor.2024.106650},
url = {https://www.sciencedirect.com/science/article/pii/S0305054824001229},
author = {Xuan Yang and Xinyao Nie and Hao Luo and George Q. Huang},
keywords = {Logistics, Public transport, Co-modality, Parcel assignment},
abstract = {Co-modality transportation advocates using urban public transport to support urban freight operations. This study considers the implementation of co-modality in a fixed-route transit network comprising multiple lines following predetermined routes and schedules. We first develop a schedule-based parcel assignment model to formulate the synchronized co-modality transportation problem (SCTP). The effectiveness of the proposed arc-based meta-heuristic algorithm is substantiated through a comprehensive computational analysis, comparing its performance with that of an exact approach and genetic algorithm. Our findings reveal a nuanced trade-off between transportation efficiency and co-modal stop utilization, identifying a threshold beyond which additional stops do not improve efficiency but increase costs. We also discover a 'buckets effect' in co-modal capacities, suggesting that balanced vehicle and stop capacities are crucial for optimizing system performance. A case study with real urban transit data validates our model's potential for significant efficiency gains in co-modality transportation systems, offering actionable insights for urban logistics.}
}
@article{LEE2025111886,
title = {Project symphony: Composing a masterpiece in a science laboratory},
journal = {iScience},
volume = {28},
number = {2},
pages = {111886},
year = {2025},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2025.111886},
url = {https://www.sciencedirect.com/science/article/pii/S2589004225001464},
author = {Sunghee Lee and Jamie Gudyka and Marnie Skinner and Jasmin Ceja-Vega and Amani Rabadi and Christopher Poust and Caroline Scott and Micaela Panella and Elizabeth Andersen and Jessica Said},
abstract = {In the spirit of collaborative science, Prof. Sunghee Lee (Chemistry Professor at Iona University in New York, USA) embarked on her academic career with a vision to bring an interdisciplinary approach to undergraduate education. At a Predominantly Undergraduate Institution (PUI) such as Iona, she saw a unique opportunity to weave together teaching and research, creating a rich tapestry of learning experiences for students. Her goal was simple yet ambitious: to use research as a bridge connecting classroom theory to real-world interdisciplinary scientific practice. In this Backstory, Sunghee and her students and recent graduates reflect on the development and experiences that shaped their journey through Project Symphony and the resulting skills they’ve learned. The symphony they’ve created together is a testament to the transformative power of collaborative undergraduate research – a melody of discovery that continues to evolve and inspire.}
}
@article{THEODOROU20075697,
title = {Hierarchical modelling of polymeric materials},
journal = {Chemical Engineering Science},
volume = {62},
number = {21},
pages = {5697-5714},
year = {2007},
issn = {0009-2509},
doi = {https://doi.org/10.1016/j.ces.2007.04.048},
url = {https://www.sciencedirect.com/science/article/pii/S000925090700382X},
author = {Doros N. Theodorou},
keywords = {Polymers, Mathematical modelling, Simulation, Rheology, Nanostructure, Diffusion},
abstract = {Within the last 20 years, computer simulations of materials have evolved from an academic curiosity to a predictive tool for addressing structure–property–processing–performance relations that are critical to the design of new products and processes. Chemical engineers, with their problem-oriented thinking and their systems approach, have played a significant role in this development. The computational prediction of physical properties is particularly challenging for polymeric materials, because of the extremely broad spectra of length and time scales governing structure and molecular motion in these materials. This challenge can only be met through the development of hierarchical analysis and simulation strategies encompassing many interconnected levels, each level addressing phenomena over a specific window of time and length scales. In this paper we will briefly discuss the fundamental underpinnings and example applications of new methods and algorithms for the hierarchical modelling of polymers. Questions to be addressed include: How can one equilibrate atomistic models of long-chain polymer melts at all length scales and thereby predict thermodynamic and conformational properties reliably? How can one quantify the structure of entanglement networks present in these melts through topological analysis and relate it to rheological properties? Are there ways to predict the microphase-separated morphology and stress–strain behaviour of multicomponent block copolymer-based materials, such as pressure sensitive adhesives? Is it possible to anticipate changes in the barrier properties of glassy amorphous polymers used in packaging applications as a consequence of modifications in the chemical constitution of chains?}
}
@incollection{PETROVIC2025163,
title = {A Historical and Current Look at Chemical Design for Reduced Hazard},
editor = {Béla Török},
booktitle = {Encyclopedia of Green Chemistry (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {163-172},
year = {2025},
isbn = {978-0-443-28923-1},
doi = {https://doi.org/10.1016/B978-0-443-15742-4.00072-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780443157424000727},
author = {Predrag V. Petrovic and Philip Coish and Paul T. Anastas},
keywords = {Benign by design, CAMD, First do no harm, Green chemistry, Hazard reduction, Safer chemical design, Sustainable chemistry},
abstract = {Green chemistry aims to design chemical products and processes that reduce or eliminate the use and generation of hazardous substances. The design of chemicals for reduced hazard selects the properties and attributes that provide the desired function while avoiding unintended consequences for human health and the environment. The tools and knowledge that are available to designers have increased considerably in recent years. Importantly, the approach and goals of design have evolved and become more holistic, multi-disciplinary, and inclusive. This is notable as the global population׳s continued economic and social well-being cannot be maintained with measures that destroy our natural environment. Sustainable chemistry promotes, advances, enables, and empowers the implementation of the chemistry of sustainability and includes chemical design for reduced hazard. In this chapter, we consider the design of chemicals for reduced hazard (safer chemical design) focusing on the past and present, and importantly, looking to the future on what lies ahead.}
}
@incollection{LIANG2023263,
title = {Teacher skills and knowledge for technology integration},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {263-271},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.04037-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305040379},
author = {Leming Liang and Nancy Law},
keywords = {Technology-enhanced learning, Teacher skills, Teacher knowledge, TPACK, Teacher learning, AR/VR, AI, Ethics, Pedagogical innovation, Teacher leadership},
abstract = {With the pervasive use of digital technology in all aspects of our lives, and the rapid advances and deployment of new technologies in education, the use and teaching of technology in education is a necessary professional repertoire for teachers to ensure students' well-being. Influential education policy frameworks posit that teachers need competence in the integration of technology for agile delivery of teaching in blended/fully online modes and for fostering students' digital literacy. Future-oriented professional development programs need to go beyond individual learning and engage leadership in creating the necessary conditions for technology-enhanced learning innovations at institutional and system levels.}
}
@article{SHI2024100685,
title = {Drug development in the AI era: AlphaFold 3 is coming!},
journal = {The Innovation},
volume = {5},
number = {5},
pages = {100685},
year = {2024},
issn = {2666-6758},
doi = {https://doi.org/10.1016/j.xinn.2024.100685},
url = {https://www.sciencedirect.com/science/article/pii/S2666675824001231},
author = {Yi Shi}
}
@article{ANDIC2023490,
title = {A robust crow search algorithm based power system state estimation},
journal = {Energy Reports},
volume = {9},
pages = {490-501},
year = {2023},
note = {Proceedings of 2022 7th International Conference on Renewable Energy and Conservation},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2023.09.075},
url = {https://www.sciencedirect.com/science/article/pii/S2352484723013124},
author = {Cenk Andic and Ali Ozturk and Belgin Turkay},
keywords = {Crow search algorithm, Power systems, State estimation},
abstract = {The State Estimation (SE) computational procedure plays a crucial role in modern electric power system security control by monitoring and analyzing operational conditions and predicting any emergency. In order to estimate state variables, Power System State Estimation (PSSE) takes into account the magnitudes and phases of voltage on each bus. To address the state estimation challenges in power systems, in this paper, we propose a novel application of the Crow Search Algorithm (CSA) specifically tailored for the state estimation problem. We have assessed the introduced algorithm using the frameworks of both the IEEE 14-bus and IEEE 30-bus test systems. The first formulation is the Weighted Least Square (WLS) method, and the second is the Weighted Least Absolute Value (WLAV) method, both of which are objective function formulations. By comparing the results, it is clear that CSA-based SE is superior to the other metaheuristic algorithms considered, namely Genetic Algorithm (GA), Particle Swarm Optimization (PSO), and Artificial Bee Swarm Optimization (ABSO). As a point of comparison, we use the Newton–Raphson method for calculating load flow. It has been shown that the proposed CSA-based SE technique has better accuracy than the other two algorithms in all different test systems. With this study, the power system is operated more accurately and reliably by the operators operating the system.}
}
@incollection{GOUGH2024547,
title = {25 - Mycelium-based materials for the built environment: a case study on simulation, fabrication and repurposing myco-materials},
editor = {Emina Kristina Petrović and Morten Gjerde and Fabricio Chicca and Guy Marriage},
booktitle = {Sustainability and Toxicity of Building Materials},
publisher = {Woodhead Publishing},
pages = {547-571},
year = {2024},
series = {Woodhead Publishing Series in Civil and Structural Engineering},
isbn = {978-0-323-98336-5},
doi = {https://doi.org/10.1016/B978-0-323-98336-5.00025-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032398336500025X},
author = {Phillip Gough and Anastasia Globa and Dagmar Ingrid Elfriede Reinhardt},
keywords = {Biodegradable building materials, mycelium, circular economy, materials science, case study research, digital printing},
abstract = {Remanufacturing organic waste for composite materials is an opportunity to create circular economic approaches in construction. Research shows how the mycelium (root network) of some fungi, such as Reishi mushrooms, can be guided and formed to create sustainable, biodegradable composite myco-materials for a range of applications. As they degrade wood, paper or coffee waste, Reishi mushrooms create a new material with potential applications in architecture. Research into biocomposites and eco-materials such as mycelium integrates advanced digital fabrication processes, complex structures and algorithmically generated forms for packaging, thermal and sound insulation or as cladding and structural materials. Significantly, the capacity of mycelium composites to seamlessly return as resource material after use in a sustainable ecological cycle indicates the potential for a new approach to sustainability in building and construction processes across the lifespan of buildings. The adaptation of a living organism as an interactive architecture building module could allow strategies for signalling and negotiating climatic variations and local site conditions around the building. Moreover, the capacity of mycelia to not only thrive in contaminated industrial wastelands but to actively detoxify and colonise could further be employed for bioremediation. The research discussed here presents an empirical study into mycelium composites, using computational design and desktop 3D printing to identify strengths and limitations of material, moulds, growth and form. The case study demonstrates how substrate inclusions impact the formation of the material, opportunities to re-awaken inert myco-material for new growth and the quality of fine-detailed elements, using domestic technology and readily available materials. We contribute a taxonomy of existing uses and applications of myco-materials and a range of implications for the process of designing with myco-materials in architecture.}
}
@article{BATTLEDAY20151865,
title = {Modafinil for cognitive neuroenhancement in healthy non-sleep-deprived subjects: A systematic review},
journal = {European Neuropsychopharmacology},
volume = {25},
number = {11},
pages = {1865-1881},
year = {2015},
issn = {0924-977X},
doi = {https://doi.org/10.1016/j.euroneuro.2015.07.028},
url = {https://www.sciencedirect.com/science/article/pii/S0924977X15002497},
author = {R.M. Battleday and A.-K. Brem},
keywords = {Neuroenhancement, Modafinil, Cognitive, Psychometric, Enhancement, Nootropic},
abstract = {Modafinil is an FDA-approved eugeroic that directly increases cortical catecholamine levels, indirectly upregulates cerebral serotonin, glutamate, orexin, and histamine levels, and indirectly decreases cerebral gamma-amino-butrytic acid levels. In addition to its approved use treating excessive somnolence, modafinil is thought to be used widely off-prescription for cognitive enhancement. However, despite this popularity, there has been little consensus on the extent and nature of the cognitive effects of modafinil in healthy, non-sleep-deprived humans. This problem is compounded by methodological discrepancies within the literature, and reliance on psychometric tests designed to detect cognitive effects in ill rather than healthy populations. In order to provide an up-to-date systematic evaluation that addresses these concerns, we searched MEDLINE with the terms “modafinil” and “cognitive”, and reviewed all resultant primary studies in English from January 1990 until December 2014 investigating the cognitive actions of modafinil in healthy non-sleep-deprived humans. We found that whilst most studies employing basic testing paradigms show that modafinil intake enhances executive function, only half show improvements in attention and learning and memory, and a few even report impairments in divergent creative thinking. In contrast, when more complex assessments are used, modafinil appears to consistently engender enhancement of attention, executive functions, and learning. Importantly, we did not observe any preponderances for side effects or mood changes. Finally, in light of the methodological discrepancies encountered within this literature, we conclude with a series of recommendations on how to optimally detect valid, robust, and consistent effects in healthy populations that should aid future assessment of neuroenhancement.}
}
@article{BORIS2013113,
title = {Flux-Corrected Transport looks at forty},
journal = {Computers & Fluids},
volume = {84},
pages = {113-126},
year = {2013},
issn = {0045-7930},
doi = {https://doi.org/10.1016/j.compfluid.2013.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0045793013001874},
author = {Jay Paul Boris},
keywords = {Flux-Corrected Transport (FCT), Monotonicity, Positivity, Computational Fluid Dynamics (CFDs), Large Eddy Simulation (LES), Monotone Integrated Large Eddy Simulation (MILES), Implicit Large Eddy Simulation (ILES)},
abstract = {This year, 2013, marks the 40th anniversary of the journal article “Flux-Corrected Transport I. SHASTA, A Fluid Transport Algorithm That Works” by Jay Boris and David Book [1]. Flux-Corrected Transport (FCT) removed a serious roadblock to advances in Computational Fluid Dynamics (CFD) by enabling the accurate treatment of strong, time-dependent shock problems in blast, reactive-flow, and combustion physics, and in aerodynamics and astrophysics. Steep gradients in conserved fluid variables could now be convected across a computational grid without the appearance of spurious oscillations and physically impossible negative values. The nonlinear “flux-correction” algorithm introduced in FCT imposes the physical properties of conservation, locality, causality, and monotonicity on the numerical solutions for convection without adding a great deal of numerical diffusion. This article shows that implementing these physical properties in solving the continuity equation through high-resolution FCT also results in a serviceable Large-Eddy Simulation treatment of turbulent flows without need for additional “subgrid turbulence models.” We have named this simplified approach Monotone Integrated Large Eddy Simulation (MILES).}
}
@article{CRONIN2022100213,
title = {A review of in silico toxicology approaches to support the safety assessment of cosmetics-related materials},
journal = {Computational Toxicology},
volume = {21},
pages = {100213},
year = {2022},
issn = {2468-1113},
doi = {https://doi.org/10.1016/j.comtox.2022.100213},
url = {https://www.sciencedirect.com/science/article/pii/S2468111322000019},
author = {Mark T.D. Cronin and Steven J. Enoch and Judith C. Madden and James F. Rathman and Andrea-Nicole Richarz and Chihae Yang},
keywords = {Cosmetics, Risk assessment, , Computational, Read-across, Quantitative structure-activity relationship},
abstract = {In silico tools and resources are now used commonly in toxicology and to support the “Next Generation Risk Assessment” (NGRA) of cosmetics ingredients or materials. This review provides an overview of the approaches that are applied to assess the exposure and hazard of a cosmetic ingredient. For both hazard and exposure, databases of existing information are used routinely. In addition, for exposure, in silico approaches include the use of rules of thumb for systemic bioavailability as well as physiologically-based kinetics (PBK) and multi-scale models for estimating internal exposure at the organ or tissue level. (Internal) Thresholds of Toxicological Concern are applicable for the safety assessment of ingredients at low concentrations. The use of structural rules, (Quantitative) Structure-Activity Relationships ((Q)SARs) and read-across are the most typically applied modelling approaches to predict hazard. Data from exposure and hazard assessment are increasingly being brought together in NGRA to provide an overall assessment of the safety of a cosmetic ingredient. All in silico approaches are reviewed in terms of their maturity and robustness for use.}
}
@article{SAENZROYO2024121922,
title = {Ordering vs. AHP. Does the intensity used in the decision support techniques compensate?},
journal = {Expert Systems with Applications},
volume = {238},
pages = {121922},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.121922},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423024247},
author = {Carlos Sáenz-Royo and Francisco Chiclana and Enrique Herrera-Viedma},
keywords = {AHP, IBR, Decision Support Systems, Expertise, Intensity Judgment},
abstract = {The manifestation of the intensity in the judgment of one alternative versus another in the peer comparison processes is a central element in some decision support techniques, such as the Analytical Hierarchy Process (AHP). However, its contribution regarding quality (expected performance) with respect to the priority vector has not been evaluated so far. Using the Intentional Bounded Rationality Methodology (IBRM), this work analyzes the gains obtained from requiring the decision-maker to report an intensity judgment in pairs (AHP) with respect to a technique that only requires expressing a preference (Ordering). The results show that when decision-makers have low levels of expertise, it is possible that a less informative and computational cheap technique (Ordering) performs better than a more informative and computational expensive one (AHP). When decision-makers have medium and high levels of expertise, AHP technique obtains modest gains with respect to the Ordering technique. This study proposes a cost-benefit analysis of decision support techniques contrasting the gains of a technique that requires more resources (AHP) against other that require less resources (Ordering). Our results can change the managing approach of the information obtained from experts’ judgments.}
}
@article{MCCRADDEN2023100864,
title = {A normative framework for artificial intelligence as a sociotechnical system in healthcare},
journal = {Patterns},
volume = {4},
number = {11},
pages = {100864},
year = {2023},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2023.100864},
url = {https://www.sciencedirect.com/science/article/pii/S2666389923002489},
author = {Melissa D. McCradden and Shalmali Joshi and James A. Anderson and Alex John London},
abstract = {Summary
Artificial intelligence (AI) tools are of great interest to healthcare organizations for their potential to improve patient care, yet their translation into clinical settings remains inconsistent. One of the reasons for this gap is that good technical performance does not inevitably result in patient benefit. We advocate for a conceptual shift wherein AI tools are seen as components of an intervention ensemble. The intervention ensemble describes the constellation of practices that, together, bring about benefit to patients or health systems. Shifting from a narrow focus on the tool itself toward the intervention ensemble prioritizes a “sociotechnical” vision for translation of AI that values all components of use that support beneficial patient outcomes. The intervention ensemble approach can be used for regulation, institutional oversight, and for AI adopters to responsibly and ethically appraise, evaluate, and use AI tools.}
}
@article{JALALIAN2023103602,
title = {Learning about me and you: Only deterministic stimulus associations elicit self-prioritization},
journal = {Consciousness and Cognition},
volume = {116},
pages = {103602},
year = {2023},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2023.103602},
url = {https://www.sciencedirect.com/science/article/pii/S1053810023001393},
author = {Parnian Jalalian and Marius Golubickis and Yadvi Sharma and C. {Neil Macrae}},
keywords = {Self, Instrumental learning, Probabilistic selection task, Self-prioritization, Drift diffusion model},
abstract = {Self-relevant material has been shown to be prioritized over stimuli relating to others (e.g., friend, stranger), generating benefits in attention, memory, and decision-making. What is not yet understood, however, is whether the conditions under which self-related knowledge is acquired impacts the emergence of self-bias. To address this matter, here we used an associative-learning paradigm in combination with a stimulus-classification task to explore the effects of different learning experiences (i.e., deterministic vs. probabilistic) on self-prioritization. The results revealed an effect of prior learning on task performance, with self-prioritization only emerging when participants acquired target-related associations (i.e., self vs. friend) under conditions of certainty (vs. uncertainty). A further computational (i.e., drift diffusion model) analysis indicated that differences in the efficiency of stimulus processing (i.e., rate of information uptake) underpinned this self-prioritization effect. The implications of these findings for accounts of self-function are considered.}
}
@incollection{SINGH2025427,
title = {Chapter Seventeen - Internet of Things legal landscape: Privacy, security, liability, and regulatory issues},
editor = {Pethuru Raj and Kavita Saini and Brij B. Gupta},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {138},
pages = {427-447},
year = {2025},
booktitle = {Post-Quantum Cryptography Algorithms and Approaches for IoT and Blockchain Security},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2025.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0065245825000269},
author = {Sushma Singh and Ravi Chandra Prakash and M. Arvindhan},
keywords = {Internet of Things, Legal landscape, Privacy, Security, Liability, Regulatory challenges, Intellectual property, Emerging trends, Case studies},
abstract = {Rapid adoption of Internet of Things (IoT) technology has created a new era of physical-digital convergence. This chapter examines the complex legal landscape of the Internet of Things, including privacy, security, liability, and regulatory issues. The goal is to help policymakers and industry professionals comprehend the complex IoT legal landscape. The chapter opens by contextualizing the extraordinary convergence of technology and law and emphasizing the need for a proactive legal framework. A full examination of IoT privacy risks follows. This includes an overview of broad data collecting, informed consent issues, and the changing role of individuals in personal data control. Existing data protection rules are evaluated to ensure privacy in the IoT context. The next part examines the legal ramifications of IoT ecosystem security breaches. Manufacturing, software, and service providers’ roles in device and data security are examined. The debate includes cybersecurity rules and the changing legal attitude needed to combat security breaches. In the next section, IoT-related legal frameworks are examined for liability and accountability. To explain IoT liability, the section navigates the complex relationships between manufacturers, software developers, and service providers using case studies and precedents. The next part discusses IoT stakeholders’ regulatory compliance challenges. A review of present legislation’ ability to meet the dynamic nature of IoT technology is provided, along with suggestions for new regulatory frameworks that balance innovation and ethical and legal standards. The chapter examines IoT patents, trademarks, and copyrights. Exploring interoperability, licencing, and IP rights challenges provides practical insights for stakeholders navigating the legal complexity of the quickly evolving IoT. New IoT trends and their legal ramifications are examined throughout the chapter. An adaptive legal framework is needed to suit the changing technology landscape of artificial intelligence, blockchain, and IoT. Strategically interwoven real-world case studies demonstrate how legal principles apply to IoT implementations. These case studies demonstrate how legal principles are used and teach us from important cases. Internet of Things, Legal Landscape, Privacy, Security, Liability, Regulatory Challenges, IP, Emerging Trends, Case Studies.}
}
@article{KIM2025,
title = {Machine Learning–Based Prediction of Substance Use in Adolescents in Three Independent Worldwide Cohorts: Algorithm Development and Validation Study},
journal = {Journal of Medical Internet Research},
volume = {27},
year = {2025},
issn = {1438-8871},
doi = {https://doi.org/10.2196/62805},
url = {https://www.sciencedirect.com/science/article/pii/S1438887125002699},
author = {Soeun Kim and Hyejun Kim and Seokjun Kim and Hojae Lee and Ahmed Hammoodi and Yujin Choi and Hyeon Jin Kim and Lee Smith and Min Seo Kim and Guillaume Fond and Laurent Boyer and Sung Wook Baik and Hayeon Lee and Jaeyu Park and Rosie Kwon and Selin Woo and Dong Keon Yon},
keywords = {adolescents, machine learning, substance, prediction, XGBoost, random forest, ML, substance use, adolescents, adolescent, South Korea, United States, Norway, web-based survey, survey, risk behavior, smoking, alcohol, intervention, interventions},
abstract = {Background
To address gaps in global understanding of cultural and social variations, this study used a high-performance machine learning (ML) model to predict adolescent substance use across three national datasets.
Objective
This study aims to develop a generalizable predictive model for adolescent substance use using multinational datasets and ML.
Methods
The study used the Korea Youth Risk Behavior Web-Based Survey (KYRBS) from South Korea (n=1,098,641) to train ML models. For external validation, we used the Youth Risk Behavior Survey (YRBS) from the United States (n=2,511,916) and Norwegian nationwide Ungdata surveys (Ungdata) from Norway (n=700,660). After developing various ML models, we evaluated the final model’s performance using multiple metrics. We also assessed feature importance using traditional methods and further analyzed variable contributions through SHapley Additive exPlanation values.
Results
The study used nationwide adolescent datasets for ML model development and validation, analyzing data from 1,098,641 KYRBS adolescents, 2,511,916 YRBS participants, and 700,660 from Ungdata. The XGBoost model was the top performer on the KYRBS, achieving an area under receiver operating characteristic curve (AUROC) score of 80.61% (95% CI 79.63-81.59) and precision of 30.42 (95% CI 28.65-32.16) with detailed analysis on sensitivity of 31.30 (95% CI 29.47-33.20), specificity of 99.16 (95% CI 99.12-99.20), accuracy of 98.36 (95% CI 98.31-98.42), balanced accuracy of 65.23 (95% CI 64.31-66.17), F1-score of 30.85 (95% CI 29.25-32.51), and area under precision-recall curve of 32.14 (95% CI 30.34-33.95). The model achieved an AUROC score of 79.30% and a precision of 68.37% on the YRBS dataset, while in external validation using the Ungdata dataset, it recorded an AUROC score of 76.39% and a precision of 12.74%. Feature importance and SHapley Additive exPlanation value analyses identified smoking status, BMI, suicidal ideation, alcohol consumption, and feelings of sadness and despair as key contributors to the risk of substance use, with smoking status emerging as the most influential factor.
Conclusions
Based on multinational datasets from South Korea, the United States, and Norway, this study shows the potential of ML models, particularly the XGBoost model, in predicting adolescent substance use. These findings provide a solid basis for future research exploring additional influencing factors or developing targeted intervention strategies.}
}
@article{HAO2025115545,
title = {Temperature history reconstruction in steel box girders using limited data and proper orthogonal decomposition-based dimension reduction representation},
journal = {Measurement},
volume = {240},
pages = {115545},
year = {2025},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2024.115545},
url = {https://www.sciencedirect.com/science/article/pii/S0263224124014301},
author = {Jing Hao and Hailin Lu and Hongyou Cao and Yunlai Zhou},
keywords = {Temperature history reconstruction, High-frequency temperature component, Stochastic vector processes, Dimension reduction, Proper orthogonal decomposition},
abstract = {The monitoring temperature history of steel box girders inevitably contains gaps or anomalies due to the malfunctions of the structural health monitoring system. Traditional methods for reconstructing temperature histories face the challenge in accounting for the randomness of temperature variations caused by the uncertainty of complex environmental factors. This research proposes a framework for reconstructing the long-term temperature of steel box girders by combining the limited measurements with a proper orthogonal decomposition (POD) based dimension reduction representation approach, to account for the stochastic nature of daily temperature variations. Unlike the conventional Monte Carlo-based POD method, the developed POD-based dimension reduction representation approach effectively reduces the number of elementary random variables from thousands to two by introducing random functions serving as constraints, overcoming the challenge of high-dimensional random variables inherent in the Monte Carlo methods. The proposed approach divides the measured temperature histories into random high-frequency (HF) and deterministic low-frequency (LF) components and establishes the theoretical models of power spectral density and coherence functions of HF temperature components to accommodate the generation of HF temperature component samples, and finally reconstructs temperature samples by superimposing the LF components and generated HF component samples. The results from a practical example demonstrate that the statistical characteristics of representative HF temperature component samples generated by the POD-based dimension reduction representation align well with the corresponding targeted values, and the proposed method outperforms the traditional POD method, yielding a 60% efficiency enhancement without compromising computational accuracy. The developed framework owns apparent superiority in accuracy compared to the traditional POD and the long short-term memory methods, particularly in continuous and extensive missing data. Moreover, the reconstructed temperature samples with assigned probabilities present complete probability information from the level of total probability. These results advance the probabilistic methods in tackling long-term temperature history reconstruction.}
}
@article{CHAVAS2024476,
title = {Bridging the microscopic divide: a comprehensive overview of micro-crystallization and in vivo crystallography},
journal = {IUCrJ},
volume = {11},
number = {4},
pages = {476-485},
year = {2024},
issn = {2052-2525},
doi = {https://doi.org/10.1107/S205225252400513X},
url = {https://www.sciencedirect.com/science/article/pii/S205225252400054X},
author = {Leonard Michel Gabriel Chavas and Fasséli Coulibaly and Damià Garriga and E. N. Baker},
keywords = {micro-crystallization,  crystallography, structural biology, macromolecular research, X-ray diffraction, XFELs, MicroED},
abstract = {The 26th IUCr congress held in Melbourne brought discussions on micro-crystallization and in vivo crystallography within structural biology to the forefront, highlighting innovative approaches and collaborative efforts to advance macromolecular research.
A series of events underscoring the significant advancements in micro-crystallization and in vivo crystallography were held during the 26th IUCr Congress in Melbourne, positioning microcrystallography as a pivotal field within structural biology. Through collaborative discussions and the sharing of innovative methodologies, these sessions outlined frontier approaches in macromolecular crystallography. This review provides an overview of this rapidly moving field in light of the rich dialogues and forward-thinking proposals explored during the congress workshop and microsymposium. These advances in microcrystallography shed light on the potential to reshape current research paradigms and enhance our comprehension of biological mechanisms at the molecular scale.}
}
@article{IVANITSKY2009101,
title = {Brain science: On the way to solving the problem of consciousness},
journal = {International Journal of Psychophysiology},
volume = {73},
number = {2},
pages = {101-108},
year = {2009},
note = {Neural Processes in Clinical Psychophysiology},
issn = {0167-8760},
doi = {https://doi.org/10.1016/j.ijpsycho.2009.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167876009001044},
author = {Alexey M. Ivanitsky and George A. Ivanitsky and Olga V. Sysoeva},
keywords = {Consciousness and brain problem, Event-related potentials, EEG rhythms, Semantic brain systems, Artificial intelligence},
abstract = {Four issues are discussed: the possible mechanism of subjective events, conscious versus unconscious brain functions, the rhythmic coding of mental operations and the possible brain basis of understanding.i.Several approaches have been developed to explain how subjective experience emerges from brain activity. One of them is the return of the nervous impulses to the sites of their primary projections, providing a synthesis of sensory information with memory and motivation [Ivanitsky, A.M., 1976. Brain Mechanisms of the Signal Evaluation. Medicina, Moscow 264 pp. (in Russian)]. Support for the existence of such a mechanism stems from studies upon the brain activity that subserves perception (visual and somato-sensory) and thought (verbal and imaginative). The cortical centres for information synthesis have been found. For perception, these are located in projection areas; for thinking — in frontal and temporal-parietal associative cortex. Closely related ideas were also developed by G. Edelman [Edelman, G.M., 1978. Group selection and phasic reentrant signaling: A theory of higher brain function. In: Eds. Edelman, G.M., Mountcastle, V.B. The Mindful Brain. Cortical Organization and the Group-selective Theory of Higher Brain Function. Cambridge, MA, MIT Press, pp 51–100.] in his re-entry theory of consciousness. Both theories emphasize the key role of memory and motivation in the origin of conscious function.ii.Conscious experience elucidates not all, but only salient brain functions. As a rule, voluntary control is switched on when additional cognitive resources are needed. Even a rather complicated mental operation, such as the discrimination between concrete and abstract words, could be executed very rapidly and implicitly; explicit analysis being engaged only in more difficult tasks. Furthermore, these two different kinds of mental operations, i.e., automatic and conscious, are predominantly associated with two different kinds of memory: a recognition memory for implicit analysis, and an episodic memory for explicit functions.iii.Rearrangements of EEG rhythms underlie mental functions. Certain rhythmical patterns are related with definite types of mental activity. The dependence of one upon the other is rather pronounced and expressive, so it becomes possible to recognize the type of mental operation being performed in mind with few seconds of the ongoing EEG, provided that the analysis of rhythms is accomplished using an artificial neural network.iv.It is commonly recognized that the computer, in contrast to the living brain, can calculate, yet cannot understand [Penrose, R., 1996. Shadows of the Mind: A Search for the Missing Science of Consciousness New York, Oxford, Oxford University Press 480 pp.]. Comprehension implies the comparison of new and old information that requires the ability to search for associations, grouping similar objects together, and distinguishing different objects from one another. However, these functions may also be implemented on a computer. Still, it is believed that computers perform these complicated operations without genuine understanding. Evidently, comprehension additionally has to be based upon some biologically significant ground. It is hypothesized that the subjective feeling of understanding appears when current information is attributed to a definite need, which is scaled in sign (+/−) coordinates. This coordinate system ceases the brain calculations, when “comprehension” is reached, i.e., the acceptable level of need satisfaction is attained.}
}
@article{HOME200255,
title = {Fluids and forces in eighteenth-century electricity},
journal = {Endeavour},
volume = {26},
number = {2},
pages = {55-59},
year = {2002},
issn = {0160-9327},
doi = {https://doi.org/10.1016/S0160-9327(02)01411-4},
url = {https://www.sciencedirect.com/science/article/pii/S0160932702014114},
author = {Roderick W. Home},
abstract = {Our understanding of the history of electricity in the eighteenth century has changed significantly since the early 1960s, when Thomas Kuhn presented it as a leading example to support his general view of the history of science. In particular, while the ideas of Benjamin Franklin are still seen as important, they are no longer seen as constituting a revolution in the theory of electricity. They appear instead as merely one stage in a long drawn-out process of evolution in electrical thinking.}
}
@article{YANG2023105120,
title = {Thoughts of brain EEG signal-to-text conversion using weighted feature fusion-based Multiscale Dilated Adaptive DenseNet with Attention Mechanism},
journal = {Biomedical Signal Processing and Control},
volume = {86},
pages = {105120},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2023.105120},
url = {https://www.sciencedirect.com/science/article/pii/S1746809423005530},
author = {Jing Yang and Muhammad Awais and Md. Amzad Hossain and Por {Lip Yee} and Ma. Haowei and Ibrahim M. Mehedi and A.I.M. Iskanderani},
keywords = {Thought-to-text conversion, Electroencephalography signal, Optimal weighted feature fusion, Eurasian oystercatcher wild geese migration optimization, Multiscale Dilated Adaptive DenseNet with Attention Mechanism},
abstract = {Individuals with visual inefficiencies or different abilities face difficulties using their hands to operate smartphones and computers, necessitating reliance on others to enter data. Such dependence may lead to security and privacy issues, especially when sensitive information is shared with helpers. To address this problem, we present Think2Type, an efficient Brain-Computer Interface (BCI) that enables users to translate their active intentions into text format based on Morse code. BCI leverages brain activity to facilitate interaction with computers, often captured via Electroencephalography (EEG). This work proposes an enhanced attention-based deep learning strategy to develop an efficient text conversion mechanism from EEG signals. We begin by collecting EEG signals from standard benchmark datasets and extracting spectral and statistical features in phase 1, concatenating them into concatenated feature set 1 (F1). In phase 2, we extract spatial and temporal features via a One-Dimensional Convolutional Neural Network (1DCNN) and a Recurrent Neural Network (RNN), respectively, concatenating them into concatenated feature set 2 (F2). Weighted feature fusion is performed on concatenated features F1 and F2, with the hybrid optimization algorithm Eurasian Oystercatcher Wild Geese Migration Optimization (EOWGMO) optimizing the weight for improved fusion efficiency. The text conversion phase utilizes the Multiscale Dilated Adaptive DenseNet with Attention Mechanism (MDADenseNet-AM) to obtain the converted text information. The MDADenseNet-A's parameters are optimized to improve thought-to-text conversion performance. The developed model's performance is evaluated via experimental analysis and compared to conventional techniques, resulting in a higher accuracy value of 96.41%, facilitating appropriate text conversion.}
}
@article{NELSON2019100758,
title = {Designing and transforming yield-stress fluids},
journal = {Current Opinion in Solid State and Materials Science},
volume = {23},
number = {5},
pages = {100758},
year = {2019},
issn = {1359-0286},
doi = {https://doi.org/10.1016/j.cossms.2019.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1359028619300762},
author = {Arif Z. Nelson and Kenneth S. Schweizer and Brittany M. Rauzan and Ralph G. Nuzzo and Jan Vermant and Randy H. Ewoldt},
keywords = {Soft matter, Yield-stress fluid, Design, Engineering, Extension, Thixotropy, Elasticity, Colloids, Emulsions, Polymers, 3D printing, Chemistry, Physics, Rheology, Complex fluids},
abstract = {We review progress in designing and transforming multi-functional yield-stress fluids and give a perspective on the current state of knowledge that supports each step in the design process. We focus mainly on the rheological properties that make yield-stress fluids so useful and the trade-offs which need to be considered when working with these materials. Thinking in terms of “design with” and “design of” yield-stress fluids motivates how we can organize our scientific understanding of this field. “Design with” involves identification of rheological property requirements independent of the chemical formulation, e.g. for 3D direct-write printing which needs to accommodate a wide range of chemistry and material structures. “Design of” includes microstructural considerations: conceptual models relating formulation to properties, quantitative models of formulation-structure-property relations, and chemical transformation strategies for converting effective yield-stress fluids to be more useful solid engineering materials. Future research directions are suggested at the intersection of chemistry, soft-matter physics, and material science in the context of our desire to design useful rheologically-complex functional materials.}
}
@article{LIU2024100129,
title = {Extracting multi-objective multigraph features for the shortest path cost prediction: Statistics-based or learning-based?},
journal = {Green Energy and Intelligent Transportation},
volume = {3},
number = {1},
pages = {100129},
year = {2024},
issn = {2773-1537},
doi = {https://doi.org/10.1016/j.geits.2023.100129},
url = {https://www.sciencedirect.com/science/article/pii/S2773153723000658},
author = {Songwei Liu and Xinwei Wang and Michal Weiszer and Jun Chen},
keywords = {Multi-objective multigraph, Feature extraction, Shortest path cost prediction, Node patterns, Node embeddings, Regression},
abstract = {Efficient airport airside ground movement (AAGM) is key to successful operations of urban air mobility. Recent studies have introduced the use of multi-objective multigraphs (MOMGs) as the conceptual prototype to formulate AAGM. Swift calculation of the shortest path costs is crucial for the algorithmic heuristic search on MOMGs, however, previous work chiefly focused on single-objective simple graphs (SOSGs), treated cost enquires as search problems, and failed to keep a low level of computational time and storage complexity. This paper concentrates on the conceptual prototype MOMG, and investigates its node feature extraction, which lays the foundation for efficient prediction of shortest path costs. Two extraction methods are implemented and compared: a statistics-based method that summarises 22 node physical patterns from graph theory principles, and a learning-based method that employs node embedding technique to encode graph structures into a discriminative vector space. The former method can effectively evaluate the node physical patterns and reveals their individual importance for distance prediction, while the latter provides novel practices on processing multigraphs for node embedding algorithms that can merely handle SOSGs. Three regression models are applied to predict the shortest path costs to demonstrate the performance of each. Our experiments on randomly generated benchmark MOMGs show that (i) the statistics-based method underperforms on characterising small distance values due to severe overestimation; (ii) A subset of essential physical patterns can achieve comparable or slightly better prediction accuracy than that based on a complete set of patterns; and (iii) the learning-based method consistently outperforms the statistics-based method, while maintaining a competitive level of computational complexity.}
}
@article{BUI2017115,
title = {Envisioning the future of ‘big data’ biomedicine},
journal = {Journal of Biomedical Informatics},
volume = {69},
pages = {115-117},
year = {2017},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2017.03.017},
url = {https://www.sciencedirect.com/science/article/pii/S1532046417300709},
author = {Alex A.T. Bui and John Darrell {Van Horn}},
keywords = {Biomedicine, Data science, Software, Computing, Training},
abstract = {Through the increasing availability of more efficient data collection procedures, biomedical scientists are now confronting ever larger sets of data, often finding themselves struggling to process and interpret what they have gathered. This, while still more data continues to accumulate. This torrent of biomedical information necessitates creative thinking about how the data are being generated, how they might be best managed, analyzed, and eventually how they can be transformed into further scientific understanding for improving patient care. Recognizing this as a major challenge, the National Institutes of Health (NIH) has spearheaded the “Big Data to Knowledge” (BD2K) program – the agency’s most ambitious biomedical informatics effort ever undertaken to date. In this commentary, we describe how the NIH has taken on “big data” science head-on, how a consortium of leading research centers are developing the means for handling large-scale data, and how such activities are being marshalled for the training of a new generation of biomedical data scientists. All in all, the NIH BD2K program seeks to position data science at the heart of 21st Century biomedical research.}
}
@incollection{BARBER20251,
title = {Cultural contributions to cognitive aging},
editor = {Jordan Henry Grafman},
booktitle = {Encyclopedia of the Human Brain (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {1-16},
year = {2025},
isbn = {978-0-12-820481-8},
doi = {https://doi.org/10.1016/B978-0-12-820480-1.00042-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204801000425},
author = {Nicolette Barber and Ioannis Valoumas and Chaipat Chunharas and Sirawaj Itthipuripat and Angela Gutchess},
keywords = {Aging, Attention, Autobiographical memory, Cognition, Cognitive aging, Cognitive neuroscience, Cross-cultural, Culture, Long-term memory, Memory},
abstract = {In this article, we review research on the influences of culture on cognitive aging, with a focus on long-term memory and attention. Given the small number of studies directly investigating cognitive aging across cultures, we draw on existing cross-cultural studies comparing brain and behavior in young adult samples. We outline the potential for future research and discuss the importance of adopting a cross-cultural lens to support cognition and well-being in older adults in diverse cultural contexts.}
}
@article{LASSITER201927,
title = {Language and simplexity: A powers view},
journal = {Language Sciences},
volume = {71},
pages = {27-37},
year = {2019},
note = {Simplexity, agency and language},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2018.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0388000118300366},
author = {Charles Lassiter},
keywords = {Distributed language, Simplexity, Causal powers, Speech acts},
abstract = {The notion of simplexity is that complex problems are often solved by novel combinations of simple mechanisms. These solutions aren't simple; they're simplex. Language use, as a complex behavior, is ripe for simplex analysis. In this paper, I argue that the notion of powers—an organism's capacity to instigate or undergo change—is doubly useful. First, powers, as opposed to mental representations, are a suitable object for simplex analysis. So conceptualizing languaging in terms of powers gets us one step closer to a simplex analysis of language. But thinking of languaging in terms of powers has an additional payoff. Berthoz asserts that the concept of simplexity is related to the concept of meaning. How they're related is unclear. Conceptualizing languaging in terms of powers injects meaningfulness into lived world of the organism. Consequently, the concept of powers can act as a bridge between the concepts of meaningfulness and simplexity.}
}
@article{COLTHER2024100625,
title = {Artificial intelligence: Driving force in the evolution of human knowledge},
journal = {Journal of Innovation & Knowledge},
volume = {9},
number = {4},
pages = {100625},
year = {2024},
issn = {2444-569X},
doi = {https://doi.org/10.1016/j.jik.2024.100625},
url = {https://www.sciencedirect.com/science/article/pii/S2444569X24001641},
author = {Cristian Colther and Jean Pierre Doussoulin},
keywords = {Artificial intelligence, Evolution knowledge, Noosphere, Ethical considerations, Future scenarios},
abstract = {This article proposes that artificial intelligence (AI) is positioned as a key driver of a new evolutionary stage of human knowledge, complementing human intelligence and facilitating the creation and development of sophisticated collective intelligence, defined as the noosphere, understood as the sphere of collective human thought. The study reveals several key insights into the transformative potential of AI, including its capacity to accelerate, mediate, and diffuse human knowledge. It concludes that AI not only catalyzes the existence of the noosphere but also redefines the structures and mechanisms through which human knowledge is expanded and democratized. Additionally, the document presents potential risks and significant ethical, social, and legal challenges of an AI-mediated noosphere, offering recommendations and a research agenda around the topic, and limitations and proposals for improvement to be considered in the future.}
}
@incollection{HEFNER20163,
title = {Chapter 1 - A Brief History of Biological Distance Analysis},
editor = {Marin A. Pilloud and Joseph T. Hefner},
booktitle = {Biological Distance Analysis},
publisher = {Academic Press},
address = {San Diego},
pages = {3-22},
year = {2016},
isbn = {978-0-12-801966-5},
doi = {https://doi.org/10.1016/B978-0-12-801966-5.00001-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128019665000019},
author = {J.T. Hefner and M.A. Pilloud and J.E. Buikstra and C.C.M. Vogelsberg},
keywords = {aDNA, Analytical scales, Biodistance, Cranial nonmetric traits, Craniometrics, Kinship, Odontometrics, Typology},
abstract = {Biological distance, or biodistance, analysis employs data derived from skeletal remains to reflect population relatedness (similarity/dissimilarity) through the application of multivariate statistical methods. The approaches used in biodistance studies have changed markedly over recent centuries, exploring phenotypic expressions assumed to be informative. Biodistance analysis began as the study of anomalous variants in the human skull, but the field has transformed over the centuries now seeking to incorporate skeletal morphology in the interpretation of genetic affinity, providing insight into the genetics governing trait expression, and providing understanding into the role of developmental biology on the expression of morphological variants. As methodological approaches improve, so too has the application of these analyses. We present here a brief historical overview of biodistance analysis research, focusing on meta-themes in the field, shifts in thinking among researchers in biological anthropology, and several of the outside influences that impact biodistance analysis.}
}
@incollection{SNYDER200089,
title = {Chapter 5 - Hope as a Common Factor across Psychotherapy Approaches: A Lesson from the Dodo's Verdict},
editor = {C.R. Snyder},
booktitle = {Handbook of Hope},
publisher = {Academic Press},
address = {San Diego},
pages = {89-108},
year = {2000},
isbn = {978-0-12-654050-5},
doi = {https://doi.org/10.1016/B978-012654050-5/50007-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780126540505500075},
author = {C.R. Snyder and Julia D. Taylor},
abstract = {Publisher Summary
Despite providing different explanations and targeting disparate symptoms, various psychological approaches for producing change appear to be equally effective. The chapter compares this phenomenon to the Dodo's verdict in “Alice in Wonderland,” in the end race. It explores the question—specifically, what mechanism (or mechanisms) underlie the equal, high efficacy produced by differing types of psychological interventions. Agency reflects people's thoughts about their capacity to use the pathways they have selected to reach their goals. Agency is crucial for the psychotherapy process because it provides mental energy so that a client can undertake various therapy-related activities. This type of goal-directed motivation is reflected in self-affirming mental statements. Operating across differing samples and methodologies, agentic thinking both initiates and helps sustain clients' improvements in psychotherapy. Furthermore, enlisting the literature on placebo effects to illustrate the impact of agentic thinking. The chapter demonstrates how agency alone propels clinical improvement. Agentic thought is the motivational force or engine in hope theory. All the mental energy imaginable, however, cannot guarantee successful goal attainment in psychotherapy. Perceptions that one can produce the routes to those goals is a second necessary component. The chapter also explores pathways thinking in the context of varying psychotherapies.}
}
@article{ROBERTS2020116758,
title = {Creative, internally-directed cognition is associated with reduced BOLD variability},
journal = {NeuroImage},
volume = {219},
pages = {116758},
year = {2020},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2020.116758},
url = {https://www.sciencedirect.com/science/article/pii/S1053811920302457},
author = {Reece P. Roberts and Cheryl L. Grady and Donna Rose Addis},
keywords = {Episodic simulation, Imagination, Creativity, BOLD variability},
abstract = {In a range of externally-directed tasks, intra-individual variability of fMRI BOLD signal has been shown to be a stronger predictor of cognitive performance than mean BOLD signal. BOLD variability’s strong association with cognitive performance is hypothesised to be due to it capturing the dynamic range of neural systems. Although increased BOLD variability is also speculated to play a role in internally-directed thought, particularly when creative and flexible cognition is required, there is a relative lack of research exploring whether BOLD variability is related to internally-directed cognition. Thus, we investigated the relationship between BOLD variability and a key component of creativity – divergent thinking – in various tasks that required participants to think flexibly. We also determined whether any associations between BOLD variability and creativity overlapped with, or differed, from associations between mean BOLD signal and creativity. First, we performed task Partial Least Squares (PLS) analyses that compared BOLD signal (either mean or variability) during two future imagination conditions that differed in the amount of cognitive flexibility required: a Congruent condition in which autobiographical details (people, places, objects) comprising an imagined event belonged to the same social sphere (e.g., university) and an Incongruent condition in which details belonged to different social spheres and required greater cognitive flexibility to integrate. Results indicated that the Incongruent condition was associated with a widespread reduction in both BOLD variability and mean signal (relative to the Congruent condition), but in largely non-overlapping regions. Next, we used behavioral PLS to determine whether individual differences in performance on future simulation tasks as well as the Alternate Uses Task relates to BOLD variability and mean BOLD signal. Better performance on these tasks was predominantly associated with increases in mean BOLD signal and decreases in BOLD variability, in a range of disparate brain regions. Together, the results suggest that, unlike tasks requiring externally-directed cognition, superior performance on tasks requiring creative internal mentation is associated with less (not more) variability.}
}
@article{BENDETOWICZ2017216,
title = {Brain morphometry predicts individual creative potential and the ability to combine remote ideas},
journal = {Cortex},
volume = {86},
pages = {216-229},
year = {2017},
note = {Is a "single" brain model sufficient?},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2016.10.021},
url = {https://www.sciencedirect.com/science/article/pii/S0010945216303161},
author = {David Bendetowicz and Marika Urbanski and Clarisse Aichelburg and Richard Levy and Emmanuelle Volle},
keywords = {Creativity, Semantic associations, Rostral prefrontal, Frontal pole, Morphometry},
abstract = {For complex mental functions such as creative thinking, inter-individual variability is useful to better understand the underlying cognitive components and brain anatomy. Associative theories propose that creative individuals have flexible semantic associations, which allows remote elements to be formed into new combinations. However, the structural brain variability associated with the ability to combine remote associates has not been explored. To address this question, we performed a voxel-based morphometry (VBM) study and explored the anatomical connectivity of significant regions. We developed a Remote Combination Association Task adapted from Mednick's test, in which subjects had to find a solution word related to three cue words presented to them. In our adaptation of the task, we used free association norms to quantify the associative distance between the cue words and solution words, and we varied this distance. The tendency to solve the task with insight and the ability to evaluate the appropriateness of a proposed solution were also analysed. Fifty-four healthy volunteers performed this task and underwent a structural MRI. Structure–function relationships were analysed using regression models between grey matter (GM) volume and task performance. Significant clusters were mapped onto an atlas of white matter (WM) tracts. The ability to solve the task, which depended on the associative distance of the solution word, was associated with structural variation in the left rostrolateral prefrontal and posterior parietal regions; the left rostral prefrontal region was connected to distant regions through long-range pathways. By using a creative combination task in which the semantic distance between words varied, we revealed a brain network centred on the left frontal pole that appears to support the ability to combine information in new ways by bridging the semantic distance between pieces of information.}
}
@article{ARAGONES20141,
title = {Rhetoric and analogies},
journal = {Research in Economics},
volume = {68},
number = {1},
pages = {1-10},
year = {2014},
issn = {1090-9443},
doi = {https://doi.org/10.1016/j.rie.2013.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S1090944313000410},
author = {Enriqueta Aragones and Itzhak Gilboa and Andrew Postlewaite and David Schmeidler},
keywords = {Rhetoric, Analogies, Complexity},
abstract = {The art of rhetoric may be defined as changing other people's minds (opinions, beliefs) without providing them new information. One technique heavily used by rhetoric employs analogies. Using analogies, one may draw the listener's attention to similarities between cases and to re-organize existing information in a way that highlights certain regularities. In this paper we offer two models of analogies, discuss their theoretical equivalence, and show that finding good analogies is a computationally hard problem.}
}
@incollection{SIEGEL20163,
title = {Chapter 1 - Introduction: Defining the Role of Statistics in Business},
editor = {Andrew F. Siegel},
booktitle = {Practical Business Statistics (Seventh Edition)},
publisher = {Academic Press},
edition = {Seventh Edition},
pages = {3-17},
year = {2016},
isbn = {978-0-12-804250-2},
doi = {https://doi.org/10.1016/B978-0-12-804250-2.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128042502000018},
author = {Andrew F. Siegel},
abstract = {We begin this chapter with an overview of the competitive advantage provided by a knowledge of statistical methods, followed by some basic facts about statistics and probability and their role in business. Statistical activities can be grouped into five main activities (designing, exploring, modeling, estimating, and hypothesis testing), and one way to clarify statistical thinking is to be able to match the business task at hand with the correct collection of statistical methods. This chapter sets the stage for the rest of the book, which follows up with many important detailed procedures for accomplishing business goals that involve these activities. Next follows an overview of data mining of Big Data (which involves these main activities) and its importance in business. Then we distinguish the field of probability (where, based on assumptions, we reach conclusions about what is likely to happen—a useful exercise in business where nobody knows for sure what will happen) from the field of statistics (where we know from the data what happened, from which we infer conclusions about the system that produced these data) while recognizing that probability and statistics will work well together in future chapters. The chapter concludes with some words of advice on how to integrate statistical thinking with other business viewpoints and activities.}
}
@article{HEDE2015522,
title = {TRIZ and the Paradigms of Social Sustainability in Product Development Endeavors},
journal = {Procedia Engineering},
volume = {131},
pages = {522-538},
year = {2015},
note = {TRIZ and Knowledge-Based Innovation in Science and Industry},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2015.12.447},
url = {https://www.sciencedirect.com/science/article/pii/S1877705815043398},
author = {Shantesh Hede and Paula Verandas Ferreira and Manuel Nunes Lopes and Luis Alexandre Rocha},
keywords = {Sustainability, Decision Modeling, TRIZ, Product Development ;},
abstract = {The Business practices of an industrialized civilization are responsible for intensifying the dynamics of the interdependent environmental, social and economic domains of our ecosystem. The worldwide objective to accomplish Sustainability is invariably addressed by Policy makers and Institutions by means of moderately disparate co-relations between Environmental and Social considerations. The dimension of Social Sustainability has a direct co-relation towards the extended continuation of a globalized Enterprise. The stated co-relation is an interconnected and interdependent network comprising of growth in Innovation and Sustainability at the Environmental and Economic frontiers. From the standpoint of Innovation, the 20th century has been dominated by both TRIZ with OTSM and Kurzweil's Law of Accelerating Returns to steer the future of revolutionary innovations. Moreover, TRIZ and its evolved counterpart OTSM have been extensively utilized for macro-scale problem solving scenarios, while Kurzweil's Law has reached up to quantum scale whereby matter as we know exhibits an entire range of unique properties with a potential to dramatically transform our human civilization. Accordingly, the perceived limitations and vague applicability of TRIZ in sub-macro scale innovations has been discussed. The contemporary tools for project evaluation (e.g.: cost benefit analysis) and product development (e.g.: linear stage-gate process) quintessential for commercializing innovations are identified to be limited, both in scope and accuracy for delivering a long term ‘sustainable’ competitive advantage to an Enterprise. Consequently, the proposed conceptual Multifaceted Framework addresses the issue of social sustainability in Product Development. The underpinnings of Systems Thinking, TRIZ and OTSM, Complex Adaptive Systems, Socio-Economics & Human Behavior forms the fundamental basis of the proposed Multifaceted Framework. The novel perspective offered by the proposed Framework enables product development teams to overcome the inherent myopia and other limitations associated with the contemporary Environmental Life Cycle Analysis and Sustainability related Decision Models. An Expert opinion based evaluation technique in conjugation with a Multilayered Decision Modeling Method have been incorporated as a salient features in the proposed framework. The evaluation technique is utilized for assigning numerical values to the pertinent sustainability related criteria of the Multilayered Decision Model. The proposed Framework plays a crucial role in product development and decision modeling across the Idea Screening Phase (Stage 2) up to the Feasibility Analysis Phase (Stage 4). In addition, a modified version Taguchi Loss Function is included to exemplify a tangible relation between Product Quality parameters and Sustainability. The objective of the proposed framework is to provide an efficient, yet comprehensive evaluation as well as an effective product development strategy with a distinct and a holistic outlook on Social Sustainability.}
}
@article{NOLAN2025108094,
title = {Efficient Bayesian functional principal component analysis of irregularly-observed multivariate curves},
journal = {Computational Statistics & Data Analysis},
volume = {203},
pages = {108094},
year = {2025},
issn = {0167-9473},
doi = {https://doi.org/10.1016/j.csda.2024.108094},
url = {https://www.sciencedirect.com/science/article/pii/S0167947324001786},
author = {Tui H. Nolan and Sylvia Richardson and Hélène Ruffieux},
keywords = {Functional principal component analysis, Hierarchical modelling, Multivariate functional data, Variational message passing},
abstract = {The analysis of multivariate functional curves has the potential to yield important scientific discoveries in domains such as healthcare, medicine, economics and social sciences. However, it is common for real-world settings to present longitudinal data that are both irregularly and sparsely observed, which introduces important challenges for the current functional data methodology. A Bayesian hierarchical framework for multivariate functional principal component analysis is proposed, which accommodates the intricacies of such irregular observation settings by flexibly pooling information across subjects and correlated curves. The model represents common latent dynamics via shared functional principal component scores, thereby effectively borrowing strength across curves while circumventing the computationally challenging task of estimating covariance matrices. These scores also provide a parsimonious representation of the major modes of joint variation of the curves and constitute interpretable scalar summaries that can be employed in follow-up analyses. Estimation is conducted using variational inference, ensuring that accurate posterior approximation and robust uncertainty quantification are achieved. The algorithm also introduces a novel variational message passing fragment for multivariate functional principal component Gaussian likelihood that enables modularity and reuse across models. Detailed simulations assess the effectiveness of the approach in sharing information from sparse and irregularly sampled multivariate curves. The methodology is also exploited to estimate the molecular disease courses of individual patients with SARS-CoV-2 infection and characterise patient heterogeneity in recovery outcomes; this study reveals key coordinated dynamics across the immune, inflammatory and metabolic systems, which are associated with long-COVID symptoms up to one year post disease onset. The approach is implemented in the R package bayesFPCA.}
}
@article{CORTENBACH2024104869,
title = {The Dial-a-Ride problem with meeting points: A problem formulation for shared demand–responsive transit},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {169},
pages = {104869},
year = {2024},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2024.104869},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X24003905},
author = {L.E. Cortenbach and K. Gkiotsalitis and E.C. {van Berkum} and E. Walraven},
keywords = {Dial-a-Ride problem, Meeting points, Demand–responsive transit, Tabu search},
abstract = {In this paper, a formulation for the Dial-a-Ride Problem with Meeting Points (DARPmp) is introduced. The problem consists of defining routes that satisfy trip requests between pick-up and drop-off points while complying with time window, ride time, vehicle load, and route duration constraints. A set of meeting points is defined, and passengers may be asked to use these meeting points as alternative pickup or drop-off points if this results in routes with lower costs. Incorporating meeting points into the DARP is achieved by formulating a mixed-integer linear program. Two preprocessing steps and three valid inequalities are introduced, which improve the computational performance when solving the DARPmp to global optimality. Two versions of the Tabu Search metaheuristic are proposed to approximate the optimal solution in large-scale networks due to the NP-hardness of DARPmp. Performing numerical experiments with benchmark instances, this study demonstrates the benefits of DARPmp compared to DARP in terms of reducing vehicle running costs.}
}
@article{2024100678,
title = {Erratum regarding missing Declaration of Competing Interest statements in previously published articles},
journal = {International Journal of Child-Computer Interaction},
volume = {41},
pages = {100678},
year = {2024},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100678},
url = {https://www.sciencedirect.com/science/article/pii/S2212868924000473}
}
@article{KELLER2011174,
title = {Towards a science of informed matter},
journal = {Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences},
volume = {42},
number = {2},
pages = {174-179},
year = {2011},
note = {When Physics Meets Biology},
issn = {1369-8486},
doi = {https://doi.org/10.1016/j.shpsc.2010.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S1369848610001172},
author = {Evelyn Fox Keller},
keywords = {Information, Self-assembly, Evolution, Selection, Embodiment, Supramolecular chemistry},
abstract = {Over the last couple of decades, a call has begun to resound in a number of distinct fields of inquiry for a reattachment of form to matter, for an understanding of ‘information’ as inherently embodied, or, as Jean-Marie Lehn calls it, for a “science of informed matter.” We hear this call most clearly in chemistry, in cognitive science, in molecular computation, and in robotics—all fields looking to biological processes to ground a new epistemology. The departure from the values of a more traditional epistemological culture can be seen most clearly in changing representations of biological development. Where for many years now, biological discourse has accepted a sharp distinction (borrowed directly from classical computer science) between information and matter, software and hardware, data and program, encoding and enactment, a new discourse has now begun to emerge in which these distinctions have little meaning. Perhaps ironically, much of this shift depends on drawing inspiration from just those biological processes which the discourse of disembodied information was intended to describe.}
}
@article{GUPTA2022103,
title = {The interaction between technology, business environment, society, and regulation in ICT industries},
journal = {IIMB Management Review},
volume = {34},
number = {2},
pages = {103-115},
year = {2022},
issn = {0970-3896},
doi = {https://doi.org/10.1016/j.iimb.2022.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0970389622000465},
author = {Subhashish Gupta},
keywords = {ICT, Technology, Disruption, Convergence, Interaction, Big data, Network effects, Innovation, Competition law, Strategy},
abstract = {This paper focusses on the interaction between technology, the business environment, regulation, and society in ICT industries. The role of technological advances in communication (e.g., cellular mobile, 5 G, spectrum allocation) and in computational advances (e.g., cloud, Internet of Things, artificial intelligence) along with developments in the business environment (e.g., disruption, convergence, Industry 4.0) and the regulatory environment (e.g., competition law and market regulation) in the model is explained. The economics of network industries and competition law and strategies such as vertical integration, bundling, and tying are described. The role of regulation and innovation is discussed along with some cases.}
}
@article{REN2024122745,
title = {Pooling-based Visual Transformer with low complexity attention hashing for image retrieval},
journal = {Expert Systems with Applications},
volume = {241},
pages = {122745},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122745},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423032475},
author = {Huan Ren and Jiangtao Guo and Shuli Cheng and Yongming Li},
keywords = {Pooling-based Visual Transformers, Attention, Image retrieval, Deep hash},
abstract = {Retrieving similar images is becoming an urgent need for us with the continuous growth of large-scale data. However, whether the dominant image retrieval methods are Convolutional Neural Networks (CNNs) or the recently emerging Visual Transformer (ViT), their complex computation, insufficient feature extraction, and mismatched weights greatly influence the efficiency and retrieval accuracy. In this paper, we propose a Pooling-based Visual Transformer with low complexity attention hashing (PTLCH) for image retrieval. First, a backbone network for Pooling-based Vision Transformer (PiT) feature learning is designed to combine the pooling in CNN and the ViT to achieve the purpose of spatial dimensionality reduction while learning rich semantic information. Second, a low complexity attention (LCA) module is incorporated into PiT, which works by combining the positional deviation with the key matrix and the value matrix and then matrix multiplying with the query matrix. LCA explores rich contextual information to enable network learning of more granular feature information. Finally, a new loss framework is proposed where we focus on the effect of difficult and erroneous samples on accuracy. By using different improved cross-entropy losses, better weights are assigned to the learning samples of our network, which effectively improves learning hash coding. We have conducted extensive experiments on three public datasets, CIFAR-10, ImageNet100, and MS-COCO, which have the highest mean average precision of 93.76%, 92.62%, and 90.60%, respectively.}
}
@article{MOHANAN2024100997,
title = {Integrating Ayurveda and modern mainstream medicine},
journal = {Journal of Ayurveda and Integrative Medicine},
volume = {15},
number = {5},
pages = {100997},
year = {2024},
issn = {0975-9476},
doi = {https://doi.org/10.1016/j.jaim.2024.100997},
url = {https://www.sciencedirect.com/science/article/pii/S0975947624001128},
author = {K.P. Mohanan},
abstract = {This article is an attempt to understand the challenge of integrating the education provided by BAMS programs and MBBS programs, in order to initiate the process of integrating research and practice in Ayurveda and Modern Mainstream Medicine. The specific issues discussed in the article are framed within the broader context of the challenge of integrating any two bodies of knowledge, theories, or knowledge systems in education and research.}
}
@article{ZHANG20247,
title = {Large language model in electrocatalysis},
journal = {Chinese Journal of Catalysis},
volume = {59},
pages = {7-14},
year = {2024},
issn = {1872-2067},
doi = {https://doi.org/10.1016/S1872-2067(23)64612-1},
url = {https://www.sciencedirect.com/science/article/pii/S1872206723646121},
author = {Chengyi Zhang and Xingyu Wang and Ziyun Wang},
keywords = {Large language model, Electrocatalysis, Artificial intelligence, Multimodal large language model},
abstract = {ABSTRACT
Large language models have recently brought a massive storm on modern society in all fields. While many view them as mere search engines for specific answers or text refinement tools like a chatbot, their broader applications remain largely unexplored. These large language models, consisting of billions of interconnected neurons, derived from all knowledge of the human, possess the remarkable ability to engage in smooth and precise conversations with individuals across the globe. Human-like intelligence enables them to address modern challenges and display immense potential in various scientific domains. In this perspective, we delve into the potential applications of modern large language model and its future iterations within the field of catalysis, aiming to shed light on how these AI-driven models can contribute to a deeper understanding of catalysis science and the intelligent design of catalysts.}
}
@incollection{MORA2019215,
title = {Chapter 7 - The social shaping of smart cities},
editor = {Luca Mora and Mark Deakin},
booktitle = {Untangling Smart Cities},
publisher = {Elsevier},
pages = {215-234},
year = {2019},
isbn = {978-0-12-815477-9},
doi = {https://doi.org/10.1016/B978-0-12-815477-9.00007-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128154779000074},
author = {Luca Mora and Mark Deakin},
keywords = {Interdisciplinarity, Expectations, Social shaping, Open innovation, Technological advancement, Urban innovations, Technological change, Hype, Smart city, Smart city research, Lessons, Recommendations, Open community, Co-design, Collaboration, Collaborative environment, Quadruple helix, Triple helix, Organization dynamics, Urban innovation, Urban sustainability, Sustainable urban development, Hyped behaviors, Urban utopia, Expectations, Smart city development, Smart urbanism},
abstract = {This last chapter concludes the investigation by summing up the key lessons and recommendations that this book can offer to the community of stakeholders involved in smart city research, policy, and practice. The series of complementary analyses that the previous chapters report on demonstrate that, when untangled from the technocentric urban utopia pictured by the corporate sector, smart cities have the potential to develop into innovation systems that set the stage for a technology-enabled approach to urban sustainability. But realizing this opportunity requires to move beyond traditional boundaries, separate the hype from reality, and strengthen the focus on the social shaping of smart cities. The investigation demonstrates that, in order for such a social shaping to develop, the design of smart cities needs to be understood as a collective action in which two complementary forces are combined. On the one hand, the faith in the technological advancement exposed in the utopian thinking. On the other, the knowledge, skills, and interests of a quadruple-helix collaborative environment where the need for technological innovation in response to urban sustainability goals is not shaped by the corporate sector and its technocentric and market-oriented logic, but an open community whose actions serve the public interest and are based on a holistic interpretation of smart city development.}
}
@article{ISMAILOVA2020291,
title = {Hereditary information processes with semantic modeling structures},
journal = {Procedia Computer Science},
volume = {169},
pages = {291-296},
year = {2020},
note = {Postproceedings of the 10th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2019 (Tenth Annual Meeting of the BICA Society), held August 15-19, 2019 in Seattle, Washington, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.181},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920303045},
author = {Larisa Ismailova and Viacheslav Wolfengagen and Sergey Kosikov},
keywords = {semantic information processing, computational model, variable domains},
abstract = {In practice, when developing an information model, inheritance and composition mechanisms are used, which allows the model developer to extend the properties of the class. In this paper, we establish and use the difference between these two closely related representations when applied in aspect-oriented modeling. In particular, when an aspect is applied to extend the base class of the original model, the designer must choose to use composition. Depending on the composition order, indexing occurs, which can lead to the expansion of the base class by dynamic effects. With a different compositional order, a class narrowing occurs, since it becomes necessary to take into account an additional property. If you intend to define an alternative to a base class with advanced functionality, then inheritance should be used. The work demonstrates the power of the combined use of inheritance and composition, which allows us to develop an aspect-oriented modeling of a family of property transformations, in which a line of intermediate models of the working information process arises.}
}
@article{BRIAS2016151,
title = {Computing the reliability kernel of a time-variant system: Application to a corroded beam},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {12},
pages = {151-155},
year = {2016},
note = {8th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.07.566},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316308242},
author = {A. Brias and J-D. Mathias and G. Deffuant},
keywords = {Discrete systems, Dynamic systems, Initial states, Reliability analysis, Reliability kernel, System failures, Transition matrix, Reliable design},
abstract = {Time-variant reliability analysis aims at assessing the probability of failure of a time-variant system within a given time horizon. We illustrate in this paper the computation of the reliability kernel which is the set of initial states for which the probability of failure remains under a threshold within the considered time horizon. This paper supposes that the time-variant system is discrete in time and space with given probabilities of transition between space states. We use a recursive relation for computing the cumulative probability of failure of the system, linking the probability of failure at time t with the probability of being at a given state x (for all possible states) at time t — 1. Applying this relation, it is possible to compute the probability of failure at any starting point in the state space and hence to derive the reliability kernel. The computation of this kernel gives informations about the system which can be further helpful in reliable design. The approach is illustrated on an example of a steel beam under corrosion.}
}
@article{SPARKES202515,
title = {Quantum-inspired algorithm could give us better weather forecasts},
journal = {New Scientist},
volume = {265},
number = {3529},
pages = {15},
year = {2025},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(25)00218-0},
url = {https://www.sciencedirect.com/science/article/pii/S0262407925002180},
author = {Matthew Sparkes}
}
@article{CRAGG201463,
title = {Skills underlying mathematics: The role of executive function in the development of mathematics proficiency},
journal = {Trends in Neuroscience and Education},
volume = {3},
number = {2},
pages = {63-68},
year = {2014},
issn = {2211-9493},
doi = {https://doi.org/10.1016/j.tine.2013.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S2211949313000422},
author = {Lucy Cragg and Camilla Gilmore},
keywords = {Mathematics, Executive function, Working memory, Development},
abstract = {The successful learning and performance of mathematics relies on a range of individual, social and educational factors. Recent research suggests that executive function skills, which include monitoring and manipulating information in mind (working memory), suppressing distracting information and unwanted responses (inhibition) and flexible thinking (shifting), play a critical role in the development of mathematics proficiency. This paper reviews the literature to assess concurrent relationships between mathematics and executive function skills, the role of executive function skills in the performance of mathematical calculations, and how executive function skills support the acquisition of new mathematics knowledge. In doing so, we highlight key theoretical issues within the field and identify future avenues for research.}
}
@article{DAVIS2024307,
title = {AI rising in higher education: opportunities, risks and limitations},
journal = {Asian Education and Development Studies},
volume = {13},
number = {4},
pages = {307-319},
year = {2024},
issn = {2046-3162},
doi = {https://doi.org/10.1108/AEDS-01-2024-0017},
url = {https://www.sciencedirect.com/science/article/pii/S2046316224000154},
author = {Adrian John Davis},
keywords = {Human mind, Human intelligence, Human consciousness, Artificial intelligence (AI), Artificial consciousness, Quality teaching},
abstract = {Purpose
The aim of this paper is twofold: to explore the significance and implications of the rise of AI technology for the field of tertiary education in general and, in particular, to answer the question of whether teachers can be replaced by intelligent AI systems such as androids, what that requires in terms of human capabilities and what that might mean for teaching and learning in higher education.
Design/methodology/approach
Given the interdisciplinary nature of this conceptual paper, a literature review serves as a methodological tool to access data pertaining to the research question posed in the paper.
Findings
This exploratory paper gathers a range of evidence from the philosophy of mind (the mind-body problem), Kahneman’s (2011) System 1 and System 2 models of the mind, Gödel’s (1951) Two Incompleteness Theorems, Polanyi’s (1958, 1966) theory of tacit knowing and Searle’s (1980) Chinese Room thought experiment to the effect that no AI system can ever fully replace a human being because no machine can replicate the human mind and its capacity for intelligence, consciousness and highly developed social skills such as empathy and cooperation.
Practical implications
AI is rising, but there are inherent limits to what machines can achieve when compared to human capabilities. An android can at most attain “weak AI”, that is, it can be smart but lack awareness or empathy. Therefore, an analysis of good teaching at the tertiary level shows that learning, knowledge and understanding go far beyond any quantitative processing that an AI machine does so well, helping us to appreciate the qualitative dimension of education and knowledge acquisition. ChatGPT is robotic, being AI-generated, but human beings thrive on the human-to-human interface – that is, human relationships and meaningful connections – and that is where the true qualitative value of educational attainment will be gauged.
Social implications
This paper has provided evidence that human beings are irreplaceable due to our unique strengths as meaning-makers and relationship-builders, our capacity for morality and empathy, our creativity, our expertise and adaptability and our capacity to build unity and cooperate in building social structures and civilization for the benefit of all. Furthermore, as society is radically automated, the purpose of human life and its reevaluation will also come into question. For instance, as more and more occupations are replaced by ChatGPT services, more and more people will be freed up to do other things with their time, such as caring for relatives, undertaking creative projects, studying further and having children.
Originality/value
The investigation of the scope and limitations of AI is significant for two reasons. First, the question of the nature and functions of a mind becomes critical to the possibility of replication because if the human mind is like a super-sophisticated computer, then the relationship between a brain and mind is similar (if not identical) to the relationship between a computer as machine hardware and its programme or software (Dreyfus, 1979). [ ] If so, it should be theoretically possible to understand its mechanism and reproduce it, and then it is just a matter of time before AI research and development can replicate the human mind and eventually replace a human teacher, especially if an AI machine can teach just as intelligently yet more efficiently and economically. But if AI has inherent limitations that preclude the possibility of ever having a human-like mind and thought processes, then our investigation can at least clarify in what ways AI/AGI – such as ChatGPT – could support teaching and learning at universities.}
}
@article{LEE2024101413,
title = {Cognitive flexibility training for impact in real-world settings},
journal = {Current Opinion in Behavioral Sciences},
volume = {59},
pages = {101413},
year = {2024},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2024.101413},
url = {https://www.sciencedirect.com/science/article/pii/S2352154624000640},
author = {Liz Y Lee and Máiréad P Healy and Nastassja L Fischer and Ke Tong and Annabel SH Chen and Barbara J Sahakian and Zoe Kourtzi},
abstract = {Interacting with complex and dynamic environments challenges the brain’s ability to adapt to change. This key ability known as cognitive flexibility involves learning the structure of the environment, switching attention between features, dimensions and tasks, and adopting new rules in the face of uncertainty. Training cognitive flexibility has strong potential to improve adaptive behavior across the lifespan with impact in real-world settings (e.g. educational, clinical). Here, we review evidence on the role of cognitive training in improving executive functions and the factors that may enhance the effectiveness of training programs. We propose that personalized and adaptive training programs that focus on the multifaceted abilities comprising cognitive flexibility are key for promoting adaptive behavior and lifelong learning in real-world settings.}
}
@article{LI2025124674,
title = {Aerodynamic analysis and hygrothermal transfer characteristics of cellulose evaporative cooling pads: A case study applied to protected agriculture},
journal = {Applied Thermal Engineering},
volume = {258},
pages = {124674},
year = {2025},
issn = {1359-4311},
doi = {https://doi.org/10.1016/j.applthermaleng.2024.124674},
url = {https://www.sciencedirect.com/science/article/pii/S1359431124023421},
author = {He Li and Chengji Zong and Jiarui Lu and Shumei Zhao and Weitang Song and Dongyan Yang},
keywords = {Direct evaporative cooling, CFD, Saturation efficiency, Pressure drop, Energy saving, Wind tunnel experiment},
abstract = {Evaporative cooling pads are clean media for cost-effective temperature management. However, the spatially integrated microclimate resulting from heat and mass transfer in the evaporative cooling pad is an uncertain and nonlinear complexity. Therefore, this purpose of this study is to present an innovative prediction model with computational fluid dynamics and evaluate the operating scenarios and geometrical properties of wet pads from quantitative metrics such as saturation efficiency and pressure drop. The reliability of the established numerical model of wet pads was verified by wind tunnel experiments and existing experiments, which predicted the outlet conditions in satisfactory conformity. The results showed that a wet pad with 8 mm ripple height and 19.6 mm ripple distance exhibits the best prospective, with energy consumption and specific water consumption reduced by 45.28 % and 26.26 %, respectively. Meanwhile, the cross strategy of 45_45° corrugation obliquity reduces the pressure drop by 18.95 %, providing uniform supply air distributions. The heat exchange of the wet pad is mainly concentrated within the first 35 mm from the inlet section, while the mass exchange of the wet pad is concentrated within the first 60 mm. The range of frontal air velocity with 0.9–2.5 m/s is recommend for the evaporative cooling system.}
}
@article{BOTTI2017481,
title = {Integrating ergonomics and lean manufacturing principles in a hybrid assembly line},
journal = {Computers & Industrial Engineering},
volume = {111},
pages = {481-491},
year = {2017},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2017.05.011},
url = {https://www.sciencedirect.com/science/article/pii/S0360835217302152},
author = {Lucia Botti and Cristina Mora and Alberto Regattieri},
keywords = {Lean manufacturing, Occupational safety, Ergonomics, Automation, Human factors, Hybrid assembly line},
abstract = {Lean manufacturing is a production method that was established in the wake of the Japanese Toyota Production System and rapidly established in the worldwide manufacturing industry. Lean characteristics combine just-in-time practices, work-in-progress and waste reduction, improvement strategies, defect-free production, and standardization. The primary goal of lean thinking is to improve profits and create value by minimizing waste. This study introduces a novel mathematical model to design lean processes in hybrid assembly lines. The aim was to provide an effective, efficient assembly line design tool that meets the lean principles and ergonomic requirements of safe assembly work. Given the production requirements, product characteristics and assembly tasks, the model defines the assembly process for hybrid assembly lines with both manual workers and automated assembly machines. Each assembly line solution ensures an acceptable risk level of repetitive movements, as required by current law. This model helps managers and practitioners to design hybrid assembly lines with both manual workers and automated assembly machines. The model was tested in a case study of an assembly line for hard shell tool cases. Results show that worker ergonomics is a key parameter of the assembly process design, as other lean manufacturing parameters, e.g. takt time, cycle time and work in progress.}
}
@article{LI2023297,
title = {BalanceHRNet: An effective network for bottom-up human pose estimation},
journal = {Neural Networks},
volume = {161},
pages = {297-305},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.01.036},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000485},
author = {Yaoping Li and Shuangcheng Jia and Qian Li},
keywords = {Multi-branch structure, Fusion, Balance structure, Branch attention},
abstract = {In the study of human pose estimation, which is widely used in safety and sports scenes, the performance of deep learning methods is greatly reduced in high overlap rate and crowded scenes. Therefore, we propose a bottom-up model, called BalanceHRNet, which is based on balanced high-resolution module and a new branch attention module. BalanceHRNet draws on the multi-branch structure and fusion method of a popular model HigherHRNet. And our model overcomes the shortcoming of HigherHRNet that cannot obtain a large enough receptive field. Specifically, through the connecting structure in balanced high-resolution module, we can connect almost all convolutional layers and obtain a sufficiently large receptive field. At the same time, the multi-resolution representation can be maintained due to the use of balanced high-resolution module, which enable our model to recognize objects with richer scales and obtain more complex semantics information. And for branch fusion method, we design branch attention to obtain the importance of different branches at different stages. Finally, our model improves the accuracy while ensuring a smaller amount of computation than HigherHRNet. The CrowdPose dataset is used as test dataset, and HigherHRNet, AlphaPose, OpenPose and so on are taken as comparison models. The AP measured by BalanceHRNet is 63.0%, increased by 3.1% compared to best model — HigherHRNet. We also demonstrate the effectiveness of our network through the COCO(2017) keypoint detection dataset. Compared with HigherHRNet-w32, the AP of our model is improved by 1.6%.}
}
@article{DESOYSA2025106885,
title = {Green with envy? The effects of inequality and equity within and across social groups on greenhouse gas emissions, 1990–2020},
journal = {World Development},
volume = {188},
pages = {106885},
year = {2025},
issn = {0305-750X},
doi = {https://doi.org/10.1016/j.worlddev.2024.106885},
url = {https://www.sciencedirect.com/science/article/pii/S0305750X24003565},
author = {Indra {de Soysa}},
keywords = {Inequality, Equity, Horizontal inequality, Greenhouse gas, Climate change},
abstract = {The idea that inequality and inequities drive climate change forms a strong discourse in environmental politics. Reducing inequality is promoted as a win–win solution for reducing greenhouse gases. Others view egalitarian processes as a potential threat since increasing the consumption possibilities of the bottom-rungs of society relative to the top would drive up higher overall emissions. Using the latest available data on greenhouse gas emissions and the adoption of green energy technology measured over three decades, this study finds that a variety of measurements of vertical and horizontal inequality and inequitable access to political resources correlate with lower emissions per capita and greater adoption of green energy technologies. Inequality works in the opposite way than often thought. Per capita income levels, contrarily, are robustly and consistently associated with higher emissions, results that support the view that it is overall wealth (consumption) that drives climate change, not its distribution. Reducing inequality and poverty poses a moral and practical conundrum because levelling up incomes within and between countries, given current levels of technology, will worsen the climate crisis. The basic results hold up to a barrage of robustness tests, such as alternative estimating methods, models, and data, and to formal tests of omitted variables bias. Understanding how emissions might be reduced while addressing questions of equity demands calls for much harder thinking, and potentially fewer slogans, such as “eco-social contracts” and “new green deals” that peddle win–win solutions to a ‘wicked problem.’}
}
@article{AMARAL20211,
title = {Overlapping but distinct: Distal connectivity dissociates hand and tool processing networks},
journal = {Cortex},
volume = {140},
pages = {1-13},
year = {2021},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2021.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S0010945221001088},
author = {Lénia Amaral and Fredrik Bergström and Jorge Almeida},
keywords = {Tools, Hands, Distal connectivity, Representation, Functional organization, fMRI},
abstract = {The processes and organizational principles of information involved in object recognition have been a subject of intense debate. These research efforts led to the understanding that local computations and feedforward/feedback connections are essential to our representations and their organization. Recent data, however, has demonstrated that distal computations also play a role in how information is locally processed. Here we focus on how long-range connectivity and local functional organization of information are related, by exploring regions that show overlapping category-preferences for two categories and testing whether their connections are related with distal representations in a category-specific way. We used an approach that relates functional connectivity with distal areas to local voxel-wise category-preferences. Specifically, we focused on two areas that show an overlap in category-preferences for tools and hands–the inferior parietal lobule/anterior intraparietal sulcus (IPL/aIPS) and the posterior middle temporal gyrus/lateral occipital temporal cortex (pMTG/LOTC) – and how connectivity from these two areas relate to voxel-wise category-preferences in two ventral temporal regions dedicated to the processing of tools and hands separately–the left medial fusiform gyrus and the fusiform body area respectively–as well as across the brain. We show that the functional connections of the two overlap areas correlate with categorical preferences for each category independently. These results show that regions that process both tools and hands maintain object topography in a category-specific way. This potentially allows for a category-specific flow of information that is pertinent to computing object representations.}
}
@article{TSAI20251293,
title = {VR Games for Teaching Lean Manufacturing Tools: A Case Study of Stool Manufacturing},
journal = {Procedia Computer Science},
volume = {253},
pages = {1293-1302},
year = {2025},
note = {6th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.01.191},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925001991},
author = {Tim Tsai and Zaneta Sarah Widjaja and Md Rakibul Hasan and Dhrumil Pithwa and Purna Sai {Teja Pinninti} and Rafiq Ahmad},
keywords = {Lean Manufacturing, Virtual Reality, Lean Tools, VR Games},
abstract = {This study investigates the efficacy of Virtual Reality (VR) in enhancing lean manufacturing training. By integrating VR with lean manufacturing principles, the aim is to compare performance and learning outcomes in traditional and lean scenarios. The research highlights the limitations of conventional training approaches in fully engaging learners and keeping pace with rapid technological advancements in manufacturing processes. Through the development of an interactive VR game focused on a stool manufacturing process, the study advances the use of PDCA framework to incorporate key lean manufacturing tools such as 5S Principles, Kanban, Poka-Yoke, and ergonomic improvements. The game development process is detailed, covering the preparation of 3D models, set up of virtual scenes, development of game function, design of user interface, and deployment. User testing reveals significant improvements in process efficiency and knowledge acquisition when employing lean-inspired scenarios within the VR environment. The study concludes with promising results, demonstrating the potential of VR in lean manufacturing training while also acknowledging the need for further research to validate these findings across a wider range of manufacturing processes.}
}
@article{HANOCH20021,
title = {“Neither an angel nor an ant”: Emotion as an aid to bounded rationality},
journal = {Journal of Economic Psychology},
volume = {23},
number = {1},
pages = {1-25},
year = {2002},
issn = {0167-4870},
doi = {https://doi.org/10.1016/S0167-4870(01)00065-4},
url = {https://www.sciencedirect.com/science/article/pii/S0167487001000654},
author = {Yaniv Hanoch},
keywords = {Bounded rationality, Emotion},
abstract = {The role of emotion as a source of bounded rationality has been largely ignored. Following Herbert Simon, economists as well as psychologists have mainly focused on cognitive constraints while neglecting to integrate the growing body of research on emotion which indicates that reason and emotion are interconnected. Accordingly, the present paper aims to bridge the existing gap. By establishing a link between the two domains of research, emotion and bounded rationality, it will be suggested that emotions work together with rational thinking in two distinct ways, and thereby function as an additional source of bounded rationality. The aim, therefore, is not to offer an alternative to bounded rationality; rather, the purpose is to elaborate and supplement themes emerging out of bounded rationality.}
}
@article{ANDRAS2021110734,
title = {Where do successful populations originate from?},
journal = {Journal of Theoretical Biology},
volume = {524},
pages = {110734},
year = {2021},
issn = {0022-5193},
doi = {https://doi.org/10.1016/j.jtbi.2021.110734},
url = {https://www.sciencedirect.com/science/article/pii/S0022519321001569},
author = {Peter Andras and Adam Stanton},
keywords = {Computational modelling, Socio-technical evolution, Socio-biological simulation, Human geography, Geography of speciation},
abstract = {In order to understand the dynamics of emergence and spreading of socio-technical innovations and population moves it is important to determine the place of origin of these populations. Here we focus on the role of geographical factors, such as land fertility and mountains in the context of human population evolution and distribution dynamics. We use a constrained diffusion-based computational model, computer simulations and the analysis of geographical and land-quality data. Our analysis shows that successful human populations, i.e. those which become dominant in their socio – geographical environment, originate from lands of many valleys with relatively low land fertility, which are close to areas of high land fertility. Many of the homelands predicted by our analysis match the assumed homelands of known successful populations (e.g. Bantus, Turkic, Maya). We also predict other likely homelands as well, where further archaeological, linguistic or genetic exploration may confirm the place of origin for populations with no currently identified urheimat. Our work is significant because it advances the understanding of human population dynamics by guiding the identification of the origin locations of successful populations.}
}
@article{JYOTSNA20231270,
title = {IntelEye: An Intelligent Tool for the Detection of Stressful State based on Eye Gaze Data While Watching Video},
journal = {Procedia Computer Science},
volume = {218},
pages = {1270-1279},
year = {2023},
note = {International Conference on Machine Learning and Data Engineering},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.01.105},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923001059},
author = {C. Jyotsna and J. Amudha and Amritanshu Ram and Giandomenico Nollo},
keywords = {Eye Tracking, Mental Health, Real Time Monitoring, K-Nearest Neighbour, Eye Gaze Measures, Welch Two Sample t-test},
abstract = {Technology to monitor mental health is gaining popularity as it helps to improve the cognitive and behavioral performance of an individual. Considering the growing need to monitor mental health, there is subsequent research in continuous and real-time monitoring technologies that can increase the quality of life by reducing the cost of health care. Eye tracking technology has played a significant role in monitoring a person's mental health. An intelligent system can apply several computational procedures to extract meaningful information from the massive physiological data obtained from eye tracking. The proposed model IntelEye is a tool to detect the stressful states of an individual while watching calm and stressful videos. The eye gaze measures based on pupil diameter, fixation, and blink were used for detecting stressful conditions. The data was collected from hospital employees, and the K Nearest Neighbor algorithm could successfully recognize the stressful states and the corresponding gaze location during stressful situations. IntelEye is not only identifying the stressful states but also has the novelty of identifying the scene and gaze location, making them stressful while watching the video.}
}
@article{TRELEAVEN198859,
title = {Parallel architecture overview},
journal = {Parallel Computing},
volume = {8},
number = {1},
pages = {59-70},
year = {1988},
note = {Proceedings of the International Conference on Vector and Parallel Processors in Computational Science III},
issn = {0167-8191},
doi = {https://doi.org/10.1016/0167-8191(88)90109-3},
url = {https://www.sciencedirect.com/science/article/pii/0167819188901093},
author = {Philip C. Treleaven},
keywords = {Computer architecture, parallel computing},
abstract = {An increasing number of parallel computer products are appearing in the market place. Their design motivations and market areas cover a broad spectrum: (i) Transaction Processing Systems, such as Parallel UNIX systems (e.g. SEQUENT Balance), for data processing applications; (ii) Numeric Supercomputers, such as Hypercube systems (e.g. INTEL iPSC), for scientific and engineering applications; (iii) VLSI Architectures, such as parallel microcomputers (e.g. INMOS Transputer), for exploiting very large scales of integration; (iv) High-Level Language Computers, such as Logic machines (e.g. FUJITSU Kabu-Wake), for symbolic computation; and (v) Neurocomputers, such as Connectionist computers (e.g. THINKING MACHINES Connection Machine), for general-purpose pattern matching applications. This survey paper gives an overview of these novel parallel computers and discusses the likely commercial impact of parallel computers.}
}
@article{BARTELL20051283,
title = {How hydrides misled chemists},
journal = {Spectrochimica Acta Part A: Molecular and Biomolecular Spectroscopy},
volume = {61},
number = {7},
pages = {1283-1286},
year = {2005},
note = {Honour Issue - Jim Durig},
issn = {1386-1425},
doi = {https://doi.org/10.1016/j.saa.2004.11.056},
url = {https://www.sciencedirect.com/science/article/pii/S1386142504006481},
author = {Lawrence S. Bartell},
keywords = {Models of molecular structure and force fields, Simple hydrides, Ligand-close-packing, Cautionary stories},
abstract = {Hydrogen-containing molecules are simple enough to be attractive subjects in experimental diffraction and spectroscopic studies and in quantum computations. Yet, the inferences about molecular structure and force fields originally drawn from studies of these subjects were significantly flawed. In recent developments the original models of structure invoked, such as hybridization, have been superseded. The reasons for this are briefly reviewed. What has emerged to account for molecular geometry, prevailing even over the popular VSEPR theory, is a model of geminal nonbonded interactions.}
}
@article{HERSHCOVICH2021107809,
title = {Thermal performance of sculptured tiles for building envelopes},
journal = {Building and Environment},
volume = {197},
pages = {107809},
year = {2021},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2021.107809},
url = {https://www.sciencedirect.com/science/article/pii/S036013232100216X},
author = {Cheli Hershcovich and René {van Hout} and Vladislav Rinsky and Michael Laufer and Yasha j. Grobman},
keywords = {Microclimate, Computational fluid dynamics simulations, Particle Image Velocimetry (PIV) experiments, Building envelope, Thermal insulation},
abstract = {Since effective thermal insulation of buildings could significantly decrease energy consumption worldwide, this study examined the potential of concrete tiles with complex geometries at improving thermal performance in building envelopes. The study focused on air flow characteristics that occur near the external surface of the tile, and their influence on the rate of heat transfer within the material. Six groups of sculptured tiles were developed, using a systematic approach (i.e., repetition of basic geometries), biomimicry, and inspiration from natural envelopes. Air flow behavior and heat transfer rates were examined at three different wind speeds, through both experiments and computational fluid dynamics simulations using a configuration of flow impingement that can be regarded as the worst case scenario. After successfully validating the data, additional numerical simulations were conducted for all developed tiles using the Star-CCM + commercial software. The results showed an improvement in the insulation performance of about half of all the tested cases. Moreover, significant improvements were seen in the geometries that mimicked animal fur, achieving heat transfer rates that were up to 24% lower than those achieved by smooth tiles. Our results indicate that the application of such tiles with increased thermal resistance could save on thermal insulation materials and improve the thermal performance of building façades.}
}
@article{HARA20239703,
title = {Reorganizing Cyber-Physical Configurations using User Activities for Human-in-the-Loop Cyber-Physical Systems},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {9703-9708},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.281},
url = {https://www.sciencedirect.com/science/article/pii/S240589632300633X},
author = {Tatsunori Hara and Yuki Okada and Jun Ota},
keywords = {Design, modelling and analysis of HMS, human-centered automation and design, cyber-physical system, design structure matrix, data utilization, product service system, smart home},
abstract = {This paper proposes a “human-in-the-loop design structure matrix (DSM)” method for understanding and reorganizing the structures of multiple cyber-physical systems (CPSs), focusing on user activities. By incorporating user activities as integral components in system engineering techniques, this method contributes to the design literature on human-in-the-loop CPS. Using the illustrative case of a smart home, we obtained the following types of clusters to review the structure and cyber-physical configurations of CPSs: clusters that retain the target user activity, clusters decoupled from the target user activity, and clusters across two different user activities. In the second type, we found a hub cluster regarding the cyber process of notifications to users, which enabled diverse data utilization among the clusters.}
}
@article{ASAI1991323,
title = {Discipline Pascal with descriptive environment; precise writing to learn programming and to avoid errors},
journal = {Computers & Education},
volume = {16},
number = {4},
pages = {323-335},
year = {1991},
issn = {0360-1315},
doi = {https://doi.org/10.1016/0360-1315(91)90006-D},
url = {https://www.sciencedirect.com/science/article/pii/036013159190006D},
author = {Hitohisa Asai},
abstract = {Dijkstra's article “On the cruelty of really teaching computing science” [l] has encouraged me to review my thoughts and experience. In the teaching environments of an introductory programming course (CS1, CS2 and others), I have discovered a mental gap in the minds of some students, which has often led to difficulties in class. A Pascal statement contains highly condensed information. In order to deal with this type of information, the students must apply a certain thinking level. In a Pascal statement, much information is concealed behind the written words. For example, consider a variable in a program. The data type and locality of the variable are not explicitly expressed in a statement. In other words, this condensed information is hiding, e.g. invisible. Hence a degree of the student's ability to focus on it may fade away in his mind. If programming code demands writing precise information in a statement then the student's difficulty may be alleviated because he can see it. This would be an attempt to narrow the gap by writing Discipline Pascal code closer to the precise thinking level. I believe that this proposal would also bring benefits to experienced programmers.}
}
@article{SINCLAIR2004169,
title = {Improving computer-assisted instruction in teaching higher-order skills},
journal = {Computers & Education},
volume = {42},
number = {2},
pages = {169-180},
year = {2004},
issn = {0360-1315},
doi = {https://doi.org/10.1016/S0360-1315(03)00070-8},
url = {https://www.sciencedirect.com/science/article/pii/S0360131503000708},
author = {Kelsey J Sinclair and Carl E Renshaw and Holly A Taylor},
keywords = {Evaluation methodologies, Multimedia/hypermedia systems, Secondary education, Applications in subject areas},
abstract = {Computer-assisted instruction (CAI) has been shown to enhance rote memory skills and improve higher order critical thinking skills. The challenge now is to identify what aspects of CAI improve which specific higher-order skills. This study focuses on the effectiveness of using CAI to teach logarithmic graphing and dimensional analysis. Two groups of ninth graders participated in a one-class period laboratory. Experiment 1 compared a fully automated computer laboratory to an equivalent paper-and-pencil exercise. Experiment 2 compared the same automated computer laboratory in Experiment 1 with a revised, less automated computer version. Both the paper-and-pencil exercise and the less automated computer exercise required students to perform basic mathematical calculations. The results from a post-test revealed that very few students were able to master the complex task of dimensional analysis, but students who took the paper-based and revised, less automated version scored higher overall. These results imply that students required to perform basic calculations had a better understanding of the lab as a whole. These results suggest that until students master basic skills, they do not have the cognitive resources to concentrate on higher-order concepts. This is supported by cognitive load theory.}
}
@article{NAIK2017509,
title = {Metastability in Senescence},
journal = {Trends in Cognitive Sciences},
volume = {21},
number = {7},
pages = {509-521},
year = {2017},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2017.04.007},
url = {https://www.sciencedirect.com/science/article/pii/S1364661317300797},
author = {Shruti Naik and Arpan Banerjee and Raju S. Bapi and Gustavo Deco and Dipanjan Roy},
keywords = {healthy aging, whole-brain computational modeling, metastability},
abstract = {The brain during healthy aging exhibits gradual deterioration of structure but maintains a high level of cognitive ability. These structural changes are often accompanied by reorganization of functional brain networks. Existing neurocognitive theories of aging have argued that such changes are either beneficial or detrimental. Despite numerous empirical investigations, the field lacks a coherent account of the dynamic processes that occur over our lifespan. Taking advantage of the recent developments in whole-brain computational modeling approaches, we hypothesize that the continuous process of aging can be explained by the concepts of metastability − a theoretical framework that gives a systematic account of the variability of the brain. This hypothesis can bridge the gap between existing theories and the empirical findings on age-related changes.}
}
@article{BENBOW19841643,
title = {Computational analysis of polymer processing: (Edited by J.R.A. Pearson and S.M. Richardson). Applied Science, 1983. £36.00. 343pp},
journal = {Chemical Engineering Science},
volume = {39},
number = {11},
pages = {1643},
year = {1984},
issn = {0009-2509},
doi = {https://doi.org/10.1016/0009-2509(84)80093-7},
url = {https://www.sciencedirect.com/science/article/pii/0009250984800937},
author = {J.J. Benbow}
}
@article{LU2025154,
title = {Simulation and classification optimization design of ultra-low temperature heat exchangers in dilution refrigerator using local thermal nonequilibrium model},
journal = {International Journal of Refrigeration},
volume = {174},
pages = {154-164},
year = {2025},
issn = {0140-7007},
doi = {https://doi.org/10.1016/j.ijrefrig.2025.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0140700725000866},
author = {Yian Lu and Jun Shen and Ya'nan Zhao and Danyang Liu},
keywords = {Dilution refrigeration, Ultra-low temperature heat exchanger, Classification optimization design, Numerical simulation, Local thermal nonequilibrium},
abstract = {Dilution refrigeration is widely used in a variety of cutting-edge scientific fields such as quantum computations. The ultra-low temperature heat exchangers play a crucial role for the ultimate performance of dilution refrigerator. To study the classification optimization design of ultra-low temperature heat exchangers, an integrated local thermal nonequilibrium model have used. By considering the ultra-low temperature heat exchangers as an integrated unit consisting of a number of interconnected sub-modules, an optimized design study of the geometrical parameter configurations of continuous heat exchanger, three-stage step heat exchanger, and connecting tubes has been carried out. Using the optimized configuration for continuous heat exchanger with a length of 19 m, an inner tube diameter of 10 mm, and an outer tube diameter of 15 mm at a molar flow rate of 1.5 mmol/s, the classification optimization of step heat exchanger is carried out. By adjusting the ratio of the sintered volume to the whole sintered volume (g1=0.15, g2=0.3 and g3=0.55) and the ratio of concentrated heat transfer area to the total heat transfer area of this stage (f1=0.35, f2=0.4 and f3=0.45), the heat transfer efficiency of the three-stage step heat exchanger can be further improved without additional resources. And the impact of length and diameter of connecting tubes has also been discussed in this paper. This study provides quantitative guidance for the design of ultra-low temperature heat exchangers, which is important for improving the overall performance of dilution refrigerators.}
}
@article{DRANKO2021738,
title = {Structural Analysis of Large-Scale Socio-Technical Systems Based on the Concept of Influence},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {13},
pages = {738-743},
year = {2021},
note = {20th IFAC Conference on Technology, Culture, and International Stability TECIS 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.10.540},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321019789},
author = {O.I. Dranko and Yu.G. Rykov and A.A. Karandeev},
keywords = {Complex systems, Cognitive maps, Influence, Structural analysis, Russian Federation economics structure},
abstract = {The paper is devoted to studying the stability of complex systems, which include diversified and heterogeneous elements. In order to analyze the stability of such systems, it is proposed to develop the process of dynamic determination of the main factors. For this purpose, this paper uses a variant of cognitive modeling based on the concept of “influence”. The proposed approach implies, in a sense, a broader view of the concept of the “fuzzy cognitive map” introduced by B. Kosko. “Influence” does not mean “causality”, allows a broader interpretation range, and is calculated in a special way that makes it possible to prove rigorous theorems. A complex system is represented as a digraph, and the selection of the main factors is proposed to be based on the concept of “influence”, which is introduced as follows. All nodes of the graph are assigned an abstract property “significance”, which allows you to compare heterogeneous factors, and a particular computational procedure is introduced that allows this “significance” to be calculated. The “influence” of factors on each other is defined as a mathematically formalized response in accordance with the introduced computational procedure of the “significance” of the studied factor to the variation in the “significance” of input factors. For example, the analysis of the sustainability of a large-scale model of the Russian economy in terms of the strength of the “influence” is provided.}
}
@article{VIJAYALAKSHMI20222382,
title = {Topological indices relating some nanostructures},
journal = {Materials Today: Proceedings},
volume = {68},
pages = {2382-2386},
year = {2022},
note = {4th International Conference on Advances in Mechanical Engineering},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2022.09.106},
url = {https://www.sciencedirect.com/science/article/pii/S2214785322059144},
author = {K. {Vijaya Lakshmi} and N. Parvathi},
keywords = {Topological indices, Chemical graph theory, Molecular structure, Nanotechnology, Nanotubes, Nanosheet},
abstract = {The application of mathematics mainly graph theory has a significant role in the development of chemistry. Topological indices act as a bridge between mathematics and chemistry. The topological index is nothing but a numerical quantity that has yielded valuable insights in terms of molecular structure. In nanotechnology, the structural characterization of the molecular species and the nanoparticles are indicated by the indices. These indices identify the symmetry of molecular structures and provide them a scientific terminology to predict qualities like boiling temperatures, viscosity, and gyrating radius. The main objective of the current paper is to compute topological indices with the corresponding formulae, moreover, these indices help in predicting physicochemical properties without the involvement of the laboratory. The current research paper investigated the computation of degree-based topological indices for the nanotubes HAC5C6C7[a,b] and TU(C4C6C8[a,b].}
}
@article{KHODADADI2022104354,
title = {Design exploration by using a genetic algorithm and the Theory of Inventive Problem Solving (TRIZ)},
journal = {Automation in Construction},
volume = {141},
pages = {104354},
year = {2022},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104354},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522002278},
author = {Anahita Khodadadi and Peter {von Buelow}},
keywords = {Conceptual design, Design exploration, TRIZ, Genetic algorithm, Multi-objective design},
abstract = {This paper presents a computational design exploration method called GA+TRIZ, which aids designers in defining the design problem clearly, making a parametric model where pertinent variables are included, obtaining a series of suitable solutions, and resolving existing conflicts among design objectives. The goal is to include the designer's qualitative and performance-based quantitative design goals in the design process, while promoting innovative ideas for resolving contradictory design objectives. The method employed is a Genetic Algorithm (GA), earlier implemented in an automated design exploration process called ParaGen, in combination with the Theory of Inventive Problem Solving (TRIZ), a novel methodology to assist architects and structural engineers in the conceptual phase of design. The GA+TRIZ method promotes automated design exploration, investigation of unexpected solutions, and continuous interaction with the computational generating system. Finally, this paper presents two examples that illustrate how the GA+TRIZ method assists designers in problem structuring, design exploration, and decision-making.}
}
@article{ROY2022105849,
title = {EEG based stress analysis using rhythm specific spectral feature for video game play},
journal = {Computers in Biology and Medicine},
volume = {148},
pages = {105849},
year = {2022},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2022.105849},
url = {https://www.sciencedirect.com/science/article/pii/S0010482522006023},
author = {Shidhartho Roy and Monira Islam and Md. Salah Uddin Yusuf and Nushrat Jahan},
keywords = {Beta–Alpha ratio, EEG, Stress-relaxation modeling, Topography, Video gameplay},
abstract = {Background and objective:
For the emerging significance of mental stress, various research directives have been established over time to understand better the causes of stress and how to deal with it. In recent years, the rise of video gameplay has been unprecedented, further triggered by the lockdown imposed due to the COVID-19 pandemic. Several researchers and organizations have contributed to the practical analysis of the impacts of such extended periods of gameplay, which lacks coordinated studies to underline the outcomes and reflect those in future game designing and public awareness about video gameplay. Investigations have mainly focused on the “gameplay stress” based on physical syndromes. Some studies have analyzed the effects of video gameplay with Electroencephalogram (EEG), Magnetic resonance imaging (MRI), etc., without concentrating on the relaxation procedure after video gameplay.
Methods:
This paper presents an end-to-end stress analysis for video gaming stimuli using EEG. The power spectral density (PSD) of the Alpha and Beta bands is computed to calculate the Beta-to-Alpha ratio (BAR). The Alpha and Beta band power is computed, and the Beta-to-Alpha band power ratio (BAR) has been determined. In this article, BAR is used to denote mental stress. Subjects are chosen based on various factors such as gender, gameplay experience, age, and Body mass index (BMI). EEG is recorded using Scan SynAmps2 Express equipment. There are three types of video gameplay: strategic, puzzle, and combinational. Relaxation is accomplished in this study by using music of various pitches. Two types of regression analysis are done to mathematically model stress and relaxation curve. Brain topography is rendered to indicate the stressed and relaxed region of the brain.
Results:
In the relaxed state, the subjects have BAR 0.701, which is considered the baseline value. Non-gamer subjects have an average BAR of 2.403 for 1 h of strategic video gameplay, whereas gamers have 2.218 BAR concurrently. After 12 minutes of listening to low-pitch music, gamers achieved 0.709 BAR, which is nearly the baseline value. In comparison to Quartic regression, the 4PL symmetrical sigmoid function performs regression analysis with fewer parameters and computational power.
Conclusion:
Non-gamers experience more stress than gamers, whereas strategic games stress the human brain more. During gameplay, the beta band in the frontal region is mostly activated. For relaxation, low pitch music is the most useful medium. Residual stress is evident in the frontal lobe when the subjects have listened to high pitch music. Quartic regression and 4PL symmetrical sigmoid function have been employed to find the model parameters of the relaxation curve. Among them, quartic regression performs better in terms of Akaike information criterion (AIC) and R2 measure.}
}
@article{MA2024109594,
title = {Robust adaptive learning framework for semi-supervised pattern classification},
journal = {Signal Processing},
volume = {224},
pages = {109594},
year = {2024},
issn = {0165-1684},
doi = {https://doi.org/10.1016/j.sigpro.2024.109594},
url = {https://www.sciencedirect.com/science/article/pii/S0165168424002135},
author = {Jun Ma and Guolin Yu},
keywords = {Non-convex distance metric, Semi-supervised robust classification, Generalized adaptive robust loss function, Outliers, Kernel method},
abstract = {Hessian scatter regularized twin support vector machine (HSR-TSVM) employs hinge loss function and L2-distance metric, which makes it ineffective in dealing with outliers and noise data problems. Aiming to this problem, this paper a novel robust adaptive learning framework CL2,pHSR-TSVM is developed for semi-supervised classification tasks. In CL2,pHSR-TSVM, the generalized adaptive robust loss function Lδ(u) is first innovatively introduced to overcome the problem that hinge loss function is not sensitive to noise and outliers. Intuitively, Lδ(u) can improve the robustness of the model by selecting different robust loss functions for different learning tasks during the learning process via the adaptive parameter δ. Secondly, the robust distance metric capped L2,p-norm is introduced in CL2,pHSR-TSVM to reduce and eliminate the exaggerated influence of L2-distance metric on the learning process of outliers, especially when the outliers are far from the normal data distribution, by setting the appropriate parameters. Furthermore, to improve the computational efficiency of CL2,pHSR-TSVM, the fast CL2,pHSR-TSVM is presented for semi-supervised classification tasks. Finally, two effective algorithms are designed to solve our methods respectively, and the convergence and computational complexity are analyzed theoretically. Experimental results demonstrate the effectiveness and robustness of our methods.}
}
@article{FALCONE2020110269,
title = {Soft computing techniques in structural and earthquake engineering: a literature review},
journal = {Engineering Structures},
volume = {207},
pages = {110269},
year = {2020},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2020.110269},
url = {https://www.sciencedirect.com/science/article/pii/S0141029619322540},
author = {Roberto Falcone and Carmine Lima and Enzo Martinelli},
keywords = {Structural engineering, Earthquake engineering, Fuzzy logic, Neural network, Swarm intelligence, Evolutionary computing},
abstract = {Although civil engineering problems are often characterized by significant levels of complexity, they are generally approached and solved by combining several practitioners’ skills, such as intuition, past experience, logical reasoning, mathematical elaborations, and physical sense. This is also the case of problems in structural and earthquake engineering whose solution is generally based on the so-called “engineer’s judgment”. However, heuristic theories and algorithms within the framework of “soft computing” can provide a more rational and systematic way to approach and solve problems in these areas. As a matter of fact, the aforementioned algorithms have been recently utilized in several branches of engineering and applied sciences. This paper proposes a state-of-the-art review of the main applications of soft computing techniques to relevant structural and earthquake engineering problems. Specifically, the applications of fuzzy computing, evolutionary computing, swarm intelligence, and neural networks, as well as their hybrid combinations, are analyzed with the aim to examine their capability and limitations in modeling, simulation, and optimization problems.}
}
@article{JIA2011445,
title = {Evolutionary level set method for structural topology optimization},
journal = {Computers & Structures},
volume = {89},
number = {5},
pages = {445-454},
year = {2011},
issn = {0045-7949},
doi = {https://doi.org/10.1016/j.compstruc.2010.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0045794910002567},
author = {Haipeng Jia and H.G. Beom and Yuxin Wang and Song Lin and Bo Liu},
keywords = {Evolutionary structure optimization, Structure topology optimization, Intelligent computation, Level set method},
abstract = {This paper proposes an evolutionary accelerated computational level set algorithm for structure topology optimization. It integrates the merits of evolutionary structure optimization (ESO) and level set method (LSM). Traditional LSM algorithm is largely dependent on the initial guess topology. The proposed method combines the merits of ESO techniques with those of LSM algorithm, while allowing new holes to be automatically generated in low strain energy within the nodal neighboring region during optimization. The validity and robustness of the new algorithm are supported by some widely used benchmark examples in topology optimization. Numerical computations show that optimization convergence is accelerated effectively.}
}
@article{FANOUS2023105658,
title = {Challenges and prospects of climate change impact assessment on mangrove environments through mathematical models},
journal = {Environmental Modelling & Software},
volume = {162},
pages = {105658},
year = {2023},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2023.105658},
url = {https://www.sciencedirect.com/science/article/pii/S1364815223000440},
author = {Majdi Fanous and Jonathan M. Eden and Renji Remesan and Alireza Daneshkhah},
keywords = {Mangrove environments, Climate change, Hydro-morphodynamic modelling, Adaptation policies, Machine learning, Data-driven modelling},
abstract = {The impacts of climate change, especially sea-level rise, are an increasing threat to the world’s coastal regions. Following recommendations made by the United Nations about the preservation of mangrove environments, particularly given their potential for effective natural defence against wave-driven hazards, a series of experiments have been conducted to quantify the ability of mangroves to counter climate change impacts. To date, these experiments have been limited by computational cost and inability to model multiple scenarios. With improved data quality and availability, machine learning has enormous potential to supplement, or even replace, existing numerical methods. This article presents both an outline of the importance of protecting mangrove environments and a review of methods currently used to quantify the capacity of mangroves to adapt to climate change impacts. In view of the limitations of existing numerical methods, the article also discusses the potential of machine learning as an efficient and effective alternative.}
}
@article{SURESHBABU2006277,
title = {Modeling and simulation in signal transduction pathways: a systems biology approach},
journal = {Biochimie},
volume = {88},
number = {3},
pages = {277-283},
year = {2006},
issn = {0300-9084},
doi = {https://doi.org/10.1016/j.biochi.2005.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0300908405001999},
author = {C.V. {Suresh Babu} and Eun {Joo Song} and Young Sook Yoo},
keywords = {Biological systems, Signal transduction, Systems biology, Modeling and simulation},
abstract = {Modeling, the heart of systems biology, of complex processes (example: signal transduction) is a wide scientific discipline where many approaches from different areas are confronted with the aim of better understanding, identifying and modeling of complex data coming from various sources. The purpose of this paper is to introduce the basic steps of systems biology view towards signaling pathways, which mainly deals with the computational tools. The paper emphasizes the modeling and simulation approach in the signal transduction pathways using the topologies of the biochemical reactions with an overview of the different types of software platforms. Finally, we demonstrated the epidermal growth factor receptor signaling pathway model as an example to study the growth factor mediated signaling system with biological experiments. This paper will enables new comers to underline the strengths of the computational approaches towards signal transduction, as well as to highlight the systems biology research directions.}
}
@article{LIU2025103366,
title = {LoViT: Long Video Transformer for surgical phase recognition},
journal = {Medical Image Analysis},
volume = {99},
pages = {103366},
year = {2025},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2024.103366},
url = {https://www.sciencedirect.com/science/article/pii/S1361841524002913},
author = {Yang Liu and Maxence Boels and Luis C. Garcia-Peraza-Herrera and Tom Vercauteren and Prokar Dasgupta and Alejandro Granados and Sébastien Ourselin},
keywords = {Surgical phase recognition, Long videos, Temporally-rich spatial feature, Multi-scale, Phase transition-aware},
abstract = {Online surgical phase recognition plays a significant role towards building contextual tools that could quantify performance and oversee the execution of surgical workflows. Current approaches are limited since they train spatial feature extractors using frame-level supervision that could lead to incorrect predictions due to similar frames appearing at different phases, and poorly fuse local and global features due to computational constraints which can affect the analysis of long videos commonly encountered in surgical interventions. In this paper, we present a two-stage method, called Long Video Transformer (LoViT), emphasizing the development of a temporally-rich spatial feature extractor and a phase transition map. The temporally-rich spatial feature extractor is designed to capture critical temporal information within the surgical video frames. The phase transition map provides essential insights into the dynamic transitions between different surgical phases. LoViT combines these innovations with a multiscale temporal aggregator consisting of two cascaded L-Trans modules based on self-attention, followed by a G-Informer module based on ProbSparse self-attention for processing global temporal information. The multi-scale temporal head then leverages the temporally-rich spatial features and phase transition map to classify surgical phases using phase transition-aware supervision. Our approach outperforms state-of-the-art methods on the Cholec80 and AutoLaparo datasets consistently. Compared to Trans-SVNet, LoViT achieves a 2.4 pp (percentage point) improvement in video-level accuracy on Cholec80 and a 3.1 pp improvement on AutoLaparo. Our results demonstrate the effectiveness of our approach in achieving state-of-the-art performance of surgical phase recognition on two datasets of different surgical procedures and temporal sequencing characteristics. The project page is available at https://github.com/MRUIL/LoViT.}
}
@article{WU2024100295,
title = {Analyzing K-12 AI education: A large language model study of classroom instruction on learning theories, pedagogy, tools, and AI literacy},
journal = {Computers and Education: Artificial Intelligence},
volume = {7},
pages = {100295},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100295},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000985},
author = {Di Wu and Meng Chen and Xu Chen and Xing Liu},
keywords = {AI education, Large language models, Pedagogical approaches, AI literacy},
abstract = {There is growing recognition among researchers and stakeholders about the significant impact of artificial intelligence (AI) technology on classroom instruction. As a crucial element in developing AI literacy, AI education in K-12 schools is increasingly gaining attention. However, most existing research on K-12 AI education relies on experiential methodologies and suffers from a lack of quantitative analysis based on extensive classroom data, hindering a comprehensive depiction of AI education's current state at these educational levels. To address this gap, this article employs the advanced semantic understanding capabilities of large language models (LLMs) to create an intelligent analysis framework that identifies learning theories, pedagogical approaches, learning tools, and levels of AI literacy in AI classroom instruction. Compared with the results of manual analysis, analysis based on LLMs can achieve more than 90% consistency. Our findings, based on the analysis of 98 classroom instruction videos in central Chinese cities, reveal that current AI classroom instruction insufficiently foster AI literacy, with only 35.71% addressing higher-level skills such as evaluating and creating AI. AI ethics are even less commonly addressed, featured in just 5.1% of classroom instruction. We classified AI classroom instruction into three categories: conceptual (50%), heuristic (18.37%), and experimental (31.63%). Correlation analysis suggests a significant relationship between the adoption of pedagogical approaches and the development of advanced AI literacy. Specifically, integrating Project-based/Problem-based learning (PBL) with Collaborative learning appears effective in cultivating the capacity to evaluate and create AI.}
}
@article{ALGERAFI202461,
title = {Designing of an effective e-learning website using inter-valued fuzzy hybrid MCDM concept: A pedagogical approach},
journal = {Alexandria Engineering Journal},
volume = {97},
pages = {61-87},
year = {2024},
issn = {1110-0168},
doi = {https://doi.org/10.1016/j.aej.2024.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S1110016824003867},
author = {Mohammed Abdulwahab Al-Gerafi and Shankha Shubhra Goswami and Mohammad Amir Khan and Quadri Noorulhasan Naveed and Ayodele Lasisi and Abdulaziz AlMohimeed and Ahmed Elaraby},
keywords = {E-Learning, Pedagogy, IVF, COPRAS, EDAS, PIV, MCDM},
abstract = {The demand for effective e-learning platforms requires prioritizing pedagogical excellence in online educational websites. Current approaches struggle with uncertainties, hindering optimal e-learning environments due to a lack of comprehensive evaluation in traditional methods. An integrated approach is crucial to avoid inefficiencies and incomplete understanding of learner needs. This research introduces a pioneering methodology integrating Inter-Valued Fuzzy (IVF) COPRAS-EDAS-PIV hybrid Multiple Criteria Decision-Making (MCDM) techniques, addressing existing limitations. Leveraging the IVF concept allows a holistic assessment of pedagogical parameters, ensuring a thorough understanding of the decision-making landscape. The study involves an extensive literature review, parameter identification, and data acquisition through group decision-making. The selection of a suitable e-learning website is based on seven conflicting parameters, and preference ranking orders are prescribed using EDAS, COPRAS, and PIV MCDM model. Rigorous analysis using these techniques facilitates precise ranking and informed decision-making. The findings underscore the efficacy of the proposed IVF-MCDM approach for the design of a pedagogical e-learning website. Final results reveal that alternative 5 as the most preferable, followed by alternative 2, while alternative 3 is the least favored option among the group. Comparative and sensitivity analyses validate the approach’s superiority, enabling stakeholders to make well-informed decisions for optimal e-learning websites that cater to diverse learner needs, thus enhancing the overall online learning experience.}
}
@article{YANG2024111470,
title = {A lattice-theoretic model of three-way conflict analysis},
journal = {Knowledge-Based Systems},
volume = {288},
pages = {111470},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111470},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124001059},
author = {Han Yang and Yiyu Yao and Keyun Qin},
keywords = {MS.F-bilattice, Three-way decision, Three-way conflict analysis},
abstract = {Pawlak conflict analysis uses a three-valued situation table for representing the ratings of a set of agents on a set of issues. This paper examines a lattice-theoretic basis of three-way conflict analysis. Qualitatively, we adopt a triangle, namely, an MS.F-bilattice, to characterize the structures of agents ratings, which gives an intuitive and effective tool for ordering a single agent and a pair of agents. We consider a strength ordering and a rating ordering to construct MS.F-bilattices. By applying the principles of three-way decision as thinking in threes, we trisect, according to the rating ordering, the nine pairs of ratings into three regions: potential opposition (PO), potential conflict (PC), and potential support (PS) regions. For each region, according to the strength ordering, we construct the weak, medium, and strong three subregions. Quantitatively, we introduce opposition-alliance and support-alliance measures based on the rating ordering for one issue to trisect these pairs of ratings into PO, PC, and PS regions. We study opposition strength, conflict strength, and support strength measures based on strength ordering for one issue to trisect each of the three regions into three subregions. Finally, we extend the five types of measures for a set of issues. The lattice-theoretic model of three-way conflict analysis clarifies the semantics of pairs of ratings by two agents and gives a different perspective on trisection methods in conflict analysis. To demonstrate the value of the proposed methods, we analyze a case study of the development planning of the Gansu Province of China.}
}
@article{LOPEZ2015289,
title = {DAMQT 2.0: A new version of the DAMQT package for the analysis of electron density in molecules},
journal = {Computer Physics Communications},
volume = {192},
pages = {289-294},
year = {2015},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2015.02.027},
url = {https://www.sciencedirect.com/science/article/pii/S0010465515000855},
author = {Rafael López and Jaime Fernández Rico and Guillermo Ramírez and Ignacio Ema and David Zorrilla},
keywords = {Electron density, Electrostatic potential, Electric field, Hellmann–Feynman forces, Density deformations},
abstract = {DAMQT 2.0 is a new version of the DAMQT package for the analysis of electron density in molecules and the fast computation of the density, density deformations, electrostatic potential and field, and Hellmann–Feynman forces. Algorithms for the partition of the electron density and the computation of related properties like density deformations, electrostatic potential and field and Hellmann–Feynman forces have been improved and their codes, fully rewritten. MPI versions of the most computational demanding modules are now included in the package for parallel computation. The Graphical User Interface has been also enhanced, with new features including a 2D plotter and significant improvements in the 3D viewer.
Program summary
Program title: DAMQT 2.0 Catalogue identifier: AEDL_v2_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AEDL_v2_0.html Program obtainable from: CPC Program Library, Queen’s University, Belfast, N. Ireland Licensing provisions: GPLv3 No. of lines in distributed program, including test data, etc.: 317,270 No. of bytes in distributed program, including test data, etc.: 40,193,220 Distribution format: tar.gz Programming language: Fortran90 and C++. Computer: Any. Operating system: Linux, Windows (7, 8). RAM: 200 Mbytes Classification: 16.1. Catalogue identifier of previous version: AEDL_v1_0 Journal reference of previous version: Comput. Phys. Comm. 180(2009)1654 External routines: Qt (4.8 or higher), OpenGL (3.x or higher), freeGLUT 2.8.x Nature of problem: Analysis of the molecular electron density and density deformations, including fast evaluation of electrostatic potential, electric field and Hellmann–Feynman forces on nuclei. Solution method: The method of Deformed Atoms in Molecules, reported elsewhere [1], is used for partitioning the molecular electron density into atomic fragments, which are further expanded in spherical harmonics times radial factors. The partition is used for defining molecular density deformations and for the fast calculation of several properties associated to density. Restrictions: Density must come from an LCAO calculation (any level) with spherical (not Cartesian) Slater or Gaussian functions. Unusual features: The program contains an OPEN statement to binary files (stream) in several files. This statement has not a standard syntax in Fortran 90. Two possibilities are considered in conditional compilation: Intel’s ifort and Fortran2003 standard. The latter is applied to compilers other than ifort (gfortran uses this one, for instance). Additional comments: Quick-start guide and User’s manual in PDF format included in the package. User’s manual is also accessible from the Graphical User Interface. The distribution file for this program is over 40 Mbytes and therefore is not delivered directly when downloaded or Email is requested. Instead an html file giving details of how the program can be obtained is sent. Running time: Largely dependent on the system size and the module run (from fractions of a second to hours). References:[1]J. Fernández Rico, R. López, I. Ema and G. Ramírez, J. Mol. Struct. Theochem 727 (2005) 115.}
}
@incollection{GILLESPIE2024124,
title = {Differentiation},
editor = {Samuel M. Scheiner},
booktitle = {Encyclopedia of Biodiversity (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Oxford},
pages = {124-133},
year = {2024},
isbn = {978-0-323-98434-8},
doi = {https://doi.org/10.1016/B978-0-12-822562-2.00167-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128225622001675},
author = {Rosemary G. Gillespie},
keywords = {Adaptive landscape, Cline, Coalescent process, Gene flow, Hybrid zone, Local adaptation, Natural selection, Neutral theory, Population structure and Speciation},
abstract = {Differentiation generally considers the accumulation of genetic differences between populations or species but can be applied more broadly to the diversification of genes, organisms, and populations. This article examines how elementary evolutionary and ecological processes lead to differentiation in both the narrow and broad senses and introduces successively more complex processes involved in differentiation. Most mutations are selectively neutral and neutral evolution can be considered the default process of genomic change, with natural selection being measurable from changes in allele frequencies that lead to deviation from neutrality. While the basic tenets of genetic differentiation are well established, the advent of new genomic technologies and associated computational tools have provided unprecedented insights into the mechanism of genetic differentiation and how these might lead to the formation of species. Recent work has highlighted in particular the importance of genetic admixture events and genomic interactions in shaping the process of differentiation.}
}
@article{LI20221,
title = {Computing for Chinese Cultural Heritage},
journal = {Visual Informatics},
volume = {6},
number = {1},
pages = {1-13},
year = {2022},
issn = {2468-502X},
doi = {https://doi.org/10.1016/j.visinf.2021.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S2468502X21000644},
author = {Meng Li and Yun Wang and Ying-Qing Xu},
keywords = {Cultural computing, Chinese Cultural Heritage, Computable cultural ecosystem, Mogao caves, Guqin},
abstract = {Implementing computational methods for preservation, inheritance, and promotion of Cultural Heritage (CH) has become a research trend across the world since the 1990s. In China, generations of scholars have dedicated themselves to studying the country’s rich CH resources; there are great potential and opportunities in the field of computational research on specific cultural artefacts or artforms. Based on previous works, this paper proposes a systematic framework for Chinese Cultural Heritage Computing that consists of three conceptual levels which are Chinese CH protection and development strategy, computing process, and computable cultural ecosystem. The computing process includes three modules: (1) data acquisition and processing, (2) digital modeling and database construction, and (3) data application and promotion. The modules demonstrate the computing approaches corresponding to different phases of Chinese CH protection and development, from digital preservation and inheritance to presentation and promotion. The computing results can become the basis for the generation of cultural genes and eventually the formation of computable cultural ecosystem Case studies on the Mogao caves in Dunhuang and the art of Guqin, recognized as world’s important tangible and intangible cultural heritage, are carried out to elaborate the computing process and methods within the framework. With continuous advances in data collection, processing, and display technologies, the framework can provide constructive reference for building up future research roadmaps in Chinese CH computing and related fields, for sustainable protection and development of Chinese CH in the digital age.}
}
@article{ZHAO201568,
title = {Approximate methods for optimal replacement, maintenance, and inspection policies},
journal = {Reliability Engineering & System Safety},
volume = {144},
pages = {68-73},
year = {2015},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2015.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0951832015001957},
author = {Xufeng Zhao and Khalifa N. Al-Khalifa and Toshio Nakagawa},
keywords = {Hazard function, Mean time to failure, Age replacement, Imperfect maintenance, Approximate inspection},
abstract = {It might be difficult sometimes to derive theoretical and numerical solutions for analytical maintenance modelings due to the computational complexity. This paper takes up several approximate models in maintenance theory, by using the cumulative hazard function H(t) and the newly proposed asymptotic MTTF (Mean Time to Failure) skilfully. We firstly denote by tx the time when the expected number of failures is x. Using H(tx)=x, we estimate failure times, model age and periodic replacements, and sequential imperfect maintenance. Motivated by the asymptotic method of computation of MTTF, we secondly model the expected cost rate for a parallel system when replacement is made at system failure, and give approximate computations for the sequential inspection policy. Optimizations of each model are obtained approximately in an easier way. When failure times have a Weibull distribution, it is shown from numerical examples that the obtained approximate optimal solutions have good approximations of the exact ones.}
}
@article{FOUGERES2021100025,
title = {Fuzzy engineering design semantics elaboration and application},
journal = {Soft Computing Letters},
volume = {3},
pages = {100025},
year = {2021},
issn = {2666-2221},
doi = {https://doi.org/10.1016/j.socl.2021.100025},
url = {https://www.sciencedirect.com/science/article/pii/S2666222121000149},
author = {Alain-Jérôme Fougères and Egon Ostrosi},
keywords = {Fuzzy collaborative design, Fuzzy collaborative system, Natural language processing, Fuzzy requirement engineering, Fuzzy engineering design platform},
abstract = {Product design activities are predicated on fuzzy modelling, given that verbalising and interpreting engineering requirements are inherently fuzzy processes. The aim of this paper is to present a method for fuzzy intelligent requirement engineering from natural language to Computer-Aided Design (CAD) models. The field exploring the dynamics of computational processes from fuzzy linguistic modelling to fuzzy design modelling is complex and remains under-explored. No existing research has been identified which focuses specifically on fuzzy requirements engineering from natural language to CAD modelling. This paper seeks to address this by providing a design formalisation system based on five key principles. These principles are used to set out a computing procedure which follows a method broken up into six phases. The results of these six phases are fuzzy semantic graphs, which provide engineering requirements according to reliable design information. The approach is put into practice using the fuzzy agent-based tool developed by the authors, called F-EGEON (Fuzzy Engineering desiGn sEmantics elabOration and applicatioN). The proposed method is illustrated through an application from the automotive industry.}
}
@article{STROM2024111887,
title = {Diborane anharmonic vibrational frequencies and Intensities: Experiment and theory},
journal = {Journal of Molecular Spectroscopy},
volume = {400},
pages = {111887},
year = {2024},
issn = {0022-2852},
doi = {https://doi.org/10.1016/j.jms.2024.111887},
url = {https://www.sciencedirect.com/science/article/pii/S0022285224000146},
author = {Aaron I. Strom and Ibrahim Muddasser and Guntram Rauhut and David T. Anderson},
keywords = {Matrix Isolation Spectroscopy, Anharmonic Vibrational Dynamics, Infrared Spectroscopy, Computational Spectroscopy, Diborane, Fermi Resonance, Darling-Dennison Resonance},
abstract = {The vibrational dynamics of diborane have been extensively studied both theoretically and experimentally ever since the bridge structure of diborane was established in the 1950s. Numerous infrared and several Raman spectroscopic studies have followed in the ensuing years at ever increasing levels of spectral resolution. In parallel, ab initio computations of the underlying potential energy surface have progressed as well as the methods to calculate the anharmonic vibration dynamics beyond the double harmonic approximation. Nevertheless, even 70 years after the bridge structure of diborane was established, there are still significant discrepancies between experiment and theory for the fundamental vibrational frequencies of diborane. In this work we use parahydrogen (pH2) matrix isolation infrared spectroscopy to characterize six fundamental vibrations of B2H6 and B2D6 and compare them with results from configuration-selective vibrational configuration interaction theory. The calculated frequencies and intensities are in very good agreement with the pH2 matrix isolation spectra, even several combination bands are well reproduced. We believe that the reason discrepancies have existed for so long is related to the large amount of anharmonicity that is associated with the bridge BH stretching modes. However, the calculated frequencies and intensities reported here for the vibrational modes of all three boron isotopologues of B2H6 and B2D6 are within ± 2.00 cm−1 and ± 1.44 cm−1, respectively, of the experimental frequencies and therefore a refined vibrational assignment of diborane has been achieved.}
}
@article{UWIZEYE2016647,
title = {A comprehensive framework to assess the sustainability of nutrient use in global livestock supply chains},
journal = {Journal of Cleaner Production},
volume = {129},
pages = {647-658},
year = {2016},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2016.03.108},
url = {https://www.sciencedirect.com/science/article/pii/S0959652616301792},
author = {Aimable Uwizeye and Pierre J. Gerber and Rogier P.O. Schulte and Imke J.M. {de Boer}},
keywords = {Nitrogen, Phosphorus, Nutrient use efficiency, Life-cycle thinking, Livestock supply chain, Soil nutrient stock change},
abstract = {The assessment of the performance of nutrient use along livestock supply chains can help to identify targeted nutrient management interventions, with a goal to benchmark and to monitor the improvement of production practices. It is necessary, therefore, to develop indicators that are capable to describe all nutrient dynamics and management along the chain. This paper proposed a comprehensive framework, based on life-cycle thinking, to assess the sustainability of nitrogen and phosphorus use. The proposed framework represents nutrient flows in typical livestock supply chain from the “cradle-to-primary-processing-gate”, including crop/pasture production, animal production, and primary processing stage as well as the transportation of feed materials, live-animals or animal products. In addition, three indicators, including the life-cycle nutrient use efficiency (life-cycle-NUE), life-cycle net nutrient balance (life-cycle-NNB) and nutrient hotspot index (NHI) were proposed and tested in a case study of mixed dairy supply chains in Europe. Proposed indicators were found to be suitable to describe different aspects of nitrogen and phosphorus dynamics and, therefore, were all needed. Moreover, the disaggregation of life-cycle-NUE and life-cycle-NNB has been investigated and the uncertainties related to the choice of the method used to estimate changes in nutrient soil stock have been discussed. Given these uncertainties, the choice of method to compute the proposed indicators is determined by data availability and by the goal and scope of the exercise.}
}
@incollection{LUDLOW2025,
title = {I-Language and E-Language},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2025},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00558-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041005585},
author = {Peter Ludlow},
keywords = {I-language, E-Language, Internalism, Externalism, Individualism, Intensional, Language faculty, Scientific method, Chomsky, Language organ},
abstract = {Chomsky (1986) introduced the distinction between I-language and E-language—a distinction that has consequences for how scientific investigation proceeds. In the former case we are investigating internal mechanisms of the language faculty. In the latter case we are investigating external, social constructs. In this essay we examine the distinguishing features of these approaches, raise questions about the coherence of each approach, and finally raise questions about whether the distinction itself stands up to close scrutiny.}
}
@article{BLACKSCHAFFER20162374289516665393,
title = {Training Pathology Residents to Practice 21st Century Medicine: A Proposal},
journal = {Academic Pathology},
volume = {3},
pages = {2374289516665393},
year = {2016},
issn = {2374-2895},
doi = {https://doi.org/10.1177/2374289516665393},
url = {https://www.sciencedirect.com/science/article/pii/S2374289521002189},
author = {W. Stephen Black-Schaffer and Jon S. Morrow and Michael B. Prystowsky and Jacob J. Steinberg},
keywords = {competency, progressive responsibility, residency training},
abstract = {Scientific advances, open information access, and evolving health-care economics are disrupting extant models of health-care delivery. Physicians increasingly practice as team members, accountable to payers and patients, with improved efficiency, value, and quality. This change along with a greater focus on population health affects how systems of care are structured and delivered. Pathologists are not immune to these disruptors and, in fact, may be one of the most affected medical specialties. In the coming decades, it is likely that the number of practicing pathologists will decline, requiring each pathologist to serve more and often sicker patients. The demand for increasingly sophisticated yet broader diagnostic skills will continue to grow. This will require pathologists to acquire appropriate professional training and interpersonal skills. Today’s pathology training programs are ill designed to prepare such practitioners. The time to practice for most pathology trainees is typically 5 to 6 years. Yet, trainees often lack sufficient experience to practice independently and effectively. Many studies have recognized these challenges suggesting that more effective training for this new century can be implemented. Building on the strengths of existing programs, we propose a redesign of pathology residency training that will meet (and encourage) a continuing evolution of American Board of Pathology and Accreditation Council for Graduate Medical Education requirements, reduce the time to readiness for practice, and produce more effective, interactive, and adaptable pathologists. The essence of this new model is clear definition and acquisition of core knowledge and practice skills that span the anatomic and clinical pathology continuum during the first 2 years, assessed by competency-based metrics with emphasis on critical thinking and skill acquisition, followed by individualized modular training with intensively progressive responsibility during the final years of training. We anticipate that implementing some or all aspects of this model will enable residents to attain a higher level of competency within the current time-based constraints of residency training.}
}
@article{SCOTNEY2020102981,
title = {The form of a ‘half-baked’ creative idea: Empirical explorations into the structure of ill-defined mental representations},
journal = {Acta Psychologica},
volume = {203},
pages = {102981},
year = {2020},
issn = {0001-6918},
doi = {https://doi.org/10.1016/j.actpsy.2019.102981},
url = {https://www.sciencedirect.com/science/article/pii/S0001691819303129},
author = {Victoria S. Scotney and Jasmine Schwartz and Nicole Carbert and Adam Saab and Liane Gabora},
keywords = {Analogy, Art, Creative process, Honing, Mental representation, Structure mapping},
abstract = {Creative thought is conventionally believed to involve searching memory and generating multiple independent candidate ideas followed by selection and refinement of the most promising. Honing theory, which grew out of the quantum approach to describing how concepts interact, posits that what appears to be discrete, separate ideas are actually different projections of the same underlying mental representation, which can be described as a superposition state, and which may take different outward forms when reflected upon from different perspectives. As creative thought proceeds, this representation loses potentiality to be viewed from different perspectives and manifest as different outcomes. Honing theory yields different predictions from conventional theories about the mental representation of an idea midway through the creative process. These predictions were pitted against one another in two studies: one closed-ended and one open-ended. In the first study, participants were interrupted midway through solving an analogy problem and wrote down what they were thinking in terms of a solution. In the second, participants were instructed to create a painting that expressed their true essence and describe how they conceived of the painting. For both studies, naïve judges categorized these responses as supportive of either the conventional view or the honing theory view. The results of both studies were significantly more consistent with the predictions of honing theory. Some implications for creative cognition, and cognition in general, are discussed.}
}
@article{URBINA2022100031,
title = {The commoditization of AI for molecule design},
journal = {Artificial Intelligence in the Life Sciences},
volume = {2},
pages = {100031},
year = {2022},
issn = {2667-3185},
doi = {https://doi.org/10.1016/j.ailsci.2022.100031},
url = {https://www.sciencedirect.com/science/article/pii/S2667318522000022},
author = {Fabio Urbina and Sean Ekins},
keywords = {Artificial intelligence, Design-make-test, Machine learning, Molecule design, Recurrent neural networks},
abstract = {Anyone involved in designing or finding molecules in the life sciences over the past few years has witnessed a dramatic change in how we now work due to the COVID-19 pandemic. Computational technologies like artificial intelligence (AI) seemed to become ubiquitous in 2020 and have been increasingly applied as scientists worked from home and were separated from the laboratory and their colleagues. This shift may be more permanent as the future of molecule design across different industries will increasingly require machine learning models for design and optimization of molecules as they become “designed by AI”. AI and machine learning has essentially become a commodity within the pharmaceutical industry. This perspective will briefly describe our personal opinions of how machine learning has evolved and is being applied to model different molecule properties that crosses industries in their utility and ultimately suggests the potential for tight integration of AI into equipment and automated experimental pipelines. It will also describe how many groups have implemented generative models covering different architectures, for de novo design of molecules. We also highlight some of the companies at the forefront of using AI to demonstrate how machine learning has impacted and influenced our work. Finally, we will peer into the future and suggest some of the areas that represent the most interesting technologies that may shape the future of molecule design, highlighting how we can help increase the efficiency of the design-make-test cycle which is currently a major focus across industries.}
}
@article{FIELDS2025,
title = {Paradox or illusion?},
journal = {Physics of Life Reviews},
year = {2025},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2025.04.009},
url = {https://www.sciencedirect.com/science/article/pii/S1571064525000600},
author = {Chris Fields},
keywords = {Free Energy Principle, Introspection, Perception}
}
@article{CUMMING2012923,
title = {Better compounds faster: the development and exploitation of a desktop predictive chemistry toolkit},
journal = {Drug Discovery Today},
volume = {17},
number = {17},
pages = {923-927},
year = {2012},
issn = {1359-6446},
doi = {https://doi.org/10.1016/j.drudis.2012.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S1359644612000840},
author = {John G. Cumming and Jon Winter and Andrew Poirrette},
abstract = {Today's drug designer has access to vast quantities of data and an impressive array of sophisticated computational methods. At the same time, there is increasing pressure on the pharmaceutical industry to improve its productivity and reduce candidate drug attrition. We set out to develop a highly integrated suite of design and data analysis tools underpinned by the best predictive chemistry methods and models, with the aim of enabling multi-disciplinary compound design teams to make better informed design decisions. In this article we address the challenges of developing a powerful, flexible and user-friendly toolkit, and of maximising its exploitation by the design community. We describe the impact the toolkit has had on drug discovery projects and give our perspective on the future direction of this activity.}
}
@article{RATTEN2023100857,
title = {Generative artificial intelligence (ChatGPT): Implications for management educators},
journal = {The International Journal of Management Education},
volume = {21},
number = {3},
pages = {100857},
year = {2023},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2023.100857},
url = {https://www.sciencedirect.com/science/article/pii/S1472811723000952},
author = {Vanessa Ratten and Paul Jones},
keywords = {Academic research, Teaching, Learning, Digital transformation, Management education, Artificial intelligence, ChatGPT},
abstract = {ChatGPT has been one of the most talked about computer programs amongst management educators in recent weeks due to its transformative ability to change how assessments are undertaken and graded. Unlike other educational technologies that can be tracked when used, ChatGPT has superior abilities that make it virtually untraceable when used. This creates a dilemma for management educators wanting to utilise the technology whilst staying relevant but also interested in authentic learning. Thus, it is critical for management educators to quickly implement policies regarding ChatGPT and subsequent new generative artificial intelligence because of its ease of use and affordability. This article is conceptual in nature and discusses ChatGPT as a generative form of artificial intelligence that presents challenges for management educators that need to be addressed through appropriate strategies. Thereby contributing to the literature on how technological innovations can be included in curriculum design and management learning practices. Practical and managerial implications are stated that highlight the critical need to re-examine existing education practices as a way of incorporating new technological innovation that can be utilised in a beneficial way.}
}
@article{XU2024100549,
title = {Prediction of environmental pollution hazard index of water conservancy system based on fuzzy logic},
journal = {International Journal of Thermofluids},
volume = {21},
pages = {100549},
year = {2024},
issn = {2666-2027},
doi = {https://doi.org/10.1016/j.ijft.2023.100549},
url = {https://www.sciencedirect.com/science/article/pii/S2666202723002641},
author = {Bingshu Xu},
keywords = {Water Conservancy System, Fuzzy Logic, Combined Forecasting, Environmental Pollution, Hazard Index Prediction},
abstract = {The water conservancy framework is a significant foundation for China's financial and social turn of events. Simultaneously monetary and social turn of events, likewise tremendously affect the biological climate. In the prediction method of the environmental pollution hazard index of water conservancy projects, a risk assessment must be carried out to enhance its applicability. This article mainly focuses on traditional mathematical prediction models and combines the characteristics of fuzzy logic mathematics (good nonlinear quality) to establish a new combination prediction method. By comparing the convergence errors of traditional methods with this method, the results show that the algorithm proposed in this paper can effectively reduce prediction errors with fewer iterations, making it stable at around 500 times, with good convergence and high computational accuracy. The fuzzy logic-based prediction method for environmental pollution hazards in water conservancy systems proposed in this article can overcome the difficulty of nonlinear combination prediction. This achieves real-time prediction of environmental pollution hazards in water conservancy systems and shortens response time, greatly improving the accuracy of prediction.}
}