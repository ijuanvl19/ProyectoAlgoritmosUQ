@article{ZHENG2022104214,
title = {Evaluation of an automated phenotyping algorithm for rheumatoid arthritis},
journal = {Journal of Biomedical Informatics},
volume = {135},
pages = {104214},
year = {2022},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2022.104214},
url = {https://www.sciencedirect.com/science/article/pii/S1532046422002192},
author = {Henry W. Zheng and Veena K. Ranganath and Lucas C. Perry and David A. Chetrit and Karla M. Criner and Angela Q. Pham and Richard Seto and Sitaram Vangala and David A. Elashoff and Alex A.T. Bui},
keywords = {Phenotyping algorithm, Computational phenotyping, Rheumatoid arthritis, PheKB},
abstract = {To better understand the challenges of generally implementing and adapting computational phenotyping approaches, the performance of a Phenotype KnowledgeBase (PheKB) algorithm for rheumatoid arthritis (RA) was evaluated on a University of California, Los Angeles (UCLA) patient population, focusing on examining its performance on ambiguous cases. The algorithm was evaluated on a cohort of 4,766 patients, along with a chart review of 300 patients by rheumatologists against accepted diagnostic guidelines. The performance revealed low sensitivity towards specific subtypes of positive RA cases, which suggests revisions in features used for phenotyping. A close examination of select cases also indicated a significant portion of patients with missing data, drawing attention to the need to consider data integrity as an integral part of phenotyping pipelines, as well as issues around the usability of various codes for distinguishing cases. We use patterns in the PheKB algorithm’s errors to further demonstrate important considerations when designing a phenotyping algorithm.}
}
@article{CHELLAM2018928,
title = {Intrusion Detection in Computer Networks using Lazy Learning Algorithm},
journal = {Procedia Computer Science},
volume = {132},
pages = {928-936},
year = {2018},
note = {International Conference on Computational Intelligence and Data Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.05.108},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918308408},
author = {Aditya Chellam and Ramanathan L and Ramani S},
keywords = {Lazy Learning, Intrusion Detection System, Machine Learning, IBk, kNN},
abstract = {Intrusion Detection Systems (IDS) are used in computer networks to safeguard the integrity and confidentiality of sensitive data. In recent years, network traffic has become sizeable enough to be considered under the big data domain. Current machine learning based techniques used in IDS are largely defined on eager learning paradigms which lose performance efficiency by trying to generalize training data before receiving queries thereby incurring overheads for trivial computations. This paper, proposes the use of lazy learning methodologies to improve overall performance of IDS. A novel heuristic weight based indexing technique has been used to overcome the drawback of high search complexity inherent in lazy learning. IBk and LWL, two popular lazy learning algorithms have been compared and applied on the NSL-KDD dataset for simulating a real-world like scenario and comparing their relative performances with hw-IBk. The results of this paper clearly indicate lazy algorithms as a viable solution for real-world network intrusion detection.}
}
@article{HADJTAIEB2014238,
title = {Ontology-based approach for measuring semantic similarity},
journal = {Engineering Applications of Artificial Intelligence},
volume = {36},
pages = {238-261},
year = {2014},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2014.07.015},
url = {https://www.sciencedirect.com/science/article/pii/S0952197614001833},
author = {Mohamed Ali {Hadj Taieb} and Mohamed {Ben Aouicha} and Abdelmajid {Ben Hamadou}},
keywords = {Semantic similarity, WordNet ontology, Taxonomic knowledge, Taxonomical parameters},
abstract = {The challenge of measuring semantic similarity between words is to find a method that can simulate the thinking process of human. The use of computers to quantify and compare semantic similarities has become an important area of research in various fields, including artificial intelligence, knowledge management, information retrieval and natural language processing. The development of efficient measures for the computation of concept similarity is fundamental for computational semantics. Several computational measures rely on knowledge resources to quantify semantic similarity, such as the WordNet « is a » taxonomy. Several of these measures are based on taxonomical parameters to achieve the best expression possible for the semantics of content. This paper presents a new measure for quantifying the degree of the semantic similarity between concepts and words based on the WordNet hierarchy and using a number of topological parameters related to the “is a” taxonomy. Our proposal combines, in a complementary way, the hyponyms and depth parameters. This measure takes the problem of fine granularity into account. It is argued, however, that WordNet sense distinctions are highly fine-grained even for humans. We, therefore, propose a new method to quantify the hyponyms subgraph of a given concept based on depth distribution. Common nouns datasets (RG65, MC30 and AG203), medical terms dataset (MED38) and verbs dataset (YP130) formed by word pairs are used in the assessment. We start by calculating semantic similarities and then compute the correlation coefficient between human judgement and computational measures. The results demonstrate that, compared to other currently available computational methods, the measure presented in this study yields into better levels of performance. Compared to several measures, it shows good accuracy covering all the pairwises of the verbs dataset YP130.}
}
@article{CLARO20121042,
title = {Assessment of 21st century ICT skills in Chile: Test design and results from high school level students},
journal = {Computers & Education},
volume = {59},
number = {3},
pages = {1042-1053},
year = {2012},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2012.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0360131512000887},
author = {Magdalena Claro and David D. Preiss and Ernesto {San Martín} and Ignacio Jara and J. Enrique Hinostroza and Susana Valenzuela and Flavio Cortes and Miguel Nussbaum},
keywords = {ICT skills, 21st century skills, Information literacy, Computer literacy, Higher-order thinking skills, Digital divide, Rasch model},
abstract = {This paper describes a study that evaluates fifteen-year-old Chilean students Information and Communication Technology (ICT) skills. The paper presents an operational definition of ICT skills, an instrument measuring these skills as well as the students' results in the test. The definition of ICT skills used considers Chile's curricular framework, functional and cognitive skills. Specifically, ICT skills were defined as the capacity to solve problems of information, communication and knowledge in digital environments. A performance-based assessment was designed in a virtual environment to measure these skills. The analysis of the results showed that the majority of students were able to solve tasks related to the use of information as consumers, i.e., approximately three quarters of the students were able to search for information and half of them were also able to organize and manage digital information. Additionally, they show that very few students were able to succeed in tasks related to the use of information as producers, i.e., only one third of the students were able to develop their own ideas in a digital environment and less than one fifth were able to refine digital information and create a representation in a digital environment. Socioeconomic group, access, daily use and confidence in doing ICT-related activities were all positively associated with higher scores, showing the need to implement strategies to compensate this inequality, possibly by explicitly defining these aims in the national curriculum.}
}
@article{SERNAM2015647,
title = {Maturity model of transdisciplinary knowledge management},
journal = {International Journal of Information Management},
volume = {35},
number = {6},
pages = {647-654},
year = {2015},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2015.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S026840121500064X},
author = {Edgar {Serna M.}},
keywords = {Management, Complexity, Interdisciplinary, Multidisciplinary, Complex thinking},
abstract = {In this article a maturity model for the management of transdisciplinary knowledge is presented, although research nowadays is transdisciplinary the different maturity models proposed in the literature are oriented towards interdisciplinary knowledge management, and, at most, they are oriented toward multidisciplinary knowledge management. The objective is proposing an evolutionary model which accepts knowledge as intensely active and dynamic and evolving in maturity from the early stages of research. But this is possible only if the research team adopt a clear, clean and joint process of disciplinary integration and transdisciplinary integration of the produced and discovered knowledge. In this way, the results of research will have a greater influence on society and they also will be adopted by society.}
}
@article{SHYER2025204018,
title = {Transcending the hegemony of the molecular machine through an organic renewal of biology and biomedicine},
journal = {Cells & Development},
pages = {204018},
year = {2025},
issn = {2667-2901},
doi = {https://doi.org/10.1016/j.cdev.2025.204018},
url = {https://www.sciencedirect.com/science/article/pii/S2667290125000257},
author = {Amy E. Shyer and Alan R. Rodrigues},
keywords = {Supracellular morphological self-organization, systems biology, molecular machine, organicism, cancer, tissue organization field theory, chronic disease},
abstract = {The dominant approach to the study of living systems in the 20th century into today has been that of a reductionist approach focused on genetics and biochemistry. The hunt for genes and the elucidation of their biochemical outputs has organized funding in research, educational curricula, academic promotion, and the distribution of prestige through awards. Such reductionism has gone hand in hand with an ontology of the machine. We will discuss how viewing life as if it emanated from a set of molecular machines is the main bottleneck in addressing key questions in biology. We will discuss how moving beyond it is not contingent on new technologies but rather a refreshed perspective of life that can be termed “organic”. Furthermore, we suggest that the study of how form arises, morphogenesis, is the key to an organic renewal of biology and biomedicine. Although morphogenesis is currently seen as a subsidiary branch of developmental biology as well as the consequence of molecular patterning processes at the subcellular scale, we will argue that morphology and its self-organizing capacity at the supracellular scale is the fundamental nexus in embryonic development as well as disease. We see the inability to appreciate form through an organic supracellular perspective as the principal bottleneck for making inroads into health issues such as cancer and the chronic disease epidemic.}
}
@article{KIM2024105050,
title = {G-TRACE: Grouped temporal recalibration for video object segmentation},
journal = {Image and Vision Computing},
volume = {147},
pages = {105050},
year = {2024},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2024.105050},
url = {https://www.sciencedirect.com/science/article/pii/S0262885624001549},
author = {Jiyun Kim and JooHo Kim and Sungeun Hong},
keywords = {Semi-supervised video object segmentation, Memory attention, Hierarchical grouping},
abstract = {In Semi-supervised Video Object Segmentation (SVOS), there is a critical emphasis on enhancing the memory and readout mechanisms for frame matching, especially in relation to temporal dynamics. Current methods predominantly use 2D CNNs for encoding video frames, which unfortunately neglects the crucial aspect of addressing temporal variations in individual frames and their associated masks during the encoding process. One potential solution would be to implement temporal models such as 3D CNNs instead of 2D CNNs, but this significantly increases computational requirements, making it impractical for real-world SVOS applications. In this paper, we introduce the Grouped Temporal Recalibration with Attention for Convolutional Encoders (G-TRACE), a novel plug-and-play module that is compatible with various existing SVOS frameworks. G-TRACE uses hierarchical memory-centric attention and integrates effortlessly with 2D CNNs, offering a novel approach to temporal modeling that operates orthogonally to traditional frame matching methods. Extensive evaluations on four widely-used benchmarks demonstrate that our method consistently delivers significant performance improvements over various baseline models.}
}
@incollection{STRUBE20012158,
title = {Cognitive Science: Overview},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {2158-2166},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/01441-8},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767014418},
author = {G. Strube},
abstract = {Cognitive science (CS) emerged in 1975 as a field spanning parts of psychology, artificial intelligence, linguistics, philosophy, anthropology, and the neurosciences. CS is unique in its basic tenet that cognitive processes are computations, a perspective which allows for direct comparison of natural and artificial intelligence and emphasizes a methodology that integrates formal and empirical analyses with computational synthesis. Computer simulations have therefore become the hallmark of CS. Today, CS is an internationally established discipline. Its dominant tradition, close to symbol-processing architectures, has been enriched by neural networks and by the recognition that human cognition rests on both biological and cultural foundations. CS studies cognitive systems: organisms, machines, or any combination of these, acting in dynamically changing environments. Cognition in CS denotes advanced control mechanisms that allow for sophisticated adaptation through computations operating on mental representations. CS recognizes that cognition in biological systems is implemented in brain processes, but emphasizes analyses at the functional level, with cognitive neuroscience relating both domains. Applications of CS may be found in the design of software, in human factors engineering, health care, and education.}
}
@article{GAO2022109390,
title = {A kernel-free fuzzy reduced quadratic surface ν-support vector machine with applications},
journal = {Applied Soft Computing},
volume = {127},
pages = {109390},
year = {2022},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2022.109390},
url = {https://www.sciencedirect.com/science/article/pii/S1568494622005336},
author = {Zheming Gao and Yiwen Wang and Min Huang and Jian Luo and Shanshan Tang},
keywords = {Data mining, -SVM, Kernel-free SVM, Fuzzy SVM, Binary classification},
abstract = {The kernel-free support vector machine (SVM) models are recently developed and studied to overcome some drawbacks induced by the kernel-based SVM models. To further improve the classification accuracy and computational efficiency of existing kernel-free quadratic surface support vector machine (QSSVM) models, a novel kernel-free ν-fuzzy reduced QSSVM model is proposed. The proposed model utilizes a reduced quadratic surface for nonlinear binary classification as well as reducing the effect of outliers in the data set. Some theoretical properties are rigorously studied, especially, the effects of the parameter ν on the dual feasibility and the number of support vectors. Computational experiments are conducted on some public benchmark data sets to indicate the superior performance of the proposed model over some well-known binary classification models. The numerical results also favors the higher training efficiency of the proposed model over those of other kernel-free SVM models. Moreover, the proposed model is successfully applied to the prodromal detection of Alzheimer’s Disease with good performance, by using the data from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database.}
}
@article{KADANE1985256,
title = {Parallel and sequential computation: a statistician's view},
journal = {Journal of Complexity},
volume = {1},
number = {2},
pages = {256-263},
year = {1985},
issn = {0885-064X},
doi = {https://doi.org/10.1016/0885-064X(85)90014-7},
url = {https://www.sciencedirect.com/science/article/pii/0885064X85900147},
author = {Joseph B Kadane},
abstract = {I borrow themes from statistics—epsecially the Bayesian ideas underlying average-case analysis and ideas of sequential design of experiments—to discuss when parallel computation is likely to be an attractive technique.}
}
@article{WATANOBE2014417,
title = {Hybrid intelligence aspects of programming in *AIDA algorithmic pictures},
journal = {Future Generation Computer Systems},
volume = {37},
pages = {417-428},
year = {2014},
note = {Special Section: Innovative Methods and Algorithms for Advanced Data-Intensive Computing Special Section: Semantics, Intelligent processing and services for big data Special Section: Advances in Data-Intensive Modelling and Simulation Special Section: Hybrid Intelligence for Growing Internet and its Applications},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2013.12.031},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X13002951},
author = {Yutaka Watanobe and Nikolay Mirenkov},
keywords = {Programming in pictures, *AIDA, Algorithmic CyberFilm},
abstract = {Programming in algorithmic pictures (a-pictures) is an approach where pictures and moving pictures are used as super-characters for representing features of computational algorithms and data structures. Within this approach some “data space structures” are traversed by “fronts of computation” and/or some “units of activity” are traversed by flows of data. There are compound a-pictures to define algorithmic steps (called Algorithmic CyberFrames) and generic a-pictures to define the contents of compound pictures. Compound a-pictures are assembled into special series to represent some algorithmic features. The series are assembled into an Algorithmic CyberFilm. The generic/compound a-pictures and their series are developed and acquired in special galleries of an open type where supportive pictures of embedded clarity annotations are also included. In this paper, *AIDA (Star-AIDA) modeling/programming language (AIDA stands for Animation and Images to Develop Algorithms) and its Filmification modeling (F-modeling) environment are briefly considered and examples of programs in a-pictures are provided. A special attention is paid to *AIDA programs as special information resources which perception, comprehension and cognition depend on interaction with, at least, a few different but mutually supplementing features of a-pictures. A scheme of data/knowledge acquisition based on clusters of different views and how this acquisition is oriented to enhancing user’s ability within works on developing application models, corresponding algorithms and programs, are presented.}
}
@article{HORVATH2015161,
title = {Ubiquitous computer aided design: A broken promise or a Sleeping Beauty?},
journal = {Computer-Aided Design},
volume = {59},
pages = {161-175},
year = {2015},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2014.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0010448514002358},
author = {Imre Horváth and Regine W. Vroom},
keywords = {Ubiquitous computing, Computer aided design, Ubiquitous design enablers, Competing technology exploitation, Ubiquitous CAD applications},
abstract = {As a novel computational approach, ubiquitous computing was emerging at the beginning of the 1980s and has reached a rather mature level by now. It assumes that computing can be available anywhere, anytime and in any context due to technological developments, social demands and calm implementations. Over the years, the opportunities of this computing paradigm have been explored and the benefits have been exploited successfully in many application fields. This survey paper addresses ubiquitous computing from the perspective of enabling computer aided design. The specific objectives of the reported survey are to: (i) give an overall account of the current status of ubiquitous computing and technologies, (ii) cast light on how ubiquitous computing has influenced the development of CAD systems, tools, and methods, and (iii) critically investigate future development opportunities of ubiquitous computing enabled computer aided design. First, the paper discusses the principles and typical technologies of ubiquitous computing. Then, the development and spectrum of the so-called standard computer aided design tasks are analyzed from a computational point of view. Afterwards, the already implemented design enabling functionalities are discussed and some additional functional possibilities are considered. The literature provides evidence that ubiquitous computing has not managed to revolutionize the methodologies or the systems of computer aided design so far, though many researchers intensively studied the affordances and the application possibilities of ubiquitous technologies. One reason is that ubiquitous computing technologies had in the last two decades to compete with other kinds of computational technologies, such as high-capacity computing, high-speed networking, immersive virtual reality, knowledge ontologies, smart software agents, mobile communication, etc., which had a much stronger influence on the development of computer aided design methods and systems. In combination with the rather conservative and conventionalist industrial practice of CAD system development and application, this may explain why the ubiquitous computing revolution remained weak in computer aided design. The literature clearly indicates that application of ubiquitous technologies did not lead to radically new functionalities that could have been exploited by the concerned industries. Consequently, it seems to be possible that computer aided design simply steps over the paradigm of ubiquitous computing and expects new functionalities from the emerging new computing paradigms, such as brain–computer interfacing, cyber–physical computing, biological computing, or quantum computing.}
}
@article{ROLLS2024102636,
title = {A theory of hippocampal function: New developments},
journal = {Progress in Neurobiology},
volume = {238},
pages = {102636},
year = {2024},
issn = {0301-0082},
doi = {https://doi.org/10.1016/j.pneurobio.2024.102636},
url = {https://www.sciencedirect.com/science/article/pii/S0301008224000728},
author = {Edmund T. Rolls and Alessandro Treves},
keywords = {Hippocampus, Episodic memory, Attractor network, Memory recall, Neocortical memory, Consolidation},
abstract = {We develop further here the only quantitative theory of the storage of information in the hippocampal episodic memory system and its recall back to the neocortex. The theory is upgraded to account for a revolution in understanding of spatial representations in the primate, including human, hippocampus, that go beyond the place where the individual is located, to the location being viewed in a scene. This is fundamental to much primate episodic memory and navigation: functions supported in humans by pathways that build ‘where’ spatial view representations by feature combinations in a ventromedial visual cortical stream, separate from those for ‘what’ object and face information to the inferior temporal visual cortex, and for reward information from the orbitofrontal cortex. Key new computational developments include the capacity of the CA3 attractor network for storing whole charts of space; how the correlations inherent in self-organizing continuous spatial representations impact the storage capacity; how the CA3 network can combine continuous spatial and discrete object and reward representations; the roles of the rewards that reach the hippocampus in the later consolidation into long-term memory in part via cholinergic pathways from the orbitofrontal cortex; and new ways of analysing neocortical information storage using Potts networks.}
}
@article{LEE2024100211,
title = {A systematic review of AI education in K-12 classrooms from 2018 to 2023: Topics, strategies, and learning outcomes},
journal = {Computers and Education: Artificial Intelligence},
volume = {6},
pages = {100211},
year = {2024},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2024.100211},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X24000122},
author = {Sang Joon Lee and Kyungbin Kwon},
keywords = {Artificial intelligence, AI education, Systematic review, K-12},
abstract = {AI education aims to teach AI concepts, essential knowledge, and skills related to the fundamental ideas in AI. As AI becomes increasingly prevalent in our daily lives, schools and educators have started to recognize the importance of AI education in K-12 schools. However, there have been a limited number of studies reporting on the implementation of AI education in classrooms. This systematic review aimed to provide an overview of the current state of AI education in K-12 schools, exploring topics, instructional approaches, and learning outcomes. Twenty-five peer-reviewed journal articles published between 2018 and 2023 were selected for this systematic review. The findings highlighted that various topics were covered in K-12 AI education, including fundamental AI concepts, different types of AI, AI applications, and ethical considerations related to AI. To facilitate meaningful learning experiences, educators frequently integrated hands-on activities and project-based learning. The findings supported the benefits of AI education in enhancing students' AI literacy, problem-solving skills, and ethical reflections on AI's societal impact. Furthermore, it fostered motivation, positive attitudes toward AI, and an interest in technology while inspiring career aspirations. It is recommended to develop tailored AI curricula, instructional strategies, and appropriate tools and resources that seamlessly integrate into various subjects within the standard school curriculum.}
}
@article{CAIRNS201964,
title = {Future design of accessibility in games: A design vocabulary},
journal = {International Journal of Human-Computer Studies},
volume = {131},
pages = {64-71},
year = {2019},
note = {50 years of the International Journal of Human-Computer Studies. Reflections on the past, present and future of human-centred technologies},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2019.06.010},
url = {https://www.sciencedirect.com/science/article/pii/S1071581919300801},
author = {Paul Cairns and Christopher Power and Mark Barlet and Greg Haynes},
keywords = {Digital games, Accessibility, Guidelines, Design vocabulary, Accessible player experiences},
abstract = {Games represent one of the most significant cultural artefacts of this century. They are a massive force in economies around the world and are enjoyed by millions of players worldwide. With their cultural significance firmly in place, it is important to ensure that all people can participate in and play games in order to feel included in our wider society. For people with disabilities, games in particular provide a cultural outlet where they can be included with everyone else, and enabled to do things on an even footing with their non-disabled peers. However, this only happens if we create the necessary design environments that provide inclusive opportunities to game alongside the rest of the player base. Guidelines have been successful in raising awareness of accessibility in games and still function well for evaluating finished games. However, they are not the generative design thinking tools that developers need. Further in being divided to address specific disabilities, they are not capturing the diversity of needs of players with disabilities and the personalised and idiosyncratic adaptations that they make in order to play. We therefore propose developing a vocabulary and language of game accessibility which is no longer about whether someone can perceive or operate an interactive technology, but instead as to whether they can have the experience they want to have. We propose the structure for such a vocabulary showing that it needs to distinguish between access to controls, enablement to meet the challenges of the game and the player experience itself. We show how the intermediate-level knowledge embodied in guidelines can be reformulated in this way to be more generative and so support designers to develop games that deliver accessible player experiences.}
}
@incollection{MEHTA2025549,
title = {16 - State of the art in machine learning for the purpose of optimizing and predicting the properties of polymeric nanocomposites},
editor = {Alokesh Pramanik and Animesh Basak and Yu Dong and Chander Prakash and J. Paulo Davim},
booktitle = {Nanocomposite Manufacturing Technologies},
publisher = {Woodhead Publishing},
pages = {549-573},
year = {2025},
series = {Woodhead Publishing Reviews: Mechanical Engineering Series},
isbn = {978-0-12-824329-9},
doi = {https://doi.org/10.1016/B978-0-12-824329-9.00016-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128243299000164},
author = {Amrinder Mehta and Hitesh Vasudev and Chander Prakash and Alokesh Pramanik and Animesh Basak and S. Shankar},
keywords = {Nanocomposite materials, machine learning, polymeric nanocomposites, thermal properties, nanofiller, matrix},
abstract = {Polymer nanocomposites are made up of a continuous matrix phase and a nano-reinforcement phase that is spread throughout the matrix. The mechanical, electrical, and thermal properties of these materials have seen major advancements as a result of these materials. They are currently put to use in a wide number of technological applications, some of which may be found in the automotive, aeronautical, aerospace, maritime, and civil sectors, respectively. Aspect ratio, geometry, size, orientation, and dispersion are examples of some of the factors that contribute to the reinforcing effect of the nanofiller. The processes of melt-blending, compression molding, solution processing, and in-situ polymerization are the ones that are utilized most commonly when it comes to the creation of polymeric nanocomposites. In the field of material science, the creation of raw computational tools for use in the design of innovative materials has been largely superseded by the use of coupled approaches. Machine learning (ML) is a subset of artificial intelligence that allows computers to automatically improve themselves by learning from their previous mistakes and obtaining new information. This is accomplished through a process known as “machine learning.” ML makes it possible to successfully analyze the behavior of the produced composites by using a wider number of different approaches. In the case of polymeric nanocomposites, we are able to make educated guesses regarding a wide variety of multifunctional features. Because it is educated on enormous amounts of data, ML also helps to keep the cost of the models down.}
}
@article{HE2025105245,
title = {A systematic review of the use of log-based process data in computer-based assessments},
journal = {Computers & Education},
volume = {228},
pages = {105245},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2025.105245},
url = {https://www.sciencedirect.com/science/article/pii/S0360131525000132},
author = {Surina He and Ying Cui},
keywords = {Computer-based assessment, Log-based process data, Systematic review},
abstract = {In recent decades, log-based process data has been increasingly used in computer-based assessments to examine test-takers' response patterns and latent traits. This study provides a systematic review of the use of log-based process data in computer-based assessments. Following the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) guideline, we identified 2548 publications, of which 330 were finally included in this study after careful screening and full-text review. The results of this study can assist researchers in better understanding: (1) what are the trends in using log-based process data in computer-based assessments, (2) which process indicators have been constructed from raw log files, (3) what latent constructs have been inferred from process indicators and at what inferential levels, and (4) what are the benefits, challenges, and future recommendations for using log-based process data. By examining these questions, we conclude that the use of log-based process data in computer-based assessment shows many potentials for enhancing the assessment. Therefore, more study using log-based process data in various fields is encouraged to better understand test-takers’ underlying response processes during assessments. Additionally, there is also a considerable demand for validating process indicators and the generalizability of findings.}
}
@article{AMBUJ2025110119,
title = {Intelligent path planning for autonomous ground vehicles in dynamic environments utilizing adaptive Neuro-Fuzzy control},
journal = {Engineering Applications of Artificial Intelligence},
volume = {144},
pages = {110119},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2025.110119},
url = {https://www.sciencedirect.com/science/article/pii/S0952197625001198},
author = { Ambuj and Rajendra Machavaram},
keywords = {Autonomous ground vehicles, Path planning, A∗ algorithm, Adaptive neuro-fuzzy inference system, Hybrid control strategy, Real-time navigation, Dynamic environments},
abstract = {Autonomous Ground Vehicles (AGVs) are increasingly deployed across diverse industries, where enhancing their operational efficiency is critical, particularly in dynamic environments. This study proposes a hybrid control strategy that integrates an improved A∗ algorithm for path planning with a Proportional-Integral-Derivative (PID) controller adaptively tuned by an Adaptive Neuro-Fuzzy Inference System (ANFIS) for path correction. The enhanced A∗ algorithm, combined with the Dynamic Window Approach (DWA), significantly reduces computational overhead while improving pathfinding speed by incorporating the vehicle's kinematic and dynamic constraints. To address non-linearities in AGV movement, the ANFIS framework continuously fine-tunes PID parameters in real-time based on sensor feedback, improving the system's ability to correct path deviations in complex terrains. Experimental results demonstrate the efficacy of the proposed method. The enhanced A∗ algorithm achieves an average path search time of 2.52 s, significantly faster than the traditional A∗ algorithm's 5.56 s. It also reduces the average search grid size from 160 to 100, yielding a shorter path length of 27.44 m compared to 32.25 m, reflecting a more efficient path search process. Additionally, the ANFIS-PID control algorithm achieves a convergence time of 0.038 s, ensuring smooth path correction with robust stability under varying load conditions. Comparisons with state-of-the-art techniques, including Rapidly-Exploring Random Trees (RRT) and Probabilistic Roadmap Algorithm, highlight the enhanced A∗ algorithm's competitive performance, particularly in resource-constrained, real-time applications. The integration of ANFIS with PID control enhances AGV navigation by enabling adaptive, real-time path correction, improving performance in dynamic environments across agricultural, industrial, logistics, and autonomous transportation applications.}
}
@article{SINHA2008955,
title = {Thermal pressure of ionic solids at high temperatures},
journal = {Solid State Sciences},
volume = {10},
number = {7},
pages = {955-959},
year = {2008},
issn = {1293-2558},
doi = {https://doi.org/10.1016/j.solidstatesciences.2007.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S1293255807003366},
author = {Pallavi Sinha},
keywords = {Thermal pressure, Volume expansion},
abstract = {In the present study, a relationship between thermal pressure and volume expansion ratio is disclosed for ionic solids at 1bar pressure. The ionic solids NaCl, KCl, MgO and CaO are considered for the analytic thinking. The analysis is based on the experimental data tabulated by Anderson and generalized data reported by Srivastava. A close agreement between the present study and the experiment reveals the validity of the present work. The extrapolated values of thermal pressure at higher temperatures are useful to understand the thermoelastic behaviour of solids.}
}
@article{CLEMENTI198713,
title = {Large-scale computations on a scalar, vector and parallel ‘supercomputer’},
journal = {Parallel Computing},
volume = {5},
number = {1},
pages = {13-44},
year = {1987},
note = {Proceedings of the International Conference on Vector and Parallel Computing-Issues in Applied Research and Development},
issn = {0167-8191},
doi = {https://doi.org/10.1016/0167-8191(87)90004-4},
url = {https://www.sciencedirect.com/science/article/pii/0167819187900044},
author = {E Clementi and J Detrich and S Chin and G Corongiu and D Folsom and D Logan and R Caltabiano and A Carnevali and J Helin and M Russo and A Gnudi and P Palamidese},
keywords = {Parallel computer systems, 1CAP-1, 1CAP-2, 1CAP-3, programming strategy, migration of code from sequential to parallel systems, performance analysis},
abstract = {We discuss two experimental parallel computer systems 1CAP-1 and 1CAP-2 which can be applied to the entire spectrum of scientific and engineering applications. These systems achieve ‘supercomputer’ levels of performance by spreading large scale computations across multiple cooperating processors—several with vector capabilities. We outline system hardware and software, and discuss our programming strategy for migrating codes from a conventional sequential system to a parallel one. The performance of a variety of applications programs is analyzed to demonstrate the merits of this approach. Finally, we discuss 1CAP-3, an extension to this computing system, which has been recently assembled.}
}
@article{BURR2020R907,
title = {Horace Barlow (1921–2020)},
journal = {Current Biology},
volume = {30},
number = {16},
pages = {R907-R910},
year = {2020},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2020.07.060},
url = {https://www.sciencedirect.com/science/article/pii/S096098222031085X},
author = {David Burr and Simon Laughlin}
}
@article{KLASIOS2016103,
title = {Evolutionizing human nature},
journal = {New Ideas in Psychology},
volume = {40},
pages = {103-114},
year = {2016},
issn = {0732-118X},
doi = {https://doi.org/10.1016/j.newideapsych.2015.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732118X15000501},
author = {John Klasios},
keywords = {Human nature, Evolutionary psychology, Adaptationism, Darwin, Homeostatic property clusters, Developmental systems},
abstract = {Many have argued that the very notion of human nature is untenable given the facts of evolution and should accordingly be discarded. This paper, by contrast, argues that the notion can be retained in a coherent and modern way. The present account expounds on the view of human nature as a collection of species-typical psychological adaptations, and outlines how it can be understood in formally modeled computational terms. The view defended is also heavily developmental and connects directly with contemporary evolutionary developmental biology. Furthermore, the notion of human nature developed here allows us to abstract away from the obfuscating variability that manifests not only between individuals across ontogeny, but also cross-culturally and throughout time.}
}
@article{WANG2024100257,
title = {A resource prediction method for air traffic cyber-physical-social system},
journal = {Transportation Engineering},
volume = {17},
pages = {100257},
year = {2024},
issn = {2666-691X},
doi = {https://doi.org/10.1016/j.treng.2024.100257},
url = {https://www.sciencedirect.com/science/article/pii/S2666691X24000320},
author = {Jintao Wang and Huaiqi Chen and Yulong Yin and Zijian Jiang and Meili Chen},
keywords = {Air traffic system, Complex network, Resource prediction, Cyber-physical-social system, Neural network},
abstract = {Air traffic is exhibiting the characteristics of large flow, strong coupling, and high time variation. Therefore, the complex network of air traffic is more vulnerable to disturbances. When it is disturbed, the failure of some nodes spreads through dependency relationships in the network, resulting in cascade failure. In the event of a cascade failure, the network may quickly collapse until it is paralyzed, with widespread delays and flight cancellations. The current flow management and deployment methods still remain in the control-oriented stage, which is mainly completed by air traffic controls (ATCs), and lack of accurate flow adjustment and effective utilization of capacity. The whole air traffic system and its peripheral factors are intricate, so human and social factors must be integrated into the control and decision-making of the system. Considering engineering and social factors such as operation environment, social environment, personnel, rules, equipment, and information processing, we analyse the air traffic in a cyber-physical-social system (CPSS). To reflect the actual system behaviour rules, dynamic response, limit state, and so on, the corresponding computational experiment and comprehensive evaluation system are established. Based on neural networks and other technologies, a resource prediction scheme based on task demand is proposed for multi-dimensional resources such as airports, air routes, and ATC, to reduce the cost of system resource scheduling and improve resource utilization through resource prediction and adjustment. Finally, the accuracy of the proposed resource prediction algorithm is verified by theoretical analysis and simulation.}
}
@incollection{SULLIVAN2008XIX,
title = {Preface},
series = {Methods in Cell Biology},
publisher = {Academic Press},
volume = {85},
pages = {XIX-XX},
year = {2008},
booktitle = {Fluorescent Proteins},
issn = {0091-679X},
doi = {https://doi.org/10.1016/S0091-679X(08)85026-1},
url = {https://www.sciencedirect.com/science/article/pii/S0091679X08850261},
author = {Kevin F. Sullivan},
abstract = {Publisher Summary
The chapter highlights the content of the book “Fluorescent proteins 2nd edition.” The book discusses the rich palette of autofluorescent proteins that now spans the spectral range from blue to deep red. From presentation of the ideas and concepts that provide the foundation for methods through discussing the factual knowledge and sources required to design experiments to the detailed exposition of actual experimental protocols, the chapters in this book combine to provide an essential tool for thinking about using genetically encoded fluorescent molecules. The experimental goals and systems discussed in the chapter present range from biophysical interrogation of individual molecules to the analysis of the behavior of cell populations in whole animals. The range of autofluorescent proteins available is presented and discussed in several chapters of the book. The construction of FP fusions is also discussed in several contexts, from developing biosensors and optimizing FRET to constructing intramolecular fusions and hemi-FP chimeras used for detecting protien–protein interactions.}
}
@incollection{BRANDT2006136,
title = {Grammatology},
editor = {Keith Brown},
booktitle = {Encyclopedia of Language & Linguistics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {136-140},
year = {2006},
isbn = {978-0-08-044854-1},
doi = {https://doi.org/10.1016/B0-08-044854-2/00355-2},
url = {https://www.sciencedirect.com/science/article/pii/B0080448542003552},
author = {P.A. Brandt},
keywords = {Barthes, deconstruction, Derrida, differance, grammatology, Lacan, logocentrism, McLuhan, phenomenology, Saussure, sign, symbolization, writing},
abstract = {Grammatology, the study of writing systems and processes, is presented through the French philosopher Jacques Derrida's critique of the concept of meaning, following from semiology and the notion of sign as such in Western thinking: ‘logocentrism.’ This critical view of writing was the starting point of deconstruction in literary criticism. It is argued in this article that written texts are different from oral utterances as concerns their enunciation, or speaker role, and that this circumstance deeply affects their interpretation. The difference could explain the sacralization of texts and text volumes, their cultural status and importance. The computer age is about to transform the relation between oral and written communication, so that we now can have written dialogue in addition to oral dialogue.}
}
@article{LIU2025108569,
title = {Enhancing student GAI literacy in digital multimodal composing through development and validation of a scale},
journal = {Computers in Human Behavior},
volume = {166},
pages = {108569},
year = {2025},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2025.108569},
url = {https://www.sciencedirect.com/science/article/pii/S0747563225000160},
author = {Meilu Liu and Lawrence Jun Zhang and Donglan Zhang},
keywords = {Generative artificial intelligence, Digital multimodal composing, Literacy},
abstract = {It is widely acknowledged that Generative Artificial Intelligence (GAI) has exerted a greater influence on EFL learners' digital multimodal composing (DMC) process. GAI focuses on creating new textual and multimodal content using large language models (LLMs), and it puts different demands on EFL learners. Although much research has been conducted on EFL learners' AI literacy in various socio-cultural contexts, more attention should now be paid to EFL learners' GAI literacy in the DMC context, a new autonomous model of literacy. It should be noted that even though some studies may concentrate on users' perceptions and experiences with GAI, which may be closely tied to GAI literacy, there lacks the development of a scale for assessing GAI literacy in DMC. Thus, this study attempted to fill these research gaps by developing and validating an applicable and generalizable instrument to measure Chinese EFL learners' multimodal GAI literacy in their DMC process. Two subsamples (n1 = 296, n2 = 294) were randomly invited to respond to the GAIDMCS, and the data were subjected to exploratory factor analysis (EFA) and confirmatory factor analysis (CFA) to test the validity and reliability of the instrument. The findings suggested that a four-factor solution with 17 items can help explain Chinese EFL learners’ GAI literacy in DMC in terms of affective learning, behavior learning, cognitive learning, and ethical learning. Our GAI literacy in DMC scale may help improve GAI education for researchers and practitioners by providing a comprehensive and plausible framework that can serve as an outline for further syllabus design.}
}
@article{HAMMOND19951593,
title = {Implementation and performance issues of a massively parallel atmospheric model},
journal = {Parallel Computing},
volume = {21},
number = {10},
pages = {1593-1619},
year = {1995},
note = {Climate and weather modeling},
issn = {0167-8191},
doi = {https://doi.org/10.1016/0167-8191(95)01017-9},
url = {https://www.sciencedirect.com/science/article/pii/0167819195010179},
author = {Steven W. Hammond and Richard D. Loft and John M. Dennis and Richard K. Sato},
keywords = {Atmospheric general circulation modeling, Climate modeling, Data parallelism, Spectral transform, Semi-Lagrangian transport},
abstract = {We present implementation and performance issues of a data parallel version of the National Center for Atmospheric Research (NCAR) Community Climate Model (CCM2). We describe automatic conversion tools used to aid in converting a production code written for a traditional vector architecture to data parallel code suitable for the Thinking Machines Corporation CM-5. Also, we describe the 3-D transposition method used to parallelize the spherical harmonic transforms in CCM2. This method employs dynamic data mapping techniques to improve data locality and parallel efficiency of these computations. We present performance data for the 3-D transposition method on the CM-5 for machine size up to 512 processors. We conclude that the parallel performance of the 3-D transposition method is adversely affected on the CM-5 by short vector lengths and array padding. We also find that the CM-5 spherical harmonic transforms spend about 70% of their execution time in communication. We detail a transposition-based data parallel implementation of the semi-Lagrangian Transport (SLT) algorithm used in CCM2. We analyze two approaches to parallelizing the SLT, called the departure point and arrival point based methods. We develop a performance model for choosing between these methods. We present SLT performance data which shows that the localized horizontal interpolation in the SLT takes 70% of the time, while the data remapping itself only require approximately 16%. We discuss the importance of scalable I/O to CCM2, and present the I/O rates measured on the CM-5. We compare the performance of the data parallel version of CCM2 on a 32-processor CM-5 with the optimized vector code running on a single processor Cray Y-MP. We show that the CM-5 code is 75% faster. We also give the overall performance of CCM2 running at higher resolutions on different numbers of CM-5 processors. We conclude by discussing the significance of these results and their implications for data parallel climate models.}
}
@article{JOHNSON2023104327,
title = {Why is biomedical informatics hard? A fundamental framework},
journal = {Journal of Biomedical Informatics},
volume = {140},
pages = {104327},
year = {2023},
issn = {1532-0464},
doi = {https://doi.org/10.1016/j.jbi.2023.104327},
url = {https://www.sciencedirect.com/science/article/pii/S1532046423000485},
author = {Todd R. Johnson and Elmer V. Bernstam},
keywords = {Biomedical informatics, Scientific discipline, Data, Information, Knowledge, Definition, Philosophy of information},
abstract = {Building on previous work to define the scientific discipline of biomedical informatics, we present a framework that categorizes fundamental challenges into groups based on data, information, and knowledge, along with the transitions between these levels. We define each level and argue that the framework provides a basis for separating informatics problems from non-informatics problems, identifying fundamental challenges in biomedical informatics, and provides guidance regarding the search for general, reusable solutions to informatics problems. We distinguish between processing data (symbols) and processing meaning. Computational systems, that are the basis for modern information technology (IT), process data. In contrast, many important challenges in biomedicine, such as providing clinical decision support, require processing meaning, not data. Biomedical informatics is hard because of the fundamental mismatch between many biomedical problems and the capabilities of current technology.}
}
@article{RIZZI19871,
title = {Selected topics in the theory and practice of computational fluid dynamics},
journal = {Journal of Computational Physics},
volume = {72},
number = {1},
pages = {1-69},
year = {1987},
issn = {0021-9991},
doi = {https://doi.org/10.1016/0021-9991(87)90072-6},
url = {https://www.sciencedirect.com/science/article/pii/0021999187900726},
author = {Arthur Rizzi and Björn Engquist},
abstract = {Computational fluid dynamics (CFD) is a large branch of scientific computing that lately has undergone explosive growth. It draws upon elements from related disciplines: fluid mechanics, numerical analysis, theory of partial differential equations, computer science, and computational geometry. By selecting certain topics we try to trace the way the dramatic growth came about and to illustrate the interplay of the related disciplines. The scope is broad and the emphasis is on discussing the underlying fundamentals in order to present an overall perspective on CFD. The focus is on the evolution of nonsmooth features in inviscid flows, primarily macroscale discontinuities like shock waves and vortex sheets admitted as solutions to the Euler equations, but also with some view to their possible unstable progression into small-scale features, ending ultimately in turbulence. Some of the current finite-difference methods, and the theory they are based upon, which are used to treat these problems are reviewed, and different grid generation techniques are introduced. Together with some principles for using advanced supercomputers, we also discuss how the methods are implemented on these machines. A number of computed results, some of them new and of large scale with up to one million grid points, are presented which reflect the limits of the theory and the current status of the field.}
}
@article{HAMEDUH20203494,
title = {Homology modeling in the time of collective and artificial intelligence},
journal = {Computational and Structural Biotechnology Journal},
volume = {18},
pages = {3494-3506},
year = {2020},
issn = {2001-0370},
doi = {https://doi.org/10.1016/j.csbj.2020.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S2001037020304748},
author = {Tareq Hameduh and Yazan Haddad and Vojtech Adam and Zbynek Heger},
keywords = {Homology modeling, Machine learning, Protein 3D structure, Structural bioinformatics, Collective intelligence, Artificial intelligence},
abstract = {Homology modeling is a method for building protein 3D structures using protein primary sequence and utilizing prior knowledge gained from structural similarities with other proteins. The homology modeling process is done in sequential steps where sequence/structure alignment is optimized, then a backbone is built and later, side-chains are added. Once the low-homology loops are modeled, the whole 3D structure is optimized and validated. In the past three decades, a few collective and collaborative initiatives allowed for continuous progress in both homology and ab initio modeling. Critical Assessment of protein Structure Prediction (CASP) is a worldwide community experiment that has historically recorded the progress in this field. Folding@Home and Rosetta@Home are examples of crowd-sourcing initiatives where the community is sharing computational resources, whereas RosettaCommons is an example of an initiative where a community is sharing a codebase for the development of computational algorithms. Foldit is another initiative where participants compete with each other in a protein folding video game to predict 3D structure. In the past few years, contact maps deep machine learning was introduced to the 3D structure prediction process, adding more information and increasing the accuracy of models significantly. In this review, we will take the reader in a journey of exploration from the beginnings to the most recent turnabouts, which have revolutionized the field of homology modeling. Moreover, we discuss the new trends emerging in this rapidly growing field.}
}
@article{BESTER2024820,
title = {Complementary conservation of South African crop wild relatives for plant improvement},
journal = {South African Journal of Botany},
volume = {174},
pages = {820-829},
year = {2024},
issn = {0254-6299},
doi = {https://doi.org/10.1016/j.sajb.2024.09.041},
url = {https://www.sciencedirect.com/science/article/pii/S0254629924005945},
author = {C Bester and NC {Le Maitre} and M Visser and WC Botes},
keywords = {Crop wild relatives (CWR), Complementary conservation, Plant breeding, Southern African development community (SADC), CAPFITOGEN},
abstract = {Crop Wild Relatives (CWR) are good sources of unexplored genetic diversity that can assist plant breeders to increase the yield and resilience of their crops. These species are valuable plant genetic resources (PGR) that have been used in more than 4,157 documented cases of plant improvement to date. South Africa has 258 prioritized CWR, selected based on their distribution, threat status and potential as gene donors. In light of ongoing habitat destruction, global warming and mismanagement of resources, the conservation of these PGR is vital. Complementary conservation approaches allow for the continuous development of CWR, while harnessing and applying the available diversity in plant breeding programs. The South African National Biodiversity Strategy and Action Plan (NBSAP) strives to utilize conservation resources to build and maintain an effective complementary, in situ to ex situ conservation pipeline. As part of the Southern African Development Community (SADC), South Africa has access to numerous resources that can assist to protect its rich floral diversity, including the SADC Plant Genetic Resource Centre (SPGRC), the SADC CWR Project and CAPFITOGEN3.}
}
@incollection{NIE20181939,
title = {Land use modeling and optimization based on food-energy-water nexus: a case study on crop-livestock systems},
editor = {Mario R. Eden and Marianthi G. Ierapetritou and Gavin P. Towler},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {44},
pages = {1939-1944},
year = {2018},
booktitle = {13th International Symposium on Process Systems Engineering (PSE 2018)},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-444-64241-7.50318-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780444642417503189},
author = {Yaling Nie and Styliani Avraamidou and Jie Li and Xin Xiao and Efstratios N. Pistikopoulos},
keywords = {land use, nexus, data-driven modeling, global optimization},
abstract = {Efficient land use in agricultural systems is a complicated decision-making problem with resource competitions and conflicting objectives. Systematic thinking based on food-energy-water (FEW) nexus is a necessity for modeling and optimization of the systems. However, challenges arise in making decisions while encountering conflicting objectives, limited data and coupling components. To address these challenges, we developed a global optimization-based land allocation framework, which provides an adaptive data-driven modeling method based on limited realistic data to predict yields for production components, a FEW index to help solve the multi-objective optimization problem and carry out assessments. Computational results indicate that the framework can provide valuable production models and a comprehensive FEW index to select strategies for optimal land allocation and limit stresses in the FEW nexus.}
}
@article{BAMMER2008875,
title = {Enhancing research collaborations: Three key management challenges},
journal = {Research Policy},
volume = {37},
number = {5},
pages = {875-887},
year = {2008},
issn = {0048-7333},
doi = {https://doi.org/10.1016/j.respol.2008.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0048733308000528},
author = {Gabriele Bammer},
keywords = {Collaboration, Integration, Boundary, Authorization, Evaluation},
abstract = {This conceptual paper explores three areas of research collaboration: (a) effectively harnessing differences, (b) setting defensible boundaries and (c) gaining legitimate authorization. The focus is on their potential lessons for individuals leading and managing research collaborations, evaluation of research partnerships and areas for further investigation. Examples from three partnerships – building the atomic bomb, the Human Genome Project and the World Commission on Dams – are used to highlight key elements of the ideas presented. The paper provides a framework for systematically thinking about integration of different perspectives and other elements essential to any particular collaboration. It also sketches out ideas for (1) managing differences which may destroy partnerships, (2) deciding what the collaboration should encompass, (3) understanding and accommodating forces which may distort what the collaboration is able to achieve, and (4) enlisting necessary supporters while preserving research independence.}
}
@article{LIU2024110132,
title = {Multimodal brain-controlled system for rehabilitation training: Combining asynchronous online brain–computer interface and exoskeleton},
journal = {Journal of Neuroscience Methods},
volume = {406},
pages = {110132},
year = {2024},
issn = {0165-0270},
doi = {https://doi.org/10.1016/j.jneumeth.2024.110132},
url = {https://www.sciencedirect.com/science/article/pii/S0165027024000773},
author = {Lei Liu and Jian Li and Rui Ouyang and Danya Zhou and Cunhang Fan and Wen Liang and Fan Li and Zhao Lv and Xiaopei Wu},
keywords = {Movement impairment, Rehabilitation, Brain–computer interface, Motor imagery, Steady-state visual evoked potential},
abstract = {Background:
Traditional therapist-based rehabilitation training for patients with movement impairment is laborious and expensive. In order to reduce the cost and improve the treatment effect of rehabilitation, many methods based on human–computer interaction (HCI) technology have been proposed, such as robot-assisted therapy and functional electrical stimulation (FES). However, due to the lack of active participation of brain, these methods have limited effects on the promotion of damaged nerve remodeling.
New method:
Based on the neurofeedback training provided by the combination of brain–computer interface (BCI) and exoskeleton, this paper proposes a multimodal brain-controlled active rehabilitation system to help improve limb function. The joint control mode of steady-state visual evoked potential (SSVEP) and motor imagery (MI) is adopted to achieve self-paced control and thus maximize the degree of brain involvement, and a requirement selection function based on SSVEP design is added to facilitate communication with aphasia patients.
Comparison with existing methods:
In addition, the Transformer is introduced as the MI decoder in the asynchronous online BCI to improve the global perception of electroencephalogram (EEG) signals and maintain the sensitivity and efficiency of the system.
Results:
In two multi-task online experiments for left hand, right hand, foot and idle states, subject achieves 91.25% and 92.50% best accuracy, respectively.
Conclusion:
Compared with previous studies, this paper aims to establish a high-performance and low-latency brain-controlled rehabilitation system, and provide an independent and autonomous control mode of the brain, so as to improve the effect of neural remodeling. The performance of the proposed method is evaluated through offline and online experiments.}
}
@article{WILKINS20258,
title = {Does DeepSeek herald AI's future?},
journal = {New Scientist},
volume = {265},
number = {3529},
pages = {8-9},
year = {2025},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(25)00207-6},
url = {https://www.sciencedirect.com/science/article/pii/S0262407925002076},
author = {Alex Wilkins},
abstract = {The success of Chinese firm DeepSeek suggests tech companies can train and run powerful AIs without consuming vast amounts of power, finds Alex Wilkins}
}
@article{BUSE2022396,
title = {Asynchronous Background Processing for accelerated simulation of wireless communication on multi-core systems},
journal = {Computer Communications},
volume = {193},
pages = {396-409},
year = {2022},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.07.032},
url = {https://www.sciencedirect.com/science/article/pii/S0140366422002791},
author = {Dominik S. Buse and Georg Echterling and Falko Dressler},
keywords = {Parallel simulation, Wireless network simulation, Asynchronous parallelization, Vehicular networking},
abstract = {Discrete event simulation (DES) is an important tool for the development and analysis of wireless networks. However, with increasing network size and complexity, the computational effort and simulation time increases significantly, often exponentially. This increase in response time may be critical if DES is interfacing real-time systems like Hardware in the Loop (HIL) or network emulation. It also slows down development cycles of users designing or debugging simulation models. Most popular DES software packages run single-threaded. Thus, they achieve only limited performance improvements from more modern multi-core CPUs. At the same time, existing approaches for parallel simulation of networks do not perform well on wireless systems or require complex paradigm shifts in simulation models. In this paper, we propose Asynchronous Background Processing (ABP) to accelerate the simulation of wireless communication on multi-core systems. By moving expensive computation from the main thread into asynchronous tasks computed by background threads, it accelerates the progression of events and thus reduces response time. Tasks are started as early as possible to exploit the time the main thread spends processing other events, ideally providing results before they are needed in the simulation. We showcase the application of ABP using Veins, a popular vehicular network simulator, demonstrating speedups of up to 3.5 on typical desktop platforms. We further perform an in-depth analysis using advanced profiling techniques to investigate the effectiveness of the parallelization and guide further optimizations.}
}
@article{GALLISTEL2017498,
title = {The Coding Question},
journal = {Trends in Cognitive Sciences},
volume = {21},
number = {7},
pages = {498-508},
year = {2017},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2017.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S1364661317300852},
author = {C.R. Gallistel},
abstract = {Recent electrophysiological results imply that the duration of the stimulus onset asynchrony in eyeblink conditioning is encoded by a mechanism intrinsic to the cerebellar Purkinje cell. This raises the general question – how is quantitative information (durations, distances, rates, probabilities, amounts, etc.) transmitted by spike trains and encoded into engrams? The usual assumption is that information is transmitted by firing rates. However, rate codes are energetically inefficient and computationally awkward. A combinatorial code is more plausible. If the engram consists of altered synaptic conductances (the usual assumption), then we must ask how numbers may be written to synapses. It is much easier to formulate a coding hypothesis if the engram is realized by a cell-intrinsic molecular mechanism.}
}
@article{DIX201013,
title = {Human–computer interaction: A stable discipline, a nascent science, and the growth of the long tail},
journal = {Interacting with Computers},
volume = {22},
number = {1},
pages = {13-27},
year = {2010},
note = {Special Issue: Festschrift for John Long},
issn = {0953-5438},
doi = {https://doi.org/10.1016/j.intcom.2009.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0953543809000952},
author = {Alan Dix},
keywords = {HCI discipline, Methodology, Theory, Peak experience, Single person study},
abstract = {This paper represents a personal view of the state of HCI as a design discipline and as a scientific discipline, and how this is changing in the face of new technological and social situations. Going back 20years a frequent topic of discussion was whether HCI was a ‘discipline’. It is unclear whether this was ever a fruitful topic, but academic disciplines are effectively about academic communities and there is ample evidence of the long-term stability of the international HCI/CHI community. However, as in computer ‘science’, the central scientific core of HCI is perhaps still unclear; for example, a strength of HCI is the closeness between theory and practice, but the corresponding danger is that the two are often confused. The paper focuses particularly on the challenge of methodological thinking in HCI, especially as the technological and social context of HCI rapidly changes. This is set alongside two other challenges: the development of reliable knowledge in HCI and the clear understanding of interlinked human roles within the discipline. As a case study of the need for methodological thinking, the paper considers the use of single person studies in research and design. These are likely to be particularly valuable as we move from a small number of applications used by many people to a ‘long tail’ where large numbers of applications are used by small numbers of people. This change calls for different practical design strategies; focusing on the peak experience of a few rather than acceptable performance for many. Moving back to the broader picture, as we see more diversity both in terms of types of systems and kinds of concerns, this may also be an opportunity to reflect on what is core across these; potential fragmentation becoming a locus to understand more clearly what defines HCI, not just for the things we see now, but for the future that we cannot see.}
}
@article{BARADARANRAHIMI2025105859,
title = {Exploring the future of urban death: Speculative design and the concept of necropolis 4.0},
journal = {Cities},
volume = {161},
pages = {105859},
year = {2025},
issn = {0264-2751},
doi = {https://doi.org/10.1016/j.cities.2025.105859},
url = {https://www.sciencedirect.com/science/article/pii/S0264275125001593},
author = {Farzan {Baradaran Rahimi}},
keywords = {Urban future, Speculative design, TOPSIS, Necropolis, Emerging technologies, Design innovation},
abstract = {As urban areas rapidly expand, they grapple with multifaceted issues such as population growth, climate change, land shortage, resource constraints, and social inequalities. Proactive planning for the future is essential to foster the development of sustainable and resilient cities capable of adapting to evolving needs. One such needs is designing for death in the urban future, given the outdated, polluting, cumbersome, and unsustainable methods currently in use. Drawing inspiration from theories of social space, hybrid space, and the historical concept of the necropolis, while integrating technological advancements such as extended reality, super artificial intelligence, quantum computing, biodegradable materials, and Web 4.0, this study aims to reimagine the design for death in the urban future through three alternative scenarios. Integrating technique for order preference by similarity to ideal solution into speculative design approach, experts' evaluation identifies Necropolis 4.0 as the closest scenario to the ideal solution. Findings serve a dual purpose. First, focusing on Necropolis 4.0, establishes a nature-human-machine relationship and paves the way for designers, planners, and policymakers to envision a novel, green, and sustainable design for death in the urban future. Second, methodological contribution of this research enhances the way we use speculative design approach in urban planning.}
}
@article{RIDDERINKHOF20143,
title = {Neurocognitive mechanisms of perception–action coordination: A review and theoretical integration},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {46},
pages = {3-29},
year = {2014},
note = {Micro- and Macro-Perspectives on Cognitive Conflict Control},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2014.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0149763414001250},
author = {K. Richard Ridderinkhof},
keywords = {Perception–action coordination, Impetus, Motivation, Prediction, Appraisal, Valuation, Impulsive action, Intuitive action, Dual systems, Conation},
abstract = {The present analysis aims at a theoretical integration of, and a systems-neuroscience perspective on, a variety of historical and contemporary views on perception–action coordination (PAC). We set out to determine the common principles or lawful linkages between sensory and motor systems that explain how perception is action-oriented and how action is perceptually guided. To this end, we analyze the key ingredients to such an integrated framework, examine the architecture of dual-system conjectures of PAC, and endeavor in an historical analysis of the key characteristics, mechanisms, and phenomena of PACs. This analysis will reveal that dual-systems views are in need of fundamental re-thinking, and its elements will be amalgamated with current views on action-oriented predictive processing into a novel integrative theoretical framework (IMPPACT: Impetus, Motivation, and Prediction in Perception–Action Coordination theory). From this framework and its neurocognitive architecture we derive a number of non-trivial predictions regarding conative, motive-driven PAC. We end by presenting a brief outlook on how IMPPACT might present novel insights into certain pathologies and into action expertise.}
}
@article{FRERICHS2018135,
title = {Mind maps and network analysis to evaluate conceptualization of complex issues: A case example evaluating systems science workshops for childhood obesity prevention},
journal = {Evaluation and Program Planning},
volume = {68},
pages = {135-147},
year = {2018},
issn = {0149-7189},
doi = {https://doi.org/10.1016/j.evalprogplan.2018.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0149718917300113},
author = {Leah Frerichs and Tiffany L. Young and Gaurav Dave and Doris Stith and Giselle Corbie-Smith and Kristen {Hassmiller Lich}},
keywords = {Concept mapping, Mental models, Network analysis, Systems science},
abstract = {Across disciplines, it is common practice to bring together groups to solve complex problems. Facilitators are often asked to help groups organize information about and better understand the problem in order to develop and prioritize solutions. However, despite existence of several methods to elicit and characterize how individuals and groups think about and conceptualize an issue, many are difficult to implement in practice-based settings where resources such as technology and participant time are limited and research questions shift over time. This paper describes an easy-to-implement diagramming technique for eliciting conceptualization and a flexible network analysis method for characterizing changes in both individual and group conceptualization. We use a case example to illustrate how we used the methods to evaluate African American adolescent’s conceptual understanding of obesity before and after participating in a series of four systems thinking workshops. The methods produced results that were sensitive to changes in conceptualization that were likely driven by the specific activities employed during the workshop sessions. The methods appear strong for capturing salient levels of conceptualization at both individual and collective levels. The paper concludes with a critical examination of strengths and weaknesses of the methods and implications for future practice and research.}
}
@article{MAXVILLE20111953,
title = {eScience: Building our Body of Knowledge},
journal = {Procedia Computer Science},
volume = {4},
pages = {1953-1963},
year = {2011},
note = {Proceedings of the International Conference on Computational Science, ICCS 2011},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2011.04.213},
url = {https://www.sciencedirect.com/science/article/pii/S1877050911002717},
author = {Valerie Maxville},
abstract = {This paper describes the need for an eScience BoK, particularly as a resource for educators. eScience is a term representing the computational technology and techniques utilised when undertaking research. As eScience matures, stakeholders, and particularly educators, can benefit from the clarity that a defined Body of Knowledge (BOK) can provide. The BOK would require domain-specific and technological aspects to be addressed. This paper describes a framework for a prototype BOK for eScience and discusses how the BOK can be used as a tool to drive education, outreach and infrastructure planning.}
}
@article{DIMARCO2020101290,
title = {(re)Producing mtEve},
journal = {Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences},
volume = {83},
pages = {101290},
year = {2020},
issn = {1369-8486},
doi = {https://doi.org/10.1016/j.shpsc.2020.101290},
url = {https://www.sciencedirect.com/science/article/pii/S1369848620300303},
author = {Marina DiMarco},
abstract = {In their 1987 Nature publication, “Mitochondrial DNA and Human Evolution,” Rebecca Cann, Mark Stoneking, and Allan C. Wilson gave a new reconstruction of human evolution on the basis of differences in mitochondrial DNA among contemporary human populations. This phylogeny included an African common ancestor for all human mitochondrial DNA (mtDNA) lineages, and Cann et al.’s reconstruction became known as the “Out of Africa” hypothesis. Since mtDNA is inherited exclusively through the maternal line, the common ancestor who was first branded African Eve later became known as Mitochondrial Eve (mtEve, for short). In this paper, I show that mtEve was not a single, successful, or purely scientific discovery. Instead, she was produced many times and in many ways, each of which informed the next. Importantly, though Wilson and colleagues heralded mitochondrial DNA as a source of certainty, objectivity, and consensus for evolutionary inference, their productions of Mitochondrial Eve depended as much on popular assumptions about the certainty of maternal inheritance as they did on new molecular and computational tools. This recognition lets us reevaluate the complex consequences of these productions, which, like mtEve herself, could not be confined to a purely social, material, or scientific dimension.}
}
@article{HEMASPAANDRA202266,
title = {The complexity of online bribery in sequential elections},
journal = {Journal of Computer and System Sciences},
volume = {127},
pages = {66-90},
year = {2022},
issn = {0022-0000},
doi = {https://doi.org/10.1016/j.jcss.2022.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S0022000022000071},
author = {Edith Hemaspaandra and Lane A. Hemaspaandra and Jörg Rothe},
keywords = {Bribery, Computational complexity, Computational social choice, Logic, Quantifier assignment, Sequential elections},
abstract = {Prior work on the complexity of bribery assumes that the bribery happens simultaneously, and that the briber has full knowledge of all votes. However, in many real-world settings votes come in sequentially, and the briber may have a use-it-or-lose-it moment to decide whether to alter a given vote, and when making that decision the briber may not know what votes remaining voters will cast. We introduce a model for, and initiate the study of, bribery in such an online, sequential setting. We show that even for election systems whose winner-determination problem is polynomial-time computable, an online, sequential setting may vastly increase the complexity of bribery, jumping the problem up to completeness for high levels of the polynomial hierarchy or even PSPACE. But we also show that for some natural, important election systems, such a dramatic complexity increase does not occur, and we pinpoint the complexity of their bribery problems.}
}
@article{HALKOS2017140,
title = {Climate change effects and their interactions: An analysis aiming at policy implications},
journal = {Economic Analysis and Policy},
volume = {53},
pages = {140-146},
year = {2017},
issn = {0313-5926},
doi = {https://doi.org/10.1016/j.eap.2017.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S031359261630217X},
author = {George E. Halkos and Kyriaki D. Tsilika},
keywords = {Graph theory, Node centrality, Mathematica, Climate related factors, Environmental economics computation},
abstract = {In this study we provide a computerized graph structure for synthesizing and displaying the data on a region’s ecosystem-economic system. By applying Mathematica-based graph modeling we create a causal network of the synergistic impact mechanism among certain climate related factors. Our computational approach identifies a climate factor that affects most immediately or most strongly the others. Important factors are indicated through the use of graph theoretical tools. Our graph-based approach and its computational aspects allow for factor ranking(s) according to their importance to the network both numerically and visually, for certain settlement types. Our contribution provides quantitative estimates of impacts and adaptation potentials of five potential effects of climate change (migration, flooding-landslides-fire, air and water pollution, human health and energy-water-other resources) which play a substantial role at the synergistic impact mechanism. By using graph visualization techniques, the structure of the synergistic impact mechanism is self-evident. Specifically, graph layouts are created to detect i) the causal relationships of the synergistic mechanism under study ii) the most influential factor(s) in the synergistic mechanism and iii) classify the factor’s roles (based on the degree of their impact) within the coping mechanism. Highlighting graph elements let information for policy implications stand out.}
}
@article{PALMER2024110848,
title = {Assessing between-individual variability in bioenergetics modelling: Opportunities, challenges, and potential applications},
journal = {Ecological Modelling},
volume = {498},
pages = {110848},
year = {2024},
issn = {0304-3800},
doi = {https://doi.org/10.1016/j.ecolmodel.2024.110848},
url = {https://www.sciencedirect.com/science/article/pii/S0304380024002369},
author = {Miquel Palmer and Irene Moro-Martínez and Joaquim Tomàs-Ferrer and Amalia Grau and María Dolores López-Belluga and Marine Herlin and Orestis Stavrakidis-Zachou and Andrea Campos-Candela},
keywords = {Between-individual differences, Dynamic Energy Budget, Bayesian},
abstract = {Population dynamics is influenced by between-individual variability. Dynamic Energy Budget (DEB) theory is an appealing framework for assessing such a variability, yet DEB parameters have rarely been estimated at the individual level. Bayesian hierarchical models show promise for inferring individual variability in DEB parameters, thought computational challenges have limited their use due to the need to solve differential equations. Timely, Stan has emerged as a general-purpose statistical tool for fitting dynamic models. This paper introduces an analytical strategy using Bayesian parametric inference and hierarchical modelling to estimate individual-specific DEB parameters. Two biologically relevant DEB parameters were successfully estimated for 69 Gilt-head breams (Sparus aurata) with up to 11 measures of length and wet weight each. The estimated between-individual variability in these two DEB parameters explained well the observed patterns in length and weight at between- and within-individual levels. Moreover, data-simulation experiments highlighted the potential and limitations of our approach, suggesting that improved data collection could enable to increase precision and the number of DEB parameters that can be estimated at the individual level. This strategy can better represent between-individual variability in DEB parameters, which ultimately may improve forecasting of population dynamics after integrating DEB into population models.}
}
@article{ALSAMHORI2024100133,
title = {Artificial intelligence for hearing loss prevention, diagnosis, and management},
journal = {Journal of Medicine, Surgery, and Public Health},
volume = {3},
pages = {100133},
year = {2024},
issn = {2949-916X},
doi = {https://doi.org/10.1016/j.glmedi.2024.100133},
url = {https://www.sciencedirect.com/science/article/pii/S2949916X24000860},
author = {Jehad Feras AlSamhori and Abdel Rahman Feras AlSamhori and Rama Mezyad Amourah and Yara AlQadi and Zina Wael Koro and Toleen Ramzi Abdallah Haddad and Ahmad Feras AlSamhori and Diala Kakish and Maya Jamal Kawwa and Margaret Zuriekat and Abdulqadir J. Nashwan},
keywords = {Artificial intelligence, Hearing loss, Machine learning, Computational audiology},
abstract = {This paper explores the transformative impact of artificial intelligence (AI), particularly machine learning (ML), on diagnosing and treating hearing loss, which affects over 5% of the global population across all ages and demographics. AI encompasses various applications, from natural language processing models like ChatGPT to image recognition systems; however, this paper focuses on ML, a subfield of AI that can revolutionize audiology by enhancing early detection, formulating personalized rehabilitation plans, and integrating electronic health records for streamlined patient care. The integration of ML into audiometry, termed "computational audiology," allows for automated, accurate hearing tests. AI algorithms can process vast data sets, provide detailed audiograms, and facilitate early detection of hearing impairments. Research shows ML's effectiveness in classifying audiograms, conducting automated audiometry, and predicting hearing loss based on noise exposure and genetics. These advancements suggest that AI can make audiological diagnostics and treatment more accessible and efficient. The future of audiology lies in the seamless integration of AI technologies. Collaborative efforts between audiologists, AI experts, and individuals with hearing loss are essential to overcome challenges and leverage AI's full potential. Continued research and development will enhance AI applications in audiology, improving patient outcomes and quality of life worldwide.}
}
@article{MAHAJAN2022103942,
title = {Participatory resilience: Surviving, recovering and improving together},
journal = {Sustainable Cities and Society},
volume = {83},
pages = {103942},
year = {2022},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2022.103942},
url = {https://www.sciencedirect.com/science/article/pii/S2210670722002633},
author = {Sachit Mahajan and Carina I. Hausladen and Javier {Argota Sánchez-Vaquerizo} and Marcin Korecki and Dirk Helbing},
keywords = {Risk, Resilience, Participation, Collective intelligence, Connective action, Sustainability},
abstract = {In the context of urbanization and a growing population, cities and citizens are becoming more exposed and vulnerable to social and environmental changes, ranging from natural disasters like earthquakes and floods to uncertainties caused by issues related to climate change and complex social dynamics or even pandemics. There have been many debates about implementing resilience thinking that allow cities and communities to prepare for possible stresses and shocks. Although there are sets of frameworks aimed at building inclusive resilience strategies fostering participation and engagement, there is limited resilience-related literature on how to conceptualize participation. Through an extensive review of various kinds of publications on resilience, policy documents, and case studies, which emphasize the concepts of participation, coordination, and co-creation, this review explores and investigates how citizen participation is discussed and applied in the context of participatory resilience. We conclude that participatory approaches possess a great potential to enhance multi-stakeholder cooperation, social innovation, and capacity building for resilience. Realization of the potential of participatory resilience will remain limited, however, unless participation strategies and frameworks are made more transparent, inclusive, and context-sensitive.}
}
@article{HU2019202,
title = {Effective Connectivity of the Fronto-Parietal Network during the Tangram Task in a Natural Environment},
journal = {Neuroscience},
volume = {422},
pages = {202-211},
year = {2019},
issn = {0306-4522},
doi = {https://doi.org/10.1016/j.neuroscience.2019.09.021},
url = {https://www.sciencedirect.com/science/article/pii/S0306452219306669},
author = {Zhishan Hu and Keng-Fong Lam and Zhen Yuan},
keywords = {Tangram, Visuospatial reasoning, Granger causality, fNIRS, Brain networks},
abstract = {Although the neural basis underlying visuospatial reasoning has been widely explored by neuroimaging techniques, the brain activation patterns during naturalistic visuospatial reasoning such as tangram remains unclear. In this study, the directional functional connectivity of fronto-parietal networks during the tangram task was carefully inspected by using combined functional near-infrared spectroscopy (fNIRS) and conditional Granger causality analysis (GCA). Meanwhile, the causal networks during the traditional spatial reasoning task were also characterized to exhibit the differences with those during the tangram task. We discovered that the tangram task in a natural environment showed enhanced activation in the fronto-parietal regions, particularly the frontal cortex. In addition, a strong directional connectivity from the right prefrontal cortex to left angular gyrus was detected for the complex spatial reasoning condition of spatial reasoning task, whereas no effective connectivity was identified between the frontal and parietal cortices during the tangram task. Further correlation analyses showed that the behavioral performance in the spatial reasoning rather than the tangram task manifested the relationship with the connectivity between the frontal and parietal cortex. Our findings demonstrate that the tangram task measures a different aspect of the visuospatial reasoning ability which requires more trial-and-error strategies and creative thinking rather than inductive reasoning. In particular, the frontal cortex is mostly involved in tangram puzzle-solving, whereas the interaction between frontal and parietal cortices is regulated by the hands-on experience during the tangram task.}
}
@article{BANCHHOR2025100194,
title = {Integration of software-based cognitive approaches and brain-like computer machinery for efficient cognitive computing},
journal = {Neuroscience Informatics},
volume = {5},
number = {2},
pages = {100194},
year = {2025},
issn = {2772-5286},
doi = {https://doi.org/10.1016/j.neuri.2025.100194},
url = {https://www.sciencedirect.com/science/article/pii/S2772528625000093},
author = {Chitrakant Banchhor and Manoj Kumar Rawat and Rahul Joshi and Dharmesh Dhabliya and Omkaresh Kulkarni and Sandeep Dwarkanath Pande and Umesh Pawar},
keywords = {Multiple intelligences theory, English learning, Data mining algorithms, Legacy problems},
abstract = {The widespread adoption of the Internet has transformed various industries, driving significant systemic reforms across different sectors. This transformation has enhanced the Internet's role in information dissemination, resource sharing, and global connectivity, allowing for more efficient distribution of knowledge and services. The development of the Internet model and its research bring significant benefits from the network, enabling people to use and learn from it. However, the traditional education model provides only limited knowledge, restricting growth and progress. Moreover, there is a vast world of knowledge yet to be explored. Nowadays, with the help of network tools, people can understand the dynamics of the whole world and accept the culture and knowledge of different regions without going out. Throughout the study of English legacy problems in various countries, efficient learning methods and high levels of English skills are the goals pursued, while the traditional English model can't meet the students' learning needs in a short time. The model construction of data mining algorithm based on large open network courses is a model for solving legacy problems adopted both domestically and internationally. According to the survey data of universities in various countries, the use of data mining algorithm can fundamentally meet the student's desire and demand for English knowledge. This research, integrates the mining algorithm into English research, which will essentially improve the English legacy problems.}
}
@article{GONZALEZ2024446,
title = {BOIS: Bayesian Optimization of Interconnected Systems},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {14},
pages = {446-451},
year = {2024},
note = {12th IFAC Symposium on Advanced Control of Chemical Processes ADCHEM 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.08.377},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324011364},
author = {Leonardo D. González and Victor M. Zavala},
keywords = {Bayesian optimization, grey-box modeling, composite functions},
abstract = {Bayesian optimization (BO) has proven to be an effective paradigm for the global optimization of expensive-to-sample systems. One of the main advantages of BO is its use of Gaussian processes (GPs) to characterize model uncertainty which can be leveraged to guide the learning and search processes. However, BO typically treats systems as black-boxes and this limits the ability to exploit structural knowledge (e.g., physics and sparse interconnections). Composite functions of the form f(x,y(x)), wherein GP modeling is shifted from the performance function f to an intermediate function y, offer an avenue for exploiting structural knowledge. However, the use of composite functions in a BO framework is complicated by the need to generate a probability density for f from the Gaussian density of y calculated by the GP (e.g., when f is nonlinear it is not possible to obtain a closed-form expression). Previous work has handled this issue using sampling techniques; these are easy to implement and flexible but are computationally intensive. In this work, we introduce a new paradigm which allows for the efficient use of composite functions in BO; this uses adaptive linearizations of f to obtain closed-form expressions for the statistical moments of the composite function. We show that this simple approach (which we call BOIS) enables the exploitation of structural knowledge, such as that arising in interconnected systems as well as systems that embed multiple GP models and combinations of physics and GP models. Using a chemical process optimization case study, we benchmark the effectiveness of BOIS against standard BO and sampling approaches. Our results indicate that BOIS achieves performance gains and accurately captures the statistics of composite functions.}
}
@article{MAASDORP2025102610,
title = {Non-tuberculosis mycobacteria identified by line probe assays in respiratory and non-respiratory samples in South Africa between 2015 and 2019},
journal = {Tuberculosis},
volume = {151},
pages = {102610},
year = {2025},
issn = {1472-9792},
doi = {https://doi.org/10.1016/j.tube.2025.102610},
url = {https://www.sciencedirect.com/science/article/pii/S1472979225000058},
author = {Elizna Maasdorp and Yonas Ghebrekristos and Amanda Khumalo and Lynthia Paul and Monique J. Williams},
keywords = {Non-tuberculosis mycobacteria, Pulmonary disease, Line probe assay},
abstract = {In recent years, a rise in non-tuberculosis mycobacteria pulmonary disease (NTM-PD) has been reported in several countries. However, data for high-burden tuberculosis settings, including South Africa, is currently limited. In this study, we conducted a retrospective analysis of routine diagnostic data obtained from one diagnostic laboratory in South Africa between 2015 and 2019. During this period, samples from 275 individuals with suspected mycobacterial infection were tested using the GenoType Mycobacterium CM (Common mycobacteria) or AS (Additional species) line probe assay (LPA) (Brucker-Hain Life science, Nehren, Germany), yielding an NTM-positive result for 163 of these individuals. Interestingly, the positivity rate in respiratory samples declined from 93 % in 2015 to 79 % in 2019. Just over half of the positive samples were of respiratory origin, and the most common species identified in respiratory samples was Mycobacterium intracellulare/Mycobacteium avium complex (28.9 %), followed by M. avium (17.4 %). Where the mycobacterial species was not identified by the LPA, a higher proportion of the subsequent cultures were negative, suggestive of colonisation rather than infection. More than half of patients with a positive NTM-LPA were HIV positive (55.9 %), and this association declined slightly during the study period (62.5 %–50 %).}
}
@article{AHMED20201,
title = {A cognitive model to predict human interest in smart environments},
journal = {Computer Communications},
volume = {161},
pages = {1-9},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0140366420306812},
author = {Tanveer Ahmed and Rishav Singh and Anil K. Pandey and Sanjay K. Singh},
keywords = {Cognition, Interest, Machine learning, Man Machine systems},
abstract = {Recently, the idea of smart cities has made several strides forward in literature. Work has hypothesize that the combination of Artificial Intelligence, Cloud Computing, and High powered computers will make technology more human-centric, even, the idea that smart cities will be able to understand the thought process of a human being seems very much likely today. This paper is along this line of thought. In particular, we try to present a method to model the cognitive state of human interest. This is done to take one more step towards the realization of a smart cognitive city. An approach which is Subjective–Objective in nature is presented to model the computation of activity inspired by interest. Based on activity, human latent state values are indirectly deduced. Inspiration is drawn from Physics and interest is modeled upon the Ornstein–Uhlenbeck (OU) process. Concepts of Adaptive filtering are used to formulate an evolving transformation function that automatically and adaptively models the conversion of interest into activity. Particle filter is employed to provide an elucidation which is computationally feasible. To validate the viability of the method, experimentation is performed with real datasets.}
}
@article{HAMALAINEN2013623,
title = {On the importance of behavioral operational research: The case of understanding and communicating about dynamic systems},
journal = {European Journal of Operational Research},
volume = {228},
number = {3},
pages = {623-634},
year = {2013},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2013.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0377221713001197},
author = {Raimo P. Hämäläinen and Jukka Luoma and Esa Saarinen},
keywords = {Process of OR, Practice of OR, Systems dynamics, Behavioral OR, Systems thinking},
abstract = {We point out the need for Behavioral Operational Research (BOR) in advancing the practice of OR. So far, in OR behavioral phenomena have been acknowledged only in behavioral decision theory but behavioral issues are always present when supporting human problem solving by modeling. Behavioral effects can relate to the group interaction and communication when facilitating with OR models as well as to the possibility of procedural mistakes and cognitive biases. As an illustrative example we use well known system dynamics studies related to the understanding of accumulation. We show that one gets completely opposite results depending on the way the phenomenon is described and how the questions are phrased and graphs used. The results suggest that OR processes are highly sensitive to various behavioral effects. As a result, we need to pay attention to the way we communicate about models as they are being increasingly used in addressing important problems like climate change.}
}
@article{CHANG2023109277,
title = {Location and timestamp-based chip contour detection using LWMG-YOLOv5},
journal = {Computers & Industrial Engineering},
volume = {180},
pages = {109277},
year = {2023},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2023.109277},
url = {https://www.sciencedirect.com/science/article/pii/S0360835223003017},
author = {Bao Rong Chang and Hsiu-Fen Tsai and Chia-Wei Hsieh},
keywords = {Chip contour detection, Real-time image recognition, LWMG-YOLOv5 model, Ghost convolution, Attention mechanism, and Minimizing production costs},
abstract = {In the fab, semiconductor manufacturers often use deep learning approaches for chip contour detection to shorten automated optical inspection to minimize the loss of production costs and lower power consumption in chip contour detection for realizing energy-efficient computing. However, YOLOv5 and GSEH-YOLOv5 models have sacrificed their accuracy to improve the operational speed. MobileNetv3-YOLOv5 model can enhance the accuracy but lacks high-speed operation. Therefore, this study presents a light version of MobileNetv3-YOLOv5 model with ghost convolution, abbreviated LWMG-YOLOv5, to speed up chip contour detection because this architecture can reduce the number of model parameters and computational burden at the same time. As a result, the proposed approach can outperform the other methods by getting a 3.62% speed-up in chip contour detection to gain a better manufacturing advantage in increasing the chip yields by 1.7% and reducing the loss of production costs by 1.83% significantly.}
}
@article{MEZINSKA2024100429,
title = {Design-driven innovation in STEM disciplines in higher education: The role and impact of transversal competences},
journal = {Journal of Open Innovation: Technology, Market, and Complexity},
volume = {10},
number = {4},
pages = {100429},
year = {2024},
issn = {2199-8531},
doi = {https://doi.org/10.1016/j.joitmc.2024.100429},
url = {https://www.sciencedirect.com/science/article/pii/S2199853124002233},
author = {Silvija Mezinska and Anda Abolina and Velta Lubkina},
keywords = {Design-driven innovations, Transversal competences, STEM disciplines, Higher education},
abstract = {The purpose of this study is to explore students’ and academic staff’ perspectives on the importance of transversal competences in creating design-driven innovations within STEM disciplines in higher education, aiming to find solutions for the enhancement of transversal competences. Accordingly, the following research questions are explored in this study: 1) What is the connection between design-driven innovation and transversal competences in STEM disciplines in higher education? 2) How can students' and academic staff’s self-assessment of transversal competences influence the creation of design-driven innovations? Throughout this study, the authors analyze the promotion of design-driven innovation by enhancing the transversal competences of academic staff of STEM disciplines and students. The exploration of qualitative components of transversal competences enable the identification of conditions that may contribute design-driven innovations in higher education. By examining the link between design-driven innovations and transversal competences, added value in competence, design and values-based higher education, based on socio-cultural environment and technological development. The results of the study, based on contemporary knowledge including survey data and focus group discussions on this topic, will be utilized as development conditions, an assessment system to promote progress towards design-driven innovations in higher education facilitated by the enhancement of transversal competences.}
}
@article{SHAFFER202041,
title = {Artificial intelligence products reshape accounting: time to re-train},
journal = {Development and Learning in Organizations: An International Journal},
volume = {34},
number = {6},
pages = {41-43},
year = {2020},
issn = {1477-7282},
doi = {https://doi.org/10.1108/DLO-10-2019-0242},
url = {https://www.sciencedirect.com/science/article/pii/S1477728220001288},
author = {Kathie J. Shaffer and Carol J. Gaumer and Kiersten P. Bradley},
keywords = {Organizational change, Accounting, Artificial intelligence},
abstract = {Purpose
Managers are expected to increase productivity in the most cost-efficient manner, using all available resources and, “work smarter.” As technology improves, there is greater incentive for managers to invest in options where automation becomes less expensive than the high cost of human capital. When repetitive tasks can be accurately duplicated through automation, the decision becomes a fait accompli. Advances in artificial intelligence (AI) or synthetic intelligence that simulates human intellectual function has significant impact potential in the service sector. This paper examines productivity efficiencies sought through artificial intelligence and the need for re-training, specifically in the accounting profession.
Design/methodology/approach
This is a conceptual paper for practitioners without research methodology.
Findings
The accounting profession 10 years from now will look noticeably different than it does now. The accountants, who embrace the new technologies, like artificial intelligence, will survive and even thrive by becoming more specialized. This will require training and, in some instances, re-training. Organizations must be willing to absorb those development costs. I hope that new graduates will enter the profession with updated skills providing added value for organizations and employers who started into the profession many years ago. The biggest challenge may lie in the re-training of accountants who have been in practice for many years and managing the resistance to change. Employers must first set the example by accepting the inevitable and then encourage and support employees to improve and update their skills. Additionally, they will have to coach employees through the changes with reassurance that those who embrace the change will experience less chance of job elimination. Embracing the available technology will enable firms to serve clients more efficiently and effectively by providing up to date business solutions regardless of the services being offered.
Research limitations/implications
There is no empirical research in this paper. It is a conceptual piece looking at the changing organization in accounting, specifically due to artificial intelligence.
Practical implications
Accounting firms that focus on basic accounting functions should find new services to offer. The same clients can be served, but at a higher-level. Accountants will offer more value to clients by detecting patterns and trends when more time can be devoted to analysis. Helping clients beyond the preparation of documents requires that accountants understand the current market conditions and potential effects of inflation and, engage in more critical thinking while at the same time be able to teach clients and help them understand at the higher level. Just as accountants’ responsibilities and duties will be transformed through the integration of AI, accounting education must be altered.
Social implications
Implications related to the workplace are only discussed in this paper.
Originality/value
It is not completely original. It is a compilation of research that is out there as a means to address critical workforce training needs in accounting as technology moves forward.}
}
@article{ASEMI2025106795,
title = {Improving EEG signal-based emotion recognition using a hybrid GWO-XGBoost feature selection method},
journal = {Biomedical Signal Processing and Control},
volume = {99},
pages = {106795},
year = {2025},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2024.106795},
url = {https://www.sciencedirect.com/science/article/pii/S174680942400853X},
author = {Hanie Asemi and Nacer Farajzadeh},
keywords = {Emotion recognition, Brain signals, EEG, Prediction, Feature selection, Machine learning},
abstract = {Emotion plays a crucial role in daily life, influencing cognitive functions such as language comprehension, decision-making, attention, and concentration. With the growing integration of computer systems into our everyday activities, it is essential to understand and detect emotional states accurately. Emotion detection through EEG signals allows direct assessment of the human’s internal state and is considered an important factor in the interaction between humans and external devices. In this paper, we introduce a novel feature selection algorithm proposed to improve the accuracy of emotion classification using EEG signals, aligned with decreasing the input dimension to reduce computations, making it more suitable for real-time applications. We performed two experiments utilizing the DEAP and the MAHNOB-HCI datasets. Various features were extracted and employed for emotion classification using SVM, KNN, and XGBoost classifiers. Initially, the highest accuracy for binary emotion classification in the DEAP dataset was achieved with statistical features and the XGBoost model, reaching 78.85% for arousal and 79.02% for valence. In the MAHNOB-HCI dataset, the highest accuracy with statistical features and the XGBoost model was 67.08% for arousal and 62.24% for valence. Subsequently, we applied the grey wolf optimization algorithm as a feature selection method, optimizing the cost function based on XGBoost accuracy. This approach significantly enhanced the classification performance. For the DEAP dataset, accuracy increased to 89.63% for arousal and 89.08% for valence using statistical features. For the MAHNOB-HCI dataset, accuracy improved to 84.94% for arousal and 82.29% for valence using statistical features.}
}
@article{TAYLOR2022109098,
title = {Path integral radiative transfer via polyline representation allowing GPU implementation},
journal = {Annals of Nuclear Energy},
volume = {173},
pages = {109098},
year = {2022},
issn = {0306-4549},
doi = {https://doi.org/10.1016/j.anucene.2022.109098},
url = {https://www.sciencedirect.com/science/article/pii/S0306454922001335},
author = {Brennen Taylor and John Keyser and Jerry Tessendorf},
keywords = {Radiative transport, Feynman path integral, Monte Carlo, Particle transport, CUDA},
abstract = {Many research areas require simulating particle transport through scattering media. For instance, radiative transfer is useful for computer graphics and for neutron transport in nuclear physics. These transport simulations tend to be computationally expensive for problems involving large amounts of multiple scattering in generic geometries, requiring significant time to compute. Finding a fast solution for these types of problems remains an open area for research. Previous work shows that radiative transfer can be represented as a Feynman Path Integral over all paths between two points in a space. The path integral assigns a weight to each path based on the local curvature of the path, accumulating a transport kernel by summing the weights of all of the paths. Previous work demonstrated a Monte Carlo method for computing the radiative transfer Feynman path integral via repeatedly perturbing paths to generate new paths, using a discrete Frenet-Serret framework and an expensive root-solve calculation. The approach is highly parallelizable on the CPU, however computations require a supercomputer to complete in reasonable time. While a GPU implementation was considered for this previous approach, the root-solve and path representation mapped poorly to the GPU, hindering a reasonable implementation. In the present work, we propose a representation of paths as polylines, and a new curve perturbation method that guarantees production of a new unique path each execution while maintaining the boundary conditions of each path, but without the time-consuming root-solve. The new perturbation method’s structure maps well to the GPU, allowing implementation in CUDA which outperforms the CPU, and allows the utilization of GPU hardware.}
}
@article{KARIM2024742,
title = {Repetitive Negative Thoughts and the Brain as a Resource-Limited Machine},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {9},
number = {8},
pages = {742-743},
year = {2024},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2024.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S2451902224001691},
author = {Helmet T. Karim}
}
@article{COCHRAN20201237,
title = {Sustainable Enterprise Design 4.0: Addressing Industry 4.0 Technologies from the Perspective of Sustainability},
journal = {Procedia Manufacturing},
volume = {51},
pages = {1237-1244},
year = {2020},
note = {30th International Conference on Flexible Automation and Intelligent Manufacturing (FAIM2021)},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.10.173},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920320308},
author = {David S. Cochran and Erwin Rauch},
keywords = {Industry 4.0, sustainability, triple bottom line, sustainable enterprise design, industrial revolution},
abstract = {The introduction of Industry 4.0 and sustainability in production is currently on everyone’s mind. However, companies face difficulties to address these trends in their long-term enterprise strategy and design. Industry 4.0 promises strategic advantages for companies in many respects, but there is a lack of instruments and concepts for integrating emerging technologies in an overall enterprise system design. Similarly, the multiple perspectives regarding economic, environmental and social sustainability provide a framework for thinking about a strategy for sustainable enterprise design. Based on the three principles presented in this paper for Sustainable Enterprise Design, this article aims to present an approach to better address sustainability as well as Industry 4.0 in terms of a long-term strategic, enterprise design that is sustainable. As a result, a list of needs, functional requirements as well as possible Industry 4.0 physical solutions is proposed to achieve a long-term sustainable enterprise design. The consequence of the perspective of an enterprise as a system that can be designed provides a rigorous approach that takes advantage of Industry 4.0 technologies and the multiple perspectives and candidate physical solutions that the research community offers.}
}
@article{DELEURAN201671,
title = {Exploratory Topology Modelling of Form-active Hybrid Structures},
journal = {Procedia Engineering},
volume = {155},
pages = {71-80},
year = {2016},
note = {TENSINET – COST TU1303 International Symposium 2016 "Novel structural skins - Improving sustainability and efficiency through new structural textile materials and designs"},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2016.08.008},
url = {https://www.sciencedirect.com/science/article/pii/S187770581632149X},
author = {Anders Holden Deleuran and Mark Pauly and Martin Tamke and Ida Friis Tinning and Mette Ramsgaard Thomsen},
keywords = {Form-Active, Hybrid Structures, Topology, Shaping, Form-Finding, Interactive Modelling, Design Space Search},
abstract = {The development of novel form-active hybrid structures (FAHS) is impeded by a lack of modelling tools that allow for exploratory topology modelling of shaped assemblies. We present a flexible and real-time computational design modelling pipeline developed for the exploratory modelling of FAHS that enables designers and engineers to iteratively construct and manipulate form-active hybrid assembly topology on the fly. The pipeline implements Kangaroo2's projection-based methods for modelling hybrid structures consisting of slender beams and cable networks. A selection of design modelling sketches is presented in which the developed modelling pipeline has been integrated to explore the design space delineated by FAHS.}
}
@article{PETIT2018135,
title = {Combining eco-social and environmental indicators to assess the sustainability performance of a food value chain: A case study},
journal = {Journal of Cleaner Production},
volume = {191},
pages = {135-143},
year = {2018},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2018.04.156},
url = {https://www.sciencedirect.com/science/article/pii/S0959652618311831},
author = {Gaëlle Petit and Caroline Sablayrolles and Gwenola {Yannou-Le Bris}},
keywords = {Life cycle assessment, Pork value chain, Sustainability, Metrics, Indicator, Framework},
abstract = {Stakeholders are increasingly demanding transparency on food value chain sustainability performance. Today there is no standard framework to meet this demand and support defining indicators to be used to conduct an overall sustainable performance assessment. This paper mobilizes existing frameworks and indicators to build new sustainable performance metrics for actors willing to work together for their value chain sustainability. Popular methods or tools for assessing dimensions of agrifood products or activities are selected and analyzed to determine how they could contribute to this metric. The analysis aims to distinguish the sustainable development pillars addressed (economic, environmental and/or social), the frames concerned (life cycle thinking or not; multi-actor or not), and the focus of performance measured (drivers, pressures, states, impacts, responses). This categorization is then used to develop a proposal for specifications adapted to food value chain sustainability performance assessment. The applicability of the framework is demonstrated through a case study in a pork agrifood value chain.}
}
@article{GU2024110161,
title = {A Bayesian decision network–based pre-disaster mitigation model for earthquake-induced cascading events to balance costs and benefits on a limited budget},
journal = {Computers & Industrial Engineering},
volume = {191},
pages = {110161},
year = {2024},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2024.110161},
url = {https://www.sciencedirect.com/science/article/pii/S0360835224002821},
author = {Wenjing Gu and Jiangnan Qiu and Jilei Hu and Xiaowei Tang},
keywords = {Earthquake-induced cascading disasters, Pre-disaster mitigation, Bayesian decision network, Cost–benefit analyses, Limited budgets},
abstract = {Cascading disasters induced by earthquakes amplify the severity of the initial impact on environment. Although decisionmakers may face uncertainty, an effective mitigation strategy is critical in environmental management. We propose an earthquake-induced cascading disaster mitigation–Bayesian decision network (ECDM-BDN) model to assess pre-disaster mitigating strategies under limited budgets from the perspective of systematic thinking. This model graphically represents the complex relationship among various variables in a seismic hazard system and resistance system based on disaster system theory. It can predict the triggering of cascading events through probabilistic reasoning and identify the key variable accountable for the range of outputs observed through sensitivity analysis. In addition, cost–benefit analyses are carried out by combining Bayesian decision network utility nodes and dynamic programming to obtain a balance between costs and benefits in the context of limited budgets. An earthquake-induced liquefaction served as a case study to demonstrate the proposed model’s effectiveness. Experimental results indicate that the ECDM-BDN model can balance the costs and effects of each pre-disaster mitigating strategy as well as select the optimal one according to the utility value. The proposed model can perform a “white-box” decision-making process, which is expected to guide earthquake-induced cascading event pre-disaster mitigation in cases of limited budgets.}
}
@article{DOWLING201949,
title = {Interactive Visual Analytics for Sensemaking with Big Text},
journal = {Big Data Research},
volume = {16},
pages = {49-58},
year = {2019},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2019.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S2214579618302995},
author = {Michelle Dowling and Nathan Wycoff and Brian Mayer and John Wenskovitch and Scotland Leman and Leanna House and Nicholas Polys and Chris North and Peter Hauck},
keywords = {Text analytics, Big data, Visualization, Interactive visual analytics, Semantic interaction, Topic modeling},
abstract = {Analysts face many steep challenges when performing sensemaking tasks on collections of textual information larger than can be reasonably analyzed without computational assistance. To scale up such sensemaking tasks, new methods are needed to interactively integrate human cognitive sensemaking activity with machine learning. Towards that goal, we offer a human-in-the-loop computational model that mirrors the human sensemaking process, and consists of foraging and synthesis sub-processes. We model the synthesis loop as an interactive spatial projection and the foraging loop as an interactive relevance ranking combined with topic modeling. We combine these two components of the sensemaking process using semantic interaction such that the human's spatial synthesis actions are transformed into automated foraging and synthesis of new relevant information. Ultimately, the model's ability to forage as a result of the analyst's synthesis activities makes interacting with big text data easier and more efficient, thereby facilitating analysts' sensemaking ability. We discuss the interaction design and theory behind our interactive sensemaking model. The model is embodied in a novel visual analytics prototype called Cosmos in which analysts synthesize structure within the larger corpus by directly interacting with a reduced-dimensionality space to express relationships on a subset of data. We then demonstrate how Cosmos supports sensemaking tasks with a realistic scenario that investigates the affect of natural disasters in Adelaide, Australia in September 2016 using a database of over 30,000 news articles.}
}
@incollection{GOLDSCHMIDT202050,
title = {Architecture☆},
editor = {Mark Runco and Steven Pritzker},
booktitle = {Encyclopedia of Creativity (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Oxford},
pages = {50-56},
year = {2020},
isbn = {978-0-12-815615-5},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.23520-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093245235207},
author = {Gabriela Goldschmidt},
keywords = {Architectural design, Architectural education, Culture, Digital design, Form, Function, Ideas, Performance, Starchitect, Style},
abstract = {Architecture is a cultural arena based on ideas, which jointly produce styles and individually, at their best, generate outstanding buildings. In our era architecture is expected to innovate in its forms, while ensuring perfect performance and adaptation to the environment. Form and function handling are rough correlates of originality and practicality, by which we measure design creativity. Architecture is also a product of the technological state of its time. At present we experience computational advances that fundamentally change buildings and the way they are designed. Architectural education is groping to adjust to the changes.}
}
@article{NOORMOHAMMADIASL2025104821,
title = {Human leading or following preferences: Effects on human perception of the robot and the human–robot collaboration},
journal = {Robotics and Autonomous Systems},
volume = {183},
pages = {104821},
year = {2025},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2024.104821},
url = {https://www.sciencedirect.com/science/article/pii/S0921889024002057},
author = {Ali Noormohammadi-Asl and Kevin Fan and Stephen L. Smith and Kerstin Dautenhahn},
keywords = {Human–robot collaboration, Adaptive task planning, Leading/following preference, Team performance, Perception of the robot and collaboration},
abstract = {Achieving effective and seamless human–robot collaboration requires two key outcomes: enhanced team performance and fostering a positive human perception of both the robot and the collaboration. This paper investigates the capability of the proposed task planning framework to realize these objectives by integrating human leading/following preferences and performance into its task allocation and scheduling processes. We designed a collaborative scenario wherein the robot autonomously collaborates with participants. The outcomes of the user study indicate that the proactive task planning framework successfully attains the aforementioned goals. We also explore the impact of participants’ leadership and followership styles on their collaboration. The results reveal intriguing relationships between these factors which warrant further investigation in future studies.}
}
@incollection{VOIT20211,
title = {Networks and Dynamic Models in Systems Medicine: Overview},
editor = {Olaf Wolkenhauer},
booktitle = {Systems Medicine},
publisher = {Academic Press},
address = {Oxford},
pages = {1-7},
year = {2021},
isbn = {978-0-12-816078-7},
doi = {https://doi.org/10.1016/B978-0-12-801238-3.11661-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128012383116617},
author = {Eberhard O. Voit},
keywords = {Complexity, Dynamic system, Emergent properties, Exposome, Graph, Network, Personalized medicine, Precision medicine, Predictive health, Reductionism, Systems biology},
abstract = {Systems medicine approaches health and disease as a holistic response of the human body. Its main challenge is the complexity of the web of uncounted processes that govern normal physiology, as well as deviations toward disease. Expanding the vast knowledge amassed by traditional medicine, systems medicine addresses this challenge by adopting and adapting concepts and methods from systems biology and by making use of powerful new datasets stemming, for instance, from wearable devices and electronic health records. Systems medicine shares concepts and goals with exposome research and can be considered the foundation for personalized or precision medicine and predictive health. Systems medicine is in its early infancy, and its character, concepts, and methodologies will probably change over time. Nevertheless, many of its computational approaches will continue to benefit from the representation of health and disease processes as networks and dynamic systems. This overview describes the basic concepts of computational systems medicine and briefly summarizes the subsequent chapters, which discuss methods, challenges, and applications of networks and dynamic models in systems medicine.}
}
@article{ASHTIANI201549,
title = {A survey of quantum-like approaches to decision making and cognition},
journal = {Mathematical Social Sciences},
volume = {75},
pages = {49-80},
year = {2015},
issn = {0165-4896},
doi = {https://doi.org/10.1016/j.mathsocsci.2015.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0165489615000165},
author = {Mehrdad Ashtiani and Mohammad Abdollahi Azgomi},
abstract = {There has always been a steady interest in how humans make decisions amongst researchers from various fields. Based on this interest, many approaches such as rational choice theory or expected utility hypothesis have been proposed. Although these approaches provide a suitable ground for modeling the decision making process of humans, they are unable to explain the corresponding irrationalities and existing paradoxes and fallacies. Recently, a new formulation of decision theory that can correctly describe these paradoxes and possibly provide a unified and general theory of decision making has been proposed. This new formulation is founded based on the application of the mathematical structure of quantum theory to the fields of human decision making and cognition. It is shown that by applying these quantum-like models, one can better describe the uncertainty, ambiguity, emotions and risks involved in the human decision making process. Even in computational environments, an agent that follows the correct patterns of human decision making will have a better functionality in performing its role as a proxy for a real user. In this paper, we present a comprehensive survey of the researches and the corresponding recent developments. Finally, the benefits of leveraging the quantum-like modeling approaches in computational domains and the existing challenges and limitations currently facing the field are discussed.}
}
@article{BEESON1988297,
title = {Towards a computation system based on set theory},
journal = {Theoretical Computer Science},
volume = {60},
number = {3},
pages = {297-340},
year = {1988},
issn = {0304-3975},
doi = {https://doi.org/10.1016/0304-3975(88)90115-6},
url = {https://www.sciencedirect.com/science/article/pii/0304397588901156},
author = {Michael J. Beeson},
abstract = {An axiomatic theory of sets and rules is formulated, which permits the use of sets as data structures and allows rules to operate on rules, numbers, or sets. We might call it a “polymorphic set theory”. Our theory combines the λ-calculus with traditional set theories. A natural set-theoretic model of the theory is constructed, establishing the consistency of the theory and bounding its proof-theoretic strength, and giving in a sense its denotational semantics. Another model, a natural recursion-theoretic model, is constructed, in which only recursive operations from integers to integers are represented, even though the logic can be classical. Some related philosophical considerations on the notions of set, type, and data structure are given in an appendix.}
}
@incollection{HARTSON2019293,
title = {Chapter 14 - Generative Design: Ideation, Sketching, and Critiquing},
editor = {Rex Hartson and Pardha Pyla},
booktitle = {The UX Book (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {293-325},
year = {2019},
isbn = {978-0-12-805342-3},
doi = {https://doi.org/10.1016/B978-0-12-805342-3.00014-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012805342300014X},
author = {Rex Hartson and Pardha Pyla},
keywords = {Design thinking, Immersion, Synthesis, Ideas, Ideation, Sketching, Critiquing, Ideation informers, Ideation catalysts, Ideation techniques, Rules of engagement},
abstract = {In this chapter we get into the UX design process, starting with generative design or design creation. The overarching objective of design creation is to formulate a plan for how the system will be structured to satisfy the ecological, interaction, and emotional needs of users. Compared to usage research (a study of things as they currently are), design (about lateral thinking and generating new ideas to make things better) is less procedural and more creative. Generative design is an intertwining of ideation (brainstorming), sketching (capturing and exploring design ideas visually), and critiquing (evaluation, review, and judgment). Ideation is collaborative, iterative, and exploratory, being informed via usage research data and models and design catalysts and is supported by a number of ideation techniques. Sketching is an essential embodied partner of ideation; you are not doing design if you are not sketching. A sketch is a conversation about design. Much of the work in design occurs within the ecological perspective, the interaction perspective, and/or the emotional perspective. The process works best if the team follows certain “rules of engagement.”}
}
@article{JIN2024105113,
title = {An enhanced approach for few-shot segmentation via smooth downsampling mask and label smoothing loss},
journal = {Image and Vision Computing},
volume = {148},
pages = {105113},
year = {2024},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2024.105113},
url = {https://www.sciencedirect.com/science/article/pii/S0262885624002178},
author = {Hailong Jin and Huiying Li},
keywords = {Few-shot segmentation, Segmentation, Few-shot learning, Deep learning},
abstract = {Few-shot semantic segmentation aims to segment new categories with only a small number of annotated images. Previous methods mainly focused on exploiting the pixel-level correlation between the support image and the query image, combined with attention-based methods, resulting in significant advancements. In this paper, we introduce a new perspective to enhance few-shot segmentation. We identify that utilizing the bilinear interpolation method to downsample the mask leads to the loss of fine-grained information from the target features. To address this issue, we propose a Smooth Downsampling Mask (SDM) method. The SDM method is designed to retain more effective target semantic features by employing a cascaded downsampling approach with a smooth kernel for mask processing. Additionally, we propose a label smoothing loss to further enhance the performance, which provides direct guidance for low-resolution feature map optimization. Both methods can be used as plug-and-play modules for existing methods. Notably, our proposed method does not involve additional learnable parameters and is computationally efficient, thus achieving painless gains. To validate the effectiveness of our method, we take three publicly available models as baselines and conduct extensive experiments on three public benchmarks PASCAL-5i, COCO-20i and FSS-1000, and achieve considerable improvement.}
}
@article{DERBYSHIRE201777,
title = {Potential surprise theory as a theoretical foundation for scenario planning},
journal = {Technological Forecasting and Social Change},
volume = {124},
pages = {77-87},
year = {2017},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2016.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S0040162516300671},
author = {James Derbyshire},
keywords = {G. L. S. Shackle, Scenario planning, Plausibility, Intuitive Logics, Crucial decisions, Uncertainty},
abstract = {Despite some recent progress, scenario planning's development as an academic discipline remains constrained by the perception it is solely a practical tool for thinking about the future, with limited theoretical foundations. The paper addresses this issue by showing that G. L. S. Shackle's ‘Potential Surprise Theory’ (PST) contains much that can lend theoretical support to scenario planning - especially its use of plausibility rather than probability, and its focus on potential extreme outcomes. Moreover, PST and scenario planning share the same ontology, viewing the future as constructed by the imagination of individuals. Yet, under PST, while the future is imagined and, therefore, subjective, individuals nevertheless seek to identify the ‘best’ option through a deductive process of elimination. PST therefore assists in overcoming the divide between the constructivist and deductivist perspectives in scenario planning as it employs both. Finally, the paper shows that theoretically underpinning scenario planning with PST would place it at the heart of contemporary debates on decision making under uncertainty taking place in economics and other fields, enhancing its status and profile as a discipline.}
}
@article{ZHENG2019109539,
title = {Development of bridge influence line identification methods based on direct measurement data: A comprehensive review and comparison},
journal = {Engineering Structures},
volume = {198},
pages = {109539},
year = {2019},
issn = {0141-0296},
doi = {https://doi.org/10.1016/j.engstruct.2019.109539},
url = {https://www.sciencedirect.com/science/article/pii/S0141029619309538},
author = {Xu Zheng and Dong-Hui Yang and Ting-Hua Yi and Hong-Nan Li},
keywords = {Bridge influence line, Inverse problem, B-WIM, Bridge evaluation, Damage detection, Model correction},
abstract = {Bridge influence line, which is defined as the response curve of a certain point of the bridge under the moving unit concentrated load, contains tremendous structural information. However, the real bridge influence lines seldom correspond well with them calculated by bridge model. Accordingly, exact identification of the bridge influence line from the direct measurement data grows up to be an important issue for bridge weight-in-motion system, performance evaluation, model correction and bridge damage detection. This paper provides a comprehensive review of current research and development activities in bridge influence line identification method. At first, the development and applications of the bridge influence line are introduced. Following that, the mathematical models of bridge influence line identification and different bridge influence line identification methods are provided. Finally, four different indexes including noise immunity, peak reconstruction accuracy, computational complexity, and adaptability for axle weight change of different methods are compared and the features of these methods are summarized. The end of this paper provides a criterion for selecting suitable influence line identification method in different conditions and an outlook for further development of bridge influence line identification method.}
}
@article{BAYDOUN2021100026,
title = {Auto-contouring FDG-PET/MR images for cervical cancer radiation therapy: An intelligent sequential approach using focally trained, shallow U-Nets},
journal = {Intelligence-Based Medicine},
volume = {5},
pages = {100026},
year = {2021},
issn = {2666-5212},
doi = {https://doi.org/10.1016/j.ibmed.2021.100026},
url = {https://www.sciencedirect.com/science/article/pii/S2666521221000028},
author = {Atallah Baydoun and Ke Xu and Latoya A. Bethell and Feifei Zhou and Jin Uk Heo and Kaifa Zhao and Elisha T. Fredman and Rodney J. Ellis and Pengjiang Qian and Raymond F. Muzic and Bryan J. Traughber},
keywords = {Cervical cancer, Deep learning, Image segmentation, PET/MR Based radiation therapy, U-net},
abstract = {Background
Manual contouring for radiation therapy planning remains the most laborious and time consuming part in the radiation therapy workflow. Particularly for cervical cancer, this task is complicated by the complex female pelvic anatomy and the concomitant dependence on 18F-labeled Fluorodeoxyglucose (FDG) positron emission tomography (PET) and magnetic resonance (MR) images. Using deep learning, we propose a new auto-contouring method for FDG-PET/MR based cervical cancer radiation therapy by combining the high level anatomical topography and radiological properties, to the low-level pixel wise deep-learning based semantic segmentation.
Materials/methods
The proposed method: 1) takes advantage of PET data and left/right anatomical symmetry, creating sub-volumes that are centered on the structures to be contoured. 2) Uses a 3D shallow U-Net (sU-Net) model with an encoder depth of 2.3) Applies the successive training of 3 consecutive sU-Nets in a feed forward strategy. 4) Employs, instead of the usual generalized dice loss function (GDL), a patch dice loss function (PDL) that takes into account the Dice similarity index (DSI) at the level of each training patch. Experimental analysis was conducted on a set of 13 PET/MR images was using a leave-one-out strategy.
Results
Despite the limited data availability, 5 anatomical structures - the gross tumor volume, bladder, anorectum, and bilateral femurs - were accurately (DSI ​= ​0.78), rapidly (1.9 ​s/structure), and automatically delineated by our algorithm. Overall, PDL achieved a better performance than GDL and DSI was higher for organs at risk (OARs) with solid tissue (e.g. femurs) than for OARs with air-filled soft tissues (e.g. anorectum).
Conclusion
The presented workflow successfully addresses the challenge of auto-contouring in FDG-PET/MR based cervical cancer. It is expected to expedite the cervical cancer radiation therapy workflow in both, conventional and adaptive radiation therapy settings.}
}
@article{FACQUE2022281,
title = {Present bias in economic choice demonstrates increased cognitive fatigability of glioma patients},
journal = {Cortex},
volume = {151},
pages = {281-293},
year = {2022},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2022.02.015},
url = {https://www.sciencedirect.com/science/article/pii/S0010945222000703},
author = {Valentine Facque and Antonius Wiehler and Emmanuelle Volle and Emmanuel Mandonnet and Mathias Pessiglione},
keywords = {Low-grade glioma, Fatigue, Impulsivity, Decision-making, Delay discounting, Cognitive control, Computational modelling},
abstract = {Fatigue is a frequent symptom in many clinical conditions that is still poorly understood despite having a major impact on quality of life. Here, we propose a novel approach using model-based analysis of choice behaviour to extract fatigue markers. We applied this approach to the case of low-grade glioma, with the aim of testing the hypothesis that fatigability in this condition may manifest as limited control over choice impulsivity. Patients with intact or resected glioma (n = 29) and matched healthy controls (n = 27) performed a series of behavioural tasks included in a 4 h-long neuropsychological assessment. Intertemporal choices, opposing smaller-sooner to larger-later monetary rewards, were intermixed with tasks designed to test cognitive and motor performance and to assess perceived fatigue with subjective ratings. All dependent variables were analysed with generalised linear models testing the main effects of group and time-on-task, as well as their interaction. While absent in standard measures of fatigue (subjective rating and objective performance), a significant group-by-time interaction was observed in the rate of impulsive choices: contrary to controls, patients developed a preference for the smaller-sooner option in the course of neuropsychological assessment. This preference shift was captured by computational modelling as an increase in the present bias, a parameter that assigns an additive bonus to immediate rewards. Thus, choice impulsivity was the only reliable marker that reflected the enhanced fatigability of patients relative to controls. These results suggest that the impact of glioma (or its resection) on brain functioning limits the exertion of cognitive control during decision-making. More generally, they pave the way to using model-based analysis of choice behaviour for future investigations of the many clinical conditions plagued with cognitive fatigue.}
}
@article{ABUKHAIT202563,
title = {The association between psychotic symptoms and suicidal ideation in a sample of patients with schizophrenia: The moderating effect of the frequency of suicidal thoughts},
journal = {Archives of Psychiatric Nursing},
volume = {54},
pages = {63-72},
year = {2025},
issn = {0883-9417},
doi = {https://doi.org/10.1016/j.apnu.2025.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0883941725000044},
author = {Abdallah {Abu Khait} and Austin Menger and Ghada Shahrour and Ayat ALhamdan and Esra'a Issa and Shaher H. Hamaideh},
keywords = {Suicidal ideation, Psychotic symptoms, Schizophrenia, Psychiatric nurses},
abstract = {Introduction
Suicidal ideation among patients with schizophrenia is ubiquitous and may lead to premature death. The ideation is a significant determinant of attempting and committing suicide.
Aim
This study aims to examine the moderating role of the frequency of suicidal thoughts on the relationship between psychotic symptoms and suicidal ideation in a sample of Jordanian patients with schizophrenia.
Materials and methods
This cross-sectional study used a non-experimental moderation design to recruit participants using convenience sampling. A total of 204 patients with schizophrenia completed self-administered questionnaires.
Results
The significant predictors of suicidal ideation were sex, whether or not the individual adhered to their medication prescription, age, the number of previous suicidal thoughts an individual had, and negative symptoms. For all suicidal ideation subscales except subscale 3 (suicide contemplation), positive psychotic symptoms were a significant predictor of suicidal ideation. The frequency of suicidal thoughts reduced (moderated) the effect of negative symptoms on suicidal ideation while amplifying the effect of positive psychotic symptoms on all suicidal ideation subscales except subscale 3 (suicide contemplation).
Conclusions
This study's results highlight the necessity of reducing suicidal thoughts to diminish the effect that positive psychotic symptoms have on suicidal ideation in patients with schizophrenia. Further research might explore the intricate relationship between psychotic symptoms and the mechanisms included in their complex link to suicidal ideation.
Implications for the practice
The results will help psychiatric nurses develop timely and accurate preventive strategies to fight suicidal ideation, assist in identifying which subgroups of patients with schizophrenia are vulnerable to suicidal ideation, and potentially lessen the suicide rate.}
}
@article{SITTI2021101340,
title = {Physical intelligence as a new paradigm},
journal = {Extreme Mechanics Letters},
volume = {46},
pages = {101340},
year = {2021},
issn = {2352-4316},
doi = {https://doi.org/10.1016/j.eml.2021.101340},
url = {https://www.sciencedirect.com/science/article/pii/S2352431621001012},
author = {Metin Sitti},
keywords = {Physical Intelligence, mechanics, meta materials, multistability, mechanical memory, mechanical computation},
abstract = {Intelligence of physical agents, such as human-made (e.g., robots, autonomous cars) and biological (e.g., animals, plants) ones, is not only enabled by their computational intelligence (CI) in their brain, but also by their physical intelligence (PI) encoded in their body. Therefore, it is essential to advance the PI of human-made agents as much as possible, in addition to their CI, to operate them in unstructured and complex real-world environments like the biological agents. This article gives a perspective on what PI paradigm is, when PI can be more significant and dominant in physical and biological agents at different length scales and how bioinspired and abstract PI methods can be created in agent bodies. PI paradigm aims to synergize and merge many research fields, such as mechanics, materials science, robotics, mechanical design, fluidics, active matter, biology, self-assembly and collective systems, to enable advanced PI capabilities in human-made agent bodies, comparable to the ones observed in biological organisms. Such capabilities would progress the future robots and other machines beyond what can be realized using the current frameworks.}
}
@article{MONTANARO2024,
title = {Computable species descriptions and nanopublications: applying ontology-based technologies to dung beetles (Coleoptera, Scarabaeinae)},
journal = {Biodiversity Data Journal},
volume = {12},
year = {2024},
issn = {1314-2836},
doi = {https://doi.org/10.3897/BDJ.12.e121562},
url = {https://www.sciencedirect.com/science/article/pii/S1314283624001660},
author = {Giulio Montanaro and James P. Balhoff and Jennifer C. Girón and Max Söderholm and Sergei Tarasov},
keywords = {Phenoscript, taxonomy, semantic data, phenotypic traits, characters, morphology,   , microCT},
abstract = {Background
Taxonomy has long struggled with analysing vast amounts of phenotypic data due to computational and accessibility challenges. Ontology-based technologies provide a framework for modelling semantic phenotypes that are understandable by computers and compliant with FAIR principles. In this paper, we explore the use of Phenoscript, an emerging language designed for creating semantic phenotypes, to produce computable species descriptions. Our case study centers on the application of this approach to dung beetles (Coleoptera, Scarabaeinae).
New information
We illustrate the effectiveness of Phenoscript for creating semantic phenotypes. We also demonstrate the ability of the Phenospy python package to automatically translate Phenoscript descriptions into natural language (NL), which eliminates the need for writing traditional NL descriptions. We introduce a computational pipeline that streamlines the generation of semantic descriptions and their conversion to NL. To demonstrate the power of the semantic approach, we apply simple semantic queries to the generated phenotypic descriptions. This paper addresses the current challenges in crafting semantic species descriptions and outlines the path towards future improvements. Furthermore, we discuss the promising integration of semantic phenotypes and nanopublications, as emerging methods for sharing scientific information. Overall, our study highlights the pivotal role of ontology-based technologies in modernising taxonomy and aligning it with the evolving landscape of big data analysis and FAIR principles.}
}
@article{OU2023e15530,
title = {Investigation and analysis of the current situation of programming education in primary and secondary schools},
journal = {Heliyon},
volume = {9},
number = {4},
pages = {e15530},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e15530},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023027378},
author = {Qizhong Ou and Weijie Liang and Zhenni He and Xiao Liu and Renxing Yang and Xiaojun Wu},
keywords = {Programming education in primary and secondary schools, Programming education for students, Programming learning, Investigation and current situation, Primary and secondary education},
abstract = {With the rapid development of the era of artificial intelligence, the application ability of programming is also highlighted. As one of the necessary abilities of social talents in the future, primary and secondary schools pay more and more attention to this, and programming education is also in full swing. Therefore, based on previous studies, this paper further clarifies the current situation when the current situation of programming education in primary and secondary schools is ambiguous. This paper is aimed at a wide range of primary and secondary school teachers. With 1500 teachers who participated in the online training class for programming teachers as the object in Chinese primary, middle and high school stages, mainly from the three levels of schools, teachers, and students. The questionnaire with good reliability and validity test was used as the research method, the survey data were statistically described and analyzed, and differences were analyzed using Microsoft Excel2019, SPSS26.0 and so on, it investigates and analyzes the current situation of programming education in primary and secondary schools. Results indicate that the overall quality of programming education offerings in elementary and secondary schools is subpar, and the construction of programming education curriculum in schools requires improvement. Nevertheless, schools prioritize improving students' comprehensive abilities, and teachers hold a positive attitude towards programming education and teaching. Although students demonstrate a strong interest in learning, their foundation is weak, resulting in poor learning outcomes. Consequently, the author provides specific recommendations regarding programming education's working mechanism, curriculum standard system, teacher training, and educational resources sharing to better develop programming education in primary and secondary schools.}
}
@article{KELTYSTEPHEN2022104810,
title = {Turing’s cascade instability supports the coordination of the mind, brain, and behavior},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {141},
pages = {104810},
year = {2022},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2022.104810},
url = {https://www.sciencedirect.com/science/article/pii/S0149763422002998},
author = {Damian G. Kelty-Stephen and Madhur Mangalam},
keywords = {Criticality, Dynamic touch, Effortful touch, Executive function, Fractality, Multifractality, Multiscale, Multiplicativity, Multimodal perception, Neural avalanche, Perception and action, Posture, Power-law},
abstract = {Turing inspired a computer metaphor of the mind and brain that has been handy and has spawned decades of empirical investigation, but he did much more and offered behavioral and cognitive sciences another metaphor—that of the cascade. The time has come to confront Turing’s cascading instability, which suggests a geometrical framework driven by power laws and can be studied using multifractal formalism and multiscale probability density function analysis. Here, we review a rapidly growing body of scientific investigations revealing signatures of cascade instability and their consequences for a perceiving, acting, and thinking organism. We review work related to executive functioning (planning to act), postural control (bodily poise for turning plans into action), and effortful perception (action to gather information in a single modality and action to blend multimodal information). We also review findings on neuronal avalanches in the brain, specifically about neural participation in body-wide cascades. Turing’s cascade instability blends the mind, brain, and behavior across space and time scales and provides an alternative to the dominant computer metaphor.}
}
@incollection{BETZ202511,
title = {Chapter 2 - Conceptual and methodological issues in insect ecomorphology},
editor = {Oliver Betz},
booktitle = {Insect Ecomorphology},
publisher = {Academic Press},
pages = {11-55},
year = {2025},
isbn = {978-0-443-18544-1},
doi = {https://doi.org/10.1016/B978-0-443-18544-1.00011-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443185441000119},
author = {Oliver Betz},
keywords = {Adaptation, Behaviour, Biomechanics, Ecological morphology, Evolutionary morphology, Functional morphology, Functional trait, Insect morphology, Macroevolution, Performance, Resource use, Trait},
abstract = {Ecomorphology can be defined as a research field that investigates the ecological and evolutionary consequences of animal construction by integrating the function (biomechanics) and form (morphology) of animals in their relationship to the environment. The conceptual framework of this research field is reviewed here, with a distinction being made between the ‘morphological-comparative concept’ (having emerged from the field of functional morphology) and the ‘ecological-correlative concept’ (representing a trait-based statistically correlative approach). Although the development of the field has mainly been driven by vertebrate morphologists, this review shows that, notably, insects represent a clade that deserves more ecomorphological research because of their tremendous species diversity connected to their enormous morphofunctional disparity. Based on the morphology–performance–ecology–fitness paradigm, research problems are examined regarding the benefits that accrue to morphologists who integrate ecological thinking into their research programmes and to ecologists who consider the functional mechanisms behind their research subjects.}
}
@article{ZHANG2019215,
title = {Food-energy-water (FEW) nexus for urban sustainability: A comprehensive review},
journal = {Resources, Conservation and Recycling},
volume = {142},
pages = {215-224},
year = {2019},
issn = {0921-3449},
doi = {https://doi.org/10.1016/j.resconrec.2018.11.018},
url = {https://www.sciencedirect.com/science/article/pii/S0921344918304361},
author = {Pengpeng Zhang and Lixiao Zhang and Yuan Chang and Ming Xu and Yan Hao and Sai Liang and Gengyuan Liu and Zhifeng Yang and Can Wang},
keywords = {Urban system, Food-energy-water nexus, Conceptual framework, Resilience},
abstract = {The emerging popularity of the nexus discussion reflects the ongoing transition from a sectoral or silo approach to an integrative approach to address the global challenges pertinent to the three essential resources: food, energy, and water (FEW). Cities are critically important for advancing regional sustainable development and are thus placed at the center of the FEW nexus. This paper provides a comprehensive literature review to debate the current concepts and methods of the FEW nexus at different scales, with the aim of developing a conceptual knowledgebase framework for scientific analysis and policy making associated with the urban FEW nexus. Although the concept of nexus thinking has been widely accepted, a consistent and explicit cognition of the FEW nexus is still lacking, and a sophisticated methodological modeling framework is urgently required at various scales. As such, we proposed a three-dimensional conceptual framework of the urban FEW nexus from the perspective of resource interdependency, resource provision and system integration. This framework is useful in steering the systematic modeling and integrative management of the complex nexus issues of urban systems with different perspectives. Finally, the future directions of urban nexus research are identified from four aspects, including systematic characterization, cross-region tele-connection mechanisms, co-decision model development, and governance transition.}
}
@article{ZHANG2024106073,
title = {Bayesian deep learning: An enhanced AI framework for legal reasoning alignment},
journal = {Computer Law & Security Review},
volume = {55},
pages = {106073},
year = {2024},
issn = {2212-473X},
doi = {https://doi.org/10.1016/j.clsr.2024.106073},
url = {https://www.sciencedirect.com/science/article/pii/S0267364924001390},
author = {Chuyue Zhang and Yuchen Meng},
keywords = {Legal AI, Legal reasoning, Deep learning, Bayesian deep learning, Bayesian neural networks},
abstract = {The integration of artificial intelligence into the field of law has penetrated the underlying logic of legal operations. Currently, legal AI systems face difficulties in representing legal knowledge, exhibit insufficient legal reasoning capabilities, have poor explainability, and are inefficient in handling causal inference and uncertainty. In legal practice, various legal reasoning methods (deductive reasoning, inductive reasoning, abductive reasoning, etc.) are often intertwined and used comprehensively. However, the reasoning modes employed by current legal AI systems are inadequate. Identifying AI models that are more suitable for legal reasoning is crucial for advancing the development of legal AI systems. Distinguished from the current high-profile large language models, we believe that Bayesian reasoning is highly compatible with legal reasoning, as it can perferm abductive reasoning, excel at causal inference, and admits the "defeasibility" of reasoning conclusions, which is consistent with the cognitive development pattern of legal professionals from apriori to posteriori. AI models based on Bayesian methods can also become the main technological support for legal AI systems. Bayesian neural networks have advantages in uncertainty modeling, avoiding overfitting, and explainability. Legal AI systems based on Bayesian deep learning frameworks can combine the advantages of deep learning and probabilistic graphical models, facilitating the exchange and supplementation of information between perception tasks and reasoning tasks. In this paper, we take perpetrator prediction systems and legal judegment prediction systems as examples to discuss the construction and basic operation modes of the Bayesian deep learning framework. Bayesian deep learning can enhance reasoning ability, improve the explainability of models, and make the reasoning process more transparent and visualizable. Furthermore, Bayesian deep learning framework is well-suited for human-machine collaborative tasks, enabling the complementary strengths of humans and machines.}
}
@article{KUBALAK2025104774,
title = {Simultaneous topology and toolpath optimization for layer-free multi-axis additive manufacturing of 3D composite structures},
journal = {Additive Manufacturing},
volume = {104},
pages = {104774},
year = {2025},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2025.104774},
url = {https://www.sciencedirect.com/science/article/pii/S2214860425001381},
author = {Joseph R. Kubalak and Alfred L. Wicks and Christopher B. Williams},
abstract = {Composite materials are extremely common in nature, with organic structures freely distributing and orienting anisotropic material properties in 3D to achieve a high degree of efficiency and functionality. Human-made composite structures do not leverage the same design thinking; they are frequently designed specifically for isotropic performance and with little geometric complexity due to limitations imposed by the manufacturing processes. While additive manufacturing (AM) provides unprecedented geometric flexibility, it typically deposits material in a series of stacked 2D layers (despite the moniker of “3D printing”); it does not enable the same freedoms of material placement and orientation seen in nature. Multi-axis (e.g., robotically-enabled) AM enables true 3D part fabrication such that material anisotropy can be advantageously oriented to enhance part performance (e.g., aligning fiber reinforcement to anticipated load paths), but existing methodologies separate the design of part geometry from its multi-axis printing toolpath. This paper presents a novel design and manufacturing workflow that integrates design optimization and multi-axis AM to algorithmically create optimal part topologies concurrently with their printing toolpaths. The workflow is aware of manufacturing and design considerations to maximize part performance while simultaneously guaranteeing multi-axis printability. Material is placed through an optimized, layer-free process to significantly improve the performance of additively manufactured composite structures. The design workflow is validated by optimizing, fabricating, and mechanically evaluating multi-axis structures and demonstrated a 56.9 % improvement in structural efficiency relative to a conventional, layer-wise AM process.}
}
@article{COCHAND2023847,
title = {Systems Anesthesiology: Systems of Care Delivery and Optimization in the Operating Room},
journal = {Anesthesiology Clinics},
volume = {41},
number = {4},
pages = {847-861},
year = {2023},
note = {Perioperative Safety Culture},
issn = {1932-2275},
doi = {https://doi.org/10.1016/j.anclin.2023.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S1932227523000460},
author = {Laure Cochand and Mark G. Filipovic and Markus Huber and Markus M. Luedi and Richard D. Urman and Corina Bello},
keywords = {Systems anesthesiology, Perioperative care, Leadership, Operations management}
}
@article{CARROLL200049,
title = {Use of student-constructed number stories in a reform-based curriculum},
journal = {The Journal of Mathematical Behavior},
volume = {19},
number = {1},
pages = {49-62},
year = {2000},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(00)00038-9},
url = {https://www.sciencedirect.com/science/article/pii/S0732312300000389},
author = {William M Carroll and Karen C Fuson and Ann Diamond},
keywords = {Reform-based curriculum, Number stories, Invented procedures},
abstract = {Twelve classes using the reform-based curriculum, Everyday Mathematics (EM), were observed early in first grade. The two lessons observed involved students generating and solving addition and subtraction number stories. In these lessons, teachers were directed to help students link these number stories to representations (pictures or objects) and equations. Because this curriculum emphasizes invented procedures and number sense, the lessons also call for whole-class discussions of students' solutions. Further, the curriculum assumes that teachers will build upon and extend the children's mathematical thinking, highlighting these alternative solution methods and supporting the students' explanations. Results show that students were successful at making up, telling, and solving number stories and used a range of solution methods, including the mathematical representations available in the classrooms. However, only about three-quarters of the teachers established explicit links between the stories and mathematical representations, with fewer than half representing the stories as numbers and equations. Although student-based explanations play an important role in helping children develop solution procedures with understanding, solution methods were only elicited in half of the classes observed, and multiple methods in one-fourth of the classes. Implications for reform curricula, especially how they might clarify new goals for teachers, are discussed.}
}
@article{ROKOSZ2025106053,
title = {Moral-dilemma judgments by individuals and groups: Are many heads really more utilitarian than one?},
journal = {Cognition},
volume = {256},
pages = {106053},
year = {2025},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2024.106053},
url = {https://www.sciencedirect.com/science/article/pii/S0010027724003391},
author = {Marta Rokosz and Michał Białek and Michał M. Stefańczyk and Bertram Gawronski},
keywords = {CNI model, Deontology, Group decision-making, Moral judgment, Utilitarianism},
abstract = {Moral dilemmas often involve a conflict between action-options that maximize outcomes for the greater good (utilitarianism) and inaction-options that conform to moral norms (deontology). Previous research suggests that, compared to individuals, groups show stronger support for outcome-maximizing actions that violate moral norms. The current study used a computational modeling approach to investigate whether this difference is driven by (1) stronger sensitivity to consequences, (2) weaker sensitivity to moral norms, or (3) weaker action aversion in moral-dilemma judgments made by groups. The results suggest that groups show a stronger sensitivity to consequences than individuals. Groups and individuals did not differ in terms of their sensitivity to moral norms and their general action aversion. The findings challenge the idea that groups are less action averse and less concerned about violating moral norms than individuals and instead suggest that group decisions are more strongly guided by outcomes for the greater good.}
}
@article{ZHANG2022104630,
title = {Accurate band gap prediction based on an interpretable Δ-machine learning},
journal = {Materials Today Communications},
volume = {33},
pages = {104630},
year = {2022},
issn = {2352-4928},
doi = {https://doi.org/10.1016/j.mtcomm.2022.104630},
url = {https://www.sciencedirect.com/science/article/pii/S2352492822014714},
author = {Lingyao Zhang and Tianhao Su and Musen Li and Fanhao Jia and Shuobo Hu and Peihong Zhang and Wei Ren},
keywords = {2D materials, Bandgap, Machine learning, DFT calculation, Interpretable},
abstract = {Most materials science datasets are not so large that the accuracy of machine learning (ML) models is relatively limited if only simple features are used. Here, we constructed an interpretable ∆-machine learning (∆-ML) model to connect the hybrid functional HSE bandgap (EgHSE) with the PBE functional bandgap (EgPBE). The former can reproduce the band gap comparable with experiments, but the computational cost is much more challenging. The training is based on our high-throughput calculations on a set of two-dimensional semiconductors. Four complex descriptors, all based on the EgPBE are constructed using the sure independence screening and sparsifying operator (SISSO) algorithm. Using these descriptors, the ∆-ML can accurately predict the EgHSE of test set with a determination coefficient (R2) of 0.96. The error satisfies a normal distribution with a mean of zero. We provide a direct functional relationship between input descriptors and target properties. We find that EgHSE and the 5/6th power of EgPBE show a significant linear correlation, which may guide rapid prediction of EgHSE from EgPBE for materials with a EgHSE greater than 0.22 eV. We also discussed the correlation between the atomic radius and the EgHSE. Our work will provide an effective and interpretable model to construct the optimal physical descriptors for ML prediction on bandgaps in screening massive new 2D materials research.}
}
@article{BLACKMAN2022101661,
title = {Persistent mysteries of jet engines, formation, propagation, and particle acceleration: Have they been addressed experimentally?},
journal = {New Astronomy Reviews},
volume = {95},
pages = {101661},
year = {2022},
issn = {1387-6473},
doi = {https://doi.org/10.1016/j.newar.2022.101661},
url = {https://www.sciencedirect.com/science/article/pii/S1387647322000197},
author = {Eric G. Blackman and Sergey V. Lebedev},
keywords = {Jets, Laboratory astrophysics, Accretion, Magnetic fields, Young stellar objects, Active galactic nuclei, Microquasars, Particle acceleration, High energy density physics},
abstract = {The physics of astrophysical jets can be divided into three regimes: (i) engine and launch (ii) propagation and collimation, (iii) dissipation and particle acceleration. Since astrophysical jets comprise a huge range of scales and phenomena, practicality dictates that most studies of jets intentionally or inadvertently focus on one of these regimes, and even therein, one body of work may be simply boundary condition for another. We first discuss long standing persistent mysteries that pertain the physics of each of these regimes, independent of the method used to study them. This discussion makes contact with frontiers of plasma astrophysics more generally. While observations theory, and simulations, and have long been the main tools of the trade, what about laboratory experiments? Jet related experiments have offered controlled studies of specific principles, physical processes, and benchmarks for numerical and theoretical calculations. We discuss what has been accomplished on these fronts. Although experiments have indeed helped us to understand certain processes, proof of principle concepts, and benchmarked codes, they have yet to solved an astrophysical jet mystery on their own. A challenge is that experimental tools used for jet-related experiments so far, are typically not machines originally designed for that purpose, or designed with specific astrophysical mysteries in mind. This presents an opportunity for a different way of thinking about the development of future platforms: start with the astrophysical mystery and build an experiment to address it.}
}
@article{DSOUZA2021100949,
title = {What characterises creativity in narrative writing, and how do we assess it? Research findings from a systematic literature search},
journal = {Thinking Skills and Creativity},
volume = {42},
pages = {100949},
year = {2021},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2021.100949},
url = {https://www.sciencedirect.com/science/article/pii/S1871187121001644},
author = {Richard D'Souza},
keywords = {Creativity, Writing, Assessment, Narrative, Review},
abstract = {This paper reports findings from a systematic search of the empirical literature from 2000 to 2020 on the assessment of creativity in narrative writing. It seeks to synthesise the designs, methods and findings on how different disciplines have gathered relevant data to the question of how creativity in writing might be assessed, and feedback made more effective. It draws together the established knowledge base around two research questions. The methodology for the systematic involved searches on five academic databases for relevant keywords, producing 1796 papers. Initial screening of the abstracts identified 97 studies for further scrutiny, and for which full texts were accessed for secondary screening based on the inclusion criteria. The final 39 papers judged to satisfy the selection criteria were subject to in-depth analysis and synthesis. The findings of the review reveal that four main techniques have been utilised in efforts to assess creativity in writing, each with their own merits and limitations, and a paucity of research in several crucial areas. The review indicates that, while several disciplines have contributed to the knowledge base in this area, few interdisciplinary studies exist that draw together multiple techniques and provide clear answers for the research questions used in the study. Furthermore, there is little empirical evidence suggesting that assessment improves student creativity with regard to writing, and new research in the field would be advanced by addressing explicit definitions of creativity, the practices of writing ‘experts’, and writing considered within its social and cultural context.}
}
@article{CUI2022110595,
title = {Neural mechanisms of aberrant self-referential processing in patients with generalized anxiety disorder},
journal = {Progress in Neuro-Psychopharmacology and Biological Psychiatry},
volume = {119},
pages = {110595},
year = {2022},
issn = {0278-5846},
doi = {https://doi.org/10.1016/j.pnpbp.2022.110595},
url = {https://www.sciencedirect.com/science/article/pii/S0278584622000872},
author = {Qian Cui and Yuyan Chen and Qin Tang and Wei Sheng and Di Li and Yuhong Zeng and Kexing Jiang and Zongling He and Huafu Chen},
keywords = {Generalized anxiety disorder, Traits, Self-related processing, Task-related fMRI, Functional connectivity},
abstract = {Massive theoretical studies in clinical psychology have implicated the self in understanding internalizing disorders (i.e., anxiety and mood disorders), in which self-related tasks were frequently used to investigate internalizing psychopathology. As one of the most frequently seen internalizing disorder in primary care, patients with generalized anxiety disorder (GAD) are characterized by inappropriate self-related processing such as negative self-referential thinking. However, relevant neural mechanisms remain unknown. In this study, participants underwent a self-related task which they were presented with several positive and negative trait words and were required to judge the extent to which these traits matched themselves when compared to their average peers. Aberrant brain activation and functional connectivity of GAD were detected during processing positive and negative traits. Compared to healthy controls (HCs), patients with GAD exhibited abnormal self-processing which manifested as lower biased self-rating scores particularly for negative traits and weaker brain activity in the left dorsomedial prefrontal cortex, inferior frontal gyrus, superior temporal sulcus (STS), and bilateral lingual gyrus when processing trait words. Abnormal functional connections between these hypoactive regions and regions associated with reward, emotion, and theory of mind were observed in subsequent psychophysiological interaction analysis. An attenuation of connectivity between the left insula and left STS was associated with greater severity of anxiety symptom in GAD patients. These findings provide insight into the abnormal neurocognitive mechanisms of biased self-related processing in GAD patients, which involves distorted self-schema accompanied by abnormal activation and functional connections of regions implicated in self-related and social cognition processing.}
}
@incollection{MEY20065,
title = {Pragmatic Acts},
editor = {Keith Brown},
booktitle = {Encyclopedia of Language & Linguistics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {5-12},
year = {2006},
isbn = {978-0-08-044854-1},
doi = {https://doi.org/10.1016/B0-08-044854-2/00386-2},
url = {https://www.sciencedirect.com/science/article/pii/B0080448542003862},
author = {J.L. Mey},
keywords = {Affordance, common sense, extralinguistic acts, intention, intentionality, interactional situation, irony, power, pract, pragmatic acts, pragmeme, presupposition, situated action, situated context, situation, speech acting, speech acts, utterance},
abstract = {Recently, the established way of thinking about speech acts (in the tradition of Austin, Searle, Grice, and their followers) has undergone a remarkable change. From being an effort to represent human words in terms of what they ‘do’ (Austin), or how they can be used to produce ‘speech acts’ (Searle) or to generate ‘implicatures’ (Grice), the focus has shifted to the situation in which words are spoken and how this contributes to understanding the utterance, or even how the situation can predefine and to a degree determine what can be said. The upshot of these considerations is that we need a new theory, one that takes into account the inter- and transactional aspects of speech acting. This article proposes such a theory under the label of ‘pragmatic acts’ – acts that work not just by their wording but also by their being embedded in a situation in which humans act, with everything that humans bring to their interactional forum, including body movements, emotions, and so on.}
}
@article{TANG2024101570,
title = {Disruptive content, cross agglomeration interaction, and agglomeration replacement: Does cohesion foster strength?},
journal = {Journal of Informetrics},
volume = {18},
number = {4},
pages = {101570},
year = {2024},
issn = {1751-1577},
doi = {https://doi.org/10.1016/j.joi.2024.101570},
url = {https://www.sciencedirect.com/science/article/pii/S1751157724000828},
author = {Kun Tang and Baiyang Li and Qiyu Zhu and Lecun Ma},
keywords = {Scholar agglomeration, Disruptive knowledge, Coauthor network, Agglomeration size},
abstract = {A trend in the academic field is agglomerations among scholars to generate knowledge with a disruptive influence on science and technology; however, the benefits have not been fully substantiated. This paper analyzes over 660,000 papers on artificial intelligence published from 1961 to 2023. We propose a method to calculate the innovative capacity of disruptive knowledge based on the similarity of historical, current, and future keywords, finding that scholars who commence their scientific endeavors earlier possess a heightened capability for disruptive knowledge innovation as Dkc index. The analysis reveals that multiagglomeration scholars have the highest average number of publications and citations, followed by agglomeration-flow scholars. Moreover, a larger agglomeration results in a lower ability to disrupt and consolidate knowledge innovation. Multiagglomeration and agglomeration-flow scholars harm disruptive/consolidative innovations. However, as the agglomeration effect intensifies, these two types of scholars from the disruptive perspective and multiagglomeration scholars from the consolidation perspective have a diminishing marginal effect on innovation capacity. The agglomeration size acts as a partial intermediary in the Multi→Size→Dkc index from the dual perspective and as a full mediator in the Flow→Size→Dkc index from the disruptive perspective, but only with a direct effect from the consolidative perspective.}
}
@article{SMITH2019473,
title = {Neurocomputational mechanisms underlying emotional awareness: Insights afforded by deep active inference and their potential clinical relevance},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {107},
pages = {473-491},
year = {2019},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2019.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S014976341930541X},
author = {Ryan Smith and Richard D. Lane and Thomas Parr and Karl J. Friston},
keywords = {Active inference, Emotional awareness, Somatic misattribution, Emotional working memory, Computational neuroscience},
abstract = {Emotional awareness (EA) is recognized as clinically relevant to the vulnerability to, and maintenance of, psychiatric disorders. However, the neurocomputational processes that underwrite individual variations remain unclear. In this paper, we describe a deep (active) inference model that reproduces the cognitive-emotional processes and self-report behaviors associated with EA. We then present simulations to illustrate (seven) distinct mechanisms that (either alone or in combination) can produce phenomena – such as somatic misattribution, coarse-grained emotion conceptualization, and constrained reflective capacity – characteristic of low EA. Our simulations suggest that the clinical phenotype of impoverished EA can be reproduced by dissociable computational processes. The possibility that different processes are at work in different individuals suggests that they may benefit from distinct clinical interventions. As active inference makes particular predictions about the underlying neurobiology of such aberrant inference, we also discuss how this type of modelling could be used to design neuroimaging tasks to test predictions and identify which processes operate in different individuals – and provide a principled basis for personalized precision medicine.}
}
@article{DAWKINS2012331,
title = {Metaphor as a possible pathway to more formal understanding of the definition of sequence convergence},
journal = {The Journal of Mathematical Behavior},
volume = {31},
number = {3},
pages = {331-343},
year = {2012},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2012.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0732312312000156},
author = {Paul Christian Dawkins},
keywords = {Real analysis, Sequence convergence, Defining, Realistic Mathematics Education, Transition to advanced mathematical thinking},
abstract = {This study presents how the introduction of a metaphor for sequence convergence constituted an experientially real context in which an undergraduate real analysis student developed a property-based definition of sequence convergence. I use elements from Zandieh and Rasmussen's (2010) Defining as a Mathematical Activity framework to trace the transformation of the student's conception from a non-standard, personal concept definition rooted in the metaphor to a concept definition for sequence convergence compatible with the standard definition. This account of the development of the definition of sequence convergence differs from prior research in the sense that it began neither with examples or visual notions, nor with the statement of the formal definition. This study contributes to the Realistic Mathematics Education literature as it documents a student's progression through the definition-of and definition-for stages of mathematical activity in an interactive lecture classroom context.}
}
@article{OUYANG2023101227,
title = {Using an integrated discourse analysis approach to analyze a group's collaborative argumentation},
journal = {Thinking Skills and Creativity},
volume = {47},
pages = {101227},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2022.101227},
url = {https://www.sciencedirect.com/science/article/pii/S1871187122002280},
author = {Fan Ouyang and Zifan Tang and Mengting Cheng and Zixuan Chen},
keywords = {Collaborative argumentation, Collaborative learning, Knowledge construction, Discourse analysis, Informal learning},
abstract = {Collaborative argumentation is widely used in K-12 and higher education to foster students' argumentation skills, facilitate deep learning, and construct collective knowledge. Collaborative argumentation requires students to coordinate their social, cognitive, and metacognitive practices with peers through interactive discourses. Discourse analysis is a traditional analytic method that has been used to understand collaborative discourses. But most previous research has either integrated computational or statistical techniques to analyze frequencies of discourses or analyzed discourse from a qualitative perspective to demonstrate the microlevel, fine-grained attributes. There has been a lack of integrated discourse analysis method to holistically investigate and understand collaborative argumentation. To fill this gap, this case study used the scripted role strategy to support a group's collaborative argumentation and proposed an integrated discourse analysis approach to illustrate the temporal changes of the group's discourse moves, structures, and turn taking processes. The results showed that four participants had different discourse attributes. Based on the results, pedagogical and analytical implications are proposed to facilitate research and practice of collaborative argumentation.}
}
@article{BRAUN2013400,
title = {Custom fabric ventures: An instructional resource in job costing for the introductory managerial accounting course},
journal = {Journal of Accounting Education},
volume = {31},
number = {4},
pages = {400-429},
year = {2013},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2013.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S0748575113000511},
author = {Karen W. Braun},
keywords = {Job costing, Introductory managerial accounting, Active learning, Instructional resource},
abstract = {Job costing is a core foundational concept in the introductory managerial accounting course. The purpose of this instructional resource (IR) is to provide a thorough hands-on, active learning resource that will allow introductory students to experience a full set of accounting and management activities necessary to produce a job and assign production costs to it. For example, the IR requires students to analyze overhead costs, determine the optimal job size, schedule production, calculate the amount of materials to purchase, complete material requisitions, update raw materials records, analyze labor time records, complete a job cost record and address critical thinking questions. The IR was developed for use in a “flipped classroom” in which students work under the guidance of the instructor, but could alternatively be assigned as an unsupervised out-of-class assignment or on-line project. Since the IR was specifically developed as a learning tool for novice introductory managerial accounting students, adequate guidance is provided throughout the activity. However, to add realism and challenge students to think beyond the confines of simple mechanics, management and accounting issues are seeded throughout. Student feedback indicates that the IR not only helps students learn how a job costing system operates, but also helps students become aware of management decisions and accounting issues that impact the costs assigned to a job.}
}
@article{PAN20241357,
title = {Construction and preliminary application of large language model for reservoir performance analysis},
journal = {Petroleum Exploration and Development},
volume = {51},
number = {5},
pages = {1357-1366},
year = {2024},
issn = {1876-3804},
doi = {https://doi.org/10.1016/S1876-3804(25)60546-5},
url = {https://www.sciencedirect.com/science/article/pii/S1876380425605465},
author = {Huanquan PAN and Jianqiao LIU and Bin GONG and Yiheng ZHU and Junhui BAI and Hu HUANG and Zhengbao FANG and Hongbin JING and Chen LIU and Tie KUANG and Yubo LAN and Tianzhi WANG and Tian XIE and Mingzhe CHENG and Bin QIN and Yujiang SHEN},
keywords = {reservoir performance analysis, artificial intelligence large model, application-specific large language model, incremental pre-training, fine-tuning, subsystems coupling, entity recognition, tool invocation},
abstract = {A large language model (LLM) is constructed to address the sophisticated demands of data retrieval and analysis, detailed well profiling, computation of key technical indicators, and the solutions to complex problems in reservoir performance analysis (RPA). The LLM is constructed for RPA scenarios with incremental pre-training, fine-tuning, and functional subsystems coupling. Functional subsystem and efficient coupling methods are proposed based on named entity recognition (NER), tool invocation, and Text-to-SQL construction, all aimed at resolving pivotal challenges in developing the specific application of LLMs for RDA. This study conducted a detailed accuracy test on feature extraction models, tool classification models, data retrieval models and analysis recommendation models. The results indicate that these models have demonstrated good performance in various key aspects of reservoir dynamic analysis. The research takes some injection and production well groups in the PK3 Block of the Daqing Oilfield as an example for testing. Testing results show that our model has significant potential and practical value in assisting reservoir engineers with RDA. The research results provide a powerful support to the application of LLM in reservoir performance analysis.}
}
@article{JOHN2022133624,
title = {How key-enabling technologies’ regimes influence sociotechnical transitions: The impact of artificial intelligence on decarbonization in the steel industry},
journal = {Journal of Cleaner Production},
volume = {370},
pages = {133624},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.133624},
url = {https://www.sciencedirect.com/science/article/pii/S0959652622032024},
author = {Nikhil John and Joeri Hendrik Wesseling and Ernst Worrell and Marko Hekkert},
keywords = {Digitalization, Energy-intensive processing industries, Barriers to innovation, Multi-regime interactions, Sociotechnical systems, Multi-level perspective},
abstract = {Key Enabling Technologies (KETs) are pervasive groups of technologies expected to enable innovation. They have been promoted as technologies with tremendous potential for boosting economic growth and sustainability in all sectors of society – a claim whose validity remains underexplored. Building on systems thinking and the Multi-Level Perspective, we develop a novel approach to assess the socio-technical impact of a KET regime on a transitioning sectoral regime. This approach is applied to the case of the Artificial Intelligence (AI) KET impacting the decarbonization of the energy-intensive steel industry. To assess AI's technical impact, we compiled an inventory of AI tools based on reviewing technical scientific articles. Our analysis shows that AI adds technological value to the full range of areas in the steel industry, like predicting process parameters; optimizing operations, scheduling, and electrical energy; and forecasting product demand, quality, and site emissions. Semi-structured interviews were the primary data source to assess AI's broader socio-institutional impact. The results indicate that AI may currently be reinforcing path dependencies of the steel industry, as AI tools are more focused on incremental improvement for existing technologies rather than novel low-carbon technologies. However, AI also offers capabilities to reduce barriers to sustainability innovation, like system integration challenges, flexibility challenges, demand-side barriers, and risk-related barriers. Finally, we reflect on the generalizability of our approach for studying other transitions, and we induce characteristics of the AI-Digital KET regimes. We find that through these characteristics, the AI-Digital KET regime alters existing and creates new system structures (actors, networks, and institutions) within the impacted sector.}
}