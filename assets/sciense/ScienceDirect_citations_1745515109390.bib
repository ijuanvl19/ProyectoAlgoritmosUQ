@article{YAN2024120835,
title = {CPS-3WS: A critical pattern supported three-way sampling method for classifying class-overlapped imbalanced data},
journal = {Information Sciences},
volume = {676},
pages = {120835},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.120835},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524007497},
author = {Yuanting Yan and Zhong Zheng and Yiwen Zhang and Yanping Zhang and Yiyu Yao},
keywords = {Three-way sampling, Class-imbalance problem, Critical pattern, Class overlap},
abstract = {Class-imbalance problem widely exists in real applications ranging from medial diagnosis to economic fraud detection, etc. As one of the mainstream techniques in dealing with imbalanced data, SMOTE (Synthetic Minority Over-sampling TEchnique) and its extensions mainly rebalance the datasets via generation of observations in specific regions with various adapted strategies. Many of them do not consider the cost of role assignment of samples, and the intractable data complexity (overlap, small disjuncts, etc.) poses additional challenges to them. This paper proposes a critical pattern supported three-way sampling method (CPS-3WS) for classifying class-overlapped imbalanced data, introducing the philosophy of thinking in threes to effective classification in imbalanced learning. Specifically, CPS-3WS uses a three-way sample partition strategy with the Bayes posterior probability by dividing majority and minority classes into three disjoint subsets: risky, critical and safe patterns. CPS-3WS conducts a three-way hybrid sampling through (i) evaluating the risky majority pattern to be eliminated and (ii) selecting critical minority pattern to synthesize new samples under local information constraint. Extensive experiments on 42 UCI benchmark datasets demonstrate the superiority of the proposed CPS-3WS compared with 11 data-level methods. The source code of CPS-3WS is available at https://github.com/ytyancp/CPS-3WS.}
}
@incollection{ULLMAN1988548,
title = {Visual routines**This report describes research done at the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. Support for the Laboratory's artificial intelligence research is provided in part by the Advanced Research Projects Agency of the Department of Defense under Office of Naval Research contract N00014-80-C-0505 and in part by National Science Foundation Grant 79-23110MCS. Reprint requests should be sent to Shimon Ullman Department of Psychology and Artificial Intelligence Laboratory, M.I.T., Cambridge, MA 02139. U.S.A.},
editor = {Allan Collins and Edward E. Smith},
booktitle = {Readings in Cognitive Science},
publisher = {Morgan Kaufmann},
pages = {548-579},
year = {1988},
isbn = {978-1-4832-1446-7},
doi = {https://doi.org/10.1016/B978-1-4832-1446-7.50047-9},
url = {https://www.sciencedirect.com/science/article/pii/B9781483214467500479},
author = {SHIMON ULLMAN},
abstract = {This paper examines the processing of visual information beyond the creation of the early representations. A fundamental requirement at this level is the capacity to establish visually abstract shape properties and spatial relations. This capacity plays a major role in object recognition, visually guided manipulation, and more abstract visual thinking. For the human visual system, the perception of spatial properties and relations that are complex from a computational standpoint nevertheless often appears deceivingly immediate and effortless. The proficiency of the human system in analyzing spatial information far surpasses the capacities of current artificial systems. The study of the computations that underlie this competence may therefore lead to the development of new more efficient methods for the spatial analysis of visual information. The perception of abstract shape properties and spatial relations raises fundamental difficulties with major implications for the overall processing of visual information. It will be argued that the computation of spatial relations divides the analysis of visual information into two main stages. The first is the bottom-up creation of certain representations of the visible environment. The second stage involves the application of processes called ‘visual routines’ to the representations constructed in the first stage. These routines can establish properties and relations that cannot be represented explicitly in the initial representations. Visual routines are composed of sequences of elemental operations. Routines for different properties and relations share elemental operations. Using a fixed set of basic operations, the visual system can assemble different routines to extract an unbounded variety of shape properties and spatial relations.}
}
@article{MONROE2020293,
title = {Moral elevation: Indications of functional integration with welfare trade-off calibration and estimation mechanisms},
journal = {Evolution and Human Behavior},
volume = {41},
number = {4},
pages = {293-302},
year = {2020},
issn = {1090-5138},
doi = {https://doi.org/10.1016/j.evolhumbehav.2020.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1090513820300581},
author = {Amy Monroe},
keywords = {Moral elevation, Welfare trade-off ratios, Competitive altruism, Emotion},
abstract = {Moral elevation is a positive social emotion, which is triggered by observing third parties behaving benevolently, and which in turn triggers a motivation to behave benevolently towards others in general. It has been suggested that this relatively obscure emotion may be the output of a naturally selected cognitive adaptation which functions to help us retain our position in the competition for access to beneficial social relationships. This suggestion is here interpreted within the framework of ‘recalibrational emotions’. This framework offers the computational vocabulary necessary to understand how mental adaptations governing affect and motivation perform their functions at the cognitive level. Parallels are drawn between the suggested function and known phenomenological attributes of moral elevation, and the recently explicated functional operation of other social emotions (such as anger, guilt, and gratitude). Specifically, these other social emotions are thought to share a common computational pathway; recalibration of our welfare trade-off ratios (WTRs). WTRs are the computational element which dictate our willingness to benefit others at some cost to ourselves. A series of studies was conducted to explore whether a reliable relationship exists between moral elevation and WTRs. The results suggest that elevation does have a positive recalibrational effect on our WTRs, and that it may also be functionally integrated with a mental mechanism designed by natural selection to estimate the WTRs of other social actors.}
}
@article{ESSEX2018554,
title = {Model falsifiability and climate slow modes},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {502},
pages = {554-562},
year = {2018},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2018.02.090},
url = {https://www.sciencedirect.com/science/article/pii/S0378437118301766},
author = {Christopher Essex and Anastasios A. Tsonis},
keywords = {Climate complexity, Computer errors, Computational over-stabilization, Dynamical and thermodynamical sensitivity, Slow climate modes},
abstract = {The most advanced climate models are actually modified meteorological models attempting to capture climate in meteorological terms. This seems a straightforward matter of raw computing power applied to large enough sources of current data. Some believe that models have succeeded in capturing climate in this manner. But have they? This paper outlines difficulties with this picture that derive from the finite representation of our computers, and the fundamental unavailability of future data instead. It suggests that alternative windows onto the multi-decadal timescales are necessary in order to overcome the issues raised for practical problems of prediction.}
}
@incollection{BLACKBURN19941,
title = {1 - Structures, Languages and Translations: the Structural Approach to Feature Logic},
editor = {C.J. Rupp and M.A. Rosner and R.L. Johnson},
booktitle = {Constraints, Language and Computation},
publisher = {Academic Press},
address = {San Diego},
pages = {1-27},
year = {1994},
series = {Cognitive Science},
isbn = {978-0-08-050296-0},
doi = {https://doi.org/10.1016/B978-0-08-050296-0.50008-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780080502960500085},
author = {Patrick Blackburn},
abstract = {Publisher Summary
This chapter reviews methodological issues involved in the structural approach used in feature logic. A direct consequence of the systematic approach to a variety of feature logics is that it clarifies the relationships between them. This is most explicit in the presentation of translations between various existing and putative feature logics which draws heavily on the correspondence theory that relates modal and classical languages. The chapter describes a general approach to the subject called the structural approach, and explains that thinking in structural terms is a useful way of thinking about unification formalisms and their interrelationships. At present, in contemporary unification-based linguistic frameworks, linguistic data is modelled by certain kinds of decorated labelled (directed) graphs. Perhaps the most prevalent way of thinking about unification-based grammar formalisms is that they are languages for expressing constraints on feature structures. Two basic ideas drive modal logic: one syntactic, and the other semantic. Modal languages are interpreted on Kripke models, which are set theoretic entities providing the following information. Propositional Dynamic Logic (PDL) is an extension of modal logic; PDL and some of its extensions are natural constraint languages for dealing with feature structures. Subsequently, Attribute Value Matrices (AVMs) are one of the most widely used methods of describing feature structures. A general setting for feature logic is the space of relational structures of model theory, together with the various languages for describing these structures, and the satisfiability preserving translations that exist among these languages. The basic ideas are very simple: feature structures are certain sorts of relational structures, and while there is a vast range of languages for talking about these structures, these languages are interrelated by satisfiability preserving translations.}
}
@article{VAIS2013718,
title = {Laplacians on flat line bundles over 3-manifolds},
journal = {Computers & Graphics},
volume = {37},
number = {6},
pages = {718-729},
year = {2013},
note = {Shape Modeling International (SMI) Conference 2013},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2013.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S0097849313000873},
author = {Alexander Vais and Daniel Brandes and Hannes Thielhelm and Franz-Erich Wolter},
keywords = {Spectral geometry, Vector bundles, Computational topology, Laplace operator, Knots, Seifert surfaces, FEM},
abstract = {The well-known Laplace–Beltrami operator, established as a basic tool in shape processing, builds on a long history of mathematical investigations that have induced several numerical models for computational purposes. However, the Laplace–Beltrami operator is only one special case of many possible generalizations that have been researched theoretically. Thereby it is natural to supplement some of those extensions with concrete computational frameworks. In this work we study a particularly interesting class of extended Laplacians acting on sections of flat line bundles over compact Riemannian manifolds. Numerical computations for these operators have recently been accomplished on two-dimensional surfaces. Using the notions of line bundles and differential forms, we follow up on that work giving a more general theoretical and computational account of the underlying ideas and their relationships. Building on this we describe how the modified Laplacians and the corresponding computations can be extended to three-dimensional Riemannian manifolds, yielding a method that is able to deal robustly with volumetric objects of intricate shape and topology. We investigate and visualize the two-dimensional zero sets of the first eigenfunctions of the modified Laplacians, yielding an approach for constructing characteristic well-behaving, particularly robust homology generators invariant under isometric deformation. The latter include nicely embedded Seifert surfaces and their non-orientable counterparts for knot complements.}
}
@article{VARDOULI2015137,
title = {Making use: Attitudes to human-artifact engagements},
journal = {Design Studies},
volume = {41},
pages = {137-161},
year = {2015},
note = {Special Issue: Computational Making},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2015.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X15000563},
author = {Theodora Vardouli},
keywords = {design theory, philosophy of design, user behavior, function theory, computational models},
abstract = {‘Function’ and ‘use’ are keywords that design researchers customarily employ when referring to human-artifact engagements. However, there is little consensus about how the concepts of function and use relate to each other, to the intentions of ‘designers’ and ‘users’, or to their actions and encompassing contexts. In this paper, I synthesize literature from design research, material culture studies, design anthropology, and function theory in order to critically compare different attitudes to human-artifact engagements, implicit in characterizations of function and use. I identify design-centric, communicative, and use-centric attitudes, and discuss their assumptions and implications for design theory. I conclude by outlining principles for theoretically and computationally approaching use as an embodied and temporally contingent process – as a form of ‘making’.}
}
@article{KASONGO2023113,
title = {A deep learning technique for intrusion detection system using a Recurrent Neural Networks based framework},
journal = {Computer Communications},
volume = {199},
pages = {113-125},
year = {2023},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2022.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0140366422004601},
author = {Sydney Mambwe Kasongo},
keywords = {Machine learning, Feature selection, Intrusion detection, Feature extraction},
abstract = {In recent years, the spike in the amount of information transmitted through communication infrastructures has increased due to the advances in technologies such as cloud computing, vehicular networks systems, the Internet of Things (IoT), etc. As a result, attackers have multiplied their efforts for the purpose of rendering network systems vulnerable. Therefore, it is of utmost importance to improve the security of those network systems. In this study, an IDS framework using Machine Learning (ML) techniques is implemented. This framework uses different types of Recurrent Neural Networks (RNNs), namely, Long-Short Term Memory (LSTM), Gated Recurrent Unit (GRU) and Simple RNN. To assess the performance of the proposed IDS framework, the NSL-KDD and the UNSW-NB15 benchmark datasets are considered. Moreover, existing IDSs suffer from low test accuracy scores in detecting new attacks as the feature dimension grows. In this study, an XGBoost-based feature selection algorithm was implemented to reduce the feature space of each dataset. Following that process, 17 and 22 relevant attributes were picked from the UNSW-NB15 and NSL-KDD, respectively. The accuracy obtained through the test subsets was used as the main performance metric in conjunction with the F1-Score, the validation accuracy, and the training time (in seconds). The results showed that for the binary classification tasks using the NSL-KDD, the XGBoost-LSTM achieved the best performance with a test accuracy (TAC) of 88.13%, a validation accuracy (VAC) of 99.49% and a training time of 225.46 s. For the UNSW-NB15, the XGBoost-Simple-RNN was the most efficient model with a TAC of 87.07%. For the multiclass classification scheme, the XGBoost-LSTM achieved a TAC of 86.93% over the NSL-KDD and the XGBoost-GRU obtained a TAC of 78.40% over the UNSW-NB15 dataset. These results demonstrated that our proposed IDS framework performed optimally in comparison to existing methods.}
}
@article{KAMATH2020100944,
title = {Making grammars for material and tectonic complexity: An example of a thin-tile vault},
journal = {Design Studies},
volume = {69},
pages = {100944},
year = {2020},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2020.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X20300326},
author = {Ayodh Vasant Kamath},
keywords = {affordance, architectural design, creativity, reflective practice, making grammars},
abstract = {Shape grammars are a framework to view design as non-deterministic, creative, visual computation, and making as the deterministic execution of a design in the material world. Making grammars conceive of making as creative, multi-sensory, material computation. However, examples in the literature on making grammar are insufficiently complex to demonstrate the creativity of non-visual senses in making. This paper develops a making grammar for thin-tile vault construction as a sensory ethnography to ‘show making’ to designers as being a creative practice involving visual and non-visual senses. To do so, the role of drawing in shape grammar and making grammar is differentiated, and environmental psychology is used to develop a framework for the use of drawing to depict multi-sensory processes in making grammar.}
}
@article{NG2023116585,
title = {Development of a system model to predict flows and performance of regional waste management planning: A case study of England},
journal = {Journal of Environmental Management},
volume = {325},
pages = {116585},
year = {2023},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2022.116585},
url = {https://www.sciencedirect.com/science/article/pii/S0301479722021582},
author = {Kok Siew Ng and Aidong Yang},
keywords = {Circular economy, Recycling, Stock-and-flow, Sustainable waste management, Resource recovery, Systems thinking},
abstract = {Significant loss of valuable resources and increasing burdens on landfills are often associated with a lack of proper planning in waste management and resource recovery strategy. A sustainable waste management model is thus urgently needed to improve resource efficiency and divert more waste from landfills. This paper proposes a comprehensive system model using stock-and-flow diagram to examine the current waste management performance and project the future waste generation, treatment and disposal scenarios, using England as a case study. The model comprises three integrated modules to represent household waste generation and collection; waste treatment and disposal; and energy recovery. A detailed mass and energy balance has been established and waste management performance has been evaluated using six upstream and downstream indicators. The base case scenario that assumes constant waste composition shows that waste to landfills can be reduced to less than 10% of the total amount, by 2035. However, it entails greater diversion of waste to energy-from-waste facilities, which is not sustainable and would incur higher capital investment and gate fees. Alternative case scenarios that promote recycling instead of energy recovery result in lower capital investment and gate fees. Complete elimination of the food and organic fraction from the residual waste stream will help meet the 65% recycling target by 2035. In light of the need for achieving a more circular economy in England, enhancing material recovery through reuse and recycling, reducing reliance on energy-from-waste and deploying more advanced waste valorisation technologies should be considered in future policy and planning for waste management.}
}
@incollection{NG202451,
title = {Chapter 4 - System modeling and mapping},
editor = {Kok Siew Ng and Elias Martinez Hernandez and Aki Yamaguchi},
booktitle = {A New Systems Thinking Approach to Sustainable Resource Management},
publisher = {Elsevier},
pages = {51-140},
year = {2024},
isbn = {978-0-323-99869-7},
doi = {https://doi.org/10.1016/B978-0-323-99869-7.00003-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323998697000036},
author = {Kok Siew Ng and Elias Martinez Hernandez and Aki Yamaguchi},
keywords = {Causal loop diagram, GIS, MFA, Resource availability, Sensitivity and uncertainty analyses, System dynamics},
abstract = {This chapter presents a series of representative techniques for system modeling and mapping, including resource availability analysis, material flow analysis, system dynamics, and sensitivity and uncertainty analyses. These are among the well-established computational modeling methods adopted widely in engineering, environmental, and social science disciplines. They are particularly useful in the context of resource and waste management, providing clearer visualization of the system and enabling reliable prediction of system behavior. Resource availability assessment provides a bird's eye view of the interdependencies among resources and the overall feasibility for a system to operate with the available resources. Material flow analysis facilitates a robust mapping of resource consumption, production, and losses, offering insights for identifying opportunities to improve resource efficiency and minimize waste. System dynamics enables us to understand the complex behavior of a system through exploring the interaction of different factors, serving as a forecasting tool for future scenarios. Sensitivity analysis determines the system's responsiveness to different input values, identifying the most influential inputs in the system's response. Uncertainty analysis quantifies variations and uncertainties in potential system responses due to variations in inputs.}
}
@article{SANGAIAH2020347,
title = {Cognitive IoT system with intelligence techniques in sustainable computing environment},
journal = {Computer Communications},
volume = {154},
pages = {347-360},
year = {2020},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2020.02.049},
url = {https://www.sciencedirect.com/science/article/pii/S0140366419314616},
author = {Arun Kumar Sangaiah and Jerline Sheebha Anni Dhanaraj and Prabu Mohandas and Aniello Castiglione},
keywords = {Computational intelligence, Cognition, Multi-sensor, Data fusion, IoT},
abstract = {Forest border crossing animals creates major societal related issues, in addition to endangering their own lives. This is the objective focused in this paper targeting the species “The Elephant”, incorporating with technical methodologies namely, multi-sensor data fusion, cognition theories and computational intelligence techniques. Multi-sensor data fusion provides three level detection of target, along with its related outputs, which improves performance metrics. Cognition theory resulted in obtaining other interesting features about the target. Computational intelligence techniques integrate and conclude the presence of the target in the pseudo-boundary. The technical combination enhances the novelty of the research work, resulting in achieving remarkable accuracy and minimized false alert. An IoT kit was designed and deployed in the real time wild environment in Hosur forest region for collecting the data of Elephant. Further, the notification is sent to the registered mobile of the forest authority, as an early warning for chasing the pachyderm back to the forest.}
}
@article{ZADEH20082751,
title = {Is there a need for fuzzy logic?},
journal = {Information Sciences},
volume = {178},
number = {13},
pages = {2751-2779},
year = {2008},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2008.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S0020025508000716},
author = {Lotfi A. Zadeh},
keywords = {Fuzzy logic, Fuzzy sets, Approximate reasoning, Computing with words, Computing with perceptions, Generalized theory of uncertainty},
abstract = {“Is there a need for fuzzy logic?” is an issue which is associated with a long history of spirited discussions and debate. There are many misconceptions about fuzzy logic. Fuzzy logic is not fuzzy. Basically, fuzzy logic is a precise logic of imprecision and approximate reasoning. More specifically, fuzzy logic may be viewed as an attempt at formalization/mechanization of two remarkable human capabilities. First, the capability to converse, reason and make rational decisions in an environment of imprecision, uncertainty, incompleteness of information, conflicting information, partiality of truth and partiality of possibility – in short, in an environment of imperfect information. And second, the capability to perform a wide variety of physical and mental tasks without any measurements and any computations [L.A. Zadeh, From computing with numbers to computing with words – from manipulation of measurements to manipulation of perceptions, IEEE Transactions on Circuits and Systems 45 (1999) 105–119; L.A. Zadeh, A new direction in AI – toward a computational theory of perceptions, AI Magazine 22 (1) (2001) 73–84]. In fact, one of the principal contributions of fuzzy logic – a contribution which is widely unrecognized – is its high power of precisiation. Fuzzy logic is much more than a logical system. It has many facets. The principal facets are: logical, fuzzy-set-theoretic, epistemic and relational. Most of the practical applications of fuzzy logic are associated with its relational facet. In this paper, fuzzy logic is viewed in a nonstandard perspective. In this perspective, the cornerstones of fuzzy logic – and its principal distinguishing features – are: graduation, granulation, precisiation and the concept of a generalized constraint. A concept which has a position of centrality in the nontraditional view of fuzzy logic is that of precisiation. Informally, precisiation is an operation which transforms an object, p, into an object, p∗, which in some specified sense is defined more precisely than p. The object of precisiation and the result of precisiation are referred to as precisiend and precisiand, respectively. In fuzzy logic, a differentiation is made between two meanings of precision – precision of value, v-precision, and precision of meaning, m-precision. Furthermore, in the case of m-precisiation a differentiation is made between mh-precisiation, which is human-oriented (nonmathematical), and mm-precisiation, which is machine-oriented (mathematical). A dictionary definition is a form of mh-precisiation, with the definiens and definiendum playing the roles of precisiend and precisiand, respectively. Cointension is a qualitative measure of the proximity of meanings of the precisiend and precisiand. A precisiand is cointensive if its meaning is close to the meaning of the precisiend. A concept which plays a key role in the nontraditional view of fuzzy logic is that of a generalized constraint. If X is a variable then a generalized constraint on X, GC(X), is expressed as X isr R, where R is the constraining relation and r is an indexical variable which defines the modality of the constraint, that is, its semantics. The primary constraints are: possibilistic, (r=blank), probabilistic (r=p) and veristic (r=v). The standard constraints are: bivalent possibilistic, probabilistic and bivalent veristic. In large measure, science is based on standard constraints. Generalized constraints may be combined, qualified, projected, propagated and counterpropagated. The set of all generalized constraints, together with the rules which govern generation of generalized constraints, is referred to as the generalized constraint language, GCL. The standard constraint language, SCL, is a subset of GCL. In fuzzy logic, propositions, predicates and other semantic entities are precisiated through translation into GCL. Equivalently, a semantic entity, p, may be precisiated by representing its meaning as a generalized constraint. By construction, fuzzy logic has a much higher level of generality than bivalent logic. It is the generality of fuzzy logic that underlies much of what fuzzy logic has to offer. Among the important contributions of fuzzy logic are the following: 1.FL-generalization. Any bivalent-logic-based theory, T, may be FL-generalized, and hence upgraded, through addition to T of concepts and techniques drawn from fuzzy logic. Examples: fuzzy control, fuzzy linear programming, fuzzy probability theory and fuzzy topology.2.Linguistic variables and fuzzy if–then rules. The formalism of linguistic variables and fuzzy if–then rules is, in effect, a powerful modeling language which is widely used in applications of fuzzy logic. Basically, the formalism serves as a means of summarization and information compression through the use of granulation.3.Cointensive precisiation. Fuzzy logic has a high power of cointensive precisiation. This power is needed for a formulation of cointensive definitions of scientific concepts and cointensive formalization of human-centric fields such as economics, linguistics, law, conflict resolution, psychology and medicine.4.NL-Computation (computing with words). Fuzzy logic serves as a basis for NL-Computation, that is, computation with information described in natural language. NL-Computation is of direct relevance to mechanization of natural language understanding and computation with imprecise probabilities. More generally, NL-Computation is needed for dealing with second-order uncertainty, that is, uncertainty about uncertainty, or uncertainty2 for short. In summary, progression from bivalent logic to fuzzy logic is a significant positive step in the evolution of science. In large measure, the real-world is a fuzzy world. To deal with fuzzy reality what is needed is fuzzy logic. In coming years, fuzzy logic is likely to grow in visibility, importance and acceptance.}
}
@incollection{WHANGBO2005765,
title = {Chapter 26 - Concepts of perturbation, orbital interaction, orbital mixing and orbital occupation},
editor = {Clifford E. Dykstra and Gernot Frenking and Kwang S. Kim and Gustavo E. Scuseria},
booktitle = {Theory and Applications of Computational Chemistry},
publisher = {Elsevier},
address = {Amsterdam},
pages = {765-784},
year = {2005},
isbn = {978-0-444-51719-7},
doi = {https://doi.org/10.1016/B978-044451719-7/50069-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044451719750069X},
author = {Myung-Hwan Whangbo},
abstract = {Publisher Summary
This chapter describes the concepts of perturbation, orbital interaction, orbital mixing, and orbital occupation work at all levels of electronic structure. These qualitative concepts provide a conceptual framework in which to rationalize experimental/theoretical observations and to generate qualitative predictions. An important role of an electronic structure theory is to provide quantitative predictions. In this role theoretical predictions require developments of efficient programs for theoretical computations. Another important role of an electronic structure theory is to provide a conceptual framework in which to think and organize. In this role, the theoretical predictions need not be quantitative but should provide a bias toward correct thinking about further experimental and theoretical studies. When combined with the ideas of symmetry and overlap, the concepts of perturbation, orbital interaction, orbital mixing, and orbital occupation have been indispensable not only in understanding structure – property relationships in various chemical compounds but also in interpreting results of electronic structure calculations. These qualitative concepts work at all levels of electronic structure descriptions from one-electron theory neglecting self-consistent-field adjustments of orbitals to theories including electron correlation and to those including relativistic effects.}
}
@article{MCLOUGHLIN2022173,
title = {Midfrontal Theta Activity in Psychiatric Illness: An Index of Cognitive Vulnerabilities Across Disorders},
journal = {Biological Psychiatry},
volume = {91},
number = {2},
pages = {173-182},
year = {2022},
note = {Biomarkers of Psychosis},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2021.08.020},
url = {https://www.sciencedirect.com/science/article/pii/S0006322321015651},
author = {Gráinne McLoughlin and Máté Gyurkovics and Jason Palmer and Scott Makeig},
keywords = {Biomarker, Cognitive control, EEG, ERP, Oscillations, Theta},
abstract = {There is an urgent need to identify the mechanisms that contribute to atypical thinking and behavior associated with psychiatric illness. Behavioral and brain measures of cognitive control are associated with a variety of psychiatric disorders and conditions as well as daily life functioning. Recognition of the importance of cognitive control in human behavior has led to intensive research into behavioral and neurobiological correlates. Oscillations in the theta band (4–8 Hz) over medial frontal recording sites are becoming increasingly established as a direct neural index of certain aspects of cognitive control. In this review, we point toward evidence that theta acts to coordinate multiple neural processes in disparate brain regions during task processing to optimize behavior. Theta-related signals in human electroencephalography include the N2, the error-related negativity, and measures of theta power in the (time-)frequency domain. We investigate how these theta signals are affected in a wide range of psychiatric conditions with known deficiencies in cognitive control: anxiety, obsessive-compulsive disorder, attention-deficit/hyperactivity disorder, and substance abuse. Theta-related control signals and their temporal consistency were found to differ in most patient groups compared with healthy control subjects, suggesting fundamental deficits in reactive and proactive control. Notably, however, clinical studies directly investigating the role of theta in the coordination of goal-directed processes across different brain regions are uncommon and are encouraged in future research. A finer-grained analysis of flexible, subsecond-scale functional networks in psychiatric disorders could contribute to a dimensional understanding of psychopathology.}
}
@article{ZEITHAMMER2024,
title = {Strange Case of Dr. Bidder and Mr. Entrant: Consumer Preference Inconsistencies in Costly Price Offers},
journal = {International Journal of Research in Marketing},
year = {2024},
issn = {0167-8116},
doi = {https://doi.org/10.1016/j.ijresmar.2024.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S016781162400079X},
author = {Robert Zeithammer and Lucas Stich and Martin Spann and Gerald Häubl},
keywords = {Pricing, Auctions, Entry costs, Behavioral economics, Experiments},
abstract = {Consumers make price offers to sellers in a variety of domains, such as when buying cars or houses or when bidding in auctions for airline upgrades, art, or collectibles. Submitting an offer typically entails administrative, waiting, and opportunity costs. Making such costly price offers involves two intertwined decisions—in addition to determining how much to offer, consumers must also decide whether to make an offer in the first place. We examine the impact of offer-submission costs on consumer behavior using a series of incentive-compatible experiments. Our findings reveal a preference inconsistency, whereby the preferences implied by one of the decisions do not align with the preferences implied by the other. In particular, potential buyers enter more often than their offer amounts would predict based on standard economic models. This preference inconsistency is robust to two interventions designed to help consumers make offer-amount and entry decisions—(1)the provision of interactive-feedback decision aids and (2)the sequencing of the two sub-decisions in the normative order. Neither of these interventions resolves the inconsistency. Instead, the patterns of results suggest that consumers approach the offer-amount and entry decisions as if they were unrelated. We discuss the implications of our findings for the design of offer-submission interfaces, as well as for econometric attempts to infer consumer preferences from offer and bidding data.}
}
@article{ALRAKHAMI2021107573,
title = {A deep learning-based edge-fog-cloud framework for driving behavior management},
journal = {Computers & Electrical Engineering},
volume = {96},
pages = {107573},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107573},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621005127},
author = {Mabrook S. Al-Rakhami and Abdu Gumaei and Mohammad Mehedi Hassan and Atif Alamri and Musaed Alhussein and Md. Abdur Razzaque and Giancarlo Fortino},
keywords = {Deep learning, Car mobile edge (CME), Fog and cloud computing, Aggressive driving behaviors},
abstract = {Among the various reasons behind vehicle accidents, drivers' aggressiveness and distractions play a significant role. Deep learning (DL) algorithms inside a car mobile edge (CME) have been used for driver monitoring and to perform automated decision-making processes. Training and retraining the DL models in resource-constrained CME devices come with several challenges, especially regarding computational and memory space costs. Moreover, training the DL models periodically on representative data nearest to CME without imposing communication overheads on the cloud improves the quality of service (QoS) parameters, such as memory demand, processing time, power consumption, and bandwidth. This paper investigates the deployment of a deep neural network (DNN) model on a cloud-fog-edge computing framework for aggressive driver behavior detection and monitoring. To reach this goal, our framework proposes utilizing effective systems and databases of sensor-based metrics and data, cost-effective wireless networks, cloud-and fog-edge computing technologies, and the Internet. Experimental results of the DNN model showed that the accuracy of detection is improved by 1.84% compared with the current related work without any pre-processing window on data points that come from bio-signal sensors. Moreover, the experimental results of the networking part prove the efficiency and effectiveness of the proposed framework.}
}
@article{ZHOU2019104484,
title = {Long-term forecasts for energy commodities price: What the experts think},
journal = {Energy Economics},
volume = {84},
pages = {104484},
year = {2019},
issn = {0140-9883},
doi = {https://doi.org/10.1016/j.eneco.2019.104484},
url = {https://www.sciencedirect.com/science/article/pii/S0140988319302658},
author = {Fan Zhou and Lionel Page and Robert K. Perrons and Zuduo Zheng and Simon Washington},
keywords = {Crude oil prices, Natural gas prices, Expert elicitation, Bayesian Truth Serum, Surprisingly popular},
abstract = {The ability to forecast energy prices in the long-term is important for a wide range of reasons, from the formulation of countries' energy and transportation policies to the defensive strategies of nations to investment decisions within the private sector. Despite the importance of these predictions, however, forecasters and market pundits face a difficult challenge when trying to forecast over the long-term. While statistical models can credibly rely on assumptions about the relationship between variables in the short-term, they are frequently less reliable in the long-term as political and technological transformations profoundly change how the economy works over time. Towards improving long-term predictions for energy commodities, this paper uses the elicitation and aggregation of experts' beliefs to put forward forecasts for crude oil and natural gas prices by incentivizing experts to tell the truth and minimising their own biases through the application of the Bayesian Truth Serum. With this approach, we generated both short-term and long-term forecasts, and used the short-term forecast to validate the quality of the experts' predictions.}
}
@article{BAMOROVAT202321,
title = {Poor adherence is a major barrier to the proper treatment of cutaneous leishmaniasis: A case-control field assessment in Iran},
journal = {International Journal for Parasitology: Drugs and Drug Resistance},
volume = {21},
pages = {21-27},
year = {2023},
issn = {2211-3207},
doi = {https://doi.org/10.1016/j.ijpddr.2022.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S2211320722000331},
author = {Mehdi Bamorovat and Iraj Sharifi and Setareh {Agha Kuchak Afshari} and Ali Karamoozian and Amirhossein Tahmouresi and Amireh Heshmatkhah and Ehsan Salarkia and Ahmad Khosravi and Maryam {Hakimi Parizi} and Maryam Barghi},
keywords = {Poor adherence, Cutaneous leishmaniasis, Major barrier, Treatment, Iran},
abstract = {Leishmaniasis is an overlooked, poverty-stricken, and complex disease with growing social and public health problems. In general, leishmaniasis is a curable disease; however, there is an expansion of unresponsive cases to treatment in cutaneous leishmaniasis (CL). One of the effective and ignored determinants in the treatment outcome of CL is poor treatment adherence (PTA). PTA is an overlooked and widespread phenomenon to proper Leishmania treatment. This study aimed to explore the effect of poor adherence in unresponsiveness to treatment in patients with anthroponotic CL (ACL) by comparing conventional statistical modalities and machine learning analyses in Iran. Overall, 190 cases consisting of 50 unresponsive patients (case group), and 140 responsive patients (control group) with ACL were randomly selected. The data collecting form that included 25 queries (Q) was recorded for each case and analyzed by R software and genetic algorithm (GA) approaches. Complex treatment regimens (Q11), cultural and lay views about the disease and therapy (Q8), life stress, hopelessness and negative feelings (Q22), adverse effects of treatment (Q13), and long duration of the lesion (Q12) were the most prevalent significant variables that inhibited effective treatment adherence by the two methods, in decreasing order of significance. In the inherent algorithm approach, similar to the statistical approach, the most significant feature was complex treatment regimens (Q11). Providing essential knowledge about ACL and treatment of patients with chronic diseases and patients with misconceptions about chemical drugs are important issues directly related to the disease's unresponsiveness. Furthermore, early detection of patients to prevent the long duration of the disease and the process of treatment, efforts to minimize side effects of treatment, induction of positive thinking, and giving hope to patients with stress and anxiety by medical staff, and family can help patients adhere to the treatment.}
}
@article{BORATYNSKA20165529,
title = {FsQCA in corporate bankruptcy research. An innovative approach in food industry},
journal = {Journal of Business Research},
volume = {69},
number = {11},
pages = {5529-5533},
year = {2016},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2016.04.166},
url = {https://www.sciencedirect.com/science/article/pii/S0148296316303733},
author = {Katarzyna Boratyńska},
keywords = {Complexity theory, fsQCA, Corporate bankruptcy, Food industry},
abstract = {This study focuses on fsQCA in corporate bankruptcy research. This research aims at revealing how an fsQCA approach can overcome the knowledge gap of current conceptual and methodological attempts to expose corporate bankruptcy's architecture of causalities. The article discusses the economic literature concerning using fsQCA in corporate bankruptcy studies through complexity theory and a critical perspective. The study concentrates on implementing fsQCA and asymmetric thinking to corporate bankruptcy cases in food industry. The research examines the main reasons for corporate bankruptcy, namely: lack of financial liquidity, too high level of liabilities, losses, weak management, and too late recovery actions. The study attempts to build theory from food industry cases.}
}
@article{MARSELLA200970,
title = {EMA: A process model of appraisal dynamics},
journal = {Cognitive Systems Research},
volume = {10},
number = {1},
pages = {70-90},
year = {2009},
note = {Modeling the Cognitive Antecedents and Consequences of Emotion},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2008.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S1389041708000314},
author = {Stacy C. Marsella and Jonathan Gratch},
keywords = {Emotion, Cognitive models, Appraisal theory, Coping},
abstract = {A computational model of emotion must explain both the rapid dynamics of some emotional reactions as well as the slower responses that follow deliberation. This is often addressed by positing multiple levels of appraisal processes such as fast pattern directed vs. slower deliberative appraisals. In our view, this confuses appraisal with inference. Rather, we argue for a single and automatic appraisal process that operates over a person’s interpretation of their relationship to the environment. Dynamics arise from perceptual and inferential processes operating on this interpretation (including deliberative and reactive processes). This article discusses current developments in a computational model of emotion processes and illustrates how a single-level model of appraisal obviates a multi-level approach within the context of modeling a naturalistic emotional situation.}
}
@article{PAZZANI1991401,
title = {A computational theory of learning causal relationships},
journal = {Cognitive Science},
volume = {15},
number = {3},
pages = {401-424},
year = {1991},
issn = {0364-0213},
doi = {https://doi.org/10.1016/0364-0213(91)80003-N},
url = {https://www.sciencedirect.com/science/article/pii/036402139180003N},
author = {Michael Pazzani},
abstract = {I present a cognitive model of the human ability to acquire causal relationships. I report on experimental evidence demonstrating that human learners acquire accurate causal relationships more rapidly when training examples are consistent with a general theory of causality. This article describes a learning process that uses a general theory of causality as background knowledge. The learning process, which I call theory-driven learning (TDL), hypothesizes causal relationships consistent both with observed data and the general theory of causality. TDL accounts for data on both the rate at which human learners acquire causal relationships, and the types of causal relationships they acquire. Experiments with TDL demonstrate the advantage of TDL for acquiring causal relationships over similarity-based approaches to learning: Fewer examples are required to learn an accurate relationship.}
}
@incollection{KEENAN2015394,
title = {Psychology of Inferences},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {394-399},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.57011-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868570111},
author = {Janice M. Keenan},
keywords = {Backward inferences, Coherence, Cortical networks, Explicit inferences, Forward inferences, Implicit inferences, Individual differences, Inferences, Inferencing, computational processes of, Inferencing, methodological issues, Inferencing, neural basis of, Knowledge, Language, Right hemisphere language},
abstract = {The goal of comprehension is to understand what the speaker intended the text to mean. Inferences are driven by a desire to make the interpretation both more coherent and more elaborate than the text itself. The goal of research on inferencing is to specify how the computational processes involved in making inferences vary with the comprehender's knowledge, the conditions that promote inferencing, the various types of inferences and methodological problems involved in assessing them, and, most recently, the neural bases of inferencing.}
}
@article{NUMRICH201169,
title = {Self-similarity of parallel machines},
journal = {Parallel Computing},
volume = {37},
number = {2},
pages = {69-84},
year = {2011},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2010.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S0167819110001444},
author = {Robert W. Numrich and Michael A. Heroux},
keywords = {Parallel algorithms, Benchmark analysis, Computational intensity, Computational force, Dimensional analysis, Equivalence class, Self-similarity, Scaling, Mixing coefficient},
abstract = {Self-similarity is a property of physical systems that describes how to scale parameters such that dissimilar systems appear to be similar. Computer systems are self-similar if certain ratios of computational forces, also known as computational intensities, are equal. Two machines with different computational power, different network bandwidth and different inter-processor latency behave the same way if they have the same ratios of forces. For the parallel conjugate gradient algorithm studied in this paper, two machines are self-similar if and only if the ratio of one force describing latency effects to another force describing bandwidth effects is the same for both machines. For the two machines studied in this paper, this ratio, which we call the mixing coefficient, is invariant as problem size and processor count change. The two machines have the same mixing coefficient and belong to the same equivalence class.}
}
@article{RAVI2023151,
title = {Evolving trends in student assessment in chemical engineering education},
journal = {Education for Chemical Engineers},
volume = {45},
pages = {151-160},
year = {2023},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2023.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S1749772823000453},
author = {Manoj Ravi},
keywords = {Assessment, Authentic assessment, Digitalisation},
abstract = {Alongside innovation in teaching practice, student assessment in chemical engineering has seen significant changes in the recent past. This article undertakes a systematic review of the recent advances that have been reported in assessment practice in chemical engineering education. The main trends that emerge are: a shift towards authentic assessment methods, an increase in emphasis on peer-assessment and other approaches for group-based assignments, and a greater use of digital tools for the delivery of authentic assessments and improvement of marking and feedback practice. The analysis also examines the diversity of assessment methods used across the different chemical engineering subjects and how these map against assessment frameworks reported in the wider pedagogical literature. The emerging strand of research on synoptic and interdisciplinary assessment is used to develop an assessment framework for producing chemical engineering graduates who are also socially responsible and competent global citizens.}
}
@incollection{PROFILLIDIS2019383,
title = {Chapter 9 - Fuzzy Methods},
editor = {V.A. Profillidis and G.N. Botzoris},
booktitle = {Modeling of Transport Demand},
publisher = {Elsevier},
pages = {383-417},
year = {2019},
isbn = {978-0-12-811513-8},
doi = {https://doi.org/10.1016/B978-0-12-811513-8.00009-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128115138000091},
author = {V.A. Profillidis and G.N. Botzoris},
keywords = {4-step model, Accidents, Airports, Crisp, Fuzzification, Fuzzy logic, Fuzzy model, Fuzzy regression, Gaussian, Linear programming, Membership degree, Membership function, Objective function, Railways, Traffic, Transport economics, Trapezoidal, Triangular},
abstract = {This chapter deals with applications of fuzzy methods, which give the ability to study quantitatively problems characterized by ambiguity, imprecision, uncertainty, linguistic variables, and missing or few or no data. The fuzzy method introduces another way of thinking: a statement, instead of being true or false, may be partially true or false. Thus, instead of taking into account the typically used fixed numerical values (such as, e.g., 2.34), the fuzzy method employs a set of plausible values (e.g., around the value 2.34) within a specific domain. Although this approach may look similar to the error of statistical methods, the fuzzy method can tackle situations (such as missing or vague data), for which classic methods are inefficient. The principles of fuzzy numbers, fuzzy sets, and fuzzy logic are presented. The case of symmetric triangular fuzzy numbers is analyzed in detail. Next, linear regression analysis with the use of fuzzy numbers is explained. A detailed application of fuzzy linear regression for a transport demand problem is surveyed analytically. The chapter includes many applications of fuzzy linear regression for the forecast of a variety of transport demand problems: air transport, rail transport, road transport, transport at urban level, and transport economics. Applications of the fuzzy method to other transport problems are explained: route choice, road safety, accident analysis, logistics and routing of freight vehicles, and the optimization of capacity of airports.}
}
@article{CHENG20115100,
title = {Equilibrium Conditions In Service Supply Chain},
journal = {Procedia Engineering},
volume = {15},
pages = {5100-5104},
year = {2011},
note = {CEIS 2011},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2011.08.946},
url = {https://www.sciencedirect.com/science/article/pii/S1877705811024477},
author = {Fei Cheng and Shanlin Yang and Xijun Ma},
keywords = {service supply chain, service volume, equilibrium},
abstract = {Service supply chain features human players as service vendor, service integrator, customer and service resource. It tends to be digitally connected, such as consulting, e-business and integrated enterprises. Our study uses a formal model and simulations to develop the effect of a service supply chain on equilibrium computation. Two insights arise on how a network can obtain equilibrium computation: forming the network structure of service supply chain; exploring entities behavior and equilibrium conditions. These results highlight the importance for service supply chain of adapting its network structure to equilibrium and application.}
}
@article{LI2021151508,
title = {Reverse vaccinology approach for the identifications of potential vaccine candidates against Salmonella},
journal = {International Journal of Medical Microbiology},
volume = {311},
number = {5},
pages = {151508},
year = {2021},
issn = {1438-4221},
doi = {https://doi.org/10.1016/j.ijmm.2021.151508},
url = {https://www.sciencedirect.com/science/article/pii/S1438422121000370},
author = {Jie Li and Jingxuan Qiu and Zhiqiang Huang and Tao Liu and Jing Pan and Qi Zhang and Qing Liu},
keywords = {, Reverse vaccinology, Computational model, Vaccine target, Immunoprotective},
abstract = {Salmonella is a leading cause of foodborne pathogen which causes intestinal and systemic diseases across the world. Vaccination is the most effective protection against Salmonella, but the identification and design of an effective broad-spectrum vaccine is still a great challenge, because of the multi-serotypes of Salmonella. Reverse vaccinology is a new tool to discovery and design vaccine antigens combining human immunology, structural biology and computational biology with microbial genomics. In this study, reverse vaccinology, an in-silico approach was established to screen appropriate immunogen targets by calculating the immunogenicity score of 583 non-redundant outer membrane and secreted proteins of Salmonella. Herein among 100 proteins identified with top-ranked scores, 15 representative antigens were selected randomly. Applying the sequence conservation test, four proteins (FliK, BcsZ, FhuA and FepA) remained as potential vaccine candidates for in vivo evaluation of immunogenicity and immunoprotection. All four candidates were capable to trigger the immune response and stimulate the production of antiserum in mice. Furthermore, top-ranked proteins including FliK and BcsZ provided wide antigenic coverage among the multi-serotype of Salmonella. The S. Typhimurium LT2 challenge model used in mice immunized with FliK and BcsZ showed a high relative percentage survival (RPS) of 52.74 % and 64.71 % respectively. In conclusion, this study constructed an in-silico pipeline able to successfully pre-screen the vaccine targets characterized by high immunogenicity and protective immunity. We show that reverse vaccinology allowed screening of appropriate broad-spectrum vaccines for Salmonella.}
}
@article{LIU2023115384,
title = {Trajectory planning for unmanned surface vehicles in multi-ship encounter situations},
journal = {Ocean Engineering},
volume = {285},
pages = {115384},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.115384},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823017687},
author = {Jianjian Liu and Huizi Chen and Shaorong Xie and Yan Peng and Dan Zhang and Huayan Pu},
keywords = {Tordsdrajectory planning, Collision avoidance, Velocity obstacle, Multiship encounters, COLREGS},
abstract = {Unmanned surface vehicles (USVs) can encounter traffic ships while navigating toward the target location. For the USVs, collision avoidance (CA) trajectories need to be planned according to the international regulations for preventing collisions at sea (COLREGS). A novel trajectory planning approach is proposed for the collision-free trajectories planning of USVs in the case of multiship encounters. Unlike the existing trajectory planning approaches, the proposed approach uses the holistic thinking to simplify the analysis of encounter situations. Ships approaching from all sides of the USV are treated as one or two equivalent obstacles based on consistent offset velocity direction (COVD) method. Furthermore, planned velocity is designed using the proposed CA strategy and kinematic constraints. This strategy is compliant with COLREGS and includes an emergency CA module to further ensure a safe distance between the USV and traffic ships. The performance of the proposed trajectory planning approach is verified through physical simulations using an existing simulator. The simulation results show that the proposed trajectory planning approach can implement multiple USVs to simultaneously avoid collisions and reach their respective target positions. Moreover, the approach remains effective when other USVs do not follow the COLREGS protocols.}
}
@article{DAVIES2023100692,
title = {Idea generation and knowledge creation through maker practices in an artifact-mediated collaborative invention project},
journal = {Learning, Culture and Social Interaction},
volume = {39},
pages = {100692},
year = {2023},
issn = {2210-6561},
doi = {https://doi.org/10.1016/j.lcsi.2023.100692},
url = {https://www.sciencedirect.com/science/article/pii/S2210656123000089},
author = {Sini Davies and Pirita Seitamaa-Hakkarainen and Kai Hakkarainen},
keywords = {Constructionism, Design practices, Engineering practices, Epistemic object, Epistemic practices, Knowledge creation, Learning by making, Material mediation},
abstract = {This investigation involved carrying out interventions that engaged teams of lower-secondary (13–14-year-old) Finnish students in using traditional and digital fabrication technologies to make materially embodied collaborative inventions. By relying on video data and ethnographic observations of the student teams' collaborative invention processes, the investigation focused on investigating 1) how the teams generated and developed their design ideas in their materially anchored making process and 2) what kinds of maker practices they relied on during open-ended invention projects. The study focused on a microanalytic study of three teams of students, and we utilized and developed visual data analysis methods. Our findings reveal the complex nature of the student teams' materially contextualized ideation and the knowledge creation activities that took place within their projects. The findings suggest that open-ended, materially mediated co-invention projects offer ample opportunities for creative cultural participation and practice-based knowledge creation in schools.}
}
@article{NAKAMURA2021198,
title = {Explanation of emotion regulation mechanism of mindfulness using a brain function model},
journal = {Neural Networks},
volume = {138},
pages = {198-214},
year = {2021},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2021.01.029},
url = {https://www.sciencedirect.com/science/article/pii/S089360802100037X},
author = {Haruka Nakamura and Yoshimasa Tawatsuji and Siyuan Fang and Tatsunori Matsui},
keywords = {Emotion regulation in mindfulness, Mechanism, Brain function model, Top-down, Bottom-up},
abstract = {The emotion regulation mechanism of mindfulness plays an important role in the stress reduction effect. Many researchers in the fields of cognitive psychology and cognitive neuroscience have attempted to elucidate this mechanism by documenting the cognitive processes that occur and the neural activities that characterize each process. However, previous findings have not revealed the mechanism of information propagation in the brain that achieves emotion regulation during mindfulness. In this study, we constructed a functional brain model based on its anatomical network structure and a computational model representing the propagation of information between brain regions. We then examined the effects of mindfulness meditation on information propagation in the brain using simulations of changes in the activity of each region. These simulations of changes represent the degree of processing resource allocation to the neural activity via changes in the weights of each region’s output. As a result of the simulations, we reveal how the neural activity characteristic of emotion regulation in mindfulness, which has been reported in previous studies, is realized in the brain. Mindfulness meditation increases the weight of the output from each region of the thalamus and sensory cortex, which processes sensory stimuli from the external world. This sensory information activates the insula and anterior cingulate cortex (ACC). The orbitofrontal cortex and dorsolateral prefrontal cortex inhibit amygdala activity (i.e., top-down emotion regulation). However, when mindfulness meditation dominates bottom-up processing via sensory stimuli from the external world, amygdala activity increases through the insula and ACC activation.}
}
@article{RECANATINI20208653,
title = {Drug Research Meets Network Science: Where Are We?},
journal = {Journal of Medicinal Chemistry},
volume = {63},
number = {16},
pages = {8653-8666},
year = {2020},
issn = {1520-4804},
doi = {https://doi.org/10.1021/acs.jmedchem.9b01989},
url = {https://www.sciencedirect.com/science/article/pii/S1520480420001702},
author = {Maurizio Recanatini and Chiara Cabrelle},
abstract = {Network theory provides one of the most potent analysis tools for the study of complex systems. In this paper, we illustrate the network-based perspective in drug research and how it is coherent with the new paradigm of drug discovery. We first present data sources from which networks are built, then show some examples of how the networks can be used to investigate drug-related systems. A section is devoted to network-based inference applications, i.e., prediction methods based on interactomes, that can be used to identify putative drug–target interactions without resorting to 3D modeling. Finally, we present some aspects of Boolean networks dynamics, anticipating that it might become a very potent modeling framework to develop in silico screening protocols able to simulate phenotypic screening experiments. We conclude that network applications integrated with machine learning and 3D modeling methods will become an indispensable tool for computational drug discovery in the next years.
}
}
@article{LI2018122,
title = {Uncertainty, politics, and technology: Expert perceptions on energy transitions in the United Kingdom},
journal = {Energy Research & Social Science},
volume = {37},
pages = {122-132},
year = {2018},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2017.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S2214629617303304},
author = {Francis G.N. Li and Steve Pye},
keywords = {Climate policy, Energy policy, Uncertainty analysis, Decision-making},
abstract = {Energy policy is beset by deep uncertainties, owing to the scale of future transitions, the long-term timescales for action, and numerous stakeholders. This paper provides insights from semi-structured interviews with 31 UK experts from government, industry, academia, and civil society. Participants were asked for their views on the major uncertainties surrounding the ability of the UK to meet its 2050 climate targets. The research reveals a range of views on the most critical uncertainties, how they can be mitigated, and how the research community can develop approaches to better support strategic decision-making. The study finds that the socio-political dimensions of uncertainty are discussed by experts almost as frequently as technological ones, but that there exist divergent perspectives on the role of government in the transition and whether or not there is a requirement for increased societal engagement. Finally, the study finds that decision-makers require a new approach to uncertainty assessment that overcomes analytical limits to existing practice, is more flexible and adaptable, and which better integrates qualitative narratives with quantitative analysis. Policy design must escape from ‘caged’ thinking concerning what can or cannot be included in models, and therefore what types of uncertainties can or cannot be explored.}
}
@article{FURNHAM2025102637,
title = {Personality and the education process: Individual difference preferences for teacher, technology, testing, time and topic},
journal = {Learning and Individual Differences},
volume = {119},
pages = {102637},
year = {2025},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2025.102637},
url = {https://www.sciencedirect.com/science/article/pii/S1041608025000135},
author = {Adrian Furnham},
keywords = {Preferences, Time of teaching, Teacher, Technology, Testing method},
abstract = {The present paper looks at the relationship between well-established personality traits and five different features of the educational process. Specifically, I explore the relationship between pupil Extraversion, Neuroticism, Openness, Agreeableness and Conscientiousness and personal preferences for Teacher (who the instructor is), Technology (the mode of instruction used), Testing (how the learning is evaluated), Time (the pace, length and time-of-day of the instruction period), and Topic (what is taught/discipline). There is a scattered literature on these topics which is briefly reviewed with a particular interest in how they relate to personality trait correlates. Evidence suggests the importance of understanding the role personality trait preferences in various educational choices and outcomes.}
}
@article{WILKINS2007635,
title = {Inexpensive fusion methods for enhancing feature detection},
journal = {Signal Processing: Image Communication},
volume = {22},
number = {7},
pages = {635-650},
year = {2007},
note = {"Special Issue on Content-Based Multimedia Indexing and Retrieval"},
issn = {0923-5965},
doi = {https://doi.org/10.1016/j.image.2007.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0923596507000732},
author = {Peter Wilkins and Tomasz Adamek and Noel E. O’Connor and Alan F. Smeaton},
keywords = {Feature detection, Data fusion, TRECVID},
abstract = {Recent successful approaches to high-level feature detection in image and video data have treated the problem as a pattern classification task. These typically leverage the techniques learned from statistical machine learning, coupled with ensemble architectures that create multiple feature detection models. Once created, co-occurrence between learned features can be captured to further boost performance. At multiple stages throughout these frameworks, various pieces of evidence can be fused together in order to boost performance. These approaches whilst very successful are computationally expensive, and depending on the task, require the use of significant computational resources. In this paper we propose two fusion methods that aim to combine the output of an initial basic statistical machine learning approach with a lower-quality information source, in order to gain diversity in the classified results whilst requiring only modest computing resources. Our approaches, validated experimentally on TRECVid data, are designed to be complementary to existing frameworks and can be regarded as possible replacements for the more computationally expensive combination strategies used elsewhere.}
}
@article{MOTA2023985,
title = {Speech as a Graph: Developmental Perspectives on the Organization of Spoken Language},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {8},
number = {10},
pages = {985-993},
year = {2023},
note = {Natural Language Processing in Psychiatry and Clinical Neuroscience Research},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2023.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S2451902223000988},
author = {Natália Bezerra Mota and Janaina Weissheimer and Ingrid Finger and Marina Ribeiro and Bárbara Malcorra and Lilian Hübner},
keywords = {Clinical high risk, Cognitive development, Computational psychiatry, Dementia, Diagnosis, Psychosis},
abstract = {Language has been used as a privileged window to investigate mental processes. More recently, descriptions of psychopathological symptoms have been analyzed with the help of natural language processing tools. An example is the study of speech organization using graph theoretical approaches that began approximately 10 years ago. After its application in different areas, there is a need to better characterize what aspects can be associated with typical and atypical behavior throughout the lifespan, given the variables related to aging as well as biological and social contexts. The precise quantification of mental processes assessed through language may allow us to disentangle biological/social markers by looking at naturalistic protocols in different contexts. In this review, we discuss 10 years of studies in which word recurrence graphs were adopted to characterize the chain of thoughts expressed by individuals while producing discourse. Initially developed to understand formal thought disorder in the context of psychotic syndromes, this line of research has been expanded to understand the atypical development in different stages of psychosis and differential diagnosis (such as dementia) as well as the typical development of thought organization in school-age children/teenagers in naturalistic and school-based protocols. We comment on the effects of environmental factors, such as education and reading habits (in monolingual and bilingual contexts), in clinical and nonclinical populations at different developmental stages (from childhood to older adulthood, considering aging effects on cognition). Looking toward the future, there is an opportunity to use word recurrence graphs to address complex questions that consider biological/social factors within a developmental perspective in typical and atypical contexts.}
}
@article{LIANG20242457,
title = {Adaptive Video Dual Domain Watermarking Scheme Based on PHT Moment and Optimized Spread Transform Dither Modulation},
journal = {Computers, Materials and Continua},
volume = {81},
number = {2},
pages = {2457-2492},
year = {2024},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2024.056438},
url = {https://www.sciencedirect.com/science/article/pii/S1546221824008051},
author = {Yucheng Liang and Ke Niu and Yingnan Zhang and Yifei Meng and Fangmeng Hu},
keywords = {Dual-domain, H.264, group of pictures, polar harmonic transform, spread transform dither modulation},
abstract = {To address the challenges of video copyright protection and ensure the perfect recovery of original video, we propose a dual-domain watermarking scheme for digital video, inspired by Robust Reversible Watermarking (RRW) technology used in digital images. Our approach introduces a parameter optimization strategy that incrementally adjusts scheme parameters through attack simulation fitting, allowing for adaptive tuning of experimental parameters. In this scheme, the low-frequency Polar Harmonic Transform (PHT) moment is utilized as the embedding domain for robust watermarking, enhancing stability against simulation attacks while implementing the parameter optimization strategy. Through extensive attack simulations across various digital videos, we identify the optimal low-frequency PHT moment using adaptive normalization. Subsequently, the embedding parameters for robust watermarking are adaptively adjusted to maximize robustness. To address computational efficiency and practical requirements, the unnormalized high-frequency PHT moment is selected as the embedding domain for reversible watermarking. We optimize the traditional single-stage extended transform dithering modulation (STDM) to facilitate multi-stage embedding in the dual-domain watermarking process. In practice, the video embedded with a robust watermark serves as the candidate video. This candidate video undergoes simulation according to the parameter optimization strategy to balance robustness and embedding capacity, with adaptive determination of embedding strength. The reversible watermarking is formed by combining errors and other information, utilizing recursive coding technology to ensure reversibility without attacks. Comprehensive analyses of multiple performance indicators demonstrate that our scheme exhibits strong robustness against Common Signal Processing (CSP) and Geometric Deformation (GD) attacks, outperforming other advanced video watermarking algorithms under similar conditions of invisibility, reversibility, and embedding capacity. This underscores the effectiveness and feasibility of our attack simulation fitting strategy.}
}
@article{SCHUH20181,
title = {Exact satisfiability of linear CNF formulas},
journal = {Discrete Applied Mathematics},
volume = {251},
pages = {1-4},
year = {2018},
issn = {0166-218X},
doi = {https://doi.org/10.1016/j.dam.2018.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S0166218X18302762},
author = {Bernd R. Schuh},
keywords = {Complexity, XSAT, Exact linear formula, l-regularity, k-uniformity, NP-completeness},
abstract = {Open questions with respect to the computational complexity of linear CNF (LCNF) formulas are addressed. Focus lies on exact linear CNF formulas (XLCNF), in which any two clauses have exactly one variable in common. It is shown that l-regularity, i.e. each variable occurs exactly l times in the formula, imposes severe restrictions on the structure of XLCNF formulas. In particular it is proven that l-regularity in XLCNF implies k-uniformity, i.e. all clauses have the same number k of literals. Allowed k- values obey k (k−1)=0 (mod l), and the number of clauses m is given by m =kl-(k−1). Then the computational complexity of monotone l-regular XLCNF formulas with respect to exact satisfiability (XSAT) is determined. XSAT turns out to be either trivial, if m is not a multiple of l, or it can be decided in sub-exponential time, namely O(nn).}
}
@article{MASSO2025100112,
title = {Research ethics committees as knowledge gatekeepers: The impact of emerging technologies on social science research},
journal = {Journal of Responsible Technology},
volume = {21},
pages = {100112},
year = {2025},
issn = {2666-6596},
doi = {https://doi.org/10.1016/j.jrt.2025.100112},
url = {https://www.sciencedirect.com/science/article/pii/S2666659625000083},
author = {Anu Masso and Jevgenia Gerassimenko and Tayfun Kasapoglu and Mai Beilmann},
keywords = {Research ethics, Ethics committees, Social sciences, Research methods, Data, Algorithms, Artificial intelligence},
abstract = {This article investigates the evolution of research ethics within the social sciences, emphasising the shift from procedural norms borrowed from medical and natural sciences to social scientific discipline-specific and method-based principles. This transformation acknowledges the unique challenges and opportunities in social science research, particularly in the context of emerging data technologies such as digital data, algorithms, and artificial intelligence. Our empirical analysis, based on a survey conducted among international social scientists (N = 214), highlights the precariousness researchers face regarding these technological shifts. Traditional methods remain prevalent, despite the recognition of new digital methodologies that necessitate new ethical principles. We discuss the role of ethics committees as influential gatekeepers, examining power dynamics and access to knowledge within the research landscape. The findings underscore the need for tailored ethical guidelines that accommodate diverse methodological approaches, advocate for interdisciplinary dialogue, and address inequalities in knowledge production. This article contributes to the broader understanding of evolving research ethics in an increasingly data-driven world.}
}
@article{RUAN2022103133,
title = {Closed-form Minkowski sums of convex bodies with smooth positively curved boundaries},
journal = {Computer-Aided Design},
volume = {143},
pages = {103133},
year = {2022},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2021.103133},
url = {https://www.sciencedirect.com/science/article/pii/S0010448521001445},
author = {Sipu Ruan and Gregory S. Chirikjian},
keywords = {Minkowski sums, Computer-aided design, Computational geometry},
abstract = {This article derives closed-form parametric formulas for the Minkowski sums of convex bodies in d-dimensional Euclidean space with boundaries that are smooth and have all positive sectional curvatures at every point. Under these conditions, there is a unique relationship between the position of each boundary point and the surface normal. The main results are presented as two theorems. The first theorem directly parameterizes Minkowski sum boundaries using the unit normal vector at each surface point. Although simple to express mathematically, such a parameterization is not always practical to obtain computationally. Therefore, the second theorem derives a more useful parametric closed-form expression using the gradient that is not normalized. In the special case of two ellipsoids, the proposed expressions are identical to those derived previously using geometric interpretations. In order to examine the results, numerical validations and comparisons of the Minkowski sums between two superquadric bodies are conducted. Applications to generate configuration space obstacles in motion planning problems and to improve optimization-based collision detection algorithms are introduced and demonstrated.}
}
@incollection{PIGGOTT2022176,
title = {8.10 - Optimization of Marine Renewable Energy Systems},
editor = {Trevor M. Letcher},
booktitle = {Comprehensive Renewable Energy (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {176-220},
year = {2022},
isbn = {978-0-12-819734-9},
doi = {https://doi.org/10.1016/B978-0-12-819727-1.00179-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128197271001795},
author = {Matthew D. Piggott and Stephan C. Kramer and Simon W. Funke and David M. Culley and Athanasios Angeloudis},
keywords = {Tidal stream, Tidal range, Optimization, Modelling},
abstract = {Optimizing marine renewable energy systems to maximize performance is key to their success. However, a range of physical, environmental, engineering, economic as well as computational challenges means that this is not straightforward. This article considers this topic, focusing on those systems whose performance is coupled to the hydrodynamics providing the resource; tidal power represents a clear example of this. In such cases system design must be optimal in relation to the resource׳s magnitude as well as its spatial and temporal variation, which are all dependent on the system׳s configuration and operation and so cannot be assumed to be known at the design stage. Designing based on the ambient resource could lead to under-performance. Coupling between the design and the resource has implications for the complexity of the optimization problem and potential hydrodynamical and environmental impacts. This coupling distinguishes many marine energy systems from other renewables which do not impact in any significant manner on the resource. The optimal design of marine energy systems thus represents a challenging and somewhat unique problem. However, feedback also opens up a number of possibilities where the resource can be ‘controlled’, to maximize the cumulative power obtained from multiple devices or plants, or to achieve some other complementary goal. Design optimization is thus critical, with many issues to consider. Due to the complexity of the problem a computational based solution is a necessity in all but the simplest scenarios. However, the coupled feedback requires that an iterative solution approach be used, which combined while the vast range of spatial and temporal scales means that methodological compromises need to be made. These compromises need to be understood, with the correct computational tool used at the appropriate point in the design process. This article reviews these challenges as well as the progress that has been made in addressing them.}
}
@article{DELIGKAS2022103784,
title = {Two's company, three's a crowd: Consensus-halving for a constant number of agents},
journal = {Artificial Intelligence},
volume = {313},
pages = {103784},
year = {2022},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2022.103784},
url = {https://www.sciencedirect.com/science/article/pii/S0004370222001242},
author = {Argyrios Deligkas and Aris Filos-Ratsikas and Alexandros Hollender},
keywords = {Consensus-halving, Fair division, Computational complexity, Query complexity, Robertson-Webb},
abstract = {We consider the ε-Consensus-Halving problem, in which a set of heterogeneous agents aim at dividing a continuous resource into two (not necessarily contiguous) portions that all of them simultaneously consider to be of approximately the same value (up to ε). This problem was recently shown to be PPA-complete, for n agents and n cuts, even for very simple valuation functions. In a quest to understand the root of the complexity of the problem, we consider the setting where there is only a constant number of agents, and we consider both the computational complexity and the query complexity of the problem. For agents with monotone valuation functions, we show a dichotomy: for two agents the problem is polynomial-time solvable, whereas for three or more agents it becomes PPA-complete. Similarly, we show that for two monotone agents the problem can be solved with polynomially-many queries, whereas for three or more agents, we provide exponential query complexity lower bounds. These results are enabled via an interesting connection to a monotone Borsuk-Ulam problem, which may be of independent interest. For agents with general valuations, we show that the problem is PPA-complete and admits exponential query complexity lower bounds, even for two agents.}
}
@article{KAR2023102661,
title = {Guest Editorial: Big data-driven theory building: Philosophies, guiding principles, and common traps},
journal = {International Journal of Information Management},
volume = {71},
pages = {102661},
year = {2023},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2023.102661},
url = {https://www.sciencedirect.com/science/article/pii/S0268401223000427},
author = {Arpan Kumar Kar and Spyros Angelopoulos and H. Raghav Rao},
keywords = {Big data, Information systems, Artificial intelligence, Machine learning, Theory building, Computational social science},
abstract = {While data availability and access used to be a major challenge for information systems research, the growth and ease of access to large datasets and data analysis tools has increased interest to use such resources for publishing. Such publications, however, seem to offer weak theoretical contributions. While big data-driven studies increasingly gain popularity, they rarely introspect why a phenomenon is better explained by a theory and limit the analysis to data descriptive by mining and visualizing large volumes of big data. We address this pressing need and provide directions to move towards theory building with Big Data. We differentiate based on inductive and deductive approaches and provide guidelines how may undertake steps for theory building. In doing so, we further provide directions surrounding common pitfalls that should be avoided in this journey of Big-Data driven theory building.}
}
@article{CHARLES20243693,
title = {Weaving innovative fabrics of knowledge between institutionalized sciences and Indigenous ways of knowing},
journal = {Matter},
volume = {7},
number = {11},
pages = {3693-3698},
year = {2024},
issn = {2590-2385},
doi = {https://doi.org/10.1016/j.matt.2024.10.013},
url = {https://www.sciencedirect.com/science/article/pii/S259023852400540X},
author = {Michael Charles},
abstract = {In the rapid chase to address humanity’s grand challenges, we must embrace multiple knowledge systems, including Indigenous ways of knowing, to fuel innovation, translate science into practice, and invite institutional sciences to evolve in an increasingly globalized world.}
}
@article{HEUNG2025100361,
title = {How ChatGPT impacts student engagement from a systematic review and meta-analysis study},
journal = {Computers and Education: Artificial Intelligence},
volume = {8},
pages = {100361},
year = {2025},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2025.100361},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X25000013},
author = {Yuk Mui Elly Heung and Thomas K.F. Chiu},
keywords = {Generative artificial intelligence, Student engagement, ChatGPT, Systematic review, Meta-analysis},
abstract = {Generative artificial intelligence, such as ChatGPT, has been increasingly integrated into education to change student learning experience. Current empirical studies have mixed results on how ChatGPT impacts student behavioral, cognitive, and emotional engagement. This systematic literature review and meta-analysis explores whether and how ChatGPT impacts student behavioral, cognitive, and emotional engagement. We used the PRISMA method to select, analyze, and report the results. We screened 766 articles from four databases and identified 17 empirical studies with 1735 students for analysis. We compared the effect on student engagement between ChatGPT-based and non-ChatGPT learning. We found a medium effect size on overall student engagement in ChatGPT-based learning in the random effects model. Our analyses further suggest that ChatGPT-based learning is more effective in fostering student behavioral (medium effective size), cognitive (large effective size), and emotional engagement (medium effective size) than non-ChatGPT learning. Our findings revealed ChatGPT is an effective tool for engaging students in learning. We also suggested three roles ChatGPT plays in fostering student engagement: personalized tutoring, programming and technical assistance, and content generation and collaboration. Our systematic literature review revealed potential risks and results in student disengagement, such as over-reliance.}
}
@article{ZHUANG2024e29830,
title = {Artificial multi-verse optimisation for predicting the effect of ideological and political theory course},
journal = {Heliyon},
volume = {10},
number = {9},
pages = {e29830},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e29830},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024058614},
author = {Xingzhong Zhuang and Zhaodi Yi and Yuqing Wang and Yi Chen and Sudan Yu},
keywords = {Teaching sufficiency, Artificial multi-verse optimizer, Classification, Art ideological and political theory course},
abstract = {Enhancing teaching sufficiency is crucial because low teaching efficiency has always been a widespread issue in ideological and political theory course. Evaluating data on the course is obtained from a freshmen class of 2022 using questionnaires. The data is organised and condensed for mining and analysis. Subsequently, an intelligent artificial multi-verse optimizer (AMVO) method s developed to predict the effect of ideological and political theory course. The proposed AMVO approach was tested against various cutting-edge algorithms to demonstrate its effectiveness and stability on the benchmark functions. The experimental results indicated that AMVO ranked first among the 23 test functions. Furthermore, the binary AMVO enhanced k-nearest neighbour classifier had excellent performance in the art ideological and political theory course in terms of error rate, accuracy, specificity and sensitivity. This model can predict the overall evaluation attitude of freshmen towards the course based on the dataset. In addition, we can further analyse the potential correlations between factors that enhance the intellectual and political content of the course. This model can further refine the evaluation of ideological and political courses by teachers and students in our school, thereby achieving the fundamental goal of moral cultivation.}
}
@article{TEMIZER2011114,
title = {Thermomechanical contact homogenization with random rough surfaces and microscopic contact resistance},
journal = {Tribology International},
volume = {44},
number = {2},
pages = {114-124},
year = {2011},
issn = {0301-679X},
doi = {https://doi.org/10.1016/j.triboint.2010.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0301679X10002318},
author = {İ. Temizer},
keywords = {Contact mechanics, Homogenization, Thermal contact resistance, Randomness},
abstract = {We extend an earlier computational thermomechanical contact homogenization framework [Temizer İ, Wriggers P. International Journal for Numerical Methods in Engineering 2010; 83:27–58] to random rough surfaces generated through the random-field model based on the concepts of ensemble averaging and sample enlargement towards the effective limit. Additionally, the homogenization theory is revisited in order to incorporate thermal dissipation at the microscopic contact interface within a thermodynamically consistent approach that preserves dissipation across the scales. Large-scale three-dimensional computations were performed to demonstrate the effectiveness and feasibility of the computational framework for an accurate characterization of the macroscopic thermomechanical response of rough surfaces in contact.}
}
@article{ANDERSON1998159,
title = {Mental retardation general intelligence and modularity},
journal = {Learning and Individual Differences},
volume = {10},
number = {3},
pages = {159-178},
year = {1998},
issn = {1041-6080},
doi = {https://doi.org/10.1016/S1041-6080(99)80128-9},
url = {https://www.sciencedirect.com/science/article/pii/S1041608099801289},
author = {Mike Anderson},
abstract = {This article presents a case for distinguishing between mental retardation as a general deficit of thinking and mental retardation that might result from the global effects of a specific deficit in a cognitive module. Using Anderson's (1992a) theory of the minimal cognitive architecture of intelligence and developmental, I show how this distinction can explain the pattern of intellectual strengths and weaknesses in Savant syndrome, Williams syndrome, Down syndrome, and autism. In addition, I discuss the developmental versus difference view and the distinction between organic and cultural familial mental retardation in the light of this theory. I conclude that not only is there no inherent incompatibility between the constructs of general intelligence and modularity of mind but that both are essential to understanding the different patterns of abilities and developmental profiles found in individuals with low IQ.}
}
@article{PEZZULO2014647,
title = {Internally generated sequences in learning and executing goal-directed behavior},
journal = {Trends in Cognitive Sciences},
volume = {18},
number = {12},
pages = {647-657},
year = {2014},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2014.06.011},
url = {https://www.sciencedirect.com/science/article/pii/S1364661314001570},
author = {Giovanni Pezzulo and Matthijs A.A. {van der Meer} and Carien S. Lansink and Cyriel M.A. Pennartz},
keywords = {forward sweep, generative models, hippocampus, decision making, reinforcement learning, spatial navigation, replay, inference, prospection, theta rhythm, ventral striatum},
abstract = {A network of brain structures including hippocampus (HC), prefrontal cortex, and striatum controls goal-directed behavior and decision making. However, the neural mechanisms underlying these functions are unknown. Here, we review the role of ‘internally generated sequences’: structured, multi-neuron firing patterns in the network that are not confined to signaling the current state or location of an agent, but are generated on the basis of internal brain dynamics. Neurophysiological studies suggest that such sequences fulfill functions in memory consolidation, augmentation of representations, internal simulation, and recombination of acquired information. Using computational modeling, we propose that internally generated sequences may be productively considered a component of goal-directed decision systems, implementing a sampling-based inference engine that optimizes goal acquisition at multiple timescales of on-line choice, action control, and learning.}
}
@article{OSPINAAGUDELO2021121224,
title = {Application domain extension of incremental capacity-based battery SoH indicators},
journal = {Energy},
volume = {234},
pages = {121224},
year = {2021},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2021.121224},
url = {https://www.sciencedirect.com/science/article/pii/S0360544221014729},
author = {Brian {Ospina Agudelo} and Walter Zamboni and Eric Monmasson},
keywords = {Battery, State of health, Battery ageing, Capacity degradation, Incremental capacity, Randomised usage pattern},
abstract = {The Incremental Capacity (IC) analysis is used to characterise the capacity and the battery state of health, aged by cycling patterns with randomly selected pulsed current levels and duration. The batteries are periodically characterised at 1C current, which is a high value with respect to the typical IC tests in pseudo-equilibrium condition. The high-current IC curves generation from raw voltage/current data includes two filtering stages, one for the input voltage and one for the incremental capacity curve smoothing, which are optimised for the application on the basis of the data characteristics. The correlations between the IC main peak features and the battery full capacity for 28 Lithium–Cobalt oxide batteries with 18650 packaging were evaluated, finding that the main peak area is a general feature to evaluate the state of health under high current tests and random usage pattern, and, therefore, it can be used as a battery health indicator in practical applications. The effects of the computational parameters on the relationship between the peak area and the battery capacity are also investigated. The results are confirmed by a further analysis performed over an additional set of cells with different technology, aged with a fixed cycling pattern. Additionally, the performance of the peak area as a health indicator was compared with an ohmic resistance-based estimation approach.}
}
@article{SAAVEDRA20091324,
title = {Experimental transition state for the Corey–Bakshi–Shibata reduction},
journal = {Tetrahedron Letters},
volume = {50},
number = {12},
pages = {1324-1327},
year = {2009},
issn = {0040-4039},
doi = {https://doi.org/10.1016/j.tetlet.2009.01.033},
url = {https://www.sciencedirect.com/science/article/pii/S0040403909000793},
author = {Jaime Saavedra and Sean E. Stafford and Matthew P. Meyer},
abstract = {Asymmetric reductions of prochiral ketones are important transformations in the syntheses of natural products, pharmaceuticals, and fine chemicals. The Corey–Bakshi–Shibata reduction is unique among hydride transfer reductions in its tremendous substrate range and catalytic nature. Here, a coordinated computational and experimental approach is taken toward understanding the origins of the high selectivity and broad substrate range, which are hallmarks of this reduction.}
}
@article{MULLER2021103546,
title = {Kandinsky Patterns},
journal = {Artificial Intelligence},
volume = {300},
pages = {103546},
year = {2021},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103546},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221000977},
author = {Heimo Müller and Andreas Holzinger},
keywords = {Explainable AI, Explainability, Synthetic test data, Ground truth},
abstract = {Kandinsky Figures and Kandinsky Patterns are mathematically describable, simple, self-contained hence controllable synthetic test data sets for the development, validation and training of visual tasks and explainability in artificial intelligence (AI). Whilst Kandinsky Patterns have these computationally manageable properties, they are at the same time easily distinguishable by human observers. Consequently, controlled patterns can be described by both humans and computers. We define a Kandinsky Pattern as a set of Kandinsky Figures, where for each figure an “infallible authority” defines that the figure belongs to the Kandinsky Pattern. With this simple principle we build training and validation data sets for testing explainability, interpretability and context learning. In this paper we describe the basic idea and some underlying principles of Kandinsky Patterns. We provide a Github repository and invite the international AI research community to a challenge to experiment with our Kandinsky Patterns. The goal is to help expand and advance the field of AI, and in particular to contribute to the increasingly important field of explainable AI.}
}
@article{WU2024111235,
title = {Intelligent strategic bidding in competitive electricity markets using multi-agent simulation and deep reinforcement learning},
journal = {Applied Soft Computing},
volume = {152},
pages = {111235},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111235},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624000097},
author = {Jiahui Wu and Jidong Wang and Xiangyu Kong},
keywords = {Intelligent bidding strategy, Competitive electricity markets, Multi-agent simulation(MAS), Deep reinforcement learning(DRL), Async n-step QL, Improved Async n-step QL},
abstract = {Aiming at the lack of comprehension of agents in Multi-Agent Simulation (MAS) based on classic Reinforcement Learning algorithms of competitive electricity markets, an intelligent strategic bidding method using Deep Reinforcement Learning (DRL) and MAS is proposed in this paper, which not only can provide more intelligent strategies for market participants to maximize their profits, but can enhance the performance of simulation models dealing with high-dimensional continuous data in electricity markets. Firstly, a theoretical framework of intelligent strategic bidding in competitive electricity markets based on MAS and DRL is proposed, and the process of intelligent bidding in electricity markets based on MAS and DRL is described. Then, three MAS models of intelligent strategic bidding are built based on three classic DRL algorithms, including Deep Q-Network (DQN), Double Deep Q-Network (DDQN), and Asynchronous n-step Q-learning (Async n-step QL), and three algorithms’ convergence speed, computational efficiency, and response sensitivity are compared and analyzed. Finally, a novel Improved Async n-step QL (IAsync n-step QL) algorithm is proposed, the MAS model based on the IAsync n-step QL algorithm for intelligent strategic bidding is established. Simulation results show that the model using the novel DRL algorithm is more profitable and responsive than the classic DRL algorithms.}
}
@article{STEANE2003469,
title = {A quantum computer only needs one universe},
journal = {Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics},
volume = {34},
number = {3},
pages = {469-478},
year = {2003},
note = {Quantum Information and Computation},
issn = {1355-2198},
doi = {https://doi.org/10.1016/S1355-2198(03)00038-8},
url = {https://www.sciencedirect.com/science/article/pii/S1355219803000388},
author = {A.M Steane},
keywords = {Quantum computation, Classical computation, Parallel universes, Entanglement},
abstract = {The nature of quantum computation is discussed. It is argued that, in terms of the amount of information manipulated in a given time, quantum and classical computation are equally efficient. Quantum superposition does not permit quantum computers to “perform many computations simultaneously” except in a highly qualified and to some extent misleading sense. Quantum computation is therefore not well described by interpretations of quantum mechanics which invoke the concept of vast numbers of parallel universes. Rather, entanglement makes available types of computation processes which, while not exponentially larger than classical ones, are unavailable to classical systems. The essence of quantum computation is that it uses entanglement to generate and manipulate a physical representation of the correlations between logical entities, without the need to completely represent the logical entities themselves.}
}
@article{DAHL20231039,
title = {A Learning Approach for Future Competencies in Manufacturing using a Learning Factory},
journal = {Procedia CIRP},
volume = {118},
pages = {1039-1043},
year = {2023},
note = {16th CIRP Conference on Intelligent Computation in Manufacturing Engineering},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2023.06.178},
url = {https://www.sciencedirect.com/science/article/pii/S2212827123004055},
author = {Håkon Dahl and Nina Tvenge and Carla Susana A Assuad and Kristian Martinsen},
keywords = {Learning factory, Work Related Learning, Industry 4.0, Learning Method, Manufacturing, Future Work Competencies},
abstract = {This paper describes a study on future competence needs in manufacturing and how a learning factory utilising a Connective Model for Didactic Design can be used in teaching and learning of these competencies. The paper briefly reports on a literature study, and a set of interviews in Norwegian manufacturing companies to get a better understanding on the expected future competence needs. This was used to design a learning process with four steps: 1: Exploration, 2: Product and process design, 3: Problem solving and 4: Debriefing. The method was tested in a case study where undergraduate students are learners following the 4-step method. The approach was evaluated through feedback from the learners. The case utilised a Festo CP-Factory learning factory at NTNU.}
}
@incollection{AKAN20253,
title = {Chapter 0 - From the ground up!},
editor = {Aydin Akan and Luis F. Chaparro},
booktitle = {Signals and Systems Using MATLAB ® (Fourth Edition)},
publisher = {Academic Press},
edition = {Fourth Edition},
pages = {3-62},
year = {2025},
isbn = {978-0-443-15709-7},
doi = {https://doi.org/10.1016/B978-0-44-315709-7.00009-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443157097000094},
author = {Aydin Akan and Luis F. Chaparro},
keywords = {Signals and systems, mathematical models, digital signal processing applications, concrete mathematics, complex variables, system dynamics, MATLAB},
abstract = {This chapter provides an overview of the material in the book, briefly illustrates the applications and highlights the mathematical background needed to understand the analysis of signals and systems. A signal is a function of time like a voice signal, or of space like an image, or of time and space like a video. A system then is a mathematical model of a device, just like the ordinary differential equations representing circuits. We illustrate the importance of the theory of signals and systems by means of practical applications, hint to how to implement them, and connect concepts in Calculus with more concrete mathematics from a computational point of view—using computers. A review of complex variables and their connection with the dynamics of systems is given. We end the chapter with a soft introduction to MATLAB®, a widely used high-level computational tool for analysis and design.}
}
@article{PROCTOR2021142852,
title = {Gateway to the perspectives of the Food-Energy-Water nexus},
journal = {Science of The Total Environment},
volume = {764},
pages = {142852},
year = {2021},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2020.142852},
url = {https://www.sciencedirect.com/science/article/pii/S0048969720363828},
author = {Kyle Proctor and Seyed M.H. Tabatabaie and Ganti S. Murthy},
keywords = {Food-energy–water nexus, Water security, Life cycle assessment, Problem archetype, Resource governance, Systems thinking},
abstract = {The Food-Energy–Water (FEW) nexus has been promoted as a tool for improving food, energy, and water resource security via an interdisciplinary approach that acknowledges the inherent synergies and tradeoffs involved in managing these resources. Over the past decade discussion of the nexus has increased rapidly, along with research funding and output. However, because the nexus encompasses so many different disciplines, researchers engage with and study the nexus from differing perspectives with distinct motivations and analytical methodologies. Understanding these motivations is critical to understanding the value of a given work. This paper first uses a narrative review to identify the motivations and toolsets of five key perspectives used to view the nexus, including: ecosystem health, waste management, public and private institutional change, stakeholder trust, and the learning process. Then, a systematic review is conducted to examine how publication trends have changed over the past decade, both generally and for each of these perspectives. The Food-Energy-Water nexus is not the first systems-based approach for addressing resource management and critiques of the nexus as a “Buzzword” or simply a reinvention of previous systems are growing in the literature. Challenging authors to explicitly define the role and motivations of their research within the broader category of the FEW nexus can improve the actionability of the research, better allow researchers to build from each other's work, and help reduce the ambiguity surrounding the nexus.}
}
@article{MAMAT201311,
title = {MAR: Maximum Attribute Relative of soft set for clustering attribute selection},
journal = {Knowledge-Based Systems},
volume = {52},
pages = {11-20},
year = {2013},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2013.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0950705113001706},
author = {Rabiei Mamat and Tutut Herawan and Mustafa Mat Deris},
keywords = {Data mining, Soft set theory, Clustering attributes, Attribute relative, Complexity},
abstract = {Clustering, which is a set of categorical data into a homogenous class, is a fundamental operation in data mining. One of the techniques of data clustering was performed by introducing a clustering attribute. A number of algorithms have been proposed to address the problem of clustering attribute selection. However, the performance of these algorithms is still an issue due to high computational complexity. This paper proposes a new algorithm called Maximum Attribute Relative (MAR) for clustering attribute selection. It is based on a soft set theory by introducing the concept of the attribute relative in information systems. Based on the experiment on fourteen UCI datasets and a supplier dataset, the proposed algorithm achieved a lower computational time than the three rough set-based algorithms, i.e. TR, MMR, and MDA up to 62%, 64%, and 40% respectively and compared to a soft set-based algorithm, i.e. NSS up to 33%. Furthermore, MAR has a good scalability, i.e. the executing time of the algorithm tends to increase linearly as the number of instances and attributes are increased respectively.}
}
@incollection{MARCHAND201831,
title = {Chapter 2 - Analogical Mapping in Numerical Development},
editor = {Daniel B. Berch and David C. Geary and Kathleen {Mann Koepke}},
booktitle = {Language and Culture in Mathematical Cognition},
publisher = {Academic Press},
pages = {31-47},
year = {2018},
series = {Mathematical Cognition and Learning},
isbn = {978-0-12-812574-8},
doi = {https://doi.org/10.1016/B978-0-12-812574-8.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012812574800002X},
author = {Elisabeth Marchand and David Barner},
keywords = {Analogy, Number acquisition, Successor function, Numerical estimation, Structure mapping},
abstract = {This chapter outlines the contribution of analogical thinking in numerical cognition and specifically to number-word learning and numerical estimation. We begin with an overview of number-word learning, followed by a description of analogical mapping as defined by Gentner, 1983, Gentner, 2010, and discuss how children might acquire the meaning of counting based on analogical mapping. Next, we review the claim that very similar processes of analogical mapping may support numerical estimation, based on findings from studies of dot-array and number-line estimation. These studies suggest that children's knowledge of how the count list is structured and in particular the ordering and distance between numbers affects their ability to make accurate estimates. Finally, we discuss extensions of this idea to other cases where analogy has been proposed as a source of representational change. We conclude that analogical mappings enrich how humans transcend core numerical abilities to represent abstract content.}
}
@article{LISANA2025100896,
title = {Playing to learn: Game-based approach to financial literacy for generation Z},
journal = {Entertainment Computing},
volume = {52},
pages = {100896},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100896},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124002647},
author = {Lisana Lisana and Hendra Dinata and Gabriela {Valencia Tanudjaja}},
keywords = {Gamification, Digital learning, Simulation, Evaluation},
abstract = {This quantitative study assessed the effectiveness of game-based learning in improving financial literacy among Generation Z. Conducted with 32 urban participants, the study involved the use of a board game and a mobile application, designed with input from financial literacy experts. Participants underwent a pretest to gauge their initial financial knowledge, engaged with the game, and completed a posttest to measure learning outcomes. Statistical analysis, including paired sample t-tests, compared pretest and posttest scores, revealing a significant enhancement in financial literacy post-gameplay. Furthermore, a questionnaire evaluated user satisfaction regarding the game, assessing metrics like enjoyment, ease of use, and perceived usefulness. Results demonstrated not only an improvement in financial knowledge but also high satisfaction among users, indicating that game-based learning can be a valuable tool for teaching financial concepts to Generation Z. These findings contribute to the understanding of game-based learning’s potential in financial education and provide insights for educators and parents. However, the study’s limitations suggest areas for future research to explore and refine the use of educational games in financial literacy.}
}
@article{LIN2024122254,
title = {Reinforcement learning and bandits for speech and language processing: Tutorial, review and outlook},
journal = {Expert Systems with Applications},
volume = {238},
pages = {122254},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122254},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423027562},
author = {Baihan Lin},
keywords = {Reinforcement learning, Bandits, Speech processing, Natural language processing, Speech recognition, Large language models, Survey, Perspective},
abstract = {In recent years, reinforcement learning and bandits have transformed a wide range of real-world applications including healthcare, finance, recommendation systems, robotics, and last but not least, the speech and natural language processing. While most speech and language applications of reinforcement learning algorithms are centered around improving the training of deep neural networks with its flexible optimization properties, there are still many grounds to explore to utilize the benefits of reinforcement learning, such as its reward-driven adaptability, state representations, temporal structures and generalizability. In this survey, we present an overview of recent advancements of reinforcement learning and bandits including those in the large language models, and discuss how they can be effectively employed to solve speech and natural language processing problems with models that are adaptive, interactive and scalable.}
}
@article{WU2025109151,
title = {Examining the role and neural electrophysiological mechanisms of adjective cues in size judgment},
journal = {Neuropsychologia},
volume = {213},
pages = {109151},
year = {2025},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2025.109151},
url = {https://www.sciencedirect.com/science/article/pii/S0028393225000867},
author = {Yihan Wu and Ronglian Zheng and Huili Xing and Yining Kou and Yufeng Wang and Xin Wu and Feng Zou and Yanyan Luo and Meng Zhang},
keywords = {Size judgement, Language, ERP, EEG microstate},
abstract = {Numerous influential theories have attempted to elucidate the relationship between language and thought. The debate persists on whether language and thought are distinct entities or if language is deeply embedded in individual cognitive processes. This study employs adjective cues combined with a mental imagery size judgment task as an experimental paradigm, utilizing neurophysiological techniques to preliminarily explore the role of adjectives in size judgment tasks and their underlying neurophysiological mechanisms. Findings reveal that performance is best when adjectives are congruent with the size of the object, with EEG microstate results indicating strong activity in Class A, related to language networks under this condition. Additionally, when adjectives conflict with object size, the discovery of the Ni component suggests that individuals monitor and inhibit the conflict between adjectives and object size, leading to decreased task performance in this condition. Moreover, when object size is ambiguous, individuals' size judgments do not benefit significantly from clear adjective cues. Event-related potentials and EEG microstate results suggest that under this condition, top-down cognitive resources are recruited more extensively. In conclusion, language plays a more crucial role in simpler judgment tasks; as tasks become more complex, judgment processes engage a greater number of distributed brain regions to collaborate, while the language system remains active. This study provides initial cognitive neuroscience evidence for understanding the relationship between language and simple forms of thought, offering preliminary insights for future investigations into the connection between language and thought.}
}
@article{YU2022158,
title = {Bioinspired interactive neuromorphic devices},
journal = {Materials Today},
volume = {60},
pages = {158-182},
year = {2022},
issn = {1369-7021},
doi = {https://doi.org/10.1016/j.mattod.2022.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S1369702122002413},
author = {Jinran Yu and Yifei Wang and Shanshan Qin and Guoyun Gao and Chong Xu and Zhong {Lin Wang} and Qijun Sun},
keywords = {Neuromorphic devices, Synaptic transistors, Interactive, Neuromorphic computing, Bioinspired},
abstract = {The performance of conventional computer based on von Neumann architecture is limited due to the physical separation of memory and processor. By synergistically integrating various sensors with synaptic devices, recently emerging interactive neuromorphic devices can directly sense/store/process various stimuli information from external environments and implement functions of perception, learning, memory, and computation. In this review, we present the basic model of bioinspired interactive neuromorphic devices and discuss the performance metrics. Next, we summarize the recent progress and development of bioinspired interactive neuromorphic devices, which are classified into neuromorphic tactile systems, visual systems, auditory systems, and multisensory system. They are discussed in detail from the aspects of materials, device architectures, operating mechanisms, synaptic plasticity, and potential applications. Additionally, the bioinspired interactive neuromorphic devices that can fuse multiple/mixed sensing signals are proposed to address more realistic and sophisticated problems. Finally, we discuss the pros and cons regarding to the computing neurons and integrating sensory neurons and deliver the perspectives on interactive neuromorphic devices at the material, device, network, and system levels. It is believed the neuromorphic devices can provide promising solutions to next generation of interactive sensation/memory/computation toward the development of multimodal, low-power, and large-scale intelligent systems endowed with neuromorphic features.}
}
@article{KWIATKOWSKA201335,
title = {Fuzzy logic and semiotic methods in modeling of medical concepts},
journal = {Fuzzy Sets and Systems},
volume = {214},
pages = {35-50},
year = {2013},
note = {Soft Computing in the Humanities and Social Sciences},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2012.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S0165011412001376},
author = {Mila Kwiatkowska and Krzysztof Kielan},
keywords = {Fuzzy system models, Medicine, Cognitive sciences, Decision support systems, Depression},
abstract = {The field of medicine is a quickly growing area of application for computer-based systems. However, the use of computerized methods in this knowledge-intensive and expert-based discipline brings multiple challenges. The major problem is the modeling, representing, and interpreting of diverse medical concepts. For example, some symptoms and their etiologies are described in terms of molecular biology and genetics, physiological processes are defined using models from chemistry and physics; yet mental disorders are defined in more subjective terms of feelings, behaviours, habits, and life events. Thus, the representation of medical concepts must be sufficiently expressive to model concepts which are inherently complex, context-dependent, evolving, and often imprecise. Furthermore, the representation must be formal or, at least, sufficiently rigorous in order to be processed by computers and at the same time, the representation must be human-readable in order to be validated by humans. In this paper, we describe the modeling process of medical concepts as a mapping from the real-world medical concepts into their computational models, and further into their physical implementation. First, we define the notion of a concept as a fundamental unit of knowledge and specify the fundamental principles of the computational representation of a concept. Second, we describe the characteristics of medical concepts, specifically their historical and cultural changeability, their social and cultural ambiguity, and their varied levels of precision. Third, we present a meta-modeling framework for computational representation of medical concepts. Our framework is based on fuzzy logic and semiotic methods which allow us to explicitly model two important characteristics of medical concepts: imprecision and context-dependency. We present the framework using an example of a mental disorder, specifically, the concept of clinical depression. To exemplify the changeable and evolutionary character of medical concepts, we discuss the development of the diagnostic criteria for depression. Finally, we use the example of the assessment of depression to describe the computational representation for polythetic and multi-dimensional concepts and for categorical and non-categorical concepts. We demonstrate how the proposed modeling framework utilizes (1) a fuzzy-logic approach to represent the non-categorical (continuous) nature of the symptoms and (2) a semiotic approach to represent the polythetic (contextual interpretation) and dimensional nature of the symptoms.}
}
@article{SALOMATIN2021582,
title = {Web user identification based on browser fingerprints using machine learning methods},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {13},
pages = {582-587},
year = {2021},
note = {20th IFAC Conference on Technology, Culture, and International Stability TECIS 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.10.512},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321019492},
author = {Alexander A. Salomatin and Andrey Y. Iskhakov and Anastasia O. Iskhakova},
keywords = {browser fingerprint, cybersecurity, identification, digital footprint, machine learning, web server},
abstract = {The article developed a method for identifying users on the network based on browser fingerprints using machine learning methods. The resulting method is a modification of the user identification method based on a digital footprint, which can be more efficient due to two components. First, the selection of attributes for a digital footprint is made from a limited set of attributes to form a user browser fingerprint. Secondly, the identification accuracy can be increased through the combined use of classification methods and the probabilistic-statistical approach. To check the successful operation of the method, a computational experiment is carried out on real data, which consists in solving the problem of classifying a user based on his browser fingerprint using the K nearest neighbors method.}
}
@article{MARTIN20102089,
title = {Integrating learning theories and application-based modules in teaching linear algebra},
journal = {Linear Algebra and its Applications},
volume = {432},
number = {8},
pages = {2089-2099},
year = {2010},
note = {Special issue devoted to the 15th ILAS Conference at Cancun, Mexico, June 16-20, 2008},
issn = {0024-3795},
doi = {https://doi.org/10.1016/j.laa.2009.08.030},
url = {https://www.sciencedirect.com/science/article/pii/S0024379509004704},
author = {William Martin and Sergio Loch and Laurel Cooley and Scott Dexter and Draga Vidakovic},
keywords = {Linear algebra, Learning theory, Curriculum, Pedagogy, Constructivist theories, APOS – Action-Process-Object-Schema, Theoretical framework, Encapsulated process, Thematicized schema, Triad – intra, Inter, Trans, Genetic decomposition, Vector addition, Matrix, Matrix multiplication, Matrix representation, Basis, Column space, Row space, Null space, Eigenspace, Transformation},
abstract = {The research team of The Linear Algebra Project developed and implemented a curriculum and a pedagogy for parallel courses in (a) linear algebra and (b) learning theory as applied to the study of mathematics with an emphasis on linear algebra. The purpose of the ongoing research, partially funded by the National Science Foundation, is to investigate how the parallel study of learning theories and advanced mathematics influences the development of thinking of individuals in both domains. The researchers found that the particular synergy afforded by the parallel study of math and learning theory promoted, in some students, a rich understanding of both domains and that had a mutually reinforcing effect. Furthermore, there is evidence that the deeper insights will contribute to more effective instruction by those who become high school math teachers and, consequently, better learning by their students. The courses developed were appropriate for mathematics majors, pre-service secondary mathematics teachers, and practicing mathematics teachers. The learning seminar focused most heavily on constructivist theories, although it also examined socio-cultural and historical perspectives. A particular theory, Action–Process–Object–Schema (APOS) [10], was emphasized and examined through the lens of studying linear algebra. APOS has been used in a variety of studies focusing on student understanding of undergraduate mathematics. The linear algebra courses include the standard set of undergraduate topics. This paper reports the results of the learning theory seminar and its effects on students who were simultaneously enrolled in linear algebra and students who had previously completed linear algebra and outlines how prior research has influenced the future direction of the project.}
}
@article{FIELDS2022104714,
title = {Neurons as hierarchies of quantum reference frames},
journal = {Biosystems},
volume = {219},
pages = {104714},
year = {2022},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2022.104714},
url = {https://www.sciencedirect.com/science/article/pii/S0303264722000983},
author = {Chris Fields and James F. Glazebrook and Michael Levin},
keywords = {Activity-dependent remodeling, Bayesian inference, Bioelectricity, Computation, Learning, Memory},
abstract = {Conceptual and mathematical models of neurons have lagged behind empirical understanding for decades. Here we extend previous work in modeling biological systems with fully scale-independent quantum information-theoretic tools to develop a uniform, scalable representation of synapses, dendritic and axonal processes, neurons, and local networks of neurons. In this representation, hierarchies of quantum reference frames act as hierarchical active-inference systems. The resulting model enables specific predictions of correlations between synaptic activity, dendritic remodeling, and trophic reward. We summarize how the model may be generalized to nonneural cells and tissues in developmental and regenerative contexts.}
}
@article{KOHLER2016212,
title = {On GPU acceleration of common solvers for (quasi-) triangular generalized Lyapunov equations},
journal = {Parallel Computing},
volume = {57},
pages = {212-221},
year = {2016},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2016.05.010},
url = {https://www.sciencedirect.com/science/article/pii/S0167819116300436},
author = {Martin Köhler and Jens Saak},
keywords = {Lyapunov equations, BLAS level-3, Accelerator device},
abstract = {The solutions of Lyapunov and generalized Lyapunov equations are a key player in many applications in systems and control theory. Their stable numerical computation, when the full solution is sought, is considered solved since the seminal work of Bartels and Stewart [R. H. Bartels, G. W. Stewart, Solution of the matrix equation AX+XB=C: Algorithm 432, Comm. ACM 15 (1972) 820–826.]. A number of variants of their algorithm have been proposed, but none of them goes beyond BLAS level-2 style implementation. On modern computers, however, the formulation of BLAS level-3 type implementations is crucial to enable optimal usage of cache hierarchies and modern block scheduling methods based on directed acyclic graphs describing the interdependence of single block computations. In this contribution, we present the port of our recent BLAS level-3 algorithm [M. Köhler, J. Saak, On BLAS Level-3 implementations of common solvers for (quasi-) triangular generalized Lyapunov equations, SLICOT Working Note 2014-1, NICONET e.V., available from www.slicot.org (Sep. 2014).] to a GPU accelerator device.}
}
@article{KYRIAZOS2024105070,
title = {Quantum concepts in Psychology: Exploring the interplay of physics and the human psyche},
journal = {BioSystems},
volume = {235},
pages = {105070},
year = {2024},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2023.105070},
url = {https://www.sciencedirect.com/science/article/pii/S0303264723002459},
author = {Theodoros Kyriazos and Mary Poga},
keywords = {Quantum mechanics, Quantum psychology, Interdisciplinary, Human psyche},
abstract = {This paper delves into the innovative intersection of quantum mechanics and psychology, examining the potential of quantum principles to provide fresh insights into human emotions, cognition, and consciousness. Drawing parallels between quantum phenomena such as superposition, entanglement, tunneling, decoherence and their psychological counterparts, we present a quantum-psychological model that reimagines emotional states, cognitive breakthroughs, interpersonal relationships, and the nature of consciousness. The study uses computational models and simulations to explore this interdisciplinary fusion's implications and applications, highlighting its potential benefits and inherent challenges. While quantum concepts offer a rich metaphorical lens to view the intricacies of human experience, it is essential to approach this nascent framework with enthusiasm and skepticism. Rigorous empirical validation is paramount to realize its full potential in research and therapeutic contexts. This exploration stands as a promising thread in the tapestry of intellectual history, suggesting a deeper understanding of the human psyche through the lens of quantum mechanics.}
}
@article{LIN2023103217,
title = {A novel personality detection method based on high-dimensional psycholinguistic features and improved distributed Gray Wolf Optimizer for feature selection},
journal = {Information Processing & Management},
volume = {60},
number = {2},
pages = {103217},
year = {2023},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2022.103217},
url = {https://www.sciencedirect.com/science/article/pii/S0306457322003181},
author = {Hao Lin and Chundong Wang and Qingbo Hao},
keywords = {Personality detection, Feature selection, Symmetric uncertainty, Grey Wolf Optimizer, Spark},
abstract = {Existing personality detection methods based on user-generated text have two major limitations. First, they rely too much on pre-trained language models to ignore the sentiment information in psycholinguistic features. Secondly, they have no consensus on the psycholinguistic feature selection, resulting in the insufficient analysis of sentiment information. To tackle these issues, we propose a novel personality detection method based on high-dimensional psycholinguistic features and improved distributed Gray Wolf Optimizer (GWO) for feature selection (IDGWOFS). Specifically, we introduced the Gaussian Chaos Map-based initialization and neighbor search strategy into the original GWO to improve the performance of feature selection. To eliminate the bias generated when using mutual information to select features, we adopt symmetric uncertainty (SU) instead of mutual information as the evaluation for correlation and redundancy to construct the fitness function, which can balance the correlation between features–labels and the redundancy between features–features. Finally, we improve the common Spark-based parallelization design of GWO by parallelizing only the fitness computation steps to improve the efficiency of IDGWOFS. The experiments indicate that our proposed method obtains average accuracy improvements of 3.81% and 2.19%, and average F1 improvements of 5.17% and 5.8% on Essays and Kaggle MBTI dataset, respectively. Furthermore, IDGWOFS has good convergence and scalability.}
}
@article{LAWSON2013284,
title = {Sensory connection, interest/attention and gamma synchrony in autism or autism, brain connections and preoccupation},
journal = {Medical Hypotheses},
volume = {80},
number = {3},
pages = {284-288},
year = {2013},
issn = {0306-9877},
doi = {https://doi.org/10.1016/j.mehy.2012.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0306987712005415},
author = {Wendy Lawson},
abstract = {Does motivational interest increase gamma synchrony across neuronal networking to enable computation of related sensory inputs that might lead to greater social understanding in autism spectrum conditions (ASC)? Meaning, is it possible/likely that in autism because individuals process one aspect of sensory input at any one time (therefore missing the wider picture in general) when they are motivated/interested or attending to particular stimuli their attention window is widened due to increased gamma synchrony and they might be enabled to connect in ways that do not occur when they are not motivated? This is my current research question. If gamma synchrony is helping with the binding of information from collective sensory inputs, in ASC, when and only if the individual is motivated, then this has huge potential for how learning might be encouraged for individuals with an ASC.}
}
@article{ALDULAIMY2024101272,
title = {The computing continuum: From IoT to the cloud},
journal = {Internet of Things},
volume = {27},
pages = {101272},
year = {2024},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2024.101272},
url = {https://www.sciencedirect.com/science/article/pii/S2542660524002130},
author = {Auday Al-Dulaimy and Matthijs Jansen and Bjarne Johansson and Animesh Trivedi and Alexandru Iosup and Mohammad Ashjaei and Antonino Galletta and Dragi Kimovski and Radu Prodan and Konstantinos Tserpes and George Kousiouris and Chris Giannakos and Ivona Brandic and Nawfal Ali and André B. Bondi and Alessandro V. Papadopoulos},
keywords = {Computing continuum, Cloud computing, Fog computing, Edge computing, Mobile cloud computing, Multi-access edge computing, SDN, NFV, IoT, Use case, Reference architecture},
abstract = {In the era of the IoT revolution, applications are becoming ever more sophisticated and accompanied by diverse functional and non-functional requirements, including those related to computing resources and performance levels. Such requirements make the development and implementation of these applications complex and challenging. Computing models, such as cloud computing, can provide applications with on-demand computation and storage resources to meet their needs. Although cloud computing is a great enabler for IoT and endpoint devices, its limitations make it unsuitable to fulfill all design goals of novel applications and use cases. Instead of only relying on cloud computing, leveraging and integrating resources at different layers (like IoT, edge, and cloud) is necessary to form and utilize a computing continuum. The layers’ integration in the computing continuum offers a wide range of innovative services, but it introduces new challenges (e.g., monitoring performance and ensuring security) that need to be investigated. A better grasp and more profound understanding of the computing continuum can guide researchers and developers in tackling and overcoming such challenges. Thus, this paper provides a comprehensive and unified view of the computing continuum. The paper discusses computing models in general with a focus on cloud computing, the computing models that emerged beyond the cloud, and the communication technologies that enable computing in the continuum. In addition, two novel reference architectures are presented in this work: one for edge–cloud computing models and the other for edge–cloud communication technologies. We demonstrate real use cases from different application domains (like industry and science) to validate the proposed reference architectures, and we show how these use cases map onto the reference architectures. Finally, the paper highlights key points that express the authors’ vision about efficiently enabling and utilizing the computing continuum in the future.}
}
@article{LI2022111937,
title = {Detection method of timber defects based on target detection algorithm},
journal = {Measurement},
volume = {203},
pages = {111937},
year = {2022},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2022.111937},
url = {https://www.sciencedirect.com/science/article/pii/S0263224122011332},
author = {Dongjie Li and Zilei Zhang and Baogang Wang and Chunmei Yang and Liwei Deng},
keywords = {Wood defect detection, YOLOX, Target detection, Feature fusion},
abstract = {Deep learning has achieved certain results in the field of wood surface defect detection. To address the problems of low accuracy of the detection results of surface defects on boards, slow detection speed and large number of model parameters, this article take advantage of computer vision to improve the feature fusion module of YOLOX target detection algorithm, by adding efficient channel attention (ECA) mechanism, adaptive spatial feature fusion mechanism (ASFF) and improve the confidence loss and localization loss functions as Focal loss and Efficient Intersection over Union (EIoU) loss, to enhance the feature extraction ability and detection accuracy of the algorithm. Considering the depth and width of the model, the depth-separable convolution and optional multi-version algorithm are used to reduce the model parameters and computational effort to seek the optimal model. Experiments show that the improved model detects four types of defects in rubber timber with a considerable improvement and has significant advantages over other target detection algorithms.}
}
@article{PINEDA2024101204,
title = {The mode of computing},
journal = {Cognitive Systems Research},
volume = {84},
pages = {101204},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.101204},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723001389},
author = {Luis A. Pineda},
keywords = {Mode of computing, Natural computing, Representation, Interpretation, Consciousness},
abstract = {The Turing Machine is the paradigmatic case of computing machines, but there are others such as analogical, connectionist, quantum and diverse forms of unconventional computing, each based on a particular intuition of the phenomenon of computing. This variety can be captured in terms of system levels, re-interpreting and generalizing Newell’s hierarchy, which includes the knowledge level at the top and the symbol level immediately below it. In this re-interpretation the knowledge level consists of human knowledge and the symbol level is generalized into a new level that here is called The Mode of Computing. Mental processes performed by natural brains are often thought of informally as computing processes and that the brain is alike to computing machinery. However, if natural computing does exist it should be characterized on its own. A proposal to such an effect is that natural computing appeared when interpretations were first made by biological entities, so natural computing and interpreting are two aspects of the same phenomenon, or that consciousness and experience are the manifestations of computing/interpreting. By analogy with computing machinery, there must be a system level at the top of the neural circuitry and directly below the knowledge level that is named here The mode of Natural Computing. If it turns out that such putative object does not exist the proposition that the mind is a computing process should be dropped; but characterizing it would come with solving the hard problem of consciousness.}
}
@incollection{YACKINOUS2015193,
title = {Chapter 11 - Cellular Automata Investigations and Emerging Complex System Principles},
editor = {William S. Yackinous},
booktitle = {Understanding Complex Ecosystem Dynamics},
publisher = {Academic Press},
address = {Boston},
pages = {193-212},
year = {2015},
isbn = {978-0-12-802031-9},
doi = {https://doi.org/10.1016/B978-0-12-802031-9.00011-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128020319000115},
author = {William S. Yackinous},
keywords = {Cellular automata, Cellular automata investigations, Explicit experimentation, Cellular automata classes, Simple programs/simple rules, Complex system principles, Computational view of systems, Computational universality, Principle of Computational Equivalence},
abstract = {This chapter is primarily about Stephen Wolfram's innovative cellular automata investigations and his associated ideas on emerging complex system principles. The chapter begins with some cellular automata history and background, and then provides a description of Wolfram's cellular automata “explicit experimentation” work. The experimentation work shows that simple programs with simple rules, repeated over and over, can yield highly complex behavior. Wolfram has identified four classes of cellular automata. Those classes and their characteristics are discussed. The correspondence between cellular automata classes and the attractors of nonlinear dynamics theory is also discussed. Another of Wolfram's important insights is that the behavior of cellular automata is indicative of the behavior of systems in general. That idea is addressed in some detail. The latter part of the chapter addresses Wolfram's computational view of systems. The topics covered include computation as a framework for system principles, the concept of computational universality, and the identification of computationally universal cellular automata. Wolfram's Principle of Computational Equivalence is then described and discussed. The chapter concludes with a summary of my perspectives on the emerging complex system principles.}
}
@incollection{FRANTZ202025,
title = {3 - The “Big 3.” Simon, Katona, Leibenstein},
editor = {Roger Frantz},
booktitle = {The Beginnings of Behavioral Economics},
publisher = {Academic Press},
pages = {25-45},
year = {2020},
series = {Perspectivs in Behavioral Economics and the Economics of Beh},
isbn = {978-0-12-815289-8},
doi = {https://doi.org/10.1016/B978-0-12-815289-8.00003-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128152898000034},
author = {Roger Frantz},
keywords = {ECONS and HUMANS, Behavioral macroeconomics, Intervening variables, Gestalt psychology, Tit-for-tat, Parable of the ant, Bounded rationality, Satisficing, X-efficiency, Non-allocative efficiency, Das John Maynard Keynes rationality problem},
abstract = {Katona, Leibenstein, and Simon are the “Big 3” of the old behavioral economics. Why are they the Big 3? Their names are most often mentioned by others in terms of “early” behavioral economics. They wrote convincingly about homo economicus, and in doing so they began knocking him off his pedestal. Without this behavioral economicus would never exist. With respect to the Big 3’s writings, Leibenstein wrote about, among other things, multiple-selves, gift exchange, social norms, consumer interdependence, non-allocative efficiency, and less than perfect rationality. Katona wrote about, among other things, ECONS vs HUMANS, expectations, aspirations, adaptive behavior, macro-behavioral theory, procedural rationality, and less than perfect rationality. Among other things, Herbert Simon wrote about bounded rationality, intuition (System 1) and logical thinking (System 2), ECONS vs HUMANS, satisficing, rejection of as if theorizing, learning theories in economics and psychology, rationality in economics and psychology, the nature of human knowledge (tacit knowledge), and less than perfect rationality.}
}
@article{GRAYSON2022108844,
title = {R Markdown as a dynamic interface for teaching: Modules from math and biology classrooms},
journal = {Mathematical Biosciences},
volume = {349},
pages = {108844},
year = {2022},
issn = {0025-5564},
doi = {https://doi.org/10.1016/j.mbs.2022.108844},
url = {https://www.sciencedirect.com/science/article/pii/S0025556422000499},
author = {Kristine L. Grayson and Angela K. Hilliker and Joanna R. Wares},
keywords = {R markdown, Data visualization, Pedagogy, Herd immunity, Teaching programming},
abstract = {Advancing technologies, including interactive tools, are changing classroom pedagogy across academia. Here, we discuss the R Markdown interface, which allows for the creation of partial or complete interactive classroom modules for courses using the R programming language. R Markdown files mix sections of R code with formatted text, including LaTeX, which are rendered together to form complete reports and documents. These features allow instructors to create classroom modules that guide students through concepts, while providing areas for coding and text response by students. Students can also learn to create their own reports for more independent assignments. After presenting the features and uses of R Markdown to enhance teaching and learning, we present examples of materials from two courses. In a Computational Modeling course for math students, we used R Markdown to guide students through exploring mathematical models to understand the principle of herd immunity. In a Data Visualization and Communication course for biology students, we used R Markdown for teaching the fundamentals of R programming and graphing, and for students to learn to create reproducible data investigations. Through these examples, we demonstrate the benefits of R Markdown as a dynamic teaching and learning tool.}
}
@article{LI20231485,
title = {A Data Driven Security Correction Method for Power Systems with UPFC},
journal = {Energy Engineering},
volume = {120},
number = {6},
pages = {1485-1502},
year = {2023},
issn = {0199-8595},
doi = {https://doi.org/10.32604/ee.2023.022856},
url = {https://www.sciencedirect.com/science/article/pii/S0199859523000519},
author = {Qun Li and Ningyu Zhang and Jianhua Zhou and Xinyao Zhu and Peng Li},
keywords = {Manuscript, security correction, data-driven, deep neural network (DNN), unified power flow controller (UPFC), overload of transmission lines},
abstract = {The access of unified power flow controllers (UPFC) has changed the structure and operation mode of power grids all across the world, and it has brought severe challenges to the traditional real-time calculation of security correction based on traditional models. Considering the limitation of computational efficiency regarding complex, physical models, a data-driven power system security correction method with UPFC is, in this paper, proposed. Based on the complex mapping relationship between the operation state data and the security correction strategy, a two-stage deep neural network (DNN) learning framework is proposed, which divides the offline training task of security correction into two stages: in the first stage, the stacked auto-encoder (SAE) classification model is established, and the node correction state (0/1) output based on the fault information; in the second stage, the DNN learning model is established, and the correction amount of each action node is obtained based on the action nodes output in the previous stage. In this paper, the UPFC demonstration project of Nanjing West Ring Network is taken as a case study to validate the proposed method. The results show that the proposed method can fully meet the real-time security correction time requirements of power grids, and avoid the inherent defects of the traditional model method without an iterative solution and can also provide reasonable security correction strategies for N-1 and N-2 faults.}
}
@article{BEIGZADEH2025107877,
title = {Mental stress detection and performance enhancement using fNIRS and wrist vibrator biofeedback},
journal = {Biomedical Signal Processing and Control},
volume = {107},
pages = {107877},
year = {2025},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2025.107877},
url = {https://www.sciencedirect.com/science/article/pii/S174680942500388X},
author = {Anita Beigzadeh and Vahid Yazdnian and Seyed Kamaledin Setarehdan},
keywords = {Stress management, Performance enhancement, Machine learning, Learning model for real-time stress classification, Brain signal processing, Biofeedback, Brain–computer interface, Functional near infrared spectroscopy},
abstract = {Daily life activities frequently expose individuals to varying levels of mental stress, which can adversely affect their performance. Therefore, it is crucial to develop effective strategies for stress management and performance improvement. This paper presents a comprehensive, portable, and real-time biofeedback system aimed at improving individuals’ stress management capabilities, ultimately leading to enhanced mental task performance. The system consists of a real-time brain signal acquisition device, a wireless vibration biofeedback unit, and a software-based program for stress level classification. Notably, the system is designed to minimize the time delay by efficiently integrating all components. Various signal processing and feature extraction techniques combined with machine learning have been employed for online stress detection. The experimental results demonstrate an accuracy of 83% and a recall of 92% in detecting true levels of mental stress in the stress classification module. In addition, the complete biofeedback system is tested on 20 participants in a controlled experimental setup, revealing a 55% reduction in stress levels and a 24.5% improvement in task accuracy. These findings support the effectiveness of the proposed system in stress management and performance improvement, validating the core premises of stress reduction and performance improvement through reward-based learning.}
}
@article{MENG2025105534,
title = {Application of a boundary-type algorithm to the inverse problems of convective heat and mass transfer},
journal = {Progress in Nuclear Energy},
volume = {179},
pages = {105534},
year = {2025},
issn = {0149-1970},
doi = {https://doi.org/10.1016/j.pnucene.2024.105534},
url = {https://www.sciencedirect.com/science/article/pii/S0149197024004840},
author = {Xiangyuan Meng and Mei Huang and Jianghao Yang and Xiaoping Ouyang and Boxue Wang and Yanping Huang and Hiroshi Matsuda and Bo Cao},
keywords = {Half boundary method, Inverse problems, Convection-diffusion, Discontinuous coefficient, The Gaussian plume model},
abstract = {The inverse problems of the convection-diffusion equation (ICDE) have received extensive attention in incomplete boundary conditions and uncertain source terms. They can be applied in thermally stratified pipe elbows and so on. Many algorithms need to combine with optimization algorithms to repeatedly calculate the direct problem in the solution process. To solve such problems, this paper employs a boundary-type algorithm named the half-boundary method (HBM). The HBM does not require additional repeated optimization of the direct problem. To test the performance of the method, the numerical simulations of some problems have been carried out, including the inverse problems of heat convection, river pollution and air pollution. The results show that the HBM has the desired accuracy by comparing with the exact solution. If there are errors in the measurement process, the solution doesn't generate a large deviation from the result. It is worth noting that the placement of internal measurement points minimally impacts the numerical results within the solution domain. And the method is also able to handle with discontinuous problems. Because the Gaussian plume model verifies the accuracy of HBM, the HBM can quickly calculate the atmospheric diffusion of the non-Gaussian plume model.}
}
@article{CRAIG2018300,
title = {Metaphors of knowing, doing and being: Capturing experience in teaching and teacher education},
journal = {Teaching and Teacher Education},
volume = {69},
pages = {300-311},
year = {2018},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2017.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X17301841},
author = {Cheryl J. Craig},
keywords = {Metaphors, Teachers' experiences, Narrative inquiry, School reform},
abstract = {In this article, Bateson's idea of human beings thinking with metaphors and learning through stories is examined as it played out within accumulated educational research studies. Five storied metaphors illuminating knowing, doing and being are highlighted from five investigations involving different research teams. In the cross-case analysis, the importance of narrative exemplars emerges, along with the significance of metaphors serving as proxies for teachers' experiences. The plotlines of the metaphors, the morals of the metaphors and the truths of the metaphors are also discussed. In the end result, the value of metaphors in surfacing teachers' embedded, embodied knowledge of experience is affirmed as well as the deftness of the narrative inquiry research method in metaphorically capturing pre-service and inservice teachers' storied experiences.}
}
@article{BOWER2024455,
title = {Model-Based Analysis of Pathway Recruitment During Subthalamic Deep Brain Stimulation},
journal = {Neuromodulation: Technology at the Neural Interface},
volume = {27},
number = {3},
pages = {455-463},
year = {2024},
issn = {1094-7159},
doi = {https://doi.org/10.1016/j.neurom.2023.02.084},
url = {https://www.sciencedirect.com/science/article/pii/S109471592300140X},
author = {Kelsey L. Bower and Angela M. Noecker and Anneke M. Frankemolle-Gilbert and Cameron C. McIntyre},
keywords = {Axons, electrode, Parkinson’s disease, subthalamic nucleus},
abstract = {Background
Subthalamic deep brain stimulation (DBS) is an established clinical therapy, but an anatomically clear definition of the underlying neural target(s) of the stimulation remains elusive. Patient-specific models of DBS are commonly used tools in the search for stimulation targets, and recent iterations of those models are focused on characterizing the brain connections that are activated by DBS.
Objective
The goal of this study was to quantify axonal pathway activation in the subthalamic region from DBS at different electrode locations and stimulation settings.
Materials and Methods
We used an anatomically and electrically detailed computational model of subthalamic DBS to generate recruitment curves for eight different axonal pathways of interest, at three generalized DBS electrode locations in the subthalamic nucleus (STN) (ie, central STN, dorsal STN, posterior STN). These simulations were performed with three levels of DBS electrode localization uncertainty (ie, 0.5 mm, 1.0 mm, 1.5 mm).
Results
The recruitment curves highlight the diversity of pathways that are theoretically activated with subthalamic DBS, in addition to the dependence of the stimulation location and parameter settings on the pathway activation estimates. The three generalized DBS locations exhibited distinct pathway recruitment curve profiles, suggesting that each stimulation location would have a different effect on network activity patterns. We also found that the use of anodic stimuli could help limit activation of the internal capsule relative to other pathways. However, incorporating realistic levels of DBS electrode localization uncertainty in the models substantially limits their predictive capabilities.
Conclusions
Subtle differences in stimulation location and/or parameter settings can impact the collection of pathways that are activated during subthalamic DBS.}
}
@article{ZHU2024111294,
title = {Grey wolf optimizer based deep learning mechanism for music composition with data analysis},
journal = {Applied Soft Computing},
volume = {153},
pages = {111294},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111294},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624000681},
author = {Qian Zhu and Achyut Shankar and Carsten Maple},
keywords = {Music composition, LSTM, GWO, MIDI, Data analysis},
abstract = {Music composition using artificial intelligence has gained increasing research attention recently. However, existing methods often generate music that needs more coherence and authenticity. This paper proposes an evolutionary computation-based deep learning approach for music composition with data analysis. Specifically, we utilize long short-term memory (LSTM) networks for generating melodic sequences and adopt a grey wolf optimizer to optimize LSTM hyperparameters. The training data is first converted to musical instrument digital interface (MIDI) format for data analysis, and melody lines are extracted using a similarity matrix method. The MIDI data is then encoded for input into the LSTM networks. The generated music is evaluated using objective metrics like mean squared error and subjective methods, including surveys of music professionals. Comparisons made to benchmark algorithms like generative adversarial networks demonstrate the advantages of our approach in accurately capturing tone, rhythm, artistic conception, and other attributes of high-quality music. The proposed mechanism provides a practical framework for AI-based music generation while ensuring authenticity.}
}
@article{GIACHETTI2025103229,
title = {Preface for “Selected papers from the 26th Ibero-American Conference on Software Engineering (CIbSE 2023)”},
journal = {Science of Computer Programming},
volume = {243},
pages = {103229},
year = {2025},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2024.103229},
url = {https://www.sciencedirect.com/science/article/pii/S0167642324001527},
author = {Giovanni Giachetti and Breno {de França} and Marcela Genero and Renata Guizzardi}
}
@article{SZYMANSKI201284,
title = {Information retrieval with semantic memory model},
journal = {Cognitive Systems Research},
volume = {14},
number = {1},
pages = {84-100},
year = {2012},
note = {Cognitive Systems Research: Special Issue on Modeling and Application of Cognitive Systems},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2011.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S1389041711000179},
author = {Julian Szymański and Włodzisław Duch},
abstract = {Psycholinguistic theories of semantic memory form the basis of understanding of natural language concepts. These theories are used here as an inspiration for implementing a computational model of semantic memory in the form of semantic network. Combining this network with a vector-based object-relation-feature value representation of concepts that includes also weights for confidence and support, allows for recognition of concepts by referring to their features, enabling a semantic search algorithm. This algorithm has been used for word games, in particular the 20-question game in which the program tries to guess a concept that a human player thinks about. The game facilitates lexical knowledge validation and acquisition through the interaction with humans via supervised dialog templates. The elementary linguistic competencies of the proposed model have been evaluated assessing how well it can represent the meaning of linguistic concepts. To study properties of information retrieval based on this type of semantic representation in contexts derived from on-going dialogs experiments in limited domains have been performed. Several similarity measures have been used to compare the completeness of knowledge retrieved automatically and corrected through active dialogs to a “golden standard”. Comparison of semantic search with human performance has been made in a series of 20-question games. On average results achieved by human players were better than those obtained by semantic search, but not by a wide margin.}
}
@article{LARSON201129,
title = {Interdisciplinary research training in a school of nursing},
journal = {Nursing Outlook},
volume = {59},
number = {1},
pages = {29-36},
year = {2011},
issn = {0029-6554},
doi = {https://doi.org/10.1016/j.outlook.2010.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0029655410004379},
author = {Elaine L. Larson and Bevin Cohen and Kristine Gebbie and Sarah Clock and Lisa Saiman},
abstract = {Although interdisciplinarity has become a favored model of scholarly inquiry, the assumption that interdisciplinary work is intuitive and can be performed without training is short-sighted. This article describes the implementation of an interdisciplinary research training program within a school of nursing. We describe the key elements of the program and the challenges we encountered. From 2007-2010, eleven trainees from 6 disciplines have been accepted into the program and 7 have completed the program; the trainees have published 12 manuscripts and presented at 10 regional or national meetings. The major challenge has been to sustain and “push the envelope” toward interdisciplinary thinking among the trainees and their mentors, and to assure that they do not revert to their “safer” disciplinary silos. This training program, funded by National Institute of Nursing Research (NINR), has become well-established within the school of nursing and across the entire University campus, and is recognized as a high quality research training program across disciplines, as exemplified by excellent applicants from a number of disciplines.}
}
@article{SHIPLEY201948,
title = {Collaboration, cyberinfrastructure, and cognitive science: The role of databases and dataguides in 21st century structural geology},
journal = {Journal of Structural Geology},
volume = {125},
pages = {48-54},
year = {2019},
note = {Back to the future},
issn = {0191-8141},
doi = {https://doi.org/10.1016/j.jsg.2018.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0191814117303164},
author = {Thomas F. Shipley and Basil Tikoff},
keywords = {Spatial cognition, Cyberinfrastructure, Expert training},
abstract = {Structural geologists support their mind with tools, and these tools are increasingly computer based. The advent of Intelligent Systems will allow creation of research teams that combine the strengths of the human mind and computer processing to produce new research results. The efficacy of these approaches will require a solid grounding in cognitive science. Critical to this approach are databases, which are potentially transformative solely in their ability to allow access to data, in a primary form. Emerging more recently, however, is the concept of a dataguide, in which computer-aided analysis informs ongoing decisions about where and what data to collect. The creation of human and computer teams can expand the types of questions that can be addressed in structural geology and tectonics research, but it will take a community-based effort to understand the value of data to experts and how computers might aid an expert in the field.}
}
@article{PITTNAUER2023382,
title = {Observing the creation of new knowledge in the economics laboratory—Do participants discover how to learn from outcome feedback in a dynamic decision problem?},
journal = {Journal of Economic Behavior & Organization},
volume = {215},
pages = {382-405},
year = {2023},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2023.09.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167268123003311},
author = {Sabine Pittnauer and Martin Hohnisch},
keywords = {Learning, Outcome feedback, Discovery, Conjecture, Heuristic simplification, Dynamic decision making},
abstract = {Domain-general learning rules often enable decision makers to learn from outcome feedback which actions tend to achieve a desired goal. However, in novel and complex environments decision makers must explore how to learn, i.e., acquire procedural knowledge of how to elicit and evaluate outcome feedback that will enable them to navigate toward a desired goal despite the vastness of the set of possible policies. Using a dynamic business simulation, this study investigated: (1) whether and how frequently participants discovered an effective procedure to learn from outcome feedback that allowed them to navigate toward a policy that maximizes long-term business profit (and hence their monetary payoff from the experiment), and (2) whether high monetary incentives affected learning procedures and performance. We found that a number of participants discovered an effective learning procedure and succeeded in approximating the optimal policy. In line with the heuristic method, this learning procedure involved a simplification of the search space and the application of domain-general learning rules to this simplified space. Although the decision histories of about half of the participants feature the key aspect of the effective learning procedure—search among the different steady states of the dynamical system—implementation errors prevented many of the participants from realizing the full potential of the learning procedure. We found no evidence to suggest that high monetary incentives affect the effectiveness of learning. Overall, the study illustrates that a “prepared mind” can discover new, effective learning procedures, although their initial implementation may require substantial refinement.}
}
@article{LIU2014330,
title = {Large scale two sample multinomial inferences and its applications in genome-wide association studies},
journal = {International Journal of Approximate Reasoning},
volume = {55},
number = {1, Part 3},
pages = {330-340},
year = {2014},
note = {Theory and applications of belief functions – Belief 2012},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2013.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X13000881},
author = {Chuanhai Liu and Jun Xie},
keywords = {Belief functions, Inference model},
abstract = {Statistical analysis of multinomial counts with a large number K of categories and a small number n of sample size is challenging to both frequentist and Bayesian methods and requires thinking about statistical inference at a very fundamental level. Following the framework of Dempster–Shafer theory of belief functions, a probabilistic inferential model is proposed for this “large K and small n” problem. The inferential model produces a probability triplet (p,q,r) for an assertion conditional on observed data. The probabilities p and q are for and against the truth of the assertion, whereas r=1−p−q is the remaining probability called the probability of “donʼt know”. The new inference method is applied in a genome-wide association study with very high dimensional count data, to identify association between genetic variants to the disease Rheumatoid Arthritis.}
}
@incollection{CHORAFAS200760,
title = {4 - Stress analysis and its tools},
editor = {Dimitris N. Chorafas},
booktitle = {Stress Testing for Risk Control Under Basel II},
publisher = {Butterworth-Heinemann},
address = {Oxford},
pages = {60-79},
year = {2007},
isbn = {978-0-7506-8305-0},
doi = {https://doi.org/10.1016/B978-075068305-0.50005-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780750683050500051},
author = {Dimitris N. Chorafas},
abstract = {Publisher Summary
This chapter explains the need for stress testing to take a scientific approach as an advanced analytical methodology for commendable results. The scientific method of investigation is the only basis for conducting tests and experiments. The chapter examines relatively novel approaches to surveys targeting a qualitative evaluation by experts, such as the Delphi method. The chapter discusses the contributions of the scientific method and financial technology to analytical thinking and testing. The characteristics of a sound methodology are discussed and the fundamentals of stress analysis under normal conditions or under stress are described. The chapter also discusses case studies with scenario analysis and talks about stress evaluation through sensitivity analysis and about the fundamentals of statistical analysis.}
}
@article{LU2017138,
title = {Quasi-generalized least squares regression estimation with spatial data},
journal = {Economics Letters},
volume = {156},
pages = {138-141},
year = {2017},
issn = {0165-1765},
doi = {https://doi.org/10.1016/j.econlet.2017.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0165176517301441},
author = {Cuicui Lu and Jeffrey M. Wooldridge},
keywords = {Quasi-GLS, Spatial correlation, Covariance tapering, Spatial HAC estimator},
abstract = {We use a particular quasi-generalized least squares (QGLS) approach to study a linear regression model with spatially correlated error terms. The QGLS estimator is consistent, asymptotically normal, computationally easier than GLS, and it appears to not lose much efficiency. A variance–covariance estimator for QGLS, which is robust to heteroskedasticity, spatial correlation and general variance–covariance misspecification is provided.}
}
@article{LEGLEITER20131,
title = {Introduction to the special issue: The field tradition in geomorphology},
journal = {Geomorphology},
volume = {200},
pages = {1-8},
year = {2013},
note = {The Field Tradition in Geomorphology 43rd Annual Binghamton Geomorphology Symposium, held 21-23 September 2012 in Jackson, Wyoming USA},
issn = {0169-555X},
doi = {https://doi.org/10.1016/j.geomorph.2013.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0169555X13003140},
author = {Carl J. Legleiter and Richard A. Marston},
keywords = {Binghamton Geomorphology Symposium, Preface, Field work, Jackson Hole, Wyoming},
abstract = {In recognition of the critical role of field observations in the ongoing development of our discipline, the 43rd annual Binghamton Geomorphology Symposium (BGS) celebrated The Field Tradition in Geomorphology. By organizing a conference devoted to this theme, we sought to honor the contributions of pioneering, field-based geomorphologists and to encourage our community to contemplate how field work might continue to provide unique insight into a new, more technologically-driven era. For example, given recent advances in remote sensing methods such as LiDAR, what kind of added value can field work provide? Similarly, how can field-based studies contribute to societally relevant, large-scale questions related to climate change and sustainable management of the Earth system? Motivated by such questions, the 2012 BGS was convened in Jackson Hole, WY, a new, Western location that enabled participation by Rocky Mountain and west coast research groups underrepresented at previous Binghamton symposia. Also, in keeping with the field tradition theme, the 2012 BGS emphasized field trips, including a rafting excursion down the Snake River and an overview of the tectonic and glacial history of Jackson Hole. The on-site portion of the symposium consisted of invited oral and poster presentations and contributed posters, including many by graduate students. Topics ranged from an historical overview of the development of geomorphic thinking to long-term sediment tracer studies to a commentary on the synergy between LiDAR and field mapping. This special issue of Geomorphology consists of papers by invited authors from the 2012 BGS, and this overview provides some context for these contributions. Looking forward, we hope that the 43rd annual BGS will stimulate further discussion of the role of field work as the discipline of geomorphology continues to evolve, carrying on the field tradition into the future.}
}
@article{VALLE2025105242,
title = {Task-value motivational prompts in a descriptive dashboard can increase anxiety among anxious learners},
journal = {Computers & Education},
volume = {229},
pages = {105242},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2025.105242},
url = {https://www.sciencedirect.com/science/article/pii/S0360131525000107},
author = {Natercia Valle and Pavlo Antonenko and Denis Valle and Benjamin Baiser},
keywords = {Data science applications in education, Distance education and online learning, Human-computer interface, Pedagogical issues, Post-secondary education},
abstract = {Despite the ubiquitous use of learning analytics dashboards in computer-mediated learning environments, there is still a knowledge gap on how these tools can support learners’ academic performance and motivation. This article describes an experimental study that investigated the influence of motivational prompts (task-value scaffolding) in a descriptive learning analytics dashboard on learners’ motivation, statistics anxiety, and learning performance in an authentic semester-long online statistics course. The study was based on a two-group experimental design during two semesters (Fall 2020 and Spring 2021). A total of 122 graduate students completed the study. The results showed that despite learners’ mostly positive perceptions of the dashboard, the use of motivational prompts did not influence learners’ cognitive outcomes. Test anxiety was the only affective outcome influenced by the intervention, with motivational prompts having a negative effect on learners who started the course with a higher level of test anxiety. This study provides needed empirical evidence on how the design of these tools can influence learners’ affective outcomes, with implications for theory and practice. However, additional experimental studies that account for sources of heterogeneity (e.g., intrapersonal characteristics, contextual factors) are necessary to uncover theoretical gaps and opportunities in the design of effective learning analytics dashboards.}
}
@article{MOSS2013611,
title = {Senior Academic Physicians and Retirement Considerations},
journal = {Progress in Cardiovascular Diseases},
volume = {55},
number = {6},
pages = {611-615},
year = {2013},
note = {Symposium on Psychosocial Factors in Cardiovascular Disease},
issn = {0033-0620},
doi = {https://doi.org/10.1016/j.pcad.2013.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S003306201300056X},
author = {Arthur J. Moss and Henry Greenberg and Edward M. Dwyer and Helmut Klein and Daniel Ryan and Charles Francis and Frank Marcus and Shirley Eberly and Jesaia Benhorin and Monty Bodenheimer and Mary Brown and Robert Case and John Gillespie and Robert Goldstein and Mark Haigney and Ronald Krone and Edgar Lichstein and Emanuela Locati and David Oakes and Poul Erik Bloch Thomsen and Wojciech Zareba},
keywords = {Academic physicians, Retirement issues, Retirement options},
abstract = {An increasing number of academic senior physicians are approaching their potential retirement in good health with accumulated clinical and research experience that can be a valuable asset to an academic institution. Considering the need to let the next generation ascend to leadership roles, when and how should a medical career be brought to a close? We explore the roles for academic medical faculty as they move into their senior years and approach various retirement options. The individual and institutional considerations require a frank dialogue among the interested parties to optimize the benefits while minimizing the risks for both. In the United States there is no fixed age for retirement as there is in Europe, but European physicians are initiating changes. What is certain is that careful planning, innovative thinking, and the incorporation of new patterns of medical practice are all part of this complex transition and timing of senior academic physicians into retirement.}
}
@article{LIU2024122986,
title = {Tackling fuel poverty and decarbonisation in a distributed heating system through a three-layer whole system approach},
journal = {Applied Energy},
volume = {362},
pages = {122986},
year = {2024},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2024.122986},
url = {https://www.sciencedirect.com/science/article/pii/S0306261924003696},
author = {Xinyao Liu and Floris Bierkens and Ishanki {De Mel} and Matthew Leach and Michael Short and Mona Chitnis and Boyue Zheng and Lirong Liu},
keywords = {Residential heating decarbonisation, Fuel poverty, Cambridge housing model, Mixed-integer linear programming, Input-output-simulation},
abstract = {Residential heating displays huge decarbonisation potential towards Net-Zero. The complexity of heating system and socio-economic system appeals for a systematic design to avoid exacerbating fuel poverty. This study develops a three-layer heat-for-all model which integrates building stocks analysis, distributed heating system optimisation, economic and environmental impacts simulation to tackle heating decarbonisation and fuel poverty simultaneously. This whole system model is a powerful decision support tool that can help conceive heating decarbonisation strategies for wider regions and countries. More than 400,000 scenarios are created, considering the effects of future policy schemes (No Grant, Business as Usual, Proposed), minimum emission reduction target, carbon intensity of grid, future natural gas, and electricity prices. Results show that optimised heating system decarbonisation plan heavily relies on future energy prices. In the case study, only air source heat pumps are chosen when electricity price is lower than 3 times gas price. Secondly, investment in heating system could stimulate the greenhouse gas emission of whole supply chain, hedging the emission reduction achieved in heating system. This further reveals that life cycle thinking is imperative in GHG emission mitigation. Thirdly, electricity decarbonisation plays a vital role in achieving whole system emission reduction. The grid carbon intensity reduction makes substantial contribution to the emission reduction of heating system and industry system. In tackling fuel poverty, it's worth noticing that the fuel poverty is aggravated with more grant support under certain scenarios, since current policy schemes focus on capital investment in heating system but overlook the increased energy bills. It appeals for a more comprehensive policy design considering all stakeholders.}
}
@article{GAO2019242,
title = {Expert knowledge recommendation systems based on conceptual similarity and space mapping},
journal = {Expert Systems with Applications},
volume = {136},
pages = {242-251},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.06.013},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419304130},
author = {Li Gao and Kun Dai and Liping Gao and Tao Jin},
keywords = {Conceptual similarity, Space mapping, Core resource database (CRD), Institutional repository (IR), Expert Knowledge Recommendation System (EKRS)},
abstract = {The semantic analysis method of structured big data generated based on human knowledge is important in expert recommendation systems and scientific and technological information analysis. In these fields, the most important problem is the calculation of concept similarity. The study aims to explore the spatial mapping relationship between the general knowledge base and the professional knowledge base for the application of the general knowledge map in professional fields. With the core resource database (CRD) as the main body of the general knowledge and the institutional repository (IR) as the main body of the professional knowledge, the conceptual features of institutional expert knowledge were firstly abstracted from IR and inferred from small-scale datasets and the mathematical model was established based on the similarity of text concepts and related ranking results. Then, a two-set concept space mapping algorithm between CRD and IR was designed. In the algorithm, the more granular concept nodes were extracted from the information on the shortest paths among concepts to obtain a new knowledge set, the Expert Knowledge Recommendation System (EKRS). Finally, the simulation experiment was carried out with open datasets to verify the algorithm. The simulation results showed that the algorithm reduced the structural complexity in the calculation of large datasets. The proposed system model had a clear knowledge structure and the recommended accuracy of the text similarity was high. For small-scale knowledge base datasets with different sparsity, the system showed the stable performance, indicating the better convergence and robustness of the algorithm.}
}
@article{MALINVERNI2021100305,
title = {Educational Robotics as a boundary object: Towards a research agenda},
journal = {International Journal of Child-Computer Interaction},
volume = {29},
pages = {100305},
year = {2021},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2021.100305},
url = {https://www.sciencedirect.com/science/article/pii/S2212868921000349},
author = {Laura Malinverni and Cristina Valero and Marie Monique Schaper and Isabel Garcia {de la Cruz}},
keywords = {Educational robotics, Children, Robots, Boundary object, Intelligent technologies},
abstract = {Educational robotics has become each time more present in the educational experiences of children and young people. Nonetheless, often, the way in which robotics is introduced in educational settings has been considered as unnecessarily narrow. The paper aims at widening the scope of Educational Robotics and expanding the pedagogical possibilities of this field. To this end, the paper draws on the outcomes of two case studies carried out with primary and secondary school children aimed at investigating their views about robots. These studies allow framing and identifying five themes we believe are particularly relevant to rethink the pedagogy of Educational Robotics. Using these themes as cornerstones for reflection, we delineate a set of dimensions and paths to move Educational Robotics beyond the focus on technical skills but instead explore its potential as a boundary object to involve children in reflective processes around the ethical, social and cultural implications of emerging intelligent technologies.}
}
@article{ROWBOTTOM2013161,
title = {Kuhn vs. Popper on criticism and dogmatism in science, part II: How to strike the balance},
journal = {Studies in History and Philosophy of Science Part A},
volume = {44},
number = {2},
pages = {161-168},
year = {2013},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2012.11.011},
url = {https://www.sciencedirect.com/science/article/pii/S0039368112001161},
author = {Darrell P. Rowbottom},
abstract = {This paper is a supplement to, and provides a proof of principle of, Kuhn vs. Popper on Criticism and Dogmatism in Science: A Resolution at the Group Level. It illustrates how calculations may be performed in order to determine how the balance between different functions in science—such as imaginative, critical, and dogmatic—should be struck, with respect to confirmation (or corroboration) functions and rules of scientific method.}
}
@article{SHUKLA2024e31397,
title = {AI as a user of AI: Towards responsible autonomy},
journal = {Heliyon},
volume = {10},
number = {11},
pages = {e31397},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e31397},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024074280},
author = {Amit K. Shukla and Vagan Terziyan and Timo Tiihonen},
keywords = {Artificial Intelligence (AI), Autonomy, Responsible AI, ChatGPT, Prompt engineering, AI accountability},
abstract = {Recent advancements in Artificial Intelligence (AI), particularly in generative language models and algorithms, have led to significant impacts across diverse domains. AI capabilities to address prompts are growing beyond human capability but we expect AI to perform well also as a prompt engineer. Additionally, AI can serve as a guardian for ethical, security, and other predefined issues related to generated content. We postulate that enforcing dialogues among AI-as-prompt-engineer, AI-as-prompt-responder, and AI-as-Compliance-Guardian can lead to high-quality and responsible solutions. This paper introduces a novel AI collaboration paradigm emphasizing responsible autonomy, with implications for addressing real-world challenges. The paradigm of responsible AI-AI conversation establishes structured interaction patterns, guaranteeing decision-making autonomy. Key implications include enhanced understanding of AI dialogue flow, compliance with rules and regulations, and decision-making scenarios exemplifying responsible autonomy. Real-world applications envision AI systems autonomously addressing complex challenges. We have made preliminary testing of such a paradigm involving instances of ChatGPT autonomously playing various roles in a set of experimental AI-AI conversations and observed evident added value of such a framework.}
}
@article{SHUKLA2025128716,
title = {Deep belief network with fuzzy parameters and its membership function sensitivity analysis},
journal = {Neurocomputing},
volume = {614},
pages = {128716},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128716},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224014875},
author = {Amit K. Shukla and Pranab K. Muhuri},
keywords = {Deep learning, Deep belief networks, Restricted Boltzmann machine, Fuzzy sets, Type-1 fuzzy sets, Contrastive divergence},
abstract = {Over the last few years, deep belief networks (DBNs) have been extensively utilized for efficient and reliable performance in several complex systems. One critical factor contributing to the enhanced learning of the DBN layers is the handling of network parameters, such as weights and biases. The efficient training of these parameters significantly influences the overall enhanced performance of the DBN. However, the initialization of these parameters is often random, and the data samples are normally corrupted by unwanted noise. This causes the uncertainty to arise among weights and biases of the DBNs, which ultimately hinders the performance of the network. To address this challenge, we propose a novel DBN model with weights and biases represented using fuzzy sets. The approach systematically handles inherent uncertainties in parameters resulting in a more robust and reliable training process. We show the working of the proposed algorithm considering four widely used benchmark datasets such as: MNSIT, n-MNIST (MNIST with additive white Gaussian noise (AWGN) and MNIST with motion blur) and CIFAR-10. The experimental results show superiority of the proposed approach as compared to classical DBN in terms of robustness and enhanced performance. Moreover, it has the capability to produce equivalent results with a smaller number of nodes in the hidden layer; thus, reducing the computational complexity of the network architecture. Additionally, we also study the sensitivity analysis for stability and consistency by considering different membership functions to model the uncertain weights and biases. Further, we establish the statistical significance of the obtained results by conducting both one-way and Kruskal-Wallis analyses of variance tests.}
}