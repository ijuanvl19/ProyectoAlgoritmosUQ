@incollection{DASILVASOARES202349,
title = {Chapter Three - Exploring the potential of eye tracking on personalized learning and real-time feedback in modern education},
editor = {Mariuche Gomides and Isabela Starling-Alves and Flávia H. Santos},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {282},
pages = {49-70},
year = {2023},
booktitle = {Brain and Maths in Ibero-America},
issn = {0079-6123},
doi = {https://doi.org/10.1016/bs.pbr.2023.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S0079612323000936},
author = {Raimundo {da Silva Soares} and Amanda Yumi Ambriola Oku and Cândida da Silva Ferreira Barreto and João Ricardo Sato},
keywords = {Mathematics education, Eye tracking, Teaching practice, Student gaze},
abstract = {Eye tracking is one of the techniques used to investigate cognitive mechanisms involved in the school context, such as joint attention and visual perception. Eye tracker has portability, straightforward application, cost-effectiveness, and infant-friendly neuroimaging measures of cognitive processes such as attention, engagement, and learning. Furthermore, the ongoing software enhancements coupled with the implementation of artificial intelligence algorithms have improved the precision of collecting eye movement data and simplified the calibration process. These characteristics make it plausible to consider eye-tracking technology a promising tool to assist the teaching-learning process in school routines. However, eye tracking needs to be explored more as an educational instrument for real-time classroom activities and teachers' feedback. This perspective article briefly presents the fundamentals of the eye-tracking technique and four illustrative examples of employing this method in everyday school life. The first application shows how eye tracker information may contribute to teacher assessment of students' computational thinking in coding classes. In the second and third illustrations, we discuss the additional information provided by the eye-tracker to the teacher assessing the student's strategies to solve fraction problems and chart interpretation. The last illustration demonstrates the potential of eye tracking to provide Real-time feedback on learning difficulties/disabilities. Thus, we highlight the potential of the eye tracker as a complementary tool to promote personalized education and discuss future perspectives. In conclusion, we suggest that an eye-tracking system could be helpful by providing real-time student gaze leading to immediate teacher interventions and metacognition strategies.}
}
@article{RUBENSTEIN2022101030,
title = {Exploring creativity's complex relationship with learning in early elementary students},
journal = {Thinking Skills and Creativity},
volume = {44},
pages = {101030},
year = {2022},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2022.101030},
url = {https://www.sciencedirect.com/science/article/pii/S1871187122000335},
author = {Lisa DaVia Rubenstein and Jenna Thomas and W. Holmes Finch and Lisa M. Ridgley},
keywords = {Creativity, Learning, Early elementary, Academic achievement, Kindergarten},
abstract = {The purpose of this study was to examine the relationship between learning and creativity in early elementary students using both static and growth achievement scores in reading and mathematics. Participants were kindergarten and first grade students from the Midwestern United States. Initial correlations demonstrated significant positive relationships between students’ performance on the Torrance Test of Creative Thinking –Figural (TTCT-F) and static academic achievement scores in both reading and mathematics, but that same relationship did not exist with academic growth scores. Specifically, when academic growth was examined further using Generalized Additive Models (GAMs), a complex picture emerged, such that grade level (i.e., kindergarten v. first grade) and subscale type (e.g., Fluency v. Originality) influenced the significance and nature of the relationship (i.e., linear v. nonlinear). In general, as students increased in creativity performance, they demonstrated less academic growth. Future work should explore the underlying mechanisms explaining these relationships to better help students leverage their creative abilities for positive academic gains in the classroom setting.}
}
@article{PAL20133944,
title = {Title Paper: Natural computing: A problem solving paradigm with granular information processing},
journal = {Applied Soft Computing},
volume = {13},
number = {9},
pages = {3944-3955},
year = {2013},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2013.06.026},
url = {https://www.sciencedirect.com/science/article/pii/S1568494613002159},
author = {Sankar K. Pal and Saroj K. Meher},
keywords = {Natural computing, Granular computing, Soft computing, Hybrid model, Decision systems},
abstract = {Natural computing, inspired by biological course of action, is an interdisciplinary field that formalizes processes observed in living organisms to design computational methods for solving complex problems, or designing artificial systems with more natural behaviour. Based on the tasks abstracted from natural phenomena, such as brain modelling, self-organization, self-repetition, self evaluation, Darwinian survival, granulation and perception, nature serves as a source of inspiration for the development of computational tools or systems that are used for solving complex problems. Nature inspired main computing paradigms used for such development include artificial neural networks, fuzzy logic, rough sets, evolutionary algorithms, fractal geometry, DNA computing, artificial life and granular or perception-based computing. Information granulation in granular computing is an inherent characteristic of human thinking and reasoning process performed in everyday life. The present article provides an overview of the significance of natural computing with respect to the granulation-based information processing models, such as neural networks, fuzzy sets and rough sets, and their hybridization. We emphasize on the biological motivation, design principles, application areas, open research problems and challenging issues of these models.}
}
@article{PEREZRIVEROL2013134,
title = {Computational proteomics pitfalls and challenges: HavanaBioinfo 2012 Workshop report},
journal = {Journal of Proteomics},
volume = {87},
pages = {134-138},
year = {2013},
issn = {1874-3919},
doi = {https://doi.org/10.1016/j.jprot.2013.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S1874391913000493},
author = {Yasset Perez-Riverol and Henning Hermjakob and Oliver Kohlbacher and Lennart Martens and David Creasy and Jürgen Cox and Felipe Leprevost and Baozhen Paul Shan and Violeta I. Pérez-Nueno and Michal Blazejczyk and Marco Punta and Klemens Vierlinger and Pedro A. Valiente and Kalet Leon and Glay Chinea and Osmany Guirola and Ricardo Bringas and Gleysin Cabrera and Gerardo Guillen and Gabriel Padron and Luis Javier Gonzalez and Vladimir Besada},
keywords = {Bioinformatics workshop, Mass spectrometry, Course, Protein identification, Database searching, Proteomic repositories},
abstract = {The workshop “Bioinformatics for Biotechnology Applications (HavanaBioinfo 2012)”, held December 8–11, 2012 in Havana, aimed at exploring new bioinformatics tools and approaches for large-scale proteomics, genomics and chemoinformatics. Major conclusions of the workshop include the following: (i) development of new applications and bioinformatics tools for proteomic repository analysis is crucial; current proteomic repositories contain enough data (spectra/identifications) that can be used to increase the annotations in protein databases and to generate new tools for protein identification; (ii) spectral libraries, de novo sequencing and database search tools should be combined to increase the number of protein identifications; (iii) protein probabilities and FDR are not yet sufficiently mature; (iv) computational proteomics software needs to become more intuitive; and at the same time appropriate education and training should be provided to help in the efficient exchange of knowledge between mass spectrometrists and experimental biologists and bioinformaticians in order to increase their bioinformatics background, especially statistics knowledge.}
}
@article{JIA2024101456,
title = {Memory backtracking strategy: An evolutionary updating mechanism for meta-heuristic algorithms},
journal = {Swarm and Evolutionary Computation},
volume = {84},
pages = {101456},
year = {2024},
issn = {2210-6502},
doi = {https://doi.org/10.1016/j.swevo.2023.101456},
url = {https://www.sciencedirect.com/science/article/pii/S2210650223002286},
author = {Heming Jia and Chenghao Lu and Zhikai Xing},
keywords = {Memory backtracking strategies, New evolutionary updating strategy, Meta-heuristic optimization algorithm},
abstract = {The search domain of meta-heuristic algorithms is always constantly changing, which make it difficult to adapt the diverse optimization issues. To overcome above issue, an evolutionary updating mechanism called Memory Backtracking Strategy (MBS) is proposed, which contains thinking stage, recall stage, and memory stage. Overall, the adoption of the MBS enhances the efficiency of MHSs by incorporating group memory, clue recall, and memory forgetting mechanisms. These strategies improve the algorithm's ability to explore the search space, optimize the search process, and escape local optima. MBS will be applied to three different types of MHS algorithms: evolutionary based (LSHADE_SPACMA), physical based (Stochastic Fractal Search, SFS), and biological based (Marine Predators Algorithmnm, MPA) to demonstrate the universality of MBS. In the experimental section including 57 engineering problems, algorithm complexity analysis, CEC2020 Friedman ranking, convergence curve, Wilcoxon statistical, and box plot. Among them, 21 algorithms participated in the Friedman experiment, including MBS_LSHADE_SPACMA ranked first, LSHADE_SPACMA ranked second, MBS_MPA ranked 6th, MPA ranked 8th, MBS_SFS ranked 9th and SFS ranked 12th. Combined with the analysis of "MBS testing analysis" and the experimental results of engineering problems, it has proven that MBS has universality and good ability to improve optimization algorithm performance. The source codes of the proposed MBS (MBS_MPA) can be accessed by https://github.com/luchenghao2022/Memory-Backtracking-Strategy}
}
@article{KONDINSKI20226397,
title = {Composition-driven archetype dynamics in polyoxovanadates††Electronic supplementary information (ESI) available. CCDC 2128841 and 2130016. For ESI and crystallographic data in CIF or other electronic format see https://doi.org/10.1039/d2sc01004f},
journal = {Chemical Science},
volume = {13},
number = {21},
pages = {6397-6412},
year = {2022},
issn = {2041-6520},
doi = {https://doi.org/10.1039/d2sc01004f},
url = {https://www.sciencedirect.com/science/article/pii/S2041652023011690},
author = {Aleksandar Kondinski and Maren Rasmussen and Sebastian Mangelsen and Nicole Pienack and Viktor Simjanoski and Christian Näther and Daniel L. Stares and Christoph A. Schalley and Wolfgang Bensch},
abstract = {ABSTRACT
Molecular metal oxides often adopt common structural frameworks (i.e. archetypes), many of them boasting impressive structural robustness and stability. However, the ability to adapt and to undergo transformations between different structural archetypes is a desirable material design feature offering applicability in different environments. Using systems thinking approach that integrates synthetic, analytical and computational techniques, we explore the transformations governing the chemistry of polyoxovanadates (POVs) constructed of arsenate and vanadate building units. The water-soluble salt of the low nuclearity polyanion [V6As8O26]4− can be effectively used for the synthesis of the larger spherical (i.e. kegginoidal) mixed-valent [V12As8O40]4− precipitate, while the novel [V10As12O40]8− POVs having tubular cyclic structures are another, well soluble product. Surprisingly, in contrast to the common observation that high-nuclearity polyoxometalate (POM) clusters are fragmented to form smaller moieties in solution, the low nuclearity [V6As8O26]4− anion is in situ transformed into the higher nuclearity cluster anions. The obtained products support a conceptually new model that is outlined in this article and that describes a continuous evolution between spherical and cyclic POV assemblies. This new model represents a milestone on the way to rational and designable POV self-assemblies.}
}
@article{ARJMANDI202350,
title = {Embedding computer programming into a chemical engineering course: The impact on experiential learning},
journal = {Education for Chemical Engineers},
volume = {43},
pages = {50-57},
year = {2023},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2023.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S1749772823000064},
author = {Mohammadreza Arjmandi and Meng Wai Woo and Cody Mankelow and Thomas Loho and Kaveh Shahbaz and Amar Auckaili and Ashvin Thambyah},
keywords = {Engineering education, Qualitative study, Programming with MATLAB, Problem-based learning, Student experience},
abstract = {The need for autonomous engineering graduates who demonstrate hands-on skills has increased in the industry. Computer programming helps engineering students solve real-world problems systematically and accurately by applying governing physical and mathematical models into a format that a computer can read and execute. This study describes the pedagogical approach of incorporating programming workshops and assessments into a second-year chemical engineering course. The impact of this intervention on experiential learning amongst the students was then evaluated by analysing the feedback provided by voluntary participants during several focus group sessions. The feedback gave further insight into teaching pedagogy with respect to Kolb's experiential learning cycle. It was found the programming background of an individual clearly affects the phase of the learning cycle they predominantly experience during the workshops. Furthermore, programming background affected an individual's critical thinking while approaching an engineering problem. Constructive feedback provided by the student participants offered an invaluable opportunity for the teaching team to reflect on what went well and the areas for improvement in future iterations. The findings of this study can advance knowledge around design and implementation of a programming module within an engineering course.}
}
@article{SAVIN2021106878,
title = {Free associations of citizens and scientists with economic and green growth: A computational-linguistics analysis},
journal = {Ecological Economics},
volume = {180},
pages = {106878},
year = {2021},
issn = {0921-8009},
doi = {https://doi.org/10.1016/j.ecolecon.2020.106878},
url = {https://www.sciencedirect.com/science/article/pii/S0921800920309484},
author = {Ivan Savin and Stefan Drews and Jeroen {van den Bergh}},
keywords = {Structural topic modelling, Growth-vs-environment debate, Public opinion, Scientific opinion, Green growth},
abstract = {The debate about the relationship between economic growth and environmental sustainability triggers a range of associations. Here we analyze open-ended textual responses of citizens and scientists concerning their associations with the terms “economic growth” and “green growth”. We derive from the responses a number of topics and examine how associations differ across distinct opinion segments of people, namely supporters of Green growth, Agrowth and Degrowth. The results indicate that the general public is more critical of the notion of economic growth than academic researchers. Citizens stress problems of corruption, social inequality, unemployment and poverty, with less variation among the three opinion segments compared to scientists. The latter more strongly emphasize the environmental consequences of economic growth. Concerning associations of scientists with the term “green growth”, we find topics questioning its feasibility to be more likely expressed by Degrowth supporters, while topics stressing the possibility of sustainable economic growth by Green growth supporters. We find that topic polarization is stronger for scientists than citizens. Our results provide further validation for opinion clusters identified in previous studies and uncover additional insights about related views on growth and sustainability.}
}
@incollection{YANG20161,
title = {Chapter 1 - Bio-inspired computation and its applications in image processing: an overview},
editor = {Xin-She Yang and João Paulo Papa},
booktitle = {Bio-Inspired Computation and Applications in Image Processing},
publisher = {Academic Press},
pages = {1-24},
year = {2016},
isbn = {978-0-12-804536-7},
doi = {https://doi.org/10.1016/B978-0-12-804536-7.00001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128045367000016},
author = {X.-S. Yang and J.P. Papa},
keywords = {algorithm, ant algorithm, artificial neural networks, bee algorithm, bat algorithm, bio-inspired computation, cuckoo search, firefly algorithm, harmony search, particle swarm optimization, metaheuristics, swarm intelligence, support vector machine, signal and image processing},
abstract = {Almost all design problems in the sciences and engineering can be formulated as optimization problems, and many image processing problems can also be related to or formulated as optimization problems. These optimization problems can be solved by optimization techniques. However, these problems are often highly nonlinear and are subject to multiple nonlinear constraints, which makes them very challenging to solve. The further complication to these challenges is the stringent time requirements and high dimensionality, which means that traditional optimization techniques, such as gradient-based methods cannot deal with such kinds of problems well. Recent trends tend to use bio-inspired optimization techniques as a promising alternative, and it is usually combined with traditional methods, especially in the area of image processing. These bio-inspired computational methods are usually based on swarm intelligence and can be very effective in coping with nonlinearity in real-world problems. This chapter presents an overview of bio-inspired computation and its application in image processing, including some current trends and important issues, such as efficiency and time constraints.}
}
@article{CHEN20121,
title = {Varieties of agents in agent-based computational economics: A historical and an interdisciplinary perspective},
journal = {Journal of Economic Dynamics and Control},
volume = {36},
number = {1},
pages = {1-25},
year = {2012},
issn = {0165-1889},
doi = {https://doi.org/10.1016/j.jedc.2011.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S0165188911001692},
author = {Shu-Heng Chen},
keywords = {Cellular automata, Autonomous agents, Tournaments, Genetic algorithms, Genetic programming, Cognitive capacity},
abstract = {In this paper, we trace four origins of agent-based computational economics (ACE), namely, the markets origin, the cellular-automata origin, the tournaments origin, and the experiments origin. Along with this trace, we examine how these origins have motivated different concepts and designs of agents in ACE, which starts from the early work on simple programmed agents, randomly behaving agents, zero-intelligence agents, human-written programmed agents, autonomous agents, and empirically calibrated agents, and extends to the newly developing cognitive agents, psychological agents, and culturally sensitive agents. The review also shows that the intellectual ideas underlying these varieties of agents cross several disciplines, which may be considered as a part of a general attempt to study humans (and their behavior) with an integrated interdisciplinary foundation.}
}
@incollection{VANCOUVER2020463,
title = {Chapter 12 - Perceptions of control theory in industrial-organizational psychology: disturbances and counter-disturbances},
editor = {Warren Mansell},
booktitle = {The Interdisciplinary Handbook of Perceptual Control Theory},
publisher = {Academic Press},
pages = {463-501},
year = {2020},
isbn = {978-0-12-818948-1},
doi = {https://doi.org/10.1016/B978-0-12-818948-1.00012-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128189481000125},
author = {Jeffrey B. Vancouver},
keywords = {Control theory, Self-regulation, Self-efficacy, Computational modeling},
abstract = {The history of perceptual control theory's growing influence in the field of Industrial-Organizational Psychology is described. This history began in the early 1980's and included mostly conceptual work that described how control theory concepts might be used to understand applied phenomena. Both conceptual and empirical work on control theory ideas continued throughout the 1990's despite a substantial backlash against the theory by prominent scholars in the field. However, it was conceptual and empirical work in the 21st century that defined its potential integrative value and its theoretical rigor. Moreover, research regarding self-efficacy demonstrated how informal theories of human behavior might be better understood from a control theory perspective. Much of the current work with perceptual control theory involves the construction and testing of computational models that represent the links among perceptual, learning, and thinking modes of self-regulation and control.}
}
@article{HONG2006255,
title = {Bruno Buchberger — A life devoted to symbolic computation},
journal = {Journal of Symbolic Computation},
volume = {41},
number = {3},
pages = {255-258},
year = {2006},
note = {Logic, Mathematics and Computer Science: Interactions in honor of Bruno Buchberger (60th birthday)},
issn = {0747-7171},
doi = {https://doi.org/10.1016/j.jsc.2005.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0747717105001306},
author = {Hoon Hong and Deepak Kapur and Peter Paule and Franz Winkler and  {Faculty of RISC-Linz}}
}
@incollection{DIBBLE20061511,
title = {Chapter 31 Computational Laboratories for Spatial Agent-Based Models},
editor = {L. Tesfatsion and K.L. Judd},
series = {Handbook of Computational Economics},
publisher = {Elsevier},
volume = {2},
pages = {1511-1548},
year = {2006},
issn = {1574-0021},
doi = {https://doi.org/10.1016/S1574-0021(05)02031-9},
url = {https://www.sciencedirect.com/science/article/pii/S1574002105020319},
author = {Catherine Dibble},
keywords = {agent-based simulation, computational laboratory, computational social science, computational economics, spatial economics, spatial social science, spatial networks, small-world networks, scale-free networks, synthetic landscape, inference},
abstract = {An agent-based model is a virtual world comprising distributed heterogeneous agents who interact over time. In a spatial agent-based model the agents are situated in a spatial environment and are typically assumed to be able to move in various ways across this environment. Some kinds of social or organizational systems may also be modeled as spatial environments, where agents move from one group or department to another and where communications or mobility among groups may be structured according to implicit or explicit channels or transactions costs. This chapter focuses on the potential usefulness of computational laboratories for spatial agent-based modeling. Speaking broadly, a computational laboratory is any computational framework permitting the exploration of the behaviors of complex systems through systematic and replicable simulation experiments. By that definition, most of the research discussed in this handbook would be considered to be work with computational laboratories. A narrower definition of computational laboratory (or comp lab for short) refers specifically to specialized software tools to support the full range of agent-based modeling and complementary tasks. These tasks include model development, model evaluation through controlled experimentation, and both the descriptive and normative analysis of model outcomes. The objective of this chapter is to explore how comp lab tools and activities facilitate the systematic exploration of spatial agent-based models embodying complex social processes critical for social welfare. Examples include the spatial and temporal coordination of human activities, the diffusion of new ideas or of infectious diseases, and the emergence and ecological dynamics of innovative ideas or of deadly new diseases.}
}
@article{SIMMONS2012311,
title = {Bats use a neuronally implemented computational acoustic model to form sonar images},
journal = {Current Opinion in Neurobiology},
volume = {22},
number = {2},
pages = {311-319},
year = {2012},
note = {Neuroethology},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2012.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0959438812000293},
author = {James A Simmons},
abstract = {This paper reexamines neurophysiological results from echolocating big brown bats to propose a new perspective on FM biosonar processing in the auditory system. Individual auditory neurons are frequency-tuned and respond to brief, 2–10ms FM sweeps with an average of one spike per sound to register their tuned frequencies, to detect echo arrival, or to register a local null in the echo spectrum. When initiated by the broadcast, these responses comprise a cascade of single spikes distributed across time in neurons tuned to different frequencies that persists for 30–50ms, long after the sound has ended. Their progress mirrors the broadcast's propagation away from the bat and the return of echoes for distances out to 5–8m. Each returning echo evokes a similar pattern of single spikes that coincide with ongoing responses to the broadcast to register the target's distance and shape. The hypothesis advanced here is that this flow of responses over time acts as an internal model of sonar acoustics that the bat executes using neuronal computations distributed across many neurons to accumulate a dynamic image of the bat's surroundings.}
}
@article{ZHANG2024109147,
title = {Three-phase multi-criteria ranking considering three-way decision framework and criterion fuzzy concept},
journal = {International Journal of Approximate Reasoning},
volume = {168},
pages = {109147},
year = {2024},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2024.109147},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X24000343},
author = {Kai Zhang and Jianhua Dai},
keywords = {Three-way decision, Criterion fuzzy concept, Three-phase ranking, Multi-criteria ranking},
abstract = {The criterion fuzzy concept refers to a fuzzy set that represents the decision-maker's subjective preference for each criterion within the universe of criteria. Addressing the challenge of ranking all alternatives based on a given criterion fuzzy concept is a novel research direction in the field of fuzzy multi-criteria ranking issues. This paper proposes a three-phase approach for multi-criteria ranking in fuzzy environments, which combines the criterion fuzzy concept and three-way decision thinking. The proposed approach not only analyzes the decision-making characteristics of all alternatives but also facilitates their ranking. During the first phase, a qualitative classification method based on the criterion fuzzy concept and ideal solutions is defined, which divides all alternatives into three independent decision sub-regions. During the second phase, by analyzing the priority relationships among the alternatives within every sub-region, three local ranking rules for alternatives are proposed to determine the ranking of alternatives in each classification region. During the third phase, the semantic relations among three classification regions are considered to give an overall ranking of all alternatives. Finally, combined with two existing quantitative ranking indicators, multiple data sets are employed to verify the feasibility and superiority of the proposed three-phase multi-criteria ranking approach.}
}
@article{DAUCE20101,
title = {Computational neuroscience, from multiple levels to multi-level},
journal = {Journal of Physiology-Paris},
volume = {104},
number = {1},
pages = {1-4},
year = {2010},
note = {Computational Neuroscience, from Multiple Levels to Multi-level},
issn = {0928-4257},
doi = {https://doi.org/10.1016/j.jphysparis.2009.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0928425709000837},
author = {Emmanuel Daucé and Laurent Perrinet}
}
@article{DANOS200773,
title = {Distributed Measurement-based Quantum Computation},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {170},
pages = {73-94},
year = {2007},
note = {Proceedings of the 3rd International Workshop on Quantum Programming Languages (QPL 2005)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2006.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S1571066107000564},
author = {Vincent Danos and Ellie D'Hondt and Elham Kashefi and Prakash Panangaden},
keywords = {Formal language, quantum communication, quantum computing, semantics},
abstract = {We develop a formal model for distributed measurement-based quantum computations, adopting an agent-based view, such that computations are described locally where possible. Because the network quantum state is in general entangled, we need to model it as a global structure, reminiscent of global memory in classical agent systems. Local quantum computations are described as measurement patterns. Since measurement-based quantum computation is inherently distributed, this allows us to extend naturally several concepts of the measurement calculus [V. Danos, E. Kashefi and P. Panangaden, The measurement calculus (2004), arXiv:quant-ph/0412135], a formal model for such computations. Our goal is to define an assembly language, i.e. we assume that computations are well-defined and we do not concern ourselves with verification techniques. The operational semantics for systems of agents is given by a probabilistic transition system, and we define operational equivalence in a way that it corresponds to the notion of bisimilarity. With this in place, we prove that teleportation is bisimilar to a direct quantum channel, and this also within the context of larger networks.}
}
@article{SCHULZ2024210,
title = {Political reinforcement learners},
journal = {Trends in Cognitive Sciences},
volume = {28},
number = {3},
pages = {210-222},
year = {2024},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2023.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1364661323002875},
author = {Lion Schulz and Rahul Bhui},
keywords = {computational models, reinforcement learning, political psychology},
abstract = {Politics can seem home to the most calculating and yet least rational elements of humanity. How might we systematically characterize this spectrum of political cognition? Here, we propose reinforcement learning (RL) as a unified framework to dissect the political mind. RL describes how agents algorithmically navigate complex and uncertain domains like politics. Through this computational lens, we outline three routes to political differences, stemming from variability in agents’ conceptions of a problem, the cognitive operations applied to solve the problem, or the backdrop of information available from the environment. A computational vantage on maladies of the political mind offers enhanced precision in assessing their causes, consequences, and cures.}
}
@article{VAZQUEZ2017550,
title = {Price computation in electricity auctions with complex rules: An analysis of investment signals},
journal = {Energy Policy},
volume = {105},
pages = {550-561},
year = {2017},
issn = {0301-4215},
doi = {https://doi.org/10.1016/j.enpol.2017.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0301421517300770},
author = {Carlos Vazquez and Michelle Hallack and Miguel Vazquez},
keywords = {Electricity auctions, Investment signals, Side payments, Integer decisions, Marginal cost},
abstract = {This paper discusses the problem of defining marginal costs when integer variables are present, in the context of short-term power auctions. Most of the proposals for price computation existing in the literature are concerned with short-term competitive equilibrium (generators should not be willing to change the dispatch assigned to them by the auctioneer), which implies operational-cost recovery for all of the generators accepted in the auction. However, this is in general not enough to choose between the different pricing schemes. We propose to include an additional criterion in order to discriminate among different pricing schemes: prices have to be also signals for generation expansion. Using this condition, we arrive to a single solution to the problem of defining prices, where they are computed as the shadow prices of the balance equations in a linear version of the unit commitment problem. Importantly, not every linearization of the unit commitment is valid; we develop the conditions for this linear model to provide adequate investment signals. Compared to other proposals in the literature, our results provide a strong motivation for the pricing scheme and a simple method for price computation.}
}
@article{GOLIGHER20241067,
title = {Bayesian statistics for clinical research},
journal = {The Lancet},
volume = {404},
number = {10457},
pages = {1067-1076},
year = {2024},
issn = {0140-6736},
doi = {https://doi.org/10.1016/S0140-6736(24)01295-9},
url = {https://www.sciencedirect.com/science/article/pii/S0140673624012959},
author = {Ewan C Goligher and Anna Heath and Michael O Harhay},
abstract = {Summary
Frequentist and Bayesian statistics represent two differing paradigms for the analysis of data. Frequentism became the dominant mode of statistical thinking in medical practice during the 20th century. The advent of modern computing has made Bayesian analysis increasingly accessible, enabling growing use of Bayesian methods in a range of disciplines, including medical research. Rather than conceiving of probability as the expected frequency of an event (purported to be measurable and objective), Bayesian thinking conceives of probability as a measure of strength of belief (an explicitly subjective concept). Bayesian analysis combines previous information (represented by a mathematical probability distribution, the prior) with information from the study (the likelihood function) to generate an updated probability distribution (the posterior) representing the information available for clinical decision making. Owing to its fundamentally different conception of probability, Bayesian statistics offers an intuitive, flexible, and informative approach that facilitates the design, analysis, and interpretation of clinical trials. In this Review, we provide a brief account of the philosophical and methodological differences between Bayesian and frequentist approaches and survey the use of Bayesian methods for the design and analysis of clinical research.}
}
@article{BANDOPADHAYA2020100378,
title = {Integrated healthcare monitoring solutions for soldier using the internet of things with distributed computing},
journal = {Sustainable Computing: Informatics and Systems},
volume = {26},
pages = {100378},
year = {2020},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2020.100378},
url = {https://www.sciencedirect.com/science/article/pii/S2210537919304081},
author = {Shuvabrata Bandopadhaya and Rajiv Dey and Ashok Suhag},
keywords = {Healthcare monitoring system, Internet of things (IoT), Distributed computing, Fuzzy classification, Partten recognistion, Long Range wide area network (LoRaWAN)},
abstract = {This paper has proposed an integrated healthcare monitoring solution for the soldiers deployed in adverse environmental conditions, using the internet of things (IoT) with distributed computing. For these soldiers, the health parameters of every individual need to be monitored on a real-time basis and subsequent analysis of the dataset to be made for initiating appropriate medical support with the lowest possible delay. In this paper, a three-layer service-oriented IoT architecture has been proposed where the computational functionalities are distributed among all the layers. The proposed distributed computing mechanism has implemented two levels of filtration of redundant information that belongs to safe soldiers. The first level of filtering is done at the end-node using the Fuzzy classification approach and the second level of filtering is done at the intermediate node using the time-series pattern analysis approach. This layer-wise filtration process results in a reduction in data flooding and computational burden on the cloud due to which system response time improves to suit emergency applications. A prototype has been developed to validate the effectiveness of the proposed solution.}
}
@article{SAHADEVAN2025103141,
title = {Knowledge augmented generalizer specializer: A framework for early stage design exploration},
journal = {Advanced Engineering Informatics},
volume = {65},
pages = {103141},
year = {2025},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2025.103141},
url = {https://www.sciencedirect.com/science/article/pii/S1474034625000345},
author = {Vijayalaxmi Sahadevan and Rohin Joshi and Kane Borg and Vishal Singh and Abhishek Raj Singh and Bilal Muhammed and Soban Babu Beemaraj and Amol Joshi},
abstract = {In non-routine engineering design projects, the design outcome is determined by how the problem is formulated and represented in the early conceptual stage. The problem representation comprises schemas, ontologies, variables, and parameters relevant to the given problem class. Despite the critical role of early conceptual decisions in shaping the eventual design outcome, most of the computational support and automation are focused on the latter stages of parametric modelling, problem-solving, and optimization. There is inadequate support for aiding and automating problem formulation, variable and parameter identification and representation, and early-stage conceptual decisions. Therefore, this paper presents an innovative, transparent, and explainable method employing semantic reasoning to automate the step-by-step conceptual design generation process, including problem formulation, identification and representation of the variables and parameters and their dependencies. The method is realized through a novel framework called Knowledge Augmented Generalizer Specializer (KAGS). KAGS employs the Function-Behavior-Structure (FBS) ontology and the Graph-of-Thought (GoT) mechanism to enable automated reasoning with a Large Language Model (LLM). The workflow comprises various stages: problem breakdown, design prototype creation, assessment, and prototype merging. The framework is implemented and tested on a Subsea Layout (SSL) planning problem, a special class of infrastructure planning projects in deep-sea oil and gas production systems. The experimentations with KAGS demonstrate its capacity to support problem formulation, hierarchical decomposition, and solution generation. The research also provides new insights into the FBS framework and meta-level reasoning in early design stages.}
}
@article{GIANNOPULU2022e09017,
title = {Synchronised neural signature of creative mental imagery in reality and augmented reality},
journal = {Heliyon},
volume = {8},
number = {3},
pages = {e09017},
year = {2022},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2022.e09017},
url = {https://www.sciencedirect.com/science/article/pii/S240584402200305X},
author = {I. Giannopulu and G. Brotto and T.J. Lee and A. Frangos and D. To},
keywords = {Creativity, Synchronisation, Mental imagery, Real environment, Augmented reality, Complexity},
abstract = {Creativity, transforming imaginative thinking into reality, is a mental imagery simulation in essence. It can be incorporeal, concerns sophisticated and/or substantial thinking, and involves objects. In the present study, a mental imagery task consisting of creating a scene using familiar (FA) or abstract (AB) physical or virtual objects in real (RMI) and augmented reality (VMI) environments, and an execution task involving effectively creating a scene in augmented reality (VE), were utilised. The beta and gamma neural oscillations of healthy participants were recorded via a 32 channel wireless 10/20 international EGG system. In real and augmented environments and for both the mental imagery and execution tasks, the participants displayed a similar cortico-cortical neural signature essentially based on synchronous vs asynchronous beta and gamma oscillatory activities between anterior (i.e. frontal) and posterior (i.e. parietal, occipito-parietal and occipito-temporal) areas bilaterally. The findings revealed a transient synchronised neural architecture that appears to be consistent with the hypothesis according to which, creativity, because of its inherent complexity, cannot be confined to a single brain area but engages various interconnected networks.}
}
@article{EGRINAGY2008135,
title = {Algebraic properties of automata associated to Petri nets and applications to computation in biological systems},
journal = {Biosystems},
volume = {94},
number = {1},
pages = {135-144},
year = {2008},
note = {Seventh International Workshop on Information Processing in Cells and Tissues},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2008.05.019},
url = {https://www.sciencedirect.com/science/article/pii/S0303264708001366},
author = {Attila Egri-Nagy and Chrystopher L. Nehaniv},
keywords = {Algebraic automata theory, Petri nets, Krohn-Rhodes theorem, Algebraic biology},
abstract = {Biochemical and genetic regulatory networks are often modeled by Petri nets. We study the algebraic structure of the computations carried out by Petri nets from the viewpoint of algebraic automata theory. Petri nets comprise a formalized graphical modeling language, often used to describe computation occurring within biochemical and genetic regulatory networks, but the semantics may be interpreted in different ways in the realm of automata. Therefore, there are several different ways to turn a Petri net into a state-transition automaton. Here, we systematically investigate different conversion methods and describe cases where they may yield radically different algebraic structures. We focus on the existence of group components of the corresponding transformation semigroups, as these reflect symmetries of the computation occurring within the biological system under study. Results are illustrated by applications to the Petri net modelling of intermediary metabolism. Petri nets with inhibition are shown to be computationally rich, regardless of the particular interpretation method. Along these lines we provide a mathematical argument suggesting a reason for the apparent all-pervasiveness of inhibitory connections in living systems.}
}
@article{KALPOKIENE2023102197,
title = {Creative encounters of a posthuman kind – anthropocentric law, artificial intelligence, and art},
journal = {Technology in Society},
volume = {72},
pages = {102197},
year = {2023},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2023.102197},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X23000027},
author = {Julija Kalpokiene and Ignas Kalpokas},
keywords = {Anthropocentrism, Artificial intelligence, Creativity, Copyright},
abstract = {Artificial Intelligence (AI) is becoming an increasingly transformative force in human life. Crucially, its impact is already extending beyond automation of routine tasks and encroaching on creativity – a domain once seen as exclusively human. Hence, this article first surveys the discriminatory and exploitative underpinnings of the anthropocentric thinking that lies beyond attempts at sidelining the creative capacities of AI. Next, four different approaches to creativity and art are analyzed, ultimately conceptualizing art-ness as externally ascribed. Ultimately, the article moves to one way of such ascription – copyrightability – demonstrating the anthropocentric thinking behind attempts to both deny and award copyright protection to AI-generated content. Moreover, it transpires that human authors are under threat whichever of such strategies ends up dominant.}
}
@article{BATISTA2003189,
title = {A Computational Basis to Object?},
journal = {Neuron},
volume = {37},
number = {2},
pages = {189-190},
year = {2003},
issn = {0896-6273},
doi = {https://doi.org/10.1016/S0896-6273(03)00029-1},
url = {https://www.sciencedirect.com/science/article/pii/S0896627303000291},
author = {Aaron P. Batista},
abstract = {To use an object, we must be able to perceive the spatial relationship between the object's parts. The accepted view of how the brain coherently encodes an object is that some neurons in the frontal cortex employ an object-centered coordinate frame. A new computational model challenges this view, using the rich conceptual framework of neural basis functions.}
}
@incollection{MARINESCU20171,
title = {Chapter 1 - Complex Systems},
editor = {Dan C. Marinescu},
booktitle = {Complex Systems and Clouds},
publisher = {Elsevier},
address = {Boston},
pages = {1-32},
year = {2017},
series = {Computer Science Reviews and Trends},
isbn = {978-0-12-804041-6},
doi = {https://doi.org/10.1016/B978-0-12-804041-6.00001-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128040416000013},
author = {Dan C. Marinescu},
keywords = {Complexity, Emergence, Phase transitions, Open systems, Nondeterminism, Self-similarity, Fractal geometry, Power Law distribution},
abstract = {After a brief review of the evolution of thinking about systems, consisting of an ensemble of components, the chapter analyzes the nondeterminism, nonlinearity, and phase transitions in complex systems. A range of topics pertinent to complexity, such as self-organization, self-organized criticality, power law distributions, computational irreducibility, and quantitative characterization of complexity are then covered. Cybernetics and the interdisciplinary nature of complexity conclude the chapter.}
}
@article{VIGNONCLEMENTEL20103,
title = {A primer on computational simulation in congenital heart disease for the clinician},
journal = {Progress in Pediatric Cardiology},
volume = {30},
number = {1},
pages = {3-13},
year = {2010},
note = {Proceedings of the 1st International Conference on Computational Simulation in Congenital Heart Disease},
issn = {1058-9813},
doi = {https://doi.org/10.1016/j.ppedcard.2010.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S1058981310000767},
author = {Irene E. Vignon-Clementel and Alison L. Marsden and Jeffrey A. Feinstein},
keywords = {Hemodynamics, Computer modeling, Boundary conditions, Clinical data, Congenital heart disease},
abstract = {Interest in the application of engineering methods to problems in congenital heart disease has gained increased popularity over the past decade. The use of computational simulation to examine common clinical problems including single ventricle physiology and the associated surgical approaches, the effects of pacemaker implantation on vascular occlusion, or delineation of the biomechanical effects of implanted medical devices is now routinely appearing in clinical journals within all pediatric cardiovascular subspecialties. In practice, such collaboration can only work if both communities understand each other's methods and their limitations. This paper is intended to facilitate this communication by presenting in the context of congenital heart disease (CHD) the main steps involved in performing computational simulation—from the selection of an appropriate clinical question/problem to understanding the computational results, and all of the “black boxes” in between. We examine the current state of the art and areas in need of continued development. For example, medical image-based model-building software has been developed based on numerous different methods. However, none of them can be used to construct a model with a simple “click of a button.” The creation of a faithful, representative anatomic model, especially in pediatric subjects, often requires skilled manual intervention. In addition, information from a second imaging modality is often required to facilitate this process. We describe the technical aspects of model building, provide a definition of some of the most commonly used terms and techniques (e.g. meshes, mesh convergence, Navier-Stokes equations, and boundary conditions), and the assumptions used in running the simulations. Particular attention is paid to the assignment of boundary conditions as this point is of critical importance in the current areas of research within the realm of congenital heart disease. Finally, examples are provided demonstrating how computer simulations can provide an opportunity to “acquire” data currently unobtainable by other modalities, with essentially no risk to patients. To illustrate these points, novel simulation examples of virtual Fontan conversion (from preoperative data to predicted postoperative state) and outcomes of different surgical designs are presented. The need for validation of the currently employed techniques and predicted results are required and the methods remain in their infancy. While the daily application of these technologies to patient-specific clinical scenarios likely remains years away, the ever increasing interest in this area among both clinicians and engineers makes its eventual use far more likely than ever before and, some could argue, only a matter of [computing] time.}
}
@article{EDELMAN2007253,
title = {Behavioral and computational aspects of language and its acquisition},
journal = {Physics of Life Reviews},
volume = {4},
number = {4},
pages = {253-277},
year = {2007},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2007.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S1571064507000255},
author = {Shimon Edelman and Heidi Waterfall},
keywords = {Computational cognitive linguistics, Psycholinguistics, Machine learning, Language acquisition},
abstract = {One of the greatest challenges facing the cognitive sciences is to explain what it means to know a language, and how the knowledge of language is acquired. The dominant approach to this challenge within linguistics has been to seek an efficient characterization of the wealth of documented structural properties of language in terms of a compact generative grammar—ideally, the minimal necessary set of innate, universal, exception-less, highly abstract rules that jointly generate all and only the observed phenomena and are common to all human languages. We review developmental, behavioral, and computational evidence that seems to favor an alternative view of language, according to which linguistic structures are generated by a large, open set of constructions of varying degrees of abstraction and complexity, which embody both form and meaning and are acquired through socially situated experience in a given language community, by probabilistic learning algorithms that resemble those at work in other cognitive modalities.}
}
@article{JOHNSON20241037,
title = {Minds and markets as complex systems: an emerging approach to cognitive economics},
journal = {Trends in Cognitive Sciences},
volume = {28},
number = {11},
pages = {1037-1050},
year = {2024},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2024.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S1364661324001748},
author = {Samuel G.B. Johnson and Patrick R. Schotanus and J.A. Scott Kelso},
keywords = {decision-making, behavioral economics, narratives, agent-based models, extended mind, Coordination Dynamics},
abstract = {Cognitive economics is an emerging interdisciplinary field that uses the tools of cognitive science to study economic and social decision-making. Although most strains of cognitive economics share commitments to bridging levels of analysis (cognitive, behavioral, and systems) and embracing interdisciplinary approaches, we review a newer strand of cognitive economic thinking with a further commitment: conceptualizing minds and markets each as complex adaptive systems. We describe three ongoing research programs that strive toward these goals: (i) studying narratives as a cognitive and social representation used to guide decision-making; (ii) building cognitively informed agent-based models; and (iii) understanding markets as an extended mind – the Market Mind Hypothesis – analyzed using the concepts, methods, and tools of Coordination Dynamics.}
}
@article{LIEFGREEN2020101332,
title = {Strategies for selecting and evaluating information},
journal = {Cognitive Psychology},
volume = {123},
pages = {101332},
year = {2020},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2020.101332},
url = {https://www.sciencedirect.com/science/article/pii/S001002852030061X},
author = {Alice Liefgreen and Toby Pilditch and David Lagnado},
keywords = {Information search, OED framework, Utility functions, Inquiry, Question asking, Strategies, Probabilistic reasoning, Bayesian Networks},
abstract = {Within the domain of psychology, Optimal Experimental Design (OED) principles have been used to model how people seek and evaluate information. Despite proving valuable as computational-level methods to account for people’s behaviour, their descriptive and explanatory powers remain largely unexplored. In a series of experiments, we used a naturalistic crime investigation scenario to examine how people evaluate queries, as well as outcomes, in probabilistic contexts. We aimed to uncover the psychological strategies that people use, not just to assess whether they deviated from OED principles. In addition, we explored the adaptiveness of the identified strategies across both one-shot and stepwise information search tasks. We found that people do not always evaluate queries strictly in OED terms and use distinct strategies, such as by identifying a leading contender at the outset. Moreover, we identified aspects of zero-sum thinking and risk aversion that interact with people’s information search strategies. Our findings have implications for building a descriptive account of information seeking and evaluation, accounting for factors that currently lie outside the realm of information-theoretic OED measures, such as context and the learner’s own preferences.}
}
@article{WENG20072303,
title = {On developmental mental architectures},
journal = {Neurocomputing},
volume = {70},
number = {13},
pages = {2303-2323},
year = {2007},
note = {Selected papers from the 3rd International Conference on Development and Learning (ICDL 2004) Time series prediction competition: the CATS benchmark},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2006.07.017},
url = {https://www.sciencedirect.com/science/article/pii/S0925231206005194},
author = {Juyang Weng},
keywords = {Mental architecture, Agent architecture, Computational neural science, Cognitive development, Autonomous mental development, Developmental robots, Learning types, Developmental vision, Speech recognition, Language acquisition, Thinking, Reasoning, Autonomous planning},
abstract = {This paper presents a computational theory of developmental mental architectures for artificial and natural systems, motivated by neuroscience. The work is an attempt to approximately model biological mental architectures using mathematical tools. Six types of architecture are presented, beginning with the observation-driven Markov decision process as Type-1. From Type-1 to Type-6, the architecture progressively becomes more complete toward the necessary functions of autonomous mental development. Properties of each type are presented. Experiments are discussed with emphasis on their architectures.}
}
@article{DYER2021101055,
title = {Uncertainty and disciplinary difference: Mapping attitudes towards uncertainty across discipline boundaries},
journal = {Design Studies},
volume = {77},
pages = {101055},
year = {2021},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2021.101055},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X21000661},
author = {Loren Dyer and Jacqueline Power and Andrew Steen and Louise Wallis and Aidan Davison},
keywords = {design processes, design thinking, epistemology, interdisciplinarity, uncertainty},
abstract = {This article investigates the different ways that uncertainty is understood and approached across design disciplines. Structural attitudes toward uncertainty are assessed in design thinking literature before other possible ways of viewing uncertainty in the design process are introduced. Uncertainty is then presented as a source of epistemological difference between design disciplines, and this difference is explicated through a project that uses literature survey and analytical diagramming to map differences between discipline attitudes to uncertainty. Our review identifies uncertainty as a prevalent source of discipline difference with the goal of better describing barriers, and effective responses to them, in inter- and trans-disciplinary design agendas.}
}
@incollection{RUNCO2023115,
title = {Chapter 4 - Biological Perspectives on Creativity},
editor = {Mark A. Runco},
booktitle = {Creativity (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
pages = {115-154},
year = {2023},
isbn = {978-0-08-102617-5},
doi = {https://doi.org/10.1016/B978-0-08-102617-5.00005-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780081026175000059},
author = {Mark A. Runco},
keywords = {Adoption studies, Altered states of consciousness, Cerebellum, Corpus callosum, Dopamine, Dreams, Drugs, Exercise, Genealogies, Genetics, Prefrontal cortex, Split brain, Stress},
abstract = {This chapter discusses biological perspectives on creativity. Some of the research on creativity as of late involves the brain and biological correlates of originality, novelty, and insight. Handedness has long been used as an indication of hemispheric dominance or hemisphericity, with right-handed people being compared with left-handed people. There are several reports of left-handed persons outnumbering the right-handed ones in creative and eminent samples. Hemisphericity and other important brain structures and processes contributing to creative thinking and behavior have more recently been studied with electroencephalogram (EEG), positron emission topography (PET), cerebral blood flow, and magnetic resonance imaging (MRI) techniques. Numerous EEG studies suggest that there are particular brain wave patterns and brain structures that are associated with creative problem-solving or at least with specific phases within the problem-solving process. EEGs suggest a complex kind of activity while individuals work on tasks indicative of creative potential. Much of the complexity disappears when those same individuals work on convergent thinking tasks. Research suggests that the prefrontal cortex plays an important role in creative thinking and behaviour.}
}
@incollection{MADIAJAGAN20191,
title = {Chapter 1 - Parallel Computing, Graphics Processing Unit (GPU) and New Hardware for Deep Learning in Computational Intelligence Research},
editor = {Arun Kumar Sangaiah},
booktitle = {Deep Learning and Parallel Computing Environment for Bioengineering Systems},
publisher = {Academic Press},
pages = {1-15},
year = {2019},
isbn = {978-0-12-816718-2},
doi = {https://doi.org/10.1016/B978-0-12-816718-2.00008-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128167182000087},
author = {M. Madiajagan and S. Sridhar Raj},
keywords = {Deep learning, Parallelization, Graphics processing unit, Hardware architecture, Memory optimization, Computational intelligence},
abstract = {Graphics processing unit (GPU) is an electronic circuit which manipulates and modifies the memory for better image output. Deep learning involves huge amounts of matrix multiplications and other operations which can be massively parallelized and thus sped up on GPUs. A single GPU might have thousands of cores while a CPU usually has no more than 12 cores. GPU's practical applicability is affected by two issues: long training time and limited GPU memory, which is greatly influenced as the neural network size grows. In order to overcome these issues, this chapter presents various technologies in distributed parallel processing which improve the training time and optimize the memory, and hardware engine architectures will be explored for data size reduction. The GPUs generally used for deep learning are limited in memory size compared to CPUs, so even the latest Tesla GPU has only 16 GB of memory. Therefore, GPU memory cannot be increased to that extent easily, so networks must be designed to fit within the available memory. This could be a factor limiting progress, overcoming which would be highly beneficiary to the computational intelligence area.}
}
@incollection{VERDICCHIO2025,
title = {Language of Artificial Intelligence Discourses},
booktitle = {Reference Module in Social Sciences},
publisher = {Elsevier},
year = {2025},
isbn = {978-0-443-15785-1},
doi = {https://doi.org/10.1016/B978-0-323-95504-1.00392-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780323955041003926},
author = {Mario Verdicchio},
keywords = {Artificial intelligence, Figures of speech, Machine learning, Philosophy of mind},
abstract = {Language has played a fundamental role in Artificial Intelligence discourses from the very beginning of the establishment of the field. An early assumption was that every aspect of intelligence could be described in a manner compatible with machine operation. This assumption is critical for comparing humans and machines, given that, on the one hand, our understanding of how human brains work is insufficient to define intelligence clearly, and on the other hand, a focus on computational artifacts may lead to a limited conceptualization that overlooks significant aspects of what it means to be conscious and conscientious humans. A mindful analysis of the metaphors used to describe AI systems is key to navigating the intricate entanglements between society and technology that contribute to this endeavor.}
}
@article{PAPAVLASOPOULOU2019415,
title = {Exploring children's learning experience in constructionism-based coding activities through design-based research},
journal = {Computers in Human Behavior},
volume = {99},
pages = {415-427},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2019.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0747563219300184},
author = {Sofia Papavlasopoulou and Michail N. Giannakos and Letizia Jaccheri},
keywords = {Constructionism, Coding, Computational thinking, Engagement, Children, Design-based research},
abstract = {Over the last few years, the integration of coding activities for children in K-12 education has flourished. In addition, novel technological tools and programming environments have offered new opportunities and increased the need to design effective learning experiences. This paper presents a design-based research (DBR) approach conducted over two years, based on constructionism-based coding experiences for children, following the four stages of DBR. Three iterations (cycles) were designed and examined in total, with participants aged 8–17 years old, using mixed methods. Over the two years, we conducted workshops in which students used a block-based programming environment (i.e., Scratch) and collaboratively created a socially meaningful artifact (i.e., a game). The study identifies nine design principles that can help us to achieve higher engagement during the coding activity. Moreover, positive attitudes and high motivation were found to result in the better management of cognitive load. Our contribution lies in the theoretical grounding of the results in constructionism and the emerging design principles. In this way, we provide both theoretical and practical evidence of the value of constructionism-based coding activities.}
}
@article{HERBET2015413,
title = {Rethinking voxel-wise lesion-deficit analysis: A new challenge for computational neuropsychology},
journal = {Cortex},
volume = {64},
pages = {413-416},
year = {2015},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2014.10.021},
url = {https://www.sciencedirect.com/science/article/pii/S0010945214003517},
author = {Guillaume Herbet and Gilles Lafargue and Hugues Duffau}
}
@article{CLEMENTZ2023143,
title = {Clinical characterization and differentiation of B-SNIP psychosis Biotypes: Algorithmic Diagnostics for Efficient Prescription of Treatments (ADEPT)-1},
journal = {Schizophrenia Research},
volume = {260},
pages = {143-151},
year = {2023},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2023.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0920996423002645},
author = {Brett A. Clementz and Ishanu Chattopadhyay and Rebekah L. Trotti and David A. Parker and Elliot S. Gershon and S. Kristian Hill and Elena I. Ivleva and Sarah K. Keedy and Matcheri S. Keshavan and Jennifer E. McDowell and Godfrey D. Pearlson and Carol A. Tamminga and Robert D. Gibbons},
abstract = {Clinically defined psychosis diagnoses are neurobiologically heterogeneous. The B-SNIP consortium identified and validated more neurobiologically homogeneous psychosis Biotypes using an extensive battery of neurocognitive and psychophysiological laboratory measures. However, typically the first step in any diagnostic evaluation is the clinical interview. In this project, we evaluated if psychosis Biotypes have clinical characteristics that can support their differentiation in addition to obtaining laboratory testing. Clinical interview data from 1907 individuals with a psychosis Biotype were used to create a diagnostic algorithm. The features were 58 ratings from standard clinical scales. Extremely randomized tree algorithms were used to evaluate sensitivity, specificity, and overall classification success. Biotype classification accuracy peaked at 91 % with the use of 57 items on average. A reduced feature set of 28 items, though, also showed 81 % classification accuracy. Using this reduced item set, we found that only 10–11 items achieved a one-vs-all (Biotype-1 or not, Biotype-2 or not, Biotype-3 or not) area under the sensitivity-specificity curve of .78 to .81. The top clinical characteristics for differentiating psychosis Biotypes, in order of importance, were (i) difficulty in abstract thinking, (ii) multiple indicators of social functioning, (iii) conceptual disorganization, (iv) severity of hallucinations, (v) stereotyped thinking, (vi) suspiciousness, (vii) unusual thought content, (viii) lack of spontaneous speech, and (ix) severity of delusions. These features were remarkably different from those that differentiated DSM psychosis diagnoses. This low-burden adaptive algorithm achieved reasonable classification accuracy and will support Biotype-specific etiological and treatment investigations even in under-resourced clinical and research environments.}
}
@article{SCHWARTZ20192047,
title = {Biophysics and the Genomic Sciences},
journal = {Biophysical Journal},
volume = {117},
number = {11},
pages = {2047-2053},
year = {2019},
issn = {0006-3495},
doi = {https://doi.org/10.1016/j.bpj.2019.07.038},
url = {https://www.sciencedirect.com/science/article/pii/S0006349519306277},
author = {David C. Schwartz},
abstract = {It is now rare to find biological, or genetic investigations that do not rely on the tools, data, and thinking drawn from the genomic sciences. Much of this revolution is powered by contemporary sequencing approaches that readily deliver large, genome-wide data sets that not only provide genetic insights but also uniquely report molecular outcomes from experiments that biophysicists are increasingly using for potentiating structural and mechanistic investigations. In this perspective, I describe a path of how biophysical thinking greatly contributed to this revolution in ways that parallel advancements in computer science through discussion of several key inventions, described as “foundational devices.” These discussions also point at the future of how biophysics and the genomic sciences may become more finely integrated for empowering new measurement paradigms for biological investigations.}
}
@article{XIA20025,
title = {Applications of computational fluid dynamics (cfd) in the food industry: a review},
journal = {Computers and Electronics in Agriculture},
volume = {34},
number = {1},
pages = {5-24},
year = {2002},
issn = {0168-1699},
doi = {https://doi.org/10.1016/S0168-1699(01)00177-6},
url = {https://www.sciencedirect.com/science/article/pii/S0168169901001776},
author = {Bin Xia and Da-Wen Sun},
keywords = {Computational fluid dynamics, , Food, Refrigeration, Cooling, Drying, Sterilisation, Mixing, Chilling, Modelling, Simulation},
abstract = {Computational fluid dynamics (cfd) is a simulation tool, which uses powerful computer and applied mathematics to model fluid flow situations for the prediction of heat, mass and momentum transfer and optimal design in industrial processes. It is only in recent years that cfd has been applied in the food processing industry. This paper reviews the application of cfd in food processing industries including drying, sterilisation, refrigeration and mixing. The advantages of using cfd are discussed and the future of cfd applications is also outlined.}
}
@article{WANG20073776,
title = {Maximum likelihood computation based on the Fisher scoring and Gauss–Newton quadratic approximations},
journal = {Computational Statistics & Data Analysis},
volume = {51},
number = {8},
pages = {3776-3787},
year = {2007},
issn = {0167-9473},
doi = {https://doi.org/10.1016/j.csda.2006.12.037},
url = {https://www.sciencedirect.com/science/article/pii/S0167947306005147},
author = {Yong Wang},
keywords = {Maximum likelihood computation, Fisher scoring, Gauss–Newton method, Constrained optimization, Iteratively reweighted least-squares},
abstract = {The Fisher scoring and Gauss–Newton methods are two known methods for maximum likelihood computation. This paper provides a generalization for each method in a unified manner so that they can be used for some difficult maximum likelihood computation, when, for example, there exist constraints on the parameters. A generalized method does not use directly the Newton-type iteration formulas of these methods, but, instead, uses the corresponding quadratic functions transformed from them. It proceeds by repeatedly approximating the log-likelihood function with the quadratic functions in the neighborhoods of the current iterates and optimizing each quadratic function within the parameter space. It is shown that each quadratic function has a weighted linear regression formulation, which can be conveniently solved. This generalization also extends the applicability of the Fisher scoring method to situations when the expected Fisher information matrices are unavailable in closed form. Fast computation can generally be anticipated, owing to their small rates of convergence and a rapid solution of each linear regression problem. While the generalized Gauss–Newton method may sometimes suffer for the so-called large residual problem, the generalized Fisher scoring method has performed consistently well in the numerical experiments we conducted.}
}
@incollection{NAGURNEY1996335,
title = {Chapter 7 Parallel computation},
series = {Handbook of Computational Economics},
publisher = {Elsevier},
volume = {1},
pages = {335-404},
year = {1996},
issn = {1574-0021},
doi = {https://doi.org/10.1016/S1574-0021(96)01009-X},
url = {https://www.sciencedirect.com/science/article/pii/S157400219601009X},
author = {Anna Nagurney},
abstract = {Publisher Summary
Parallel computation represents not only a new mode of computation, but, a new intellectual paradigm. This chapter provides an overview of the technology of parallel computation in terms of hardware and programming languages; presents some of the fundamental classes of problems encountered in economics and the associated numerical methodologies for their solution; discusses the state-of-the-art computational techniques and focuses on parallel techniques and contrasts them with serial techniques for illumination and instructive purposes; and presents applications of the classes of problems and associated numerical methods to econometrics, microeconomics, macroeconomics, and finance. The emergence of computation as a basic scientific methodology in economics has given access to solutions of fundamental problems that pure analysis, observation, or experimentation could not have achieved. Parallel computation represents the wave of the future. It is considered to be cheaper and faster than serial computing and the only approach to faster computation currently foreseeable. Parallel computation is appealing, hence, for the economies of scale that are possible, for the potentially faster solution of large-scale problems, and also for the possibilities that it presents for imitating adjustment or tatonnement processes.}
}
@article{SUMAR20103980,
title = {Computational intelligence approach to PID controller design using the universal model},
journal = {Information Sciences},
volume = {180},
number = {20},
pages = {3980-3991},
year = {2010},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2010.06.026},
url = {https://www.sciencedirect.com/science/article/pii/S0020025510002872},
author = {Rodrigo Rodrigues Sumar and Antonio Augusto Rodrigues Coelho and Leandro dos Santos Coelho},
keywords = {PID control, Nonlinear systems, Fuzzy systems, Neural networks, Differential evolution, Optimization},
abstract = {Despite the popularity of PID (Proportional-Integral-Derivative) controllers, their tuning aspect continues to present challenges for researches and plant operators. Various control design methodologies have been proposed in the literature, such as auto-tuning, self-tuning, and pattern recognition. The main drawback of these methodologies in the industrial environment is the number of tuning parameters to be selected. In this paper, the design of a PID controller, based on the universal model of the plant, is derived, in which there is only one parameter to be tuned. This is an attractive feature from the viewpoint of plant operators. Fuzzy and neural approaches – bio-inspired methods in the field of computational intelligence – are used to design and assess the efficiency of the PID controller design based on differential evolution optimization in nonlinear plants. The numerical results presented herein indicate that the proposed bio-inspired design is effective for the nonlinear control of nonlinear plants.}
}
@incollection{TSATSE20212033,
title = {Reflections on the development of scenario and problem-based chemical engineering projects},
editor = {Metin Türkay and Rafiqul Gani},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {50},
pages = {2033-2038},
year = {2021},
booktitle = {31st European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-88506-5.50314-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323885065503144},
author = {A. Tsatse and E. Sorensen},
keywords = {problem-based learning, Scenarios, Process Systems Engineering},
abstract = { Abstract
This work reflects on the use of scenario- and problem-based learning as a way of conveying not only fundamental knowledge, but also to provide training in the use of computational Process Systems Engineering (PSE) tools applied to open-ended real world problems. The teaching framework also has a strong emphasis on the development of professional skills and to evaluate the recommended design solutions considering multiple perspectives such as economics, safety, environment and societal context. The framework is implemented through week-long group projects called Scenarios, taking place mainly in the first two years of study, and examples are given of different variations of Scenarios. This teaching approach has multiple benefits, including but not limited to, students’ understanding of PSE tools and the development of their critical engineering thinking.}
}
@article{MILOVANOVIC2021101044,
title = {Characterization of concept generation for engineering design through temporal brain network analysis},
journal = {Design Studies},
volume = {76},
pages = {101044},
year = {2021},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2021.101044},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X21000557},
author = {Julie Milovanovic and Mo Hu and Tripp Shealy and John Gero},
keywords = {design cognition, design process, problem solving, conceptual design, design neurocognition},
abstract = {This research explores the effect of the structuredness of design concept generation techniques on temporal network neurocognition. Engineering graduate students (n = 30) completed three concept generation tasks using techniques with different levels of structuredness: brainstorming, morphological analysis, and TRIZ. Students’ brain activation in their prefrontal cortex (PFC) was measured using functional near-infrared spectroscopy (fNIRS). The temporal dynamic of central regions in brain networks were compared between tasks. Central regions facilitate functional interaction and imply information flow through the brain. A consistent central region appears in the medial PFC. Consistent network connections occurred across both hemispheres suggesting a concurrent dual processing of divergent and convergent thinking. This study offers novel insights into the underlying neurophysiological mechanism when using these concept generation techniques.}
}
@article{MALDONADO2014177,
title = {Synchronicity among Biological and Computational Levels of an Organism: Quantum Biology and Complexity},
journal = {Procedia Computer Science},
volume = {36},
pages = {177-184},
year = {2014},
note = {Complex Adaptive Systems Philadelphia, PA November 3-5, 2014},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.09.076},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914013258},
author = {Carlos E. Maldonado and Nelson A. Gómez-Cruz},
keywords = {quantum biology, living systems, non-linear systems, complexity science, theory, health.},
abstract = {This paper argues that there is a synchronicity among biological and computational levels on an organism and provides arguments and proofs based on experimental research gathered in the literature. The leading thread is the interplay between quantum biology (QB) and complexity. As the paper asks whether QB does contribute to complexity science (CS), five arguments are provided: (i) Firstly a state-of-the art of QB and its relationship to CS is sketched out. Thereafter, the attention is directed to answering the question set out; (ii) Secondly, it digs into the understanding of life toward deeper levels of reality; (iii) It is shown that non-trivial quantum effects shed insightful lights on the information processing of and within living beings; (iv) Once the distinction is made between increasing levels of complexity and increasing levels of organization, the focus lies in the importance of QB for organization, and not so much for complexity as such; (v) The role of information rises at the center of all concerns, and the intertwining of complexity and information processing. At the end some conclusions are drawn.}
}
@article{POST1987339,
title = {Latest thinking on the Malpasset accident},
journal = {Engineering Geology},
volume = {24},
number = {1},
pages = {339-353},
year = {1987},
note = {Dam Failures},
issn = {0013-7952},
doi = {https://doi.org/10.1016/0013-7952(87)90071-8},
url = {https://www.sciencedirect.com/science/article/pii/0013795287900718},
author = {G. Post and D. Bonazzi}
}
@article{ROSEN20211,
title = {A word is worth a thousand pictures: A 20-year comparative analysis of aberrant abstraction in schizophrenia, affective psychosis, and non-psychotic depression},
journal = {Schizophrenia Research},
volume = {238},
pages = {1-9},
year = {2021},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2021.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0920996421003674},
author = {Cherise Rosen and Martin Harrow and Liping Tong and Thomas H. Jobe and Helen Harrow},
keywords = {Abstraction, Concretism, Aberrant abstraction, Schizophrenia, Affective psychosis, Unipolar depression non-psychotic},
abstract = {Abstract thinking is a cognitive process that involves the assimilation of concepts reduced from diffuse sensory input, organized, and interpreted in a manner beyond the obvious. There are multiple facets by which abstraction is measured that include semantic, visual-spatial and social comprehension. This study examined the prevalence and course of abstract and concrete responses to semantic proverbs and aberrant abstraction (composite score of semantic, visual-spatial, and social comprehension) over 20 years in 352 participants diagnosed with schizophrenia, affective psychosis, and unipolar non-psychotic depression. We utilized linear models, two-way ANOVA and contrasts to compare groups and change over time. Linear models with Generalized Estimation Equation (GEE) to determine association. Our findings show that regardless of diagnosis, semantic proverb interpretation improves over time. Participants with schizophrenia give more concrete responses to proverbs when compared to affective psychosis and unipolar depressed without psychosis. We also show that the underlying structure of concretism encompasses increased conceptual overinclusion at index hospitalization and idiosyncratic associations at follow-up; whereas, abstract thinking overtime encompasses increased visual-spatial abstraction at index and rich associations with increased social comprehension scores at follow-up. Regardless of diagnosis, premorbid functioning, descriptive characteristics, and IQ were not associated with aberrant abstraction. Delusions are highly and positively related to aberrant abstraction scores, while hallucinations are mildly and positively related to this score. Lastly, our data point to the importance of examining the underlying interconnected structures of ‘established’ constructs vis-à-vis mixed methods to provide a description of the rich interior world that may not always map onto current quantitative measures.}
}
@article{ALONSO20092683,
title = {A method to generate computationally efficient reduced order models},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {198},
number = {33},
pages = {2683-2691},
year = {2009},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2009.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0045782509001376},
author = {D. Alonso and A. Velazquez and J.M. Vega},
keywords = {Reduced order model, Proper Orthogonal Decomposition, Incompressible nonisothermal flow},
abstract = {A new method is presented to generate reduced order models (ROMs) in Fluid Dynamics problems. The method is based on the expansion of the flow variables on a Proper Orthogonal Decomposition (POD) basis, calculated from a limited number of snapshots, which are obtained via Computational Fluid Dynamics (CFD). Then, the POD-mode amplitudes are calculated as minimizers of a properly defined overall residual of the equations and boundary conditions. The residual can be calculated using only a limited number of points in the flow field, which can be scattered either all over the whole computational domain or over a smaller projection window. This means that the process is both computationally efficient (reconstructed flow fields require less than 1% of the time needed to compute a full CFD solution) and flexible (the projection window can avoid regions of large localized CFD errors). Also, various definitions of the residual are briefly discussed, along with the number and distribution of snapshots, the number of retained modes, and the effect of CFD errors, to conclude that the method is numerically robust. This is because the results are largely insensitive to the definition of the residual, to CFD errors, and to the CFD method itself, which may contain artificial stabilizing terms. Thus, the method is amenable for practical engineering applications.}
}
@article{FAUL2024255,
title = {Update on “Emotion and autobiographical memory”: 14 years of advances in understanding functions, constructions, and consequences},
journal = {Physics of Life Reviews},
volume = {51},
pages = {255-272},
year = {2024},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2024.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S1571064524001301},
author = {Leonard Faul and Jaclyn H. Ford and Elizabeth A. Kensinger},
abstract = {Holland and Kensinger (2010) reviewed the literature on “Emotion and autobiographical memory.” They focused on two broad ways that emotions influence memory: (1) emotion during an event influences how the event is remembered, and (2) emotion and emotional goals during memory retrieval influence how past events are remembered. We begin by providing a brief update on the key points from that review. Holland and Kensinger (2010) also had noted a number of important avenues for future work. Here, we describe what has been learned about the functions of autobiographical memory and their reconstructive nature. Relatedly, we review more recent research on memory reconstruction in the context of visual perspective shifts, counterfactual thinking, nostalgia, and morality. This research has emphasized the reciprocal nature of the interactions between emotion and autobiographical memory: Not only do emotions influence memory, memories influence emotions. Next, we discuss advances that have been made in understanding the reciprocal relations between stress, mood, and autobiographical memory. Finally, we discuss the research that is situating emotional autobiographical memories within a social framework, providing a bedrock for collective memories. Despite the many advances of the past 14 years, many open questions remain; throughout the review we note domains in which we hope to see advances over the next decades.}
}
@article{EGBERT2021104173,
title = {“It's a chance to make mistakes”: Processes and outcomes of coding in 2nd grade classrooms},
journal = {Computers & Education},
volume = {168},
pages = {104173},
year = {2021},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2021.104173},
url = {https://www.sciencedirect.com/science/article/pii/S0360131521000506},
author = {Joy Egbert and Seyed Abdollah Shahrokni and Reima Abobaker and Nataliia Borysenko},
keywords = {Elementary education, Robotics, Coding, Teacher learning, Computational thinking},
abstract = {Several gaps exist in the literature on coding. First, little exploration has focused on early elementary school students. In addition, close description of the overall context of coding tasks at this level is rare. Further, there is a need for both teacher and student voices around coding experiences to be heard. Moreover, a task engagement framework has not been used to evaluate the process or outcomes of early elementary coding tasks. Therefore, an exploratory holistic case study design was used to investigate student and teacher processes and outcomes of coding lessons in order to fill gaps in the literature. In this study, forty-six 2nd grade students, two teachers, and four researchers completed two one-week units on basic coding. Multiple descriptive and numeric data sources were employed to describe the process and outcomes of learning coding. Conclusions include: (1) teachers should start learning about coding first with short awareness sessions and then move to their own classrooms with knowledge brokers and other forms of assistance; (2) a focus on content and process, including problem-solving, is effective for coding with young children; (3) there can be a high level of engagement for teachers and students with the use of robots and welldesigned, age-appropriate coding tasks, and; (4) multiple data sources and the inclusion of both teacher and student data are essential in exploring coding in classrooms.}
}
@article{GUPTA2009481,
title = {Does phonological short-term memory causally determine vocabulary learning? Toward a computational resolution of the debate},
journal = {Journal of Memory and Language},
volume = {61},
number = {4},
pages = {481-502},
year = {2009},
issn = {0749-596X},
doi = {https://doi.org/10.1016/j.jml.2009.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0749596X09000825},
author = {Prahlad Gupta and Jamie Tisdale},
keywords = {Nonword repetition, Vocabulary learning, Computational modeling, Phonological memory, Word learning, Short-term memory, Language},
abstract = {The relationship between nonword repetition ability and vocabulary size and vocabulary learning has been a topic of intense research interest and investigation over the last two decades, following the demonstration that nonword repetition accuracy is predictive of vocabulary size (Gathercole & Baddeley, 1989). However, the nature of this relationship is not well understood. One prominent account posits that phonological short-term memory (PSTM) is a causal determinant both of nonword repetition ability and of phonological vocabulary learning, with the observed correlation between the two reflecting the effect of this underlying third variable (e.g., Baddeley, Gathercole, & Papagno, 1998). An alternative account proposes the opposite causality: that it is phonological vocabulary size that causally determines nonword repetition ability (e.g., Snowling, Chiat, & Hulme, 1991). We present a theory of phonological vocabulary learning, instantiated as a computational model. The model offers a precise account of the construct of PSTM, of performance in the nonword repetition task, of novel word form learning, and of the relationship between all of these. We show through simulation not only that PSTM causally affects both nonword repetition accuracy and phonological vocabulary size, but also that phonological vocabulary size causally affects nonword repetition ability. The plausibility of the model is supported by the fact that its nonword repetition accuracy displays effects of phonotactic probability and of nonword length, which have been taken as evidence for causal effects on nonword repetition accuracy of phonological vocabulary knowledge and PSTM, respectively. Thus the model makes explicit how the causal links posited by the two theoretical perspectives are both valid, in the process reconciling the two perspectives, and indicating that an opposition between them is unnecessary.}
}
@incollection{HUDEDAGADDI2017233,
title = {Chapter 7 - Quantum inspired computational intelligent techniques in image segmentation},
editor = {Siddhartha Bhattacharyya and Ujjwal Maulik and Paramartha Dutta},
booktitle = {Quantum Inspired Computational Intelligence},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {233-258},
year = {2017},
isbn = {978-0-12-804409-4},
doi = {https://doi.org/10.1016/B978-0-12-804409-4.00007-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128044094000073},
author = {D.P. Hudedagaddi and B.K. Tripathy},
keywords = {Quantum computing, Computing intelligence, Image segmentation, Evolutionary algorithms},
abstract = {Quantum computing (QC) is a new area of research which incorporates elements from mathematics, physics, and computing. Quantum computing has generated a growing interest among scientists, technologists, and industrialists. Over the past decade it provided a platform for research to people in the scientific, technical, and industrial fields. Quantum physics concepts have been used in developing the basics of QC. In QC, the parallel processing feature has reduced the algorithm complexities which are being used. This feature helped find solutions to several optimization problems and issues that were related to it. Quantum inspired intelligent computational methods have been used in several application areas. Image segmentation is one such area and the exploration of this feature in image segmentation is the primary focus of this chapter.}
}
@incollection{SEJNOWSKI20012460,
title = {Computational Neuroscience},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {2460-2465},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/03419-7},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767034197},
author = {T.J. Sejnowski},
abstract = {The goal of computational neuroscience is to explain in computational terms how brains generate behaviors. Computational models of the brain explore how populations of highly interconnected neurons are formed during development and how they come to represent, process, store, act upon, and become altered by, information present in the environment. Techniques from computer science and mathematics are used to simulate and analyze these computational models and provide links between the widely ranging levels of investigation, from the molecular to the systems levels. Computational neuroscience is a relatively young discipline that is growing rapidly. Most of the models that have been developed thus far have been aimed at interpreting experimental data and providing a conceptual framework for the dynamic properties of neural systems. A more comprehensive theory of brain function should arise as we gain a broader understanding of the computational resources of nervous systems at all levels of organization.}
}
@article{HAMALAINEN2024100050,
title = {Generating policy alternatives for decision making: A process model, behavioural issues, and an experiment},
journal = {EURO Journal on Decision Processes},
volume = {12},
pages = {100050},
year = {2024},
issn = {2193-9438},
doi = {https://doi.org/10.1016/j.ejdp.2024.100050},
url = {https://www.sciencedirect.com/science/article/pii/S2193943824000062},
author = {Raimo P. Hämäläinen and Tuomas J. Lahtinen and Kai Virtanen},
keywords = {Policy decision, Generation of policy alternatives, Portfolio decision analysis, Path dependence, Cognitive biases and heuristics},
abstract = {The generation of alternative policies is essential in complex decision tasks with multiple interests and stakeholders. A diverse set of policies is typically desirable to cover the range of options and objectives. Decision modelling literature has often assumed that clearly defined decision alternatives are readily available. This is not a realistic assumption in practice. We present a structured process model for the generation of policy alternatives in settings that include non-quantifiable elements and where portfolio optimisation approaches are not applicable. Behavioural issues and path dependence as well as heuristics and biases which can occur during the process are discussed. The behavioural experiment compares policy alternatives obtained by using two different portfolio generation techniques. The results of the experiment demonstrate that path dependence can occur in policy generation. We report thinking patterns of subjects which relate to biases and heuristics.}
}
@article{ALPUENTE20153,
title = {Exploring conditional rewriting logic computations},
journal = {Journal of Symbolic Computation},
volume = {69},
pages = {3-39},
year = {2015},
note = {Symbolic Computation in Software Science},
issn = {0747-7171},
doi = {https://doi.org/10.1016/j.jsc.2014.09.028},
url = {https://www.sciencedirect.com/science/article/pii/S0747717114000960},
author = {M. Alpuente and D. Ballis and F. Frechina and J. Sapiña},
keywords = {Rewriting logic, Trace exploration, Maude, Conditional rewrite theories},
abstract = {Trace exploration is concerned with techniques that allow computation traces to be dynamically searched for specific contents. Depending on whether the exploration is carried backward or forward, trace exploration techniques allow provenance tracking or impact tracking to be done. The aim of provenance tracking is to show how (parts of) a program output depends on (parts of) its input and to help estimate which input data need to be modified to accomplish a change in the outcome. The aim of impact tracking is to identify the scope and potential consequences of changing the program input. Rewriting Logic (RWL) is a logic of change that supplements (an extension of) the equational logic by adding rewrite rules that are used to describe (nondeterministic) transitions between states. In this paper, we present a rich and highly dynamic, parameterized technique for the forward inspection of RWL computations that allows the nondeterministic execution of a given conditional rewrite theory to be followed up in different ways. With this technique, an analyst can browse, slice, filter, or search the traces as they come to life during the program execution. The navigation of the trace is driven by a user-defined, inspection criterion that specifies the required exploration mode. By selecting different inspection criteria, one can automatically derive a family of practical algorithms such as program steppers and more sophisticated dynamic trace slicers that compute summaries of the computation tree, thereby facilitating the dynamic detection of control and data dependencies across the tree. Our methodology, which is implemented in the Anima graphical tool, allows users to evaluate the effects of a given statement or instruction in isolation, track input change impact, and gain insight into program behavior (or misbehavior).}
}
@article{POPAT2019365,
title = {Learning to code or coding to learn? A systematic review},
journal = {Computers & Education},
volume = {128},
pages = {365-376},
year = {2019},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2018.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0360131518302768},
author = {Shahira Popat and Louise Starkey},
keywords = {Coding, Programming, School, Computer, Outcome, Skills},
abstract = {The resurgence of computer programming in the school curriculum brings a promise of preparing students for the future that goes beyond just learning how to code. This study reviewed research to analyse educational outcomes for children learning to code at school. A systematic review was applied to identify relevant articles and a thematic analysis to synthesise the findings. Ten articles were included in the synthesis and an overarching model was developed which depicts the themes. The results demonstrate that although students are learning to code, a range of other educational outcomes can be learnt or practiced through the teaching of coding. These included mathematical problem-solving, critical thinking, social skills, self-management and academic skills. The review also identified the importance of instructional design for developing these educational outcomes through coding.}
}
@article{PALMERI2004378,
title = {Computational approaches to the development of perceptual expertise},
journal = {Trends in Cognitive Sciences},
volume = {8},
number = {8},
pages = {378-386},
year = {2004},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2004.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1364661304001603},
author = {Thomas J. Palmeri and Alan C-N. Wong and Isabel Gauthier},
abstract = {Dog experts, ornithologists, radiologists and other specialists are noted for their remarkable abilities at categorizing, identifying and recognizing objects within their domain of expertise. A complete understanding of the development of perceptual expertise requires a combination of thorough empirical research and carefully articulated computational theories that formalize specific hypotheses about the acquisition of expertise. A comprehensive computational theory of the development of perceptual expertise remains elusive, but we can look to existing computational models from the object-recognition, perceptual-categorization, automaticity and related literatures for possible starting points. Arguably, hypotheses about the development of perceptual expertise should first be explored within the context of existing computational models of visual object understanding before considering the creation of highly modularized adaptations for particular domains of perceptual expertise.}
}
@article{NUNES2020117761,
title = {Thinking the future of membranes: Perspectives for advanced and new membrane materials and manufacturing processes},
journal = {Journal of Membrane Science},
volume = {598},
pages = {117761},
year = {2020},
issn = {0376-7388},
doi = {https://doi.org/10.1016/j.memsci.2019.117761},
url = {https://www.sciencedirect.com/science/article/pii/S0376738819333113},
author = {Suzana P. Nunes and P. Zeynep Culfaz-Emecen and Guy Z. Ramon and Tymen Visser and Geert Henk Koops and Wanqin Jin and Mathias Ulbricht},
abstract = {The state-of-the-art of membrane technology is characterized by a number of mature applications such as sterile filtration, hemodialysis, water purification and gas separation, as well as many more niche applications of successful membrane-based separation and processing of fluid mixtures. The membrane industry is currently employing a portfolio of established materials, mostly standard polymers or inorganic materials (not originally developed for membranes), and easily scalable manufacturing processes such as phase inversion, interfacial polymerization and coating. Innovations in membranes and their manufacturing processes must meet the desired intrinsic properties that determine selectivity and flux, for specific applications. However, tunable and stable performance, as well as sustainability over the entire life cycle of membrane products are becoming increasingly important. Membrane manufacturers are progressively required to share the carbon footprint of their membrane modules with their customers. Environmental awareness among the world's population is a growing phenomenon and finds its reflection in product development and manufacturing processes. In membrane technology one can see initial steps in this direction with the replacement of hazardous solvents, the utilization of renewable materials for membrane production and the reuse of membrane modules. Other examples include increasing the stability of organic membrane polymers and lowering the cost of inorganic membranes. In a long-term perspective, many more developments in materials science will be required for making new, advanced membranes. These include “tools” such as self-assembly or micro- and nano-fabrication, and “building blocks”, e.g. tailored block copolymers or 1D, 2D and 3D materials. Such membranes must be fabricated in a simpler manner and be more versatile than existing ones. In this perspective paper, a vision of such LEGO®-like membranes with precisely adjustable properties will be illustrated with, where possible, examples that already demonstrate feasibility. These include the possibility to switch properties using an external stimulus, adapting a membrane's selectivity to a given separation, or providing the ability to assemble, disassemble and reassemble the membrane on a suitable support as scaffold, in situ, in place and on-demand. Overall, it is foreseen that the scope of future membrane applications will become much wider, based on improved existing membrane materials and manufacturing processes, as well as the combination of novel, tailor-made “building blocks” and “tools” for the fabrication of next-generation membranes tuned to specific applications.}
}
@article{GROTH20211712,
title = {A systems-based framework to computationally describe putative transcription factors and signaling pathways regulating glycan biosynthesis},
journal = {Beilstein Journal of Organic Chemistry},
volume = {17},
pages = {1712-1724},
year = {2021},
issn = {1860-5397},
doi = {https://doi.org/10.3762/bjoc.17.119},
url = {https://www.sciencedirect.com/science/article/pii/S186053972102209X},
author = {Theodore Groth and Rudiyanto Gunawan and Sriram Neelamegham},
keywords = {ChIP-Seq, glycoinformatics, glycosylation, TCGA transcription factor},
abstract = {Glycosylation is a common posttranslational modification, and glycan biosynthesis is regulated by a set of glycogenes. The role of transcription factors (TFs) in regulating the glycogenes and related glycosylation pathways is largely unknown. In this work, we performed data mining of TF–glycogene relationships from the Cistrome Cancer database (DB), which integrates chromatin immunoprecipitation sequencing (ChIP-Seq) and RNA-Seq data to constitute regulatory relationships. In total, we observed 22,654 potentially significant TF–glycogene relationships, which include interactions involving 526 unique TFs and 341 glycogenes that span 29 the Cancer Genome Atlas (TCGA) cancer types. Here, TF–glycogene interactions appeared in clusters or so-called communities, suggesting that changes in single TF expression during both health and disease may affect multiple carbohydrate structures. Upon applying the Fisher’s exact test along with glycogene pathway classification, we identified TFs that may specifically regulate the biosynthesis of individual glycan types. Integration with Reactome DB knowledge provided an avenue to relate cell-signaling pathways to TFs and cellular glycosylation state. Whereas analysis results are presented for all 29 cancer types, specific focus is placed on human luminal and basal breast cancer disease progression. Overall, the article presents a computational approach to describe TF–glycogene relationships, the starting point for experimental system-wide validation.}
}
@article{SCHERER2020106349,
title = {A meta-analysis of teaching and learning computer programming: Effective instructional approaches and conditions},
journal = {Computers in Human Behavior},
volume = {109},
pages = {106349},
year = {2020},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2020.106349},
url = {https://www.sciencedirect.com/science/article/pii/S0747563220301023},
author = {Ronny Scherer and Fazilat Siddiq and Bárbara {Sánchez Viveros}},
keywords = {Computational thinking, Computer programming, Intervention studies, Multilevel meta-analysis, Scratch programming},
abstract = {This meta-analysis maps the evidence on the effectiveness of instructional approaches and conditions for learning computer programming under three study conditions: (a) Studies focusing on the effectiveness of programming interventions per se, (b) studies focusing on the effectiveness of visualization and physicality, and (c) studies focusing on the effectiveness of dominant instructional approaches. Utilizing the data from 139 interventions and 375 effect sizes, we found (a) a strong effect of learning computer programming per se (Hedges’ g‾ = 0.81, 95% CI [0.42, 1.21]), (b) moderate to large effect sizes of visualization (g‾ = 0.44, 95% CI [0.29, 0.58]) and physicality interventions (g‾ = 0.72, 95% CI [0.23, 1.21]), and (c) moderate to large effect sizes for studies focusing on dominant instructional approaches (g‾s = 0.49–1.02). Moderator analyses indicated that the effect sizes differed only marginally between the instructional approaches and conditions—however, collaboration in metacognition instruction, problem solving instruction outside of regular lessons, short-term interventions focusing on physicality, and interventions focusing on visualization through Scratch were especially effective. Our meta-analysis synthesizes the existing research evidence on the effectiveness of computer programming instruction and, ultimately, provides references with which the effects of future studies could be compared.}
}
@article{JI20074338,
title = {A fuzzy logic-based computational recognition-primed decision model},
journal = {Information Sciences},
volume = {177},
number = {20},
pages = {4338-4353},
year = {2007},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2007.02.026},
url = {https://www.sciencedirect.com/science/article/pii/S0020025507001193},
author = {Yanqing Ji and R. Michael Massanari and Joel Ager and John Yen and Richard E. Miller and Hao Ying},
keywords = {Medical decision-making, Naturalistic decision-making, Recognition-primed decision model, Computational recognition-primed decision model, Experience-based reasoning, Adverse drug reactions, Fuzzy logic, Similarity measure},
abstract = {The recognition-primed decision (RPD) model is a primary naturalistic decision-making approach which seeks to explicitly recognize how human decision makers handle complex tasks and environment based on their experience. Motivated by the need for quantitative computer modeling and simulation of human decision processes in various application domains, including medicine, we have developed a general-purpose computational fuzzy RPD model that utilizes fuzzy sets, fuzzy rules, and fuzzy reasoning to represent, interpret, and compute imprecise and subjective information in every aspect of the model. Experiences acquired by solicitation with experts are stored in experience knowledge bases. New local and global similarity measures have been developed to identify the experience that is most applicable to the current situation in a specific decision-making context. Furthermore, an action evaluation strategy has been developed to select the workable course of action. The proposed fuzzy RPD model has been preliminarily validated by using it to calculate the extent of causality between a drug (Cisapride, withdrawn by the FDA from the market in 2000) and some of its adverse effects for 100 hypothetical patients. The simulated patients were created based on the profiles of over 1000 actual patients treated with the drug at our medical center before its withdrawal. The model validity was demonstrated by comparing the decisions made by the proposed model and those by two independent internists. The levels of agreement were established by the weighted Kappa statistic and the results suggested good to excellent agreement.}
}
@incollection{GARDNER20243,
title = {Chapter 1 - Scaling the smart city},
editor = {Nicole Gardner},
booktitle = {Scaling the Smart City},
publisher = {Elsevier},
pages = {3-25},
year = {2024},
series = {Smart Cities},
isbn = {978-0-443-18452-9},
doi = {https://doi.org/10.1016/B978-0-443-18452-9.00001-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044318452900001X},
author = {Nicole Gardner},
keywords = {Cyber-physical system, Design, Interaction, IoT, Scalar, Scale, Scaling, Smart city, Techno-urban imaginary, Urban design, Urban technology},
abstract = {This chapter explores the smart city through the conceptual lens of scale, as a scale-making project and as a project that is subject to scaling processes. It explores how scalar notions figure in smart city discourses, and how the drive to scale shapes the prevailing approach to digital technology and urban space integration. It argues that deprioritizing the smart city's scalability logic can bring into view different ways of designing the integration of digital technologies and urban space that can better connect with the contextual and material specificities of local contexts and less attended to dimensions of urban livability. Rescaling the smart city to the local urban precinct scale and paying close attention to life-technology relations is further reasoned as a way to productively re-orient and extend thinking on the ethical significance of the smart city.}
}
@article{RIESENFELD20151054,
title = {Initiating a CAD renaissance: Multidisciplinary analysis driven design: Framework for a new generation of advanced computational design, engineering and manufacturing environments},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {284},
pages = {1054-1072},
year = {2015},
note = {Isogeometric Analysis Special Issue},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2014.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S0045782514004502},
author = {Richard F. Riesenfeld and Robert Haimes and Elaine Cohen},
keywords = {Multidisciplinary analysis driven design, Integrated computational engineering, CAD/CAE/CAM/IGA},
abstract = {We present a critical analysis of the effectiveness of the current field of CAD, and discuss some of the forces that have taken it so far off course from its strikingly foresighted origins. Armed with the ensuing understanding of the operational forces that have taken CAD adrift, we conclude that the disparity between CAD’s mired state-of-the-art condition relative to more appropriate, inspired and achievable goals for CAD calls for more drastic measures. It is asserted that, well beyond the evolutionary progression of incremental steps characteristic of next version system releases, the field is overdue for developing a class of genuine design-centric, ab initio, CAD systems architectures effecting the original CAD vision through the powerful instruments of contemporary computing tools and technologies.}
}
@article{ANDERSON2015309,
title = {Reading visually embodied meaning from the brain: Visually grounded computational models decode visual-object mental imagery induced by written text},
journal = {NeuroImage},
volume = {120},
pages = {309-322},
year = {2015},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2015.06.093},
url = {https://www.sciencedirect.com/science/article/pii/S1053811915006345},
author = {Andrew James Anderson and Elia Bruni and Alessandro Lopopolo and Massimo Poesio and Marco Baroni},
keywords = {Concept representation, Embodiment, Mental imagery, Perceptual simulation, Language, Multimodal semantic models, Representational similarity},
abstract = {Embodiment theory predicts that mental imagery of object words recruits neural circuits involved in object perception. The degree of visual imagery present in routine thought and how it is encoded in the brain is largely unknown. We test whether fMRI activity patterns elicited by participants reading objects' names include embodied visual-object representations, and whether we can decode the representations using novel computational image-based semantic models. We first apply the image models in conjunction with text-based semantic models to test predictions of visual-specificity of semantic representations in different brain regions. Representational similarity analysis confirms that fMRI structure within ventral-temporal and lateral-occipital regions correlates most strongly with the image models and conversely text models correlate better with posterior-parietal/lateral-temporal/inferior-frontal regions. We use an unsupervised decoding algorithm that exploits commonalities in representational similarity structure found within both image model and brain data sets to classify embodied visual representations with high accuracy (8/10) and then extend it to exploit model combinations to robustly decode different brain regions in parallel. By capturing latent visual-semantic structure our models provide a route into analyzing neural representations derived from past perceptual experience rather than stimulus-driven brain activity. Our results also verify the benefit of combining multimodal data to model human-like semantic representations.}
}
@article{DORKO2023101036,
title = {Is it a function? Generalizing from single- to multivariable settings},
journal = {The Journal of Mathematical Behavior},
volume = {70},
pages = {101036},
year = {2023},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2023.101036},
url = {https://www.sciencedirect.com/science/article/pii/S0732312323000068},
author = {Allison Dorko},
keywords = {Actor-oriented transfer, Generalization, Multivariable calculus, Functions of two variables, Multivariable functions},
abstract = {Generalizing is a hallmark of mathematical thinking. The term ‘generalization’ is used to mean both the process of generalizing and the product of that process. This paper reports on five calculus students’ generalizing activity and what they generalized about multivariable functions. The study makes two contributions. The first is a fine-grained, actor-oriented characterization of the ways undergraduates generalized. This adds to knowledge in two areas: the use of the actor-oriented perspective and generalization in advanced mathematics. The second contribution is the products of students’ generalizing: what they generalized about what it means for a multivariable relation to represent a function). This adds to the literature about student reasoning regarding multivariable topics by characterizing the powerful ways of reasoning students possess pre-instruction.}
}
@article{BAWDEN1984205,
title = {Systems thinking and practices in the education of agriculturalists},
journal = {Agricultural Systems},
volume = {13},
number = {4},
pages = {205-225},
year = {1984},
issn = {0308-521X},
doi = {https://doi.org/10.1016/0308-521X(84)90074-X},
url = {https://www.sciencedirect.com/science/article/pii/0308521X8490074X},
author = {Richard J. Bawden and Robert D. Macadam and Roger J. Packham and Ian Valentine},
abstract = {A systems approach has been taken to a review of agricultural education programmes and as the essential theme of resultant curricula at Hawkesbury Agricultural College in Australia. The systems thinking and practices which have guided, and been shaped by, the innovations are outlined, and the rationale and framework of the major programme are described. The subsequent emphasis has been placed on effective learning for agricultural managers and their technologist advisors. It is argued that problem solving and learning are essentially the same psychological processes and that taking a systems approach to investigating problem situations provides a more useful paradigm for learning about agriculture than reductionist, discipline-based approaches. Experiential learning and autonomy in learning are seen as consistent with this and are basic features of the programmes. A conceptual framework for problem solving that incorporates soft and hard systems and scientific reductionist methodologies has been developed. A contingency approach to situation improving is emerging as a less restrictive and more realistic alternative to a normative approach to problem solving.}
}
@article{ANDERSON201738,
title = {Isolating blocks as computational tools in the circular restricted three-body problem},
journal = {Physica D: Nonlinear Phenomena},
volume = {343},
pages = {38-50},
year = {2017},
issn = {0167-2789},
doi = {https://doi.org/10.1016/j.physd.2016.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167278916303013},
author = {Rodney L. Anderson and Robert W. Easton and Martin W. Lo},
keywords = {Circular restricted three-body problem, Isolating blocks, Invariant manifolds, Invariant 3-sphere},
abstract = {Isolating blocks may be used as computational tools to search for the invariant manifolds of orbits and hyperbolic invariant sets associated with libration points while also giving additional insight into the dynamics of the flow in these regions. We use isolating blocks to investigate the dynamics of objects entering the Earth–Moon system in the circular restricted three-body problem with energies close to the energy of the L2 libration point. Specifically, the stable and unstable manifolds of Lyapunov orbits and the hyperbolic invariant set around the libration points are obtained by numerically computing the way orbits exit from an isolating block in combination with a bisection method. Invariant spheres of solutions in the spatial problem may then be located using the resulting manifolds.}
}
@article{AGGARWAL2023110458,
title = {Quantum healthcare computing using precision based granular approach},
journal = {Applied Soft Computing},
volume = {144},
pages = {110458},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110458},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623004763},
author = {Lakshita Aggarwal and Shelly Sachdeva and Puneet Goswami},
keywords = {Quantum computing, Qubits, Healthcare, Diagnosis, Classical computing, Precision},
abstract = {Previously, doctors interpreted diseases and their outcomes according to their experience in diagnosis. However, with the rapid increase in technology and population, the task of examining the patient becomes cumbersome and sometimes human efforts produce inconsistent results. Several research is being done for healthcare in terms of improving visualization and accuracy by using machine learning models. The current research targets to explore quantum computing as a different way of processing information compared to classical computer systems such as the use of quantum bits (qubits) along with superposition and entanglement for extending the computation capabilities at an unprecedented level of thinking in the healthcare domain. Quantum computing systems provide exponential benefits in terms of high-speed processing, faster and easier diagnostic assistance, unimaginable reduction in processing throughput, and many more. An extensive comparative analysis of existing approaches has been made which benchmarks the need for quantum healthcare computing. The objective of this work is to interpret whether Quantum computers prove to be more trusted when it comes to patient diagnosis, and faster analysis leading to cost optimization. In order to accelerate patient diagnosis, different approaches have been presented. The authors have proposed a precision-based granular approach for patient diagnosis that incorporates diagnosing the disease with enhanced precision and granularity. It involves reporting symptoms by the patient, encountering by healthcare expert on multiple factors, precise examination, granular health status (understanding past and present medical history), followed by a precise intervention by understanding biomolecular simulations. The algorithm has been presented to describe the flow process for patient diagnosis modeling using quantum computing. It involves qubits initialization, pairing the values, assigning probabilistic values, cross-validation, and quantum circuit formation. Precision-based granular approach has been implemented for a scenario (consisting of medical parameters such as oxygen and heart rate level, with the functionality of diagnosing oxygen level and heart range which lies as either normal or not normal (high/low)). Precision-based granular approach deals specifically with the individual ‘biomolecular simulation by understanding variations in the individual body whereas the umbrella-based approach does not deal with specifically to individual mechanisms. Granular level of encounter is not possible in umbrella-based treatment. Python Jupyter notebook and IBM Composer tool is used for the implementation of results. Bloch sphere and computational state graph are obtained as an output for better visualization and understanding. Falcon r5.11H processor is used with the version of 1.0.24 of IBM Composer to simulate the experiment. The methodology using precision based granular approach provides timely encounter of disease along with umbrella diagnosis and precise treatment. The time is taken and frequency of qubits have been presented with promising results. The diagnosis process and optimizing cost efficiency can aid in an early detection of the disease.}
}
@article{KAWITI2025100213,
title = {Indigenous knowledge, architecture, and nature in the context of Oceania},
journal = {Nature-Based Solutions},
volume = {7},
pages = {100213},
year = {2025},
issn = {2772-4115},
doi = {https://doi.org/10.1016/j.nbsj.2025.100213},
url = {https://www.sciencedirect.com/science/article/pii/S2772411525000035},
author = {Derek Kawiti and Albert Refiti and Amanda Yates and Elisapeta Heta and Sibyl Bloomfield and Victoria Chanse and Maibritt Pedersen Zari},
keywords = {Pacific, Indigenous, Architecture, Ecology, Climate change adaptation, Māori, Samoan},
abstract = {This perspective article is derived from conversations between leading Indigenous academics and practitioners in the fields of architecture and urban design recorded at a keynote panel at the 2023 NUWAO International Symposium on Nature-based Urban Climate Adaptation for Wellbeing, held at Te Herenga Waka Victoria University of Wellington, Aotearoa New Zealand. The focus of the discussion was Indigenous design for adaptation to climate change in Moananui Oceania with an emphasis on relationships to nature. Given the diversity of Moananui Oceania in terms of languages, cultures, histories, and worldviews, this discussion represented a unique convergence of Indigenous leadership and thought in the field. It highlighted key themes related to Indigenous design for climate change adaptation and offered a novel, distinctive perspective aimed at advancing thinking around nature-based solutions (NbS). It is important to recognise and integrate Indigenous values and approaches to knowledge generation, particularly within academic settings. In the context of Moananui Oceania this can require adapting oral traditions and formats, such as talanoa, and hui or kōrero, into conventional Western-based research formats such as the journal article. This paper is an attempt to capture important Indigenous knowledge and discussion in a western format to enable further dissemination and sharing. This means the format and methodologies described in the paper do not align exactly with traditional scientific journal article formats, however the discussions and findings help to meet the motivation of the authors, which is to transform traditional Indigenous ways of sharing information into a perspective article format and share insights with a wider audience. This methodology aligns well with the special issue call that this paper resides in (Just, Socio-ecological Urban Transformation: Nature-based Solutions and Traditional Ecological Knowledge), underpinning the relevance and potential contribution to the field. Two key themes were explored within the context of the importance of working with nature; relationships between ecologies and tikanga (customary practices), and looking backwards to generate innovation and resilience.}
}
@article{TURHAN20075237,
title = {Statistical and computational intelligence tools for the analyses of warp tension in different back-rest oscillations},
journal = {Information Sciences},
volume = {177},
number = {23},
pages = {5237-5252},
year = {2007},
note = {Including: Mathematics of Uncertainty},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2007.06.029},
url = {https://www.sciencedirect.com/science/article/pii/S0020025507003246},
author = {Yıldıray Turhan and Sezai Tokat and Recep Eren},
keywords = {Neural networks, Radial basis function, Cross-validation, Data regression, Warp tension, Back-rest oscillation, Weft density},
abstract = {In this paper, experimental, computational intelligence based and statistical investigations of warp tensions in different back-rest oscillations are presented. Firstly, in the experimental stage, springs having different stiffnesses are used to obtain different back-rest oscillations. For each spring, fabrics are woven in different weft densities and the warp tensions are measured and saved during weaving process. Secondly, in the statistical investigation, the experimental data are analyzed by using linear multiple and quadratic multiple-regression models. Later, in the computational intelligence based investigation, the data obtained from the experimental study are analyzed by using artificial neural networks that are universal approximators which provide a massively parallel processing and decentralized computing. Specially, radial basis function neural network structure is chosen. In this structure, cross-validation technique is used in order to determine the number of radial basis functions. Finally, the results of regression analysis, the computational intelligence based analysis and experimental measurements are compared by using the coefficient of determination. From the results, it is shown that the computational intelligence based analysis indicates a better agreement with the experimental measurement than the statistical analysis.}
}
@article{CARROLL1999111,
title = {Invented Computational Procedures of Students in a Standards-Based Curriculum},
journal = {The Journal of Mathematical Behavior},
volume = {18},
number = {2},
pages = {111-121},
year = {1999},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(99)00024-3},
url = {https://www.sciencedirect.com/science/article/pii/S0732312399000243},
author = {William M. Carroll},
abstract = {Fourth graders who had been in a standards-based elementary curriculum since kindergarten were individually interviewed and administered a whole-class test that probed their knowledge of facts and multidigit computation. Standard algorithms are not taught as part of the curriculum, which instead emphasizes student-invented procedures and discussions of solution methods. Of interest were the types of student-invented procedures that were used as well as their computational accuracy. Students used several procedures that involved sophisticated mental calculation strategies, such as decomposing numbers or adding from left to right. Many students also used the standard written algorithms. Both invented and standard algorithms used by the students were highly accurate, although invented procedures often indicated better mental flexibility and awareness of place value. On the written test, students' computational abilities were above national normative levels.}
}
@incollection{SLEPIAN2024516,
title = {4.01 - Synergistic Approaches of Cross-Fertilization and Feedback Together Driving and Advancing Health for All},
editor = {Kenneth S. Ramos},
booktitle = {Comprehensive Precision Medicine (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {516-523},
year = {2024},
isbn = {978-0-12-824256-8},
doi = {https://doi.org/10.1016/B978-0-12-824010-6.00081-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128240106000812},
author = {Marvin J. Slepian},
keywords = {Digital Health, Multiscale, Nested pots, Personalized medicine, Pharmacogenomics, Point-of-care, Precision medicine, System synergies, Systems biology, Wearable technologies},
abstract = {We are at a point in time with rapid advances occurring in digital technologies, developing a range of new quantifiable markers termed “digital biomarkers,” which are increasingly utilized for diagnostics, as well as defining new operative mechanisms of health and disease. In parallel, significant advances have occurred in precision medicine, utilizing breakthroughs in “omics biology,” coupled with our understanding of their impact across systems in “systems biology.” Contemporaneously, a new approach to thinking of how health and disease evolve and impact an individual has emerged—that of considering mechanisms and impact across scales, i.e. on a “multi-scale” level, extending from the patient down to the molecule, and similarly from the patient up to society. In this chapter details of each of these approaches, their evolution and key current concepts are outlined. Moreover, the main theme and postulate developed in this chapter outlines the interconnectedness and the way in which each approach informs each other. In essence a cyclic, reinforcing, feedback loop exists, connecting digital technologies with precision and personalization approaches, across systems and scales, leading to enhanced diagnostics, the potential for new therapeutics and increasing insight into mechanisms. This cyclic flow of information will lead to new, more exacting technologies, with the ultimate outcome of enhanced efficacy, safety and improved health outcomes for patients and society.}
}
@article{JEYASUBRAMANIAN2021120243,
title = {A complete review on biochar: Production, property, multifaceted applications, interaction mechanism and computational approach},
journal = {Fuel},
volume = {292},
pages = {120243},
year = {2021},
issn = {0016-2361},
doi = {https://doi.org/10.1016/j.fuel.2021.120243},
url = {https://www.sciencedirect.com/science/article/pii/S0016236121001198},
author = {K. Jeyasubramanian and B. Thangagiri and A. Sakthivel and J. {Dhaveethu Raja} and S. Seenivasan and P. Vallinayagam and D. Madhavan and S. {Malathi Devi} and B. Rathika},
keywords = {Biochar, Engineered Biochar, Biomass, Applications, Production methods, Mechanism},
abstract = {Burning crop residues release large amounts of greenhouse gases, particulate matter, carbon monoxide, etc. which influence a lot of environmental issues that are hazardous to all living beings including humans. One of the useful methods of using crop residues is in the form of biochar obtained after employing thermo-chemical routes. Apart from the crop residues, the carbon rich contents obtained from forest, animal compost, waste plastics, etc., can be heated in oxygen starved atmosphere, that left char having enriched carbon and trace amount of minerals finds extensive applications in agriculture, especially to make soil more fertile, as a carbon sequestration agent, increases crops yields, etc. This review focuses on the synthetic strategies adopted to obtain biochar, and the numerous applications in various fields. Further, the interactions involved in between the host–guest molecules are explained using computational studies like Density Functional Theory, Artificial Neural Network analysis, Machine Learning methods, and Visual MINTEQ program.}
}
@article{ZHOU2025103462,
title = {Co-design of analogical and embodied representations with children for child-centered AI learning experiences},
journal = {International Journal of Human-Computer Studies},
volume = {199},
pages = {103462},
year = {2025},
issn = {1071-5819},
doi = {https://doi.org/10.1016/j.ijhcs.2025.103462},
url = {https://www.sciencedirect.com/science/article/pii/S1071581925000199},
author = {Xiaofei Zhou and Yunfan Gong and Yushan Zhou and Yufei Jiang and Zhen Bai},
keywords = {Embodied learning, Analogical learning, AI recommendation systems, The filter bubble, Augmented reality, Children},
abstract = {AI recommendations shape our daily decisions and our young generation is no exception. The convenience of navigating personalized content comes with the notorious “filter bubble” effect, which can reduce exposure to diverse options and opinions. Children are particularly vulnerable to this due to their limited AI literacy and critical thinking skills. In this study, we explore how to engage children as co-designers to create child-centered experiences for learning AI concepts related to the filter bubble. Leveraging embodied and analogical learning theories, we co-designed an Augmented Reality (AR) application, BeeTrap, with children from underrepresented backgrounds in STEM. BeeTrap not only raises awareness of filter bubbles but also empowers children to understand recommendation system mechanisms. Our contributions include (1) insights into child-centered AI learning using embodied metaphors and analogies as educational representations of AI concepts; and (2) implications for enhancing children’s understanding of AI concepts through co-design processes.}
}
@article{BAR202135,
title = {Wanted: Architecture for changing minds: A comment on “The growth of cognition: Free energy minimization and the embryogenesis of cortical computation”},
journal = {Physics of Life Reviews},
volume = {36},
pages = {35-36},
year = {2021},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2020.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S1571064520300683},
author = {Moshe Bar}
}
@article{HALLINAN2001506,
title = {Thinking Beyond the Fringe},
journal = {Trends in Cognitive Sciences},
volume = {5},
number = {12},
pages = {506-507},
year = {2001},
issn = {1364-6613},
doi = {https://doi.org/10.1016/S1364-6613(00)01802-7},
url = {https://www.sciencedirect.com/science/article/pii/S1364661300018027},
author = {Jennifer Hallinan},
keywords = {explicit models of memory, stochastic generative approach, evolution of the neural modularity, metarepresentation}
}
@article{BRIDGES2012780,
title = {Thinking Outside the Cleft to Understand Synaptic Activity: Contribution of the Cystine-Glutamate Antiporter (System xc−) to Normal and Pathological Glutamatergic Signaling},
journal = {Pharmacological Reviews},
volume = {64},
number = {3},
pages = {780-802},
year = {2012},
issn = {0031-6997},
doi = {https://doi.org/10.1124/pr.110.003889},
url = {https://www.sciencedirect.com/science/article/pii/S0031699724010159},
author = {Richard Bridges and Victoria Lutgen and Doug Lobner and David A. Baker},
abstract = {System xc− represents an intriguing target in attempts to understand the pathological states of the central nervous system. Also called a cystine-glutamate antiporter, system xc− typically functions by exchanging one molecule of extracellular cystine for one molecule of intracellular glutamate. Nonvesicular glutamate released during cystine-glutamate exchange activates extrasynaptic glutamate receptors in a manner that shapes synaptic activity and plasticity. These findings contribute to the intriguing possibility that extracellular glutamate is regulated by a complex network of release and reuptake mechanisms, many of which are unique to glutamate and rarely depicted in models of excitatory signaling. Because system xc− is often expressed on non-neuronal cells, the study of cystine-glutamate exchange may advance the emerging viewpoint that glia are active contributors to information processing in the brain. It is noteworthy that system xc− is at the interface between excitatory signaling and oxidative stress, because the uptake of cystine that results from cystine-glutamate exchange is critical in maintaining the levels of glutathione, a critical antioxidant. As a result of these dual functions, system xc− has been implicated in a wide array of central nervous system diseases ranging from addiction to neurodegenerative disorders to schizophrenia. In the current review, we briefly discuss the major cellular components that regulate glutamate homeostasis, including glutamate release by system xc−. This is followed by an in-depth discussion of system xc− as it relates to glutamate release, cystine transport, and glutathione synthesis. Finally, the role of system xc− is surveyed across a number of psychiatric and neurodegenerative disorders.}
}
@article{COSTA20055,
title = {Interactive Computation: Stepping Stone in the Pathway From Classical to Developmental Computation},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {141},
number = {5},
pages = {5-31},
year = {2005},
note = {Proceedings of the Workshop on the Foundations of Interactive Computation (FInCo 2005)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2005.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S157106610505187X},
author = {Antônio Carlos da Rocha Costa and Graçaliz Pereira Dimuro},
keywords = {Interactive computation, developmental computation, domain theory, classical theory of computation},
abstract = {This paper reviews and extends previous work on the domain-theoretic notion of Machine Development. It summarizes the concept of Developmental Computation and shows how Interactive Computation can be understood as a stepping stone in the pathway from Classical to Developmental Computation. A critical appraisal is given of Classical Computation, showing in which ways its shortcomings tend to restrict the possible evolution of real computers, and how Interactive and Developmental Computation overcome such shortcomings. The idea that Developmental Computation is more encompassing than Interactive Computation is stressed. A formal framework for Developmental Computation is sketched, and the current frontier of the work on Developmental Computation is briefly exposed.}
}
@article{LITTLE20031285,
title = {The computational science major at SUNY Brockport},
journal = {Future Generation Computer Systems},
volume = {19},
number = {8},
pages = {1285-1292},
year = {2003},
note = {Selected papers from the Workshop on Education in Computational Sciences held at the International Conference on Computational Science},
issn = {0167-739X},
doi = {https://doi.org/10.1016/S0167-739X(03)00086-4},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X03000864},
author = {Leigh J. Little},
keywords = {Computational science, Education, Undergraduate, Graduate},
abstract = {The field of computational science is a recent addition to academic study. While the content of such an education is generally agreed upon, effective methods for imparting this knowledge are still being investigated. This paper describes the current state of the computational science degree programs at SUNY Brockport and the successes that have been obtained. Issues relating to the implementation of such programs in the context of a small, liberal arts college are also discussed.}
}
@article{HASUO2017404,
title = {Semantics of higher-order quantum computation via geometry of interaction},
journal = {Annals of Pure and Applied Logic},
volume = {168},
number = {2},
pages = {404-469},
year = {2017},
note = {Eighth Games for Logic and Programming Languages Workshop (GaLoP)},
issn = {0168-0072},
doi = {https://doi.org/10.1016/j.apal.2016.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S0168007216301336},
author = {Ichiro Hasuo and Naohiko Hoshino},
keywords = {Higher-order computation, Quantum computation, Programming language, Geometry of interaction, Denotational semantics, Categorical semantics},
abstract = {While much of the current study on quantum computation employs low-level formalisms such as quantum circuits, several high-level languages/calculi have been recently proposed aiming at structured quantum programming. The current work contributes to the semantical study of such languages by providing interaction-based semantics of a functional quantum programming language; the latter is, much like Selinger and Valiron's, based on linear lambda calculus and equipped with features like the ! modality and recursion. The proposed denotational model is the first one that supports the full features of a quantum functional programming language; we prove adequacy of our semantics. The construction of our model is by a series of existing techniques taken from the semantics of classical computation as well as from process theory. The most notable among them is Girard's Geometry of Interaction (GoI), categorically formulated by Abramsky, Haghverdi and Scott. The mathematical genericity of these techniques—largely due to their categorical formulation—is exploited for our move from classical to quantum.}
}
@article{EVINS2013230,
title = {A review of computational optimisation methods applied to sustainable building design},
journal = {Renewable and Sustainable Energy Reviews},
volume = {22},
pages = {230-245},
year = {2013},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2013.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S1364032113000920},
author = {Ralph Evins},
keywords = {Review, Optimisation, Sustainable, Building, Design, Multi-objective},
abstract = {This paper presents a comprehensive review of all significant research applying computational optimisation to sustainable building design problems. A summary of common heuristic optimisation algorithms is given, covering direct search, evolutionary methods and other bio-inspired algorithms. The main summary table covers 74 works that focus on the application of these methods to different fields of sustainable building design. Key fields are reviewed in detail: envelope design, including constructions and form; configuration and control of building systems; renewable energy generation; and holistic optimisations of several areas simultaneously, with particular focus on residential and retrofit. Improvements to the way optimisation is applied are also covered, including platforms and frameworks, algorithmic comparisons and developments, use of meta-models and incorporation of uncertainty. Trends, including the rise of multi-objective optimisation, are analysed graphically. Likely future developments are discussed.}
}
@incollection{VASSILOPOULOS2020349,
title = {10 - Computational intelligence methods for the fatigue life modeling of composite materials},
editor = {Anastasios P. Vassilopoulos},
booktitle = {Fatigue Life Prediction of Composites and Composite Structures (Second Edition)},
publisher = {Woodhead Publishing},
edition = {Second Edition},
pages = {349-383},
year = {2020},
series = {Woodhead Publishing Series in Composites Science and Engineering},
isbn = {978-0-08-102575-8},
doi = {https://doi.org/10.1016/B978-0-08-102575-8.00010-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780081025758000103},
author = {Anastasios P. Vassilopoulos and Efstratios F. Georgopoulos},
keywords = {Fatigue, Composites, Artificial neural network, Genetic programming, ANFIS, S-N curves},
abstract = {Novel computational methods such as artificial neural networks, adaptive neuro-fuzzy inference systems and genetic programming are used in this chapter for the modeling of the nonlinear behavior of composite laminates subjected to constant amplitude loading. The examined computational methods are stochastic nonlinear regression tools, and can therefore be used to model the fatigue behavior of any material, provided that sufficient data are available for training. They are material independent methods that simply follow the trend of the available data, in each case giving the best estimate of their behavior. Application on a wide range of experimental data gathered after fatigue testing glass/epoxy and glass/polyester laminates proved that their modeling ability compares favorably with, and is to some extent superior to, other modeling techniques.}
}
@article{BRIMKOV2005233,
title = {Exact Image Reconstruction from a Single Projection through Real Computation},
journal = {Electronic Notes in Discrete Mathematics},
volume = {20},
pages = {233-246},
year = {2005},
note = {Proceedings of the Workshop on Discrete Tomography and its Applications},
issn = {1571-0653},
doi = {https://doi.org/10.1016/j.endm.2005.05.066},
url = {https://www.sciencedirect.com/science/article/pii/S1571065305050705},
author = {Valentin E. Brimkov and Reneta P. Barneva},
keywords = {Discrete tomography, Computed tomography, Algebraic computation model, Algebraic complexity, Linear Diophantine equation},
abstract = {In Discrete Tomography one aims to reconstruct a function (image) with a known discrete range from its projection along certain directions. By modern electron-microscopy techniques, one can count the number of atoms laying on a line representing, e.g., an X-ray. The so obtained data is used in the integer programming formulation. However, in real applications the size of the problem, that is well-known to be NP-hard, is so large that no method seems to be applicable to it. Other natural restrictions can make the problem even harder. In an attempt to avoid such kind of difficulties, we present an alternative approach to the problem. With this, we also aim to shed more light on the theoretical limitations for efficient computation in Discrete Tomography. Our approach is based on image reconstruction from a single projection, under the hypothesis that all computations take place in an algebraic computation model. In terms of computational efficiency, the proposed algorithm is significantly superior to the known algorithms for the problem. We also discuss on the possibilities for practical implementation of our method.}
}
@article{GOLDBERG2011171,
title = {Computational physiology of the neural networks of the primate globus pallidus: function and dysfunction},
journal = {Neuroscience},
volume = {198},
pages = {171-192},
year = {2011},
note = {Function and Dysfunction of the Basal Ganglia},
issn = {0306-4522},
doi = {https://doi.org/10.1016/j.neuroscience.2011.08.068},
url = {https://www.sciencedirect.com/science/article/pii/S0306452211010268},
author = {J.A. Goldberg and H. Bergman},
keywords = {basal ganglia, primate, neurons, correlations, oscillations, Parkinson's disease},
abstract = {The dorsal pallidal complex is made up of the external and internal segments of the globus pallidus (GPe and GPi respectively). It is part of the main axis of the basal ganglia (BG) that connects the thalamo-cortical networks to the BG input stages (striatum and subthalamic nucleus) and continues directly, and indirectly through the GPe, to the BG output stages (GPi and substantia nigra reticulata). Here we review the unique anatomical and physiological features of the pallidal complex and argue that they support the main computational goal of the BG main axis (actor); namely, a behavioral policy that maximizes future cumulative gains and minimizes costs. The three mono-layer competitive networks of the BG main axis flexibly extract relevant features from the current state of the thalamo-cortical activity to control current (ongoing) and future actions. We hypothesize that the striatal and the subthalamic projections neurons act as mono-stable integrators (class I excitability) and the in-vivo pallidal neurons act as bi-stable resonators (class II excitability). GPe neurons exhibit pausing behavior because their membrane potential lingers in the vicinity of an unstable equilibrium point and bi-stability, and these pauses enable a less-greedy exploratory behavioral policy. Finally, degeneration of midbrain dopaminergic neurons and striatal dopamine depletion (as in Parkinson's disease) lead to augmentation of striatal excitability and competitive dynamics. As a consequence the pallidal network, whose elements tend to synchronize as a result of their bi-stable resonance behavior, shifts from a Poissonian-like non-correlated to synchronous oscillatory discharge mode. This article is part of a Special Issue entitled: Function and Dysfunction of the Basal Ganglia.}
}
@incollection{GOODSON2010175,
title = {Chapter 10 - Using Computational Modeling to Understand Microtubule Dynamics: A Primer for Cell Biologists},
editor = {Leslie Wilson and John J. Correia},
series = {Methods in Cell Biology},
publisher = {Academic Press},
volume = {95},
pages = {175-188},
year = {2010},
booktitle = {Microtubules, in vitro},
issn = {0091-679X},
doi = {https://doi.org/10.1016/S0091-679X(10)95010-3},
url = {https://www.sciencedirect.com/science/article/pii/S0091679X10950103},
author = {Holly V. Goodson and Ivan V. Gregoretti},
abstract = {Experimental cell biology, biochemistry, and structural biology have provided a wealth of information about microtubule function and mechanism, but we are reaching a limit as to what can be understood from experiment alone. Standard biochemical approaches are not sufficient to make quantitative predictions about microtubule behavior, and they are limited in their ability to test existing conceptual models of microtubule mechanism. Because microtubules are so complex, achieving a deep understanding of microtubule behavior and mechanism will require the input of mathematical and computational modeling. However, this type of analysis can be daunting to the uninitiated. The purpose of this chapter is to provide a straightforward introduction to the various types of modeling and how they can be used to study microtubule function, dynamics, and mechanism.}
}
@article{SZUBA1998321,
title = {A molecular quasi-random model of computations applied to evaluate collective intelligence},
journal = {Future Generation Computer Systems},
volume = {14},
number = {5},
pages = {321-339},
year = {1998},
note = {Bio-inspired solutions to parallel processing problems},
issn = {0167-739X},
doi = {https://doi.org/10.1016/S0167-739X(98)00037-5},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X98000375},
author = {Tadeusz Szuba},
keywords = {Collective intelligence, IQ measure, Social structure, PROLOG, Model of computations, Brownian movements},
abstract = {This paper presents a bio-inspired model of computations, the random PROLOG processor (RPP), used for analysis of collective intelligence (CI). In the RPP, clause_molecules (CMs) of facts, rules, goals, or higher-level logical structures enclosed by membranes move quasi-randomly in structured computational_PROLOG_space (CS). When CMs rendezvous, an inference process can occur iff the logical conditions are fulfilled. CI can be evaluated as follows: (1) the mapping is done of a given social structure into the RPP; (2) the beings and their behavior are translated into PROLOG expressions, carried by CMs; (3) the goal(s) of the social structure are translated into N-element inference (NEI); (4) the efficiency of the NEI is evaluated and given as the intelligence quotient of a social structure (IQS) projected onto NEI.}
}
@incollection{ZOHURI202225,
title = {Chapter 2 - A general approach to business resilience system (BRS)},
editor = {Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia},
booktitle = {Knowledge is Power in Four Dimensions: Models to Forecast Future Paradigm},
publisher = {Academic Press},
pages = {25-57},
year = {2022},
isbn = {978-0-323-95112-8},
doi = {https://doi.org/10.1016/B978-0-323-95112-8.00003-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323951128000039},
author = {Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia},
keywords = {Artificial intelligence, Data analysis and information, Market and market share, Predictive analytics, Super artificial intelligence},
abstract = {The business resilience system (BRS) with its risk atom and processing data point is based on fuzzy logic and cloud computation in real time. Its purpose and objectives define a clear set of expectations for organizations and enterprises, so their network system and supply chain are totally resilient and protected against cyberattacks, man-made threats, and natural disasters. These enterprises include financial, organizational, homeland security, and supply chain operations with multipoint manufacturing across the world. Market share and marketing advantages are expected to result from the implementation of the system. The collected information and defined objectives provide the basis to monitor and analyze the data through cloud computation and will guarantee the success of their survivability against any unexpected threats. Putting this kind of operation in place allows the executive and stakeholders within those organizations and enterprises to make the right decision when encountering threats that interrupt their normal day-to-day operations, as well as, in cases such as defense and homeland security, to predict the next move of an adversary. Given the fact that the BRS, as part of its functionality, processes the incoming data and information if not real time, then near real time with the help of superartificial intelligence in place, this gives the stakeholder an edge against and threats as well as predicting issues with operational intelligence. Artificial intelligence (AI) is one of those technologies that seem to be expanding in every direction. This technology will take center stage at Think 2018. Resilience thinking is inevitably systems thinking, at least as much as sustainable development is. In fact, “when considering systems of humans and nature (social-ecological systems), it is important to consider the system as a whole.” The term “resilience” originated in the 1970s in the field of ecology from the research of C.S. Holling, who defined resilience as “a measure of the persistence of systems and of their ability to absorb change and disturbance and still maintain the same relationships between populations or state variables.” In short, resilience is best defined as “the ability of a system to absorb disturbances and still retain its basic function and structure.” In this chapter, we explain the BRS and how it works. Please note that the with minor editing and manipulation, the materials presented in this chapter have been borrowed from the book published from Zohuri and Moghaddam10 with permission from both authors and publisher as well.}
}
@article{ZHANG2022116187,
title = {Tri-level attribute reduction in rough set theory},
journal = {Expert Systems with Applications},
volume = {190},
pages = {116187},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.116187},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421015050},
author = {Xianyong Zhang and Yiyu Yao},
keywords = {Attribute reduction, Three-way decision, Tri-level analysis, Object-specific attribute reducts, Tri-level attribute reducts, Granular computing},
abstract = {Attribute reduction serves as a pivotal topic of rough set theory for data analysis. The ideas of tri-level thinking from three-way decision can shed new light on three-level attribute reduction. Existing classification-specific and class-specific attribute reducts consider only macro-top and meso-middle levels. This paper introduces a micro-bottom level of object-specific reducts. The existing two types of reducts apply to the global classification with all objects and a local class with partial objects, respectively. The new type applies to an individual object. These three types of reducts constitute tri-level attribute reducts. Their development and hierarchy are worthy of systematical explorations. Firstly, object-specific reducts are defined by object consistency from dependency, and they improve both classification-specific and class-specific reducts. Secondly, tri-level reducts are unified by tri-level consistency. Hierarchical relationships between object-specific reducts and class-specific, classification-specific reducts are analyzed, and relevant connections of three-way classifications of attributes are given. Finally, tri-level reducts are systematically analyzed, and two approaches, i.e., the direct calculation and hierarchical transition, are suggested for constructing a specific reduct. We build a framework of tri-level thinking and analysis of attribute reduction to enrich three-way granular computing. Tri-level reducts lead to the sequential development and hierarchical deepening of attribute reduction, and their results profit intelligence processing and system reasoning.}
}
@article{HAYASHI1999507,
title = {Numerical models of HDR geothermal reservoirs—a review of current thinking and progress},
journal = {Geothermics},
volume = {28},
number = {4},
pages = {507-518},
year = {1999},
issn = {0375-6505},
doi = {https://doi.org/10.1016/S0375-6505(99)00026-7},
url = {https://www.sciencedirect.com/science/article/pii/S0375650599000267},
author = {Kazuo Hayashi and Jonathan Willis-Richards and Robert J Hopkirk and Yuichi Niibori},
keywords = {Reservoir, Hot dry rock, HDR, Modelling, Numerical simulation},
abstract = {A brief review is presented of modelling and simulation of HDR geothermal reservoirs both for hydraulic fracturing/stimulation to create artificial/engineered geothermal reservoirs and for long-term heat extraction operations. Firstly, modelling of the governing factors is surveyed and coupling among mechanical, thermal, hydraulic and chemical effects is discussed. Next the structure and modelling of reservoirs are discussed. Finally, the features of a variety of simulation codes are summarized.}
}
@article{LUCAS20218320,
title = {Implementing a Novel Software Program to Support Pharmacy Students’ Reflective Practice in Scientific Research},
journal = {American Journal of Pharmaceutical Education},
volume = {85},
number = {10},
pages = {8320},
year = {2021},
issn = {0002-9459},
doi = {https://doi.org/10.5688/ajpe8320},
url = {https://www.sciencedirect.com/science/article/pii/S0002945923015012},
author = {Cherie Lucas and Simon Buckingham Shum and Ming Liu and Mary Bebawy},
keywords = {reflection, formative feedback, pharmacy education, pharmaceutical research},
abstract = {ABSTRACT
Objective. To explore pharmacy students’ perceptions of a novel web application tool (AcaWriter) implemented in a Master of Pharmacy curriculum to support reflective thinking in scientific research. Methods. A qualitative research design involving a 50-minute focus group (n=12) was used. The focus group session was audio-taped, transcribed verbatim, and analyzed thematically using the Braun and Clarke framework. Results. Analysis generated four themes related to AcaWriter’s utility in enhancing students’ research thinking and capacity. The themes identified included: ease of use to prompt reflection, tangible tool with non-judgmental capacity; benefits for enhancing self and peer reflection on research techniques and group dynamics; benefits of the reflective writing process to enhance research capacity compared with engaging in reflective dialogue; and benefits beyond the writing process: cultivating self-improvement and self-confidence. Conclusion. The findings of this study show that a novel web application implemented within a pharmacy curriculum can assist students’ self and peer reflection on a research task. Further research is needed to explore the impact of using this tool and its relationship with academic performance and outcomes.}
}
@article{JACKSON20091397,
title = {There may be more to reaching than meets the eye: Re-thinking optic ataxia},
journal = {Neuropsychologia},
volume = {47},
number = {6},
pages = {1397-1408},
year = {2009},
note = {Perception and Action},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2009.01.035},
url = {https://www.sciencedirect.com/science/article/pii/S0028393209000475},
author = {Stephen R. Jackson and Roger Newport and Masud Husain and Jane E. Fowlie and Michael O’Donoghue and Nin Bajaj},
keywords = {Optic ataxia, Neuropsychology of action, Reaching},
abstract = {Optic ataxia (OA) is generally thought of as a disorder of visually guided reaching movements that cannot be explained by any simple deficit in visual or motor processing. In this paper we offer a new perspective on optic ataxia; we argue that the popular characterisation of this disorder is misleading and is unrepresentative of the pattern of reaching errors typically observed in OA patients. We begin our paper by reviewing recent neurophysiological, neuropsychological, and functional brain imaging studies that have led to the proposal that the medial parietal cortex in the vicinity of the parietal-occipital junction (POJ) – the key anatomical site associated with OA – represents reaching movements in eye-centred coordinates, and that this ability is impaired in optic ataxia. Our perspective stresses the importance of the POJ and superior parietal regions of the human PPC for representing reaching movements in both extrinsic (eye-centred) and intrinsic (postural) coordinates, and proposes that it is the ability to simultaneously represent multiple spatial locations that must be directly compared with one another that is impaired in non-foveal OA patients. In support of this idea we review recent fMRI and behavioural studies conducted by our group that have investigated the anatomical correlates of posturally guided movements, and the movements guided by postural cues in patients presenting with optic ataxia.}
}
@article{BELLEMAREPEPIN2022105103,
title = {Processing visual ambiguity in fractal patterns: Pareidolia as a sign of creativity},
journal = {iScience},
volume = {25},
number = {10},
pages = {105103},
year = {2022},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2022.105103},
url = {https://www.sciencedirect.com/science/article/pii/S258900422201375X},
author = {Antoine Bellemare-Pepin and Yann Harel and Jordan O’Byrne and Geneviève Mageau and Arne Dietrich and Karim Jerbi},
keywords = {Cognitive neuroscience, Social sciences, Psychology},
abstract = {Summary
Creativity is a highly valued and beneficial skill that empirical research typically probes using “divergent thinking” (DT) tasks such as problem solving and novel idea generation. Here, in contrast, we examine the perceptual aspect of creativity by asking whether creative individuals are more likely to perceive recognizable forms in ambiguous stimuli –a phenomenon known as pareidolia. To this end, we designed a visual task in which participants were asked to identify as many recognizable forms as possible in cloud-like fractal images. We found that pareidolic perceptions arise more often and more rapidly in highly creative individuals. Furthermore, high-creatives report pareidolia across a broader range of image contrasts and fractal dimensions than do low creatives. These results extend the established body of work on DT by introducing divergent perception as a complementary manifestation of the creative mind, thus clarifying the perception-creation link while opening new paths for studying creative behavior in humans.}
}
@article{PEZZANO2024100078,
title = {Are we done with (Wordy) manifestos? Towards an introverted digital humanism},
journal = {Journal of Responsible Technology},
volume = {17},
pages = {100078},
year = {2024},
issn = {2666-6596},
doi = {https://doi.org/10.1016/j.jrt.2024.100078},
url = {https://www.sciencedirect.com/science/article/pii/S2666659624000040},
author = {Giacomo Pezzano},
keywords = {Mediatic turn, Philosophy of technology, Learning, Book, Video game},
abstract = {Beginning with a reconstruction of the anthropological paradigms underlying The Vienna Manifesto and The Onlife Manifesto (§ 1.1), this paper distinguishes between two possible approaches to digital humanism: an extroverted one, principally engaged in finding a way to humanize digital technologies, and an introverted one, pointing instead attention to how digital technologies can re-humanize us, particularly our “mindframe” (§ 1.2). On this basis, I stress that if we take seriously the consequences of the “mediatic turn”, according to which human reason is finally recognized as mediatically contingent (§ 2.1), then we should accept that just as the book created the poietic context for the development of traditional humanism and its “bookish” idea of private and public reason, so too digital psycho-technologies today provide the conditions for the rise of a new humanism (§ 2.2). I then discuss the possible humanizing potential of digital simulated worlds: I compare the symbolic-reconstructive mindset to the sensorimotor mindset (§ 3.1), and I highlight their respective mediological association with the book and the video game, advocating for the peculiar thinking and reasoning affordances now offered by the new digital psycho-technologies (§ 3.2).}
}
@article{KEOGH2007249,
title = {Keeping the patient asleep and alive: Towards a computational cognitive model of disturbance management in anaesthesia},
journal = {Cognitive Systems Research},
volume = {8},
number = {4},
pages = {249-261},
year = {2007},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2006.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041707000034},
author = {K. Keogh and E.A. Sonenberg},
keywords = {Behavioural analysis, Computational cognitive modelling, Disturbance management},
abstract = {We have analysed rich, dynamic data about the behaviour of anaesthetists during the management of a simulated critical incident in the operating theatre. We use a paper based analysis and a partial implementation to further the development of a computational cognitive model for disturbance management in anaesthesia. We suggest that our data analysis pattern may be used for the analysis of behavioural data describing cognitive and observable events in other complex dynamic domains.}
}
@article{KE201426,
title = {An implementation of design-based learning through creating educational computer games: A case study on mathematics learning during design and computing},
journal = {Computers & Education},
volume = {73},
pages = {26-39},
year = {2014},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2013.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0360131513003345},
author = {Fengfeng Ke},
keywords = {Learning by design, Game-based learning, Mathematical disposition, Thinking mathematically, Computer game making},
abstract = {This mixed-method case study examined the potential of computer-assisted, math game making activities in facilitating design-based math learning for school children. Sixty-four middle school children participated in Scratch-based, math game making activities. Data were collected via activity and conversation observation, artifact analysis, interviewing, and survey. The study findings indicated that participants developed significantly more positive dispositions toward mathematics after computer game making. The study also found that experience-driven game design processes helped to activate children's reflection on everyday mathematical experiences. Mathematical thinking and content experience were intertwined within the process of computer game authoring. On the other hand, children designers were involved in game-world and story crafting more than mathematical representation. And it was still challenging for them to perform computer game coding with abstract reasoning.}
}
@article{IOAKIMIDIS2017280,
title = {Caustics, pseudocaustics and the related illuminated and dark regions with the computational method of quantifier elimination},
journal = {Optics and Lasers in Engineering},
volume = {88},
pages = {280-300},
year = {2017},
issn = {0143-8166},
doi = {https://doi.org/10.1016/j.optlaseng.2016.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0143816616301348},
author = {Nikolaos I. Ioakimidis},
keywords = {Caustics, Pseudocaustics, Illuminated and dark regions, Cracks, Plates, Elasticity},
abstract = {The method of caustics is a powerful experimental method in elasticity and particularly in fracture mechanics for crack problems. The related method of pseudocaustics is also of interest. Here we apply the computational method of quantifier elimination implemented in the computer algebra system Mathematica in order to determine (i) the non-parametric equation and two properties of the caustic at a crack tip and especially (ii) the illuminated and the dark regions related to caustics and pseudocaustics in plane elasticity and plate problems. The present computations concern: (i) The derivation of the non-parametric equation of the classical caustic about a crack tip through the elimination of the parameter involved (here the polar angle) as well as two geometrical properties of this caustic. (ii) The derivation of the inequalities defining the illuminated region on the screen in the problem of an elastic half-plane loaded normally by a concentrated load with the boundary of this illuminated region related to some extent to the caustic formed. (iii) Similarly for the problem of a clamped circular plate under a uniform loading with respect to the caustic and the pseudocaustic formed. (iv) Analogously for the problem of an equilateral triangular plate loaded by uniformly distributed moments along its whole boundary, which defines the related pseudocaustic. (v) The determination of quantities of interest in mechanics from the obtained caustics or pseudocaustics. The kind of computations in the applications (ii) to (iv), i.e. the derivation of inequalities defining the illuminated region on the screen, seems to be completely new independently of the use here of the method of quantifier elimination. Additional applications are also possible, but some of them require the expansion of the present somewhat limited power of the quantifier elimination algorithms in Mathematica. This is expected to take place in the future.}
}
@article{CHEN2005121,
title = {Computational intelligence in economics and finance: Carrying on the legacy of Herbert Simon},
journal = {Information Sciences},
volume = {170},
number = {1},
pages = {121-131},
year = {2005},
note = {Computational Intelligence in Economics and Finance},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2003.11.006},
url = {https://www.sciencedirect.com/science/article/pii/S0020025503004444},
author = {Shu-Heng Chen},
keywords = {Computational intelligence, Artificial intelligence, Agent-based computational economics, Autonomous agents, Stock price-volume relation, Micro-macro relation},
abstract = {This is an editorial guide for the special issue on computational intelligence (CI) in economics and finance. A historical introduction to the background is given. This research paradigm is traced back to Herbert Simon, who, as a founder of artificial intelligence, pioneered the applications of AI to economics. The move from the classical AI to CI indicates a continuation of the legacy of Herbert Simon. Computational intelligence has proved to be a constructive foundation for economics. In responding to what Herbert Simon referred as procedural rationality, our study of bounded rationality has been enriched by bringing autonomous agents into the economic analysis.}
}
@article{BAHLS2015104,
title = {LaTeXnics: The effect of specialized typesetting software on STEM students’ composition processes},
journal = {Computers and Composition},
volume = {37},
pages = {104-116},
year = {2015},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2015.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S8755461515000511},
author = {Patrick Bahls and Amanda Wray},
keywords = {Computer-Mediated communication, STEM writing, Writing process, LaTeX, Markup languages, Typesetting tools},
abstract = {Undergraduate science, technology, engineering, and mathematics (STEM) students are often trained to use technical typesetting software in order to produce authentic mathematical prose, though little research exists about how this writing technology impacts students’ thinking and computation process. Drawing upon survey and interview research conducted at two liberal arts institutions, the authors investigate student writing practices across several undergraduate mathematics courses that required the use of LaTeX (a common markup language allowing users to specify the appearance of text and its layout on the printed page). This article presents findings about how the use of LaTeX slowed down students’ writing process, encouraging greater revision and reflection as well as allowing students to identify errors in their work at more than one stage in the process. We also explore the affective learning outcomes of STEM students using typesetting software, including increased feelings of confidence and professionalization. This article seeks to contribute to the growing conversation about how STEM students transfer knowledge about writing across disciplines.}
}