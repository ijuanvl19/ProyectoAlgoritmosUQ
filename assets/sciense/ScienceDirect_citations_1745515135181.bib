@article{CHENG2025104454,
title = {Exploring the associations between psychiatric symptoms and cognitive functions in first-episode schizophrenia: A network analysis},
journal = {Asian Journal of Psychiatry},
volume = {107},
pages = {104454},
year = {2025},
issn = {1876-2018},
doi = {https://doi.org/10.1016/j.ajp.2025.104454},
url = {https://www.sciencedirect.com/science/article/pii/S1876201825000978},
author = {Peng Cheng and Zhening Liu and Feiwen Wang and Jun Yang and Jie Yang},
keywords = {First-episode schizophrenia, Psychiatric symptoms, Cognitive functions, Network analysis},
abstract = {Background
First-episode schizophrenia represents a critical period for intervention in the treatment of schizophrenia. Understanding the intricate relationships between psychiatric symptoms and cognitive functions is vital for early precise intervention and predicting illness outcomes. Previous research has largely overlooked this issue, and traditional analytical methods based on pre-established theoretical assumptions are insufficient. This study aims to address this gap by utilizing graph theory-based network analysis.
Methods
The study employed the Positive and Negative Syndrome Scale (PANSS) to assess psychiatric symptoms. Cognitive functions were evaluated using the Digit Symbol and Information subtests from the Wechsler Adult Intelligence Scale (WAIS), which measure information processing efficiency and general knowledge, respectively. A network of psychiatric symptoms and cognitive functions was constructed based on these assessments.
Results
The network analysis revealed that negative symptom nodes were central. Notably, node N1 (Blunted affect) showed a negative correlation with the Digit Symbol node, being the only psychiatric symptom node linked to cognitive functions. Community detection analysis indicated that cognitive, positive symptom, and negative symptom nodes tended to cluster within their respective categories, while general psychopathology nodes showed a tendency to cluster with various types of nodes. Some general psychopathology nodes were isolated, reflecting the concealed nature of certain psychiatric symptoms in first-episode schizophrenia patients.
Conclusion
This study innovatively applies network analysis to explore the characteristics of the psychiatric symptom-cognitive function network in Chinese patients with first-episode schizophrenia. The findings provide valuable theoretical insights for targeted symptom-based interventions and for predicting disease outcomes in first-episode schizophrenia.}
}
@article{ZHOU20211491,
title = {Transcriptome based functional identification and application of regulator AbrB on alkaline protease synthesis in Bacillus licheniformis 2709},
journal = {International Journal of Biological Macromolecules},
volume = {166},
pages = {1491-1498},
year = {2021},
issn = {0141-8130},
doi = {https://doi.org/10.1016/j.ijbiomac.2020.11.028},
url = {https://www.sciencedirect.com/science/article/pii/S0141813020349539},
author = {Cuixia Zhou and Huitu Zhang and Honglei Fang and Yanqing Sun and Huiying Zhou and Guangcheng Yang and Fuping Lu},
keywords = {Transcriptome analysis, Gene regulation, AbrB, Alkaline protease, },
abstract = {Bacillus licheniformis 2709 is the major alkaline protease producer, which has great potential value of industrial application, but how the high-producer can be regulated rationally is still not completely understood. It's meaningful to understand the metabolic processes during alkaline protease production in industrial fermentation medium. Here, we collected the transcription database at various enzyme-producing stages (preliminary stage, stable phase and decline phase) to specifically research the synthesized and regulatory mechanism of alkaline protease in B. licheniformis. The RNA-sequencing analysis showed differential expression of numerous genes related to several processes, among which genes correlated with regulators were concerned, especially the major differential gene abrB on enzyme (AprE) synthesis was investigated. It was further verified that AbrB is a repressor of AprE by plasmid-mediated over-expression due to the severely descending enzyme activity (11,300 U/mL to 2695 U/mL), but interestingly it is indispensable for alkaline protease production because the enzyme activity of the null abrB mutant was just about 2279 U/mL. Thus, we investigated the aprE transcription by eliminating the theoretical binding site (TGGAA) of AbrB protein predicated by computational strategy, which significantly improved the enzyme activity by 1.21-fold and gene transcription level by 1.77-fold in the mid-log phase at a cultivation time of 18 h. Taken together, it is of great significance to improve the production strategy, control the metabolic process and oriented engineering by rational molecular modification of regulatory network based on the high throughput sequencing and computational prediction.}
}
@article{SMITH2018325,
title = {The Developing Infant Creates a Curriculum for Statistical Learning},
journal = {Trends in Cognitive Sciences},
volume = {22},
number = {4},
pages = {325-336},
year = {2018},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2018.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S1364661318300275},
author = {Linda B. Smith and Swapnaa Jayaraman and Elizabeth Clerkin and Chen Yu},
keywords = {statistical learning, egocentric vision, face perception, object perception},
abstract = {New efforts are using head cameras and eye-trackers worn by infants to capture everyday visual environments from the point of view of the infant learner. From this vantage point, the training sets for statistical learning develop as the sensorimotor abilities of the infant develop, yielding a series of ordered datasets for visual learning that differ in content and structure between timepoints but are highly selective at each timepoint. These changing environments may constitute a developmentally ordered curriculum that optimizes learning across many domains. Future advances in computational models will be necessary to connect the developmentally changing content and statistics of infant experience to the internal machinery that does the learning.}
}
@article{KLEINMUNTZ1992227,
title = {Computers as clinicians: An update},
journal = {Computers in Biology and Medicine},
volume = {22},
number = {4},
pages = {227-237},
year = {1992},
issn = {0010-4825},
doi = {https://doi.org/10.1016/0010-4825(92)90062-R},
url = {https://www.sciencedirect.com/science/article/pii/001048259290062R},
author = {Benjamin Kleinmuntz},
keywords = {Clinical information processing, Clinical decision making, Clinical reasoning, Diagnostic reasoning, Computers as clinicians, Medical decision making, Human judgment, Human inference, Computer simulation of thinking, Artificial intelligence, Expert systems},
abstract = {Computers as clinicians entered medical settings relatively recently, but with limited success because they lack general intelligence—that is, though they can be experts in domain specific specialties, they cannot yet deal with clinical problems never before encountered. SOAR, a novel AI computer programming architecture, can learn from past encounters with prior problems and can generalize its learning to new ones. It may therefore take computers to a higher level of clinical performance.}
}
@article{GOMEZIGLESIAS201828,
title = {Performance evaluation of the three-point angular correlation function},
journal = {Parallel Computing},
volume = {76},
pages = {28-41},
year = {2018},
issn = {0167-8191},
doi = {https://doi.org/10.1016/j.parco.2018.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167819118301236},
author = {Antonio Gómez-Iglesias and Miguel Cárdenas-Montes},
keywords = {Performance, Benchmarking, Atom, ARM, Xeon, FPGA, GPU},
abstract = {In recent years, we have observed an increase in the diversity of the processors ecosystem. Different designs and architectures are being studied based on their performance and power characteristics. While using benchmarks for this purpose allows for reproducibility and easy understanding of the results, using real scientific applications allows researchers to realize the actual implications of each design on the overall performance of their codes. This paper analyzes the performance of different implementations of a three-point angular correlation function. This function is used in the study of Large-Scale Distribution of galaxies in a variety of computational platforms. The function is based on histogram construction and presents a large computational cost. This cost dramatically increases with the size of the datasets. We have considered two different GPUs, a set of x86 Intel machines (multi- and many-core), ARM chipsets, as well as an FPGA. We first study the best possible implementation for each platform in terms of time to solution. We then compare the power used by those platforms for a predefined number of datasets. Energy is one of the main constraints that computer architects are facing nowadays. The results will be used to evaluate the performance of this function considering those two targets – time and energy – for those platforms and to analyze the suitability of each of those platforms for this specific problem.}
}
@incollection{BERMAN201691,
title = {Chapter 3 - Indexing Text},
editor = {Jules J. Berman},
booktitle = {Data Simplification},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {91-133},
year = {2016},
isbn = {978-0-12-803781-2},
doi = {https://doi.org/10.1016/B978-0-12-803781-2.00003-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128037812000035},
author = {Jules J. Berman},
keywords = {Index, Concordance, Autocoding, Autoencoding, Page rank, Search, Retrieval},
abstract = {Data has no value if it cannot be sensibly examined. In past centuries, the index has been the key to searching and retrieving text. Today, it is tempting to think that the index is obsolete, being replaced the by the "find" box that pops onto the screen when we press the "Ctrl-F" keys. This is not the case; simple "find" searches cannot cope with the variations and complexities of textual information. A thoughtful index is a reconceptualization of the document that permits rapid retrieval of terms that are related to the search term. An index, aided by proper annotation of data, permits us to understand data in ways that were not anticipated when the original content was collected. With the use of computers, multiple indexes, designed for different purposes, can be created for a single document or data set. As data accrues, indexes can be updated. When data sets are combined, their respective indexes can be merged. A good way of thinking about indexes is that the document contains all of the complexity; the index contains all of the simplicity. Data scientists who understand how to create and use indexes will be in the best position to search, retrieve, and analyze textual data.}
}
@article{ZHANG2019610,
title = {Resolution, energy and time dependency on layer scaling in finite element modelling of laser beam powder bed fusion additive manufacturing},
journal = {Additive Manufacturing},
volume = {28},
pages = {610-620},
year = {2019},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2019.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S2214860418309576},
author = {Wenyou Zhang and Mingming Tong and Noel M. Harrison},
keywords = {Powder bed fusion, Optimisation, Process modelling, Interdependencies, Additive manufacturing},
abstract = {The Laser Beam Powder Bed Fusion (PBF-LB) category of Additive Manufacturing (AM) is currently receiving much attention for computational process modelling. Major challenges exist in how to reconcile resolution, energy and time in a real build, with the practical limitations of resolution (layer height and mesh resolution), energy (heat format and magnitude) and time (heating and cooling step times) in the computational space. A novel thermomechanical PBF-LB process model including an efficient powder-interface heat loss mechanism was developed. The effect of variations in layer height (layer scaling), energy and time on the temperature and stress evolution was investigated. The influence of heating step time and cooling step time was characterised and the recommended ratio of element size to layer scaling was presented, based on a macroscale 2D model. The layer scaling method was effective when scaling up to 4 times the layer thickness and appropriately also scaling the cooling step time. This research provides guidelines and a framework for layer scaling for finite element modelling of the PBF-LB process.}
}
@incollection{ZEMAN2013373,
title = {Chapter 31 - The nature of consciousness},
editor = {James L. Bernat and H. Richard Beresford},
series = {Handbook of Clinical Neurology},
publisher = {Elsevier},
volume = {118},
pages = {373-407},
year = {2013},
booktitle = {Ethical and Legal Issues in Neurology},
issn = {0072-9752},
doi = {https://doi.org/10.1016/B978-0-444-53501-6.00031-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780444535016000317},
author = {Adam Zeman and Jan Adriaan Coebergh},
keywords = {awareness, consciousness, philosophy, self-consciousness, theories of consciousness, unconsciousness}
}
@article{NEUMAN2014650,
title = {Personality from a cognitive-biological perspective},
journal = {Physics of Life Reviews},
volume = {11},
number = {4},
pages = {650-686},
year = {2014},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2014.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S1571064514001584},
author = {Yair Neuman},
keywords = {Personality, Threat, Trust, Distrust, Psychology, Interdisciplinarity},
abstract = {The term “personality” is used to describe a distinctive and relatively stable set of mental traits that aim to explain the organism's behavior. The concept of personality that emerged in human psychology has been also applied to the study of non-human organisms from birds to horses. In this paper, I critically review the concept of personality from an interdisciplinary perspective, and point to some ideas that may be used for developing a cognitive-biological theory of personality. Integrating theories and research findings from various fields such as cognitive ethnology, clinical psychology, and neuroscience, I argue that the common denominator of various personality theories are neural systems of threat/trust management and their emotional, cognitive, and behavioral dimensions. In this context, personality may be also conceived as a meta-heuristics both human and non-human organisms apply to model and predict the behavior of others. The paper concludes by suggesting a minimal computational model of personality that may guide future research.}
}
@article{MAO2024101988,
title = {A survey on semantic processing techniques},
journal = {Information Fusion},
volume = {101},
pages = {101988},
year = {2024},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2023.101988},
url = {https://www.sciencedirect.com/science/article/pii/S1566253523003044},
author = {Rui Mao and Kai He and Xulang Zhang and Guanyi Chen and Jinjie Ni and Zonglin Yang and Erik Cambria},
keywords = {Semantic processing, Word sense disambiguation, Anaphora resolution, Named entity recognition, Concept extraction, Subjectivity detection},
abstract = {Semantic processing is a fundamental research domain in computational linguistics. In the era of powerful pre-trained language models and large language models, the advancement of research in this domain appears to be decelerating. However, the study of semantics is multi-dimensional in linguistics. The research depth and breadth of computational semantic processing can be largely improved with new technologies. In this survey, we analyzed five semantic processing tasks, e.g., word sense disambiguation, anaphora resolution, named entity recognition, concept extraction, and subjectivity detection. We study relevant theoretical research in these fields, advanced methods, and downstream applications. We connect the surveyed tasks with downstream applications because this may inspire future scholars to fuse these low-level semantic processing tasks with high-level natural language processing tasks. The review of theoretical research may also inspire new tasks and technologies in the semantic processing domain. Finally, we compare the different semantic processing techniques and summarize their technical trends, application trends, and future directions.}
}
@article{YANG2012381,
title = {Combining means-end chain and fuzzy ANP to explore customers’ decision process in selecting bundles},
journal = {International Journal of Information Management},
volume = {32},
number = {4},
pages = {381-395},
year = {2012},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2011.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S0268401211001307},
author = {Hao-Wei Yang and Kuei-Feng Chang},
keywords = {Means-end chain, Fuzzy analytic network process, Customer value, Product attributes},
abstract = {Although some researches had submitted the hierarchical model of customer value, there are still questions remaining in the model. How to achieve a more effective method for obtaining and analyzing data from customers concerning their expectations is still lacking. Therefore, this research first applied the modification of the means-end chain (MEC) to construct a hierarchy framework of customer value to allow product attributes to be linked. Next, this research combined fuzzy analytic network process (FANP) in exploring customer preference to catch the multiple needs of customers. Meanwhile, in the measurement of customer preference, fuzzy logic and linguistic variables are utilized to overcome human subjective and imprecise thinking. Overall, this research proposes the hierarchy framework of customer value including the causal relationships of attributes–consequence–value to fill previous model's gap, and identified the factors which could enhance the value of bundle, in contrast to the many monetary research treatments of product bundles. Finally, this research presented the value implications of cosmetics bundles and implication for management and marketing.}
}
@article{STOJANOVIC2021107270,
title = {Application of distance learning in mathematics through adaptive neuro-fuzzy learning method},
journal = {Computers & Electrical Engineering},
volume = {93},
pages = {107270},
year = {2021},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2021.107270},
url = {https://www.sciencedirect.com/science/article/pii/S0045790621002536},
author = {Jelena Stojanović and Dalibor Petkovic and Ibrahim M Alarifi and Yan Cao and Nebojsa Denic and Jelena Ilic and Hamid Assilzadeh and Sead Resic and Biljana Petkovic and Afrasyab Khan and Milosav Milickovic},
keywords = {Pupils, E-learning, Distance learning, Moodle, Computational intelligent},
abstract = {The main aim of the study is analyzing of pupils’ knowledge in mathematics by adaptive neuro fuzzy inference system (ANFIS) after implementation of distance learning application or e-learning (electronic learning). Since a large number of faculties and other institutions are increasingly using e-learning, it can be stated that for this purpose the Modular object-oriented dynamic learning environment (Moodle) learning management system (LMS) is mostly used. This paper deals with the analysis of distance learning and the application of Moodle LMS in higher education institutions, taking into account the impact of such education on the quality of teaching and the acquisition of knowledge by students, and the methods teachers use in Serbia. The ANFIS is used to determine which factors are the most important for pupils’ performance in mathematics. The results show that the main influence on the pupils’ performance is their prior knowledge. The prior knowledge is more effective when it is combined with education software in the lectures of mathematics in elementary school. In secondary school, the prior knowledge is more effective if it is combined with motivation for learning mathematics.}
}
@article{INGLIS2022104424,
title = {From viewsheds to viewscapes: Trends in landscape visibility and visual quality research},
journal = {Landscape and Urban Planning},
volume = {224},
pages = {104424},
year = {2022},
issn = {0169-2046},
doi = {https://doi.org/10.1016/j.landurbplan.2022.104424},
url = {https://www.sciencedirect.com/science/article/pii/S0169204622000731},
author = {Nicole C. Inglis and Jelena Vukomanovic and Jennifer Costanza and Kunwar K. Singh},
keywords = {GIS, Landscape aesthetics, Landscape visibility, Line-of-sight, Visual assessment},
abstract = {The study of visibility and visual quality (VVQ) spans scientific disciplines, methods, frameworks and eras. Recent advances in line-of-sight computation and geographic information systems (GIS) have propelled VVQ research into the realm of high performance computing via a cache of geospatial tools accessible to a broad range of research disciplines. However, in the disciplines that use VVQ analysis most (archaeology, architecture, geosciences and planning), methods and terminology can vary markedly, which may encumber interdisciplinary progress. A multidisciplinary systematic review of past VVQ research is timely to assess past efforts and effectively advance the field. In this study, we summarize the state of VVQ research in a systematic review of peer-reviewed publications spanning the past two decades. Our search yielded 528 total studies, 176 of which we reviewed in depth. VVQ analysis in peer-reviewed research increased 21-fold in the last 20 years, applied primarily in archaeology and natural resources research. We found that methods, tools and study designs varied across disciplines and scales. Research disproportionately represented the Global North and primarily employed medium resolution bare-earth elevation models, despite their known limitations. We propose a framework for standardized reporting of methods that emphasizes cross-disciplinary collaboration to propel visibility research into the future.}
}
@article{DEPARDIEU1994110,
title = {Optical properties of cermets: 3D real space renormalization using the connection machine},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {207},
number = {1},
pages = {110-114},
year = {1994},
issn = {0378-4371},
doi = {https://doi.org/10.1016/0378-4371(94)90360-3},
url = {https://www.sciencedirect.com/science/article/pii/0378437194903603},
author = {G. Depardieu and P. Fiorini and S. Berthier},
abstract = {We propose a 3D position space renormalization approach based on Kadanoff's block method, for the calculation of the effective dielectric function of cermet type film. This model has been applied to simulated cubic lattices with particle aggregation. Our calculations were performed on a Thinking Machine CM5 massively parallel supercomputer. The renormalization process maps very naturally on to hypercube parallel architecture. We obtain good agreement with most of the experimental data.}
}
@article{SCHAFER2020100360,
title = {Lenstool-HPC: A High Performance Computing based mass modelling tool for cluster-scale gravitational lenses},
journal = {Astronomy and Computing},
volume = {30},
pages = {100360},
year = {2020},
issn = {2213-1337},
doi = {https://doi.org/10.1016/j.ascom.2019.100360},
url = {https://www.sciencedirect.com/science/article/pii/S2213133719301349},
author = {C. Schäfer and G. Fourestey and J.-P. Kneib},
keywords = {Gravitational lensing software, High performance computing algorithms, Applied computing: astronomy, Galaxies: clusters, Galaxies: halos, Lenstool},
abstract = {With the upcoming generation of telescopes, cluster scale strong gravitational lenses will act as an increasingly relevant probe of cosmology and dark matter. The better resolved data produced by current and future facilities requires faster and more efficient lens modelling software. Consequently, we present Lenstool-HPC, a strong gravitational lens modelling and map generation tool based on High Performance Computing (HPC) techniques and the renowned Lenstool software. We also showcase the HPC concepts needed for astronomers to increase computation speed through massively parallel execution on supercomputers. Lenstool-HPC was developed using lens modelling algorithms with high amounts of parallelism. Each algorithm was implemented as a highly optimised CPU, GPU and Hybrid CPU–GPU version. The software was deployed and tested on the Piz Daint cluster of the Swiss National Supercomputing Centre (CSCS). Lenstool-HPC perfectly parallel lens map generation and derivative computation achieves a factor 30 speed-up using only 1 GPU compared to Lenstool. Lenstool-HPC hybrid Lens-model fit generation tested at Hubble Space Telescope precision is scalable up to 200 CPU–GPU nodes and is faster than Lenstool using only 4 CPU–GPU nodes.}
}
@article{KRING2014725,
title = {The motivation and pleasure dimension of negative symptoms: Neural substrates and behavioral outputs},
journal = {European Neuropsychopharmacology},
volume = {24},
number = {5},
pages = {725-736},
year = {2014},
note = {Negative symptoms of schizophrenia: clinical characteristics and their measurement, experimental modelling, and opportunities for improved treatment},
issn = {0924-977X},
doi = {https://doi.org/10.1016/j.euroneuro.2013.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0924977X13001831},
author = {Ann M. Kring and Deanna M. Barch},
keywords = {Schizophrenia, Motivation, Pleasure, Neural substrates, Effort, Anticipation},
abstract = {A range of emotional and motivation impairments have long been clinically documented in people with schizophrenia, and there has been a resurgence of interest in understanding the psychological and neural mechanisms of the so-called “negative symptoms” in schizophrenia, given their lack of treatment responsiveness and their role in constraining function and life satisfaction in this illness. Negative symptoms comprise two domains, with the first covering diminished motivation and pleasure across a range of life domains and the second covering diminished verbal and non-verbal expression and communicative output. In this review, we focus on four aspects of the motivation/pleasure domain, providing a brief review of the behavioral and neural underpinnings of this domain. First, we cover liking or in-the-moment pleasure: immediate responses to pleasurable stimuli. Second, we cover anticipatory pleasure or wanting, which involves prediction of a forthcoming enjoyable outcome (reward) and feeling pleasure in anticipation of that outcome. Third, we address motivation, which comprises effort computation, which involves figuring out how much effort is needed to achieve a desired outcome, planning, and behavioral response. Finally, we cover the maintenance emotional states and behavioral responses. Throughout, we consider the behavioral manifestations and brain representations of these four aspects of motivation/pleasure deficits in schizophrenia. We conclude with directions for future research as well as implications for treatment.}
}
@article{LEHRACH201126,
title = {ITFoM – The IT Future of Medicine},
journal = {Procedia Computer Science},
volume = {7},
pages = {26-29},
year = {2011},
note = {Proceedings of the 2nd European Future Technologies Conference and Exhibition 2011 (FET 11)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2011.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S1877050911006776},
author = {Hans Lehrach and Ralf Subrak and Peter Boyle and Markus Pasterk and Kurt Zatloukal and Heimo Müller and Tim Hubbard and Angela Brand and Mark Girolami and Daniel Jameson and Frank J. Bruggeman and Hans V. Westerhoff},
keywords = {nonlinear information and communication technology, distributive computing, high throughput data analysis, personalized medicine, systems medicine, healthcare revolution: virtual human},
abstract = {Molecular medicine is undergoing a revolution, creating a data fog that may obscure understanding. The functioning human is analogous to a biological factory controlled by an incredibly complex Information and Communication (IC) network. It is proposed that 7 billion computational replicas be made of those 7 billion human IC networks to enable interrogation and manipulation, for understanding and personalized healthcare. This requires a revolutionary ICT that follows the organization of the biological information and communication flows, with implications for hardware, software and connectivity.}
}
@article{ZHOU2025106971,
title = {Integration of deep learning in the diagnosis, chemical analysis, and therapeutic approaches for neurodegenerative disorders},
journal = {Biomedical Signal Processing and Control},
volume = {100},
pages = {106971},
year = {2025},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2024.106971},
url = {https://www.sciencedirect.com/science/article/pii/S1746809424010292},
author = {Wencan Zhou and Chao Sun and Lihong Liu},
keywords = {Neurodegenerative Disorders (NDD), Diagnosis, Linear Discriminant Analysis (LDA), Chemical reaction optimization based improved generative adversarial network (CRO-IGAN)},
abstract = {Neurodegenerative disorders (NDD) are a group of progressive conditions that primarily affect neurons in the brain. The diseases gradually impair cognitive function, movement, and other neurological processes, leading to a decline in the individual’s quality of life. Reliable biomarkers that accurately detect and track the growth of neurological disorders are crucial for the development of effective therapeutics. People with NDDs have damage to the brainneurons, which causes strange walking patterns. To overcome this problem we proposed Chemical reaction optimization based improved generative adversarial network (CRO-IGAN) method is to improve the patient’s conditions that are affected in neurodegenerative diseases. The patient’s dataset is gathered, here we utilized min max normalization for data preprocessing is used to clean the data. Principal component analysis (PCA) is employed for feature extraction to extract the pre-processed data and remove the unwanted data. The appropriate data is selected using linear discriminant analysis (LDA) for feature selection. The parameter metrics used in this study are recall, sensitivity and specificity. The suggest techniques CRO-IGAN provides high performance of accuracy for diagnosing NDD to improve the patient health which provides a superior performance than other existing methods.}
}
@article{COBELLI1984291,
title = {A model of glucose kinetics and their control by insulin, compartmental and noncompartmental approaches},
journal = {Mathematical Biosciences},
volume = {72},
number = {2},
pages = {291-315},
year = {1984},
issn = {0025-5564},
doi = {https://doi.org/10.1016/0025-5564(84)90114-7},
url = {https://www.sciencedirect.com/science/article/pii/0025556484901147},
author = {Claudio Cobelli and Gianna Toffolo and Eleuterio Ferrannini},
abstract = {Compartmental and noncompartmental models are used to quantify, from multiple steady-state tracer experiments, glucose kinetics and the effect of insulin upon them. Some aspects of experiment design are discussed. A physiological three-compartment model of glucose kinetics is proposed which provides a new quantitative picture of insulin control of glucose metabolism. Noncompartmental modeling is shown to have structural errors which prevent physiological insight. Compartmental models make a better use of the informational content of kinetic data, even if more demanding both in terms of modeling and computational effort and in terms of physiological thinking.}
}
@article{CHASTAIN2002237,
title = {Square peg in a round hole or horseless carriage? Reflections on the use of computing in architecture},
journal = {Automation in Construction},
volume = {11},
number = {2},
pages = {237-248},
year = {2002},
note = {ACADIA '99},
issn = {0926-5805},
doi = {https://doi.org/10.1016/S0926-5805(00)00095-9},
url = {https://www.sciencedirect.com/science/article/pii/S0926580500000959},
author = {Thomas Chastain and Yehuda E Kalay and Christopher Peri},
keywords = {Square peg in a round hole, Horseless carriage, Architecture},
abstract = {We start with two paradigms that have been used to describe the relationship of computation methods and tools to the production of architecture. The first is that of forcing a square peg into a round hole — implying that the use of a tool is misdirected, or at least poorly fits the processes that have traditionally been part of an architectural design practice. In doing so, the design practice suffers from the use of new technology. The other paradigm describes a state of transformation in relationship to new technology as a horseless carriage in which the process is described in obsolete and ‘backward’ terms. The implication is that there is a lack of appreciation for the emerging potentials of technology to change our relationship with the task. The paper demonstrates these two paradigms through the invention of drawings in the 14th Century, which helped to define the profession of architecture. It then goes on to argue that modern computational tools follow the same paradigms, and like drawings, stand to bring profound changes to the profession of architecture as we know it.}
}
@article{POHJONEN2018244,
title = {Modelling of austenite transformation along arbitrary cooling paths},
journal = {Computational Materials Science},
volume = {150},
pages = {244-251},
year = {2018},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2018.03.052},
url = {https://www.sciencedirect.com/science/article/pii/S0927025618302052},
author = {Aarne Pohjonen and Mahesh Somani and David Porter},
keywords = {Phase transformations, Bainite, Martensite, Thermomechanical processing, Steel},
abstract = {A computational model based on the Johnson-Mehl-Avrami-Kolmogorov equation for simulating the onset and kinetics of austenite to bainite and martensite transformation has been fitted to experimental continuous cooling data for two different steels. We investigated how deformation below recrystallization temperature affected the transformation onset and kinetics in comparison to the same steel in the undeformed state. The fitted model can be used to simulate phase transformations occurring when the steel is cooled along any cooling path. The model can be fully coupled to heat transfer and conduction simulations in order to optimize cooling practice, for example in industrial thermomechanical processing of steel. The fitted model can also be used to predict the hardness of the steel after cooling.}
}
@article{PAYANDEH2006328,
title = {TOWARD UNIFICATION OF CONSTRAINED MECHANICS AND VIRTUAL FIXTURES IN HAPTIC RENDERING},
journal = {IFAC Proceedings Volumes},
volume = {39},
number = {15},
pages = {328-333},
year = {2006},
note = {8th IFAC Symposium on Robot Control},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20060906-3-IT-2910.00056},
url = {https://www.sciencedirect.com/science/article/pii/S1474667016385354},
author = {Shahram Payandeh and Pierre Joli and Zheng Feng},
keywords = {constrained mechanics, virtual fixtures (VF), computational mechanics, haptic rendering},
abstract = {Virtual Fixtures (VF) are defined as haptic or visual aids in assisting users of virtual and tele-operation environments during various tasks. They have been mostly defined similar to physical fixtures which are used in performing daily tasks such as holding-on to the railing when climbing the stairs or using a ruler to draw straight lines. Such analogous definitions can be even extended to the case of learning certain manual tasks under supervision, e.g. when one holds the child's hand in order to teach the association between the mouse movements and positions of cursor on the screen. This paper presents how notions from the computational mechanics for solving constrained dynamical systems can be extended for stable implementation of a class of VF in a haptic rendered environment. It is shown that some of the solution methodologies from computational mechanics can have direct implication in haptic rendering of stiff-constrained environment. Examples of such implementations are also presented using a haptic device interacting with a class of VF.}
}
@article{KANAAN2024143,
title = {An Introduction to Approaching Architecture in the Muslim World: Novel Paths of Investigations},
journal = {Journal of Material Cultures in the Muslim World},
volume = {4},
number = {2},
pages = {143-152},
year = {2024},
issn = {2666-6278},
doi = {https://doi.org/10.1163/26666286-12340043},
url = {https://www.sciencedirect.com/science/article/pii/S2666627824000082},
author = {Ruba Kana‘an and Avinoam Shalem}
}
@article{ZAREI2024175691,
title = {Integrated nexus approach to assessing climate change impacts on grassland ecosystem dynamics: A case study of the grasslands in Tanzania},
journal = {Science of The Total Environment},
volume = {952},
pages = {175691},
year = {2024},
issn = {0048-9697},
doi = {https://doi.org/10.1016/j.scitotenv.2024.175691},
url = {https://www.sciencedirect.com/science/article/pii/S0048969724058479},
author = {Azin Zarei and Kaveh Madani and Edeltraud Guenther and Hamid Mohammadi Nasrabadi and Holger Hoff},
keywords = {Climate change impacts, Emission scenarios, Vegetation dynamics, Nexus approach, Vulnerability},
abstract = {This study addresses the intricate interplay between climate, vegetation, and livestock dynamics in Tanzania within the Climate-Vegetation-Livestock (CVL) nexus through a quantitative assessment. By examining the temporal and spatial relationships between vegetation indices (NDVI, EVI, NPP) and key climatic variables (Precipitation, Temperature, Evapotranspiration) from 2009 to 2019, and projecting to 2050, this research aims to elucidate vegetation responses to climate change and its subsequent impacts on livestock. To this end, the relationship between the vegetation dynamics indicators (NDVI, NPP) and climate parameters is evaluated to quantify the vegetation response to climate change using statistical models. Next, an examination of multicollinearity is conducted to investigate potential interactions (nexus) between variables, incorporating the correlation among independent variables. Notably, the evaluation of performance and accuracy for the mentioned models is conducted through the cross-validation method and validation indices. Ultimately, the variation between projected NPP and NDVI (average for 2040–2060) and the present NPP and NDVI (average for 2009–2020) identifies the regions that are most likely susceptible, showcasing the vegetation cover's reaction to climate change in different emission scenarios. The results unveil significant spatio-temporal variations in vegetation dynamics influenced by climatic factors, where higher precipitation and temperatures correlate with increased vegetation health and productivity. The projected fluctuations in NDVI and NPP values indicate varying trends across different regions, with a general decrease in vegetation density and productivity from the northeast to the west under both RCP2.6 and RCP8.5 scenarios by 2050. This decline is attributed to anticipated changes in precipitation and temperature patterns driven by climate change. Furthermore, significant declines in vegetation density and productivity under emission scenarios, particularly in the southern regions compared to the present, suggest greater vulnerability to climate change impacts. This highlights the need for targeted mitigation strategies in these vulnerable areas. Meanwhile, northeast areas under both NDVI and NPP will remain unchanged across both climate scenarios. Moreover, analysis of livestock distribution maps indicates areas of vulnerability under climate change scenarios, with implications for future livestock management and agricultural practices. These findings underscore the importance of proactive planning and targeted interventions to enhance resilience and sustainable development in vulnerable regions, emphasizing the need for integrated approaches that consider the complex interactions between climate, vegetation, and livestock dynamics.}
}
@article{LAENG2014263,
title = {Scrutinizing visual images: The role of gaze in mental imagery and memory},
journal = {Cognition},
volume = {131},
number = {2},
pages = {263-283},
year = {2014},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2014.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0010027714000043},
author = {Bruno Laeng and Ilona M. Bloem and Stefania D’Ascenzo and Luca Tommasi},
keywords = {Imagery, Memory, Eye-tracking, Enactment},
abstract = {Gaze was monitored by use of an infrared remote eye-tracker during perception and imagery of geometric forms and figures of animals. Based on the idea that gaze prioritizes locations where features with high information content are visible, we hypothesized that eye fixations should focus on regions that contain one or more local features that are relevant for object recognition. Most importantly, we predicted that when observers looked at an empty screen and at the same time generated a detailed visual image of what they had previously seen, their gaze would probabilistically dwell within regions corresponding to the original positions of salient features or parts. Correlation analyses showed positive relations between gaze’s dwell time within locations visited during perception and those in which gaze dwelled during the imagery generation task. Moreover, the more faithful an observer’s gaze enactment, the more accurate was the observer’s memory, in a separate test, of the dimension or size in which the forms had been perceived. In another experiment, observers saw a series of pictures of animals and were requested to memorize them. They were then asked later, in a recall phase, to answer a question about a property of one of the encoded forms; it was found that, when retrieving from long-term memory a previously seen picture, gaze returned to the location of the part probed by the question. In another experimental condition, the observers were asked to maintain fixation away from the original location of the shape while thinking about the answer, so as to interfere with the gaze enactment process; such a manipulation resulted in measurable costs in the quality of memory. We conclude that the generation of mental images relies upon a process of enactment of gaze that can be beneficial to visual memory.}
}
@incollection{TAMIR2024263,
title = {Chapter Five - Predicting other people shapes the social mind},
editor = {Bertram Gawronski},
series = {Advances in Experimental Social Psychology},
publisher = {Academic Press},
volume = {69},
pages = {263-315},
year = {2024},
booktitle = {Advances in Experimental Social Psychology},
issn = {0065-2601},
doi = {https://doi.org/10.1016/bs.aesp.2023.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S006526012300028X},
author = {Diana I. Tamir and Mark A. Thornton},
keywords = {Social cognition, Emotion, Mentalizing, Prediction, Social neuroscience, Mental states, Actions, Traits, Dynamics},
abstract = {People have a rich understanding of the social world within which they are embedded. How do they organize this social knowledge? And to what end? We suggest that these two questions are intimately linked. We review a burgeoning literature which shows how the social mind organizes different layers of social knowledge—including knowledge of actions, mental states, personality traits, situations, and relationships—into parsimonious low-dimensional maps. By distilling much of the complexity of the social world down to coordinates on a few key psychological dimensions, people construct a highly efficient representation of the social world. We go on to review recent research showing that these maps facilitate accurate, automatic prediction of real-world social dynamics. Specifically, the placement of stimuli within these maps implicitly encodes predictions about the social future, both within the same layer of social knowledge, and across different layers. Moreover, the ability of these maps to predict the social future is no coincidence: increasing evidence suggests that the goal of prediction actively shapes the way people organize social knowledge. We conclude by discussing challenges and future directions for studying the predictive social mind.}
}
@article{PALAFOXALCANTAR2020598,
title = {The complementary use of game theory for the circular economy: A review of waste management decision-making methods in civil engineering},
journal = {Waste Management},
volume = {102},
pages = {598-612},
year = {2020},
issn = {0956-053X},
doi = {https://doi.org/10.1016/j.wasman.2019.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S0956053X19307111},
author = {P.G. Palafox-Alcantar and D.V.L. Hunt and C.D.F. Rogers},
keywords = {Circular economy, Civil engineering, Cooperation, Decision-making, Game theory, Solid waste},
abstract = {Circular economy principles aim to contribute towards sustainability and resilience through several simultaneous agendas including economic growth, social development and environmental responsibility. Stakeholders from each perspective have their own interests and priorities, which often result in conflict. There are several and varied methodologies which address the decision-making process, however in engineering spheres these techniques are usually limited to optimising resources, time or costs. Decisions that are comprehensive in scope and integrated across all affected systems are required to transition towards a circular economy, effective cross-disciplinary thinking is imperative and cooperation amongst diverse areas is essential. Game theory is a useful technique when analysing the interactions of stakeholders with multiple objectives and perspectives. This paper aims to critically review methodological approaches used in waste management practice and provide a guidance on how game theory differs from, and is complementary to, the primary decision-making tools available where cooperation is a feature too often missing. This review seeks to justify the development of game theory to complement waste management decision-making methods in civil engineering, where resource consumption and waste management is often voluminous. An application of game theory to a waste management example illustrates that this methodological approach is of complementary value. The contribution of this study to circular economy and solid waste agendas is to emphasise the capability of game theory to help facilitate conflict resolution, competition, and stakeholder consensus when capturing multiple (sometimes conflicting) values in line with circular economy principles.}
}
@article{AMMAR2024100839,
title = {Role of pedagogical approaches in fostering innovation among K-12 students in STEM education},
journal = {Social Sciences & Humanities Open},
volume = {9},
pages = {100839},
year = {2024},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2024.100839},
url = {https://www.sciencedirect.com/science/article/pii/S2590291124000366},
author = {Mohammad Ammar and Noora J. Al-Thani and Zubair Ahmad},
keywords = {Educational reform, STEM, Pedagogy, K-12, Early education, Innovation, Technology},
abstract = {The intricate challenges of the modern world demand students to be equipped with advanced skills and knowledge to thrive in an increasingly competitive global landscape. Science, technology, engineering, and mathematics (STEM) practices can help develop these capabilities in students from an early age. However, as technology continues to advance rapidly, STEM education has experienced a rapid transformation with seamless integration of various technologies. Students in the K-12 education is required to keep up with the growing innovation and to bridge this gap, pedagogical approaches play a crucial role. Therefore, this review presents the current landscape, developmental trends, and future directions of the various pedagogical practices used to integrate innovation in K-12 STEM. The characteristics and environmental perceptions that influence the development of innovation in students using such approaches are examined. Results from 42 systematically shortlisted studies indicate positive correlations of personalized pedagogical approaches in promoting innovation in students, thereby increasing STEM literacy in K-12 education. However, limitations that remain with teacher competencies and school facilities to cope with various pedagogical approaches are also discussed. Finally, we conclude with our recommendations on effective and efficient approaches that can be implemented in K-12 STEM education to develop the skills and mindset in students necessary to become innovative thinkers and prepare them for a technology-driven society.}
}
@incollection{SOUSA2021290,
title = {Material Design-for-eXcellence Framework – Application to Composites},
editor = {Dermot Brabazon},
booktitle = {Encyclopedia of Materials: Composites},
publisher = {Elsevier},
address = {Oxford},
pages = {290-301},
year = {2021},
isbn = {978-0-12-819731-8},
doi = {https://doi.org/10.1016/B978-0-12-819724-0.00105-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128197240001051},
author = {S.P.B. Sousa and A.J. Baptista and A.T. Marques},
keywords = {Advanced materials, CFRP, Composites, Eco-efficiency, GFRP, Material Design-for-eXcellence, Materials Life Cycle, Materials Performance, M-DfX Scorecards, Sustainability},
abstract = {Despite a good number of material selection methods and tools, there is a lack of straightforward material performance methods that allow an easy and multi-dimensional assessment of the material properties, relating to the inner structure of the material in a multi-scale proposition, supported by a Life-Cycle Assessment mindset. Material Design-for-eXcellence (M-DfX) is a novel approach to support the assessment of material performance in a systematic and visual way, through the evaluation of material properties (“X” dimensions) and characteristics in a normalized form. It manages the material composition complexity in different scales, adopting a modular configuration analogy. The integrated analysis of Material Performance is attained via an effectiveness assessment of the properties’ characteristics, cross-evaluated with efficiency/eco-efficiency aspects within a Life Cycle approach, resulting in new original scorecards and quadrants graphical tools. It has adopted a Lean Thinking approach for the use of visual management and waste identification in relation to production and resource efficiency. A demonstration example of M-DfX is given for the framework testing in the Composite Materials field, comparing a CRFP composite versus GRFP composite real use case application for the body of an airport bus vehicle.}
}
@incollection{MISTRY2021467,
title = {17 - Bio-inspired design},
editor = {Igor Yadroitsev and Ina Yadroitsava and Anton {du Plessis} and Eric MacDonald},
booktitle = {Fundamentals of Laser Powder Bed Fusion of Metals},
publisher = {Elsevier},
pages = {467-489},
year = {2021},
series = {Additive Manufacturing Materials and Technologies},
isbn = {978-0-12-824090-8},
doi = {https://doi.org/10.1016/B978-0-12-824090-8.00010-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012824090800010X},
author = {Yash Mistry and Daniel Anderson and Dhruv Bhate},
keywords = {Additive manufacturing, Bio-inspired design, Biomimicry, Cellular materials, Complexity, Hierarchy, Laser powder bed fusion, Periodicity, Texture, Topology optimization},
abstract = {Advances in additive manufacturing, computational design tools, and digitization techniques are converging in an exciting new era of engineering design, as humanity has never experienced before. Within this convergent domain, Bio-Inspired Design (BID) is a particularly promising area of research since the potential space for establishing structure-function correlation is vast, and the majority of it is untapped. In this chapter, bio-inspired design is first introduced, specifically in the context of the Laser Powder Bed Fusion (L-PBF) process. Practical approaches for implementing BID for L-PBF are discussed, followed by a discussion of key general design concepts that can be abstracted for BID. Examples of how BID and L-PBF have been combined are then presented, followed by a consideration of some of the most important design constraints posed by the L-PBF process that the bio-inspired designer needs to be aware of. The chapter concludes with a forward looking discussion of opportunities in this domain.}
}
@article{BENEDETTO2014167,
title = {Rebound effects due to economic choices when assessing the environmental sustainability of wine},
journal = {Food Policy},
volume = {49},
pages = {167-173},
year = {2014},
issn = {0306-9192},
doi = {https://doi.org/10.1016/j.foodpol.2014.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0306919214001250},
author = {Graziella Benedetto and Benedetto Rugani and Ian Vázquez-Rowe},
keywords = {Carbon footprint, Consequential LCA, Indirect effects, Life Cycle Assessment, Sustainable consumption},
abstract = {The identification and working mechanisms of Rebound Effects (REs) have important policy implications. The intensity of these impacts is crucial when it comes to detecting strategies to promote sustainable consumption of food and beverages, as in the case of wine. In fact, neglecting the occurrence of REs in wine production and delivery leads to under- or over-estimating the effects that novel more sustainable technologies may produce. An in-depth analysis on the ways in which the stakeholders may react to the availability of a new product (e.g. wine produced through a process oriented to the reduction of CO2 emissions) may be particularly useful to allow producers and consumers to target the REs with respect to the overall goals of desired sustainability. In this article, we first provide a definition and a classification of different types of REs and then analyse some exemplificative cases applied to the supply and consumption of wine produced through technologies that reduce environmental emissions or resource consumptions. A final step analyses the methodological tools that may be useful when including REs in life cycle thinking as applied to the wine sector.}
}
@article{YU2023119632,
title = {A graph attention network under probabilistic linguistic environment based on Bi-LSTM applied to film classification},
journal = {Information Sciences},
volume = {649},
pages = {119632},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.119632},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523012173},
author = {Bin Yu and Ruipeng Cai and Jing Zhang and Yu Fu and Zeshui Xu},
keywords = {Probabilistic linguistic term set, Bi-LSTM, Graph attention network, Film classification},
abstract = {Film reviews contain rich and complex linguistic information that can reflect the opinions and emotions of the reviewers. However, existing methods for emotion classification of film reviews rely on quantifying qualitative evaluations numerically. This approach can lead to difficulties in interpretation, information loss, and performance degradation under massive data. In this paper, we propose a novel method that utilizes a probabilistic linguistic term set (PLTS) and graph attention network (GAT) to classify films based on their emotional content in long reviews. Firstly, the Bi-directional long short-term memory (Bi-LSTM) method is used to convert film reviews into distributed emotional probabilities. This approach not only captures the emotional information in reviews, but also avoids the limitations of numerical quantification. Secondly, using PLTS to represent emotional information not only considers the relationships of linguistic features but also captures multiple emotional information simultaneously. Finally, we utilize multiple GATs to learn and aggregate the distributed emotional probabilities, enabling our method to fully perceive multiple emotional information in the reviews. Experimental results demonstrate that our method outperforms other models in classification accuracy on the IMDB film review dataset. Our method emulates human thinking to analyze emotional information in reviews and uses a human-like attention mechanism to learn the interrelationship between emotions in film reviews. Therefore, our method exhibits significant improvements in both accuracy and interpretability compared to current models, making it applicable to diverse domains that necessitate the analysis of linguistic data. Overall, the proposed method in this paper presents a novel and effective approach to analyzing and classifying films based on linguistic reviews.}
}
@article{SOLTOGGIO201848,
title = {Born to learn: The inspiration, progress, and future of evolved plastic artificial neural networks},
journal = {Neural Networks},
volume = {108},
pages = {48-67},
year = {2018},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2018.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0893608018302120},
author = {Andrea Soltoggio and Kenneth O. Stanley and Sebastian Risi},
keywords = {Artificial neural networks, Lifelong learning, Plasticity, Evolutionary computation},
abstract = {Biological neural networks are systems of extraordinary computational capabilities shaped by evolution, development, and lifelong learning. The interplay of these elements leads to the emergence of biological intelligence. Inspired by such intricate natural phenomena, Evolved Plastic Artificial Neural Networks (EPANNs) employ simulated evolution in-silico to breed plastic neural networks with the aim to autonomously design and create learning systems. EPANN experiments evolve networks that include both innate properties and the ability to change and learn in response to experiences in different environments and problem domains. EPANNs’ aims include autonomously creating learning systems, bootstrapping learning from scratch, recovering performance in unseen conditions, testing the computational advantages of particular neural components, and deriving hypotheses on the emergence of biological learning. Thus, EPANNs may include a large variety of different neuron types and dynamics, network architectures, plasticity rules, and other factors. While EPANNs have seen considerable progress over the last two decades, current scientific and technological advances in artificial neural networks are setting the conditions for radically new approaches and results. Exploiting the increased availability of computational resources and of simulation environments, the often challenging task of hand-designing learning neural networks could be replaced by more autonomous and creative processes. This paper brings together a variety of inspiring ideas that define the field of EPANNs. The main methods and results are reviewed. Finally, new opportunities and possible developments are presented.}
}
@article{ZHANG2025,
title = {Study on the gene mapping and formation mechanism of historical buildings influenced by multiculturalism: A case of Gulangyu, China},
journal = {Frontiers of Architectural Research},
year = {2025},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2024.12.008},
url = {https://www.sciencedirect.com/science/article/pii/S2095263525000056},
author = {Na Zhang and Yuan Li and Jingge Liu and Long Zhao and Mengsheng Yang and Huanxia Bai},
keywords = {Multiculturalism, Gene mapping, Historical building, Gulangyu},
abstract = {Historical buildings influenced by multiculturalism exhibit characteristics of stylistic integration and cultural complexity. Research on multicultural architecture is significant for understanding the collision and fusion of diverse cultures. Currently, studies in this field primarily focus on the qualitative descriptions of architectural styles. In this study, we analyze the floor plans of historical buildings located in the World Cultural Heritage site of Gulangyu Island. Utilizing methods such as typology, spatial schema, space syntax, cluster analysis, and regression analysis, we investigate their gene mapping and formation mechanisms. The results indicate the following: The gene mapping of multicultural plan variants (denoted as "V"), can be expressed as follows: V = L × X% + F × Y%, where "L" represents the local planar prototype, and "F" denotes the foreign planar prototype, and X+Y=100). These variants can be classified into three types. Multicultural plan variants include three generative mechanisms: "Imitation," "Compromise," and "Variation." The spatial layout, central positioning, and traffic flow lines reflect the "Imitation" of local planar prototypes. Connectivity and integration illustrate the "Compromise" between local and foreign planar prototypes. Topological structures and topological depth represent the "Variation" influenced by foreign planar prototypes.}
}
@article{LIN2006709,
title = {On integration of interface design methods: Can debates be resolved?},
journal = {Interacting with Computers},
volume = {18},
number = {4},
pages = {709-722},
year = {2006},
note = {Special Theme Papers from Special Editorial Board Members (contains Regular Papers)},
issn = {0953-5438},
doi = {https://doi.org/10.1016/j.intcom.2005.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S0953543805001104},
author = {Y. Lin and W.J. Zhang and R.J. Koubek and Ronald R. Mourant},
keywords = {Human–computer interface, Total interface design, Interface design principle},
abstract = {There have been many debates on how to design the human–computer interface (HCI). Often, one can find that different views in a debate are simply because these views are attached to different aspects which embody the same thing. In other words, prior to giving an effective judgment of a debate, one needs to establish an understanding of the ‘total’ aspects of a thing the debate is about. Following this line of thinking, in this paper, we propose an understanding of the ‘total’ aspects of designing HCI, which is called the total interface design framework. We then judge several debates under this framework with the purpose of exemplifying the judgment process for any other debate related to designing HCI. At the end, the debates used for exemplifying our judgment process can be resolved. The effectiveness of the total interface design framework for integrating the different HCI approaches is also demonstrated.}
}
@article{PEREIRA2014126,
title = {Analytical model for calculating indeterminate results interval of screening tests, the effect on seroconversion window period: A brief evaluation of the impact of uncertain results on the blood establishment budget},
journal = {Transfusion and Apheresis Science},
volume = {51},
number = {2},
pages = {126-131},
year = {2014},
issn = {1473-0502},
doi = {https://doi.org/10.1016/j.transci.2014.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S1473050214001736},
author = {Paulo Pereira and James O. Westgard and Pedro Encarnação and Jerard Seghatchian},
keywords = {Bias, Delta-value, Precision, Seroconversion window period, Total analytical error},
abstract = {The evaluation of measurement uncertainty is not required by the European Union regulation for blood establishments' laboratory tests. However, it is required for tests accredited by ISO 15189. Also, the forthcoming ISO 9001 edition requires “risk based thinking” with risk described as “the effect of uncertainty on an expected result”. ISO recommends GUM models for determination of measurement uncertainty, but their application is not intended for ordinal value measurements, such as what happens with screening test binary results. This article reviews, discusses and proposes concepts intended for measurement uncertainty of screening test results. The precision model focuses on cutoff level allowing the evaluation of the indeterminate interval using analytical sources of variance. The interval is considered in the estimation of the seroconversion window period. The delta-value of patients and healthy subjects' samples allows ranking two tests according to the probability of the two classes of indeterminate results: chance of false negative results and chance of false positive results (waste on budget).}
}
@article{TAMM2016251,
title = {Slow sluggish cognitive tempo symptoms are associated with poorer academic performance in children with ADHD},
journal = {Psychiatry Research},
volume = {242},
pages = {251-259},
year = {2016},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2016.05.054},
url = {https://www.sciencedirect.com/science/article/pii/S0165178115304686},
author = {Leanne Tamm and Annie A. Garner and Richard E.A. Loren and Jeffery N. Epstein and Aaron J. Vaughn and Heather A. Ciesielski and Stephen P. Becker},
keywords = {Learning difficulties, School children, Apathy/disinterest, Slowed behavior/thinking},
abstract = {Sluggish cognitive tempo (SCT) symptoms may confer risk for academic impairment in attention-deficit/hyperactivity disorder (ADHD). We investigated SCT in relation to academic performance and impairment in 252 children (ages 6–12, 67% boys) with ADHD. Parents and teachers completed SCT and academic impairment ratings, and achievement in reading, math, and spelling was assessed. Simultaneous regressions controlling for IQ, ADHD, and comorbidities were conducted. Total SCT predicted parent-rated impairments in writing, mathematics, and overall school but not reading. Parent-rated SCT Slow predicted poorer reading and spelling, but not math achievement. Teacher-rated SCT Slow predicted poorer spelling and math, but not reading achievement. Parent-rated SCT Slow predicted greater academic impairment ratings across all domains, whereas teacher-rated SCT Slow predicted greater impairment in writing only. Age and gender did not moderate these relationships with the exception of math impairment; SCT slow predicted math impairment for younger but not older children. Parent and teacher SCT Sleepy and Daydreamy ratings were not significant predictors. SCT Slow appears to be uniquely related to academic problems in ADHD, and may be important to assess and potentially target in intervention. More work is needed to better understand the nature of SCT Slow symptoms in relation to inattention and amotivation.}
}
@incollection{FOX2005103,
title = {7 Knowledge, arguments, and intentions in clinical decision-making},
editor = {Ray Paton and Laura A. McNamara},
series = {Studies in Multidisciplinarity},
publisher = {Elsevier},
volume = {3},
pages = {103-129},
year = {2005},
booktitle = {Multidisciplinary Approaches to Theory in Medicine},
issn = {1571-0831},
doi = {https://doi.org/10.1016/S1571-0831(06)80011-0},
url = {https://www.sciencedirect.com/science/article/pii/S1571083106800110},
author = {John Fox and David Glasspool},
abstract = {Publisher Summary
The most influential theories of reasoning and decision making were developed by mathematicians and logicians, often informed by problems in some practical domain such as medicine or economics. Their work led to theoretical concepts with great intellectual depth and formal rigor, such as statistical decision theory (SDT). There are difficulties with expected utility and other mathematical techniques for practical decision making. Any quantitative decision procedure depends upon the ability to estimate the required parameters. This can be problematic in real-world applications. Classical decision theory focuses on only a small part of the decision process—making the choice. There are deep issues about the adequacy of quantitative formalisms to represent the kinds of knowledge and forms of reasoning that are routinely employed in medical thinking. This chapter presents an alternative framework that is formally sound but avoids the shortcomings of standard quantitative decision procedures.}
}
@incollection{VALLERO2025ix,
title = {Preface},
editor = {Daniel A. Vallero},
booktitle = {Fundamentals of Water Pollution},
publisher = {Elsevier},
pages = {ix-xi},
year = {2025},
isbn = {978-0-443-28987-3},
doi = {https://doi.org/10.1016/B978-0-443-28987-3.10000-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780443289873100001},
author = {Daniel A. Vallero}
}
@article{CANDLER201537,
title = {Rate-dependent energetic processes in hypersonic flows},
journal = {Progress in Aerospace Sciences},
volume = {72},
pages = {37-48},
year = {2015},
note = {Celebrating 60 Years of the Air Force Office of Scientific Research (AFOSR): A Review of Hypersonic Aerothermodynamics},
issn = {0376-0421},
doi = {https://doi.org/10.1016/j.paerosci.2014.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0376042114000852},
author = {Graham V. Candler},
keywords = {Nonequilibrium, Hypersonics, Computational fluid dynamics},
abstract = {In celebration of the first 60 years of the Air Force Office of Scientific Research, several studies of hypersonic flows dominated by rate-dependent energetic processes are revisited. The work presented shows the evolution and advancement of computational capabilities in this area, and illustrates some key lessons learned over the previous decade or so. Early work with Leyva and Hornung in the California Institute of Technology T5 Free-Piston Shock Tunnel had the goal of validating thermochemical models for high-enthalpy flows. Several of these flows are re-analyzed with more advanced numerical methods, resulting in improved comparisons with the experimental measurements. This work was followed by a series of experiments in the Calspan-University at Buffalo Research Center (now CUBRC Inc.) facilities at lower enthalpy conditions. Initial comparisons were poor, but with a better understanding of the facility behavior and the inclusion of key finite-rate processes, excellent agreement was obtained for nitrogen flows. An interesting study related to plasmadynamics and finite-rate processes in a different type of flow is discussed. Finally, it is shown that recent advances in numerical methods that are beginning to enable the direct numerical simulation of key rate-dependent energetic processes in hypersonic flows.}
}
@article{GATI2021298,
title = {Differentially private data fusion and deep learning Framework for Cyber–Physical–Social Systems: State-of-the-art and perspectives},
journal = {Information Fusion},
volume = {76},
pages = {298-314},
year = {2021},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2021.04.017},
url = {https://www.sciencedirect.com/science/article/pii/S1566253521000890},
author = {Nicholaus J. Gati and Laurence T. Yang and Jun Feng and Xin Nie and Zhian Ren and Samwel K. Tarus},
keywords = {Differential privacy, Deep computation, Data fusion, CPSS},
abstract = {The modern technological advancement influences the growth of the cyber–physical system and cyber–social system to a more advanced computing system cyber–physical–social system (CPSS). Therefore, CPSS leads the data science revolution by promoting tri-space information resource from a single space. The establishment of CPSSs increases the related privacy concerns. To provide privacy on CPSSs data, various privacy-preserving schemes have been introduced in the recent past. However, technological advancement in CPSSs requires the modifications of previous techniques to suit its dynamics. Meanwhile, differential privacy has emerged as an effective method to safeguard CPSSs data privacy. To completely comprehend the state-of-the-art developments and learn the field’s research directions, this article provides a comprehensive review of differentially private data fusion and deep learning in CPSSs. Additionally, we present a novel differentially private data fusion and deep learning Framework for Cyber–Physical–Social Systems , and various future research directions for CPSSs.}
}
@article{SMITH2017274,
title = {The hierarchical basis of neurovisceral integration},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {75},
pages = {274-296},
year = {2017},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2017.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S014976341630673X},
author = {Ryan Smith and Julian F. Thayer and Sahib S. Khalsa and Richard D. Lane},
keywords = {Neurovisceral integration, Cardiac vagal control, Heart rate variability, Interoception, Emotion, Predictive coding, Cognitive control},
abstract = {The neurovisceral integration (NVI) model was originally proposed to account for observed relationships between peripheral physiology, cognitive performance, and emotional/physical health. This model has also garnered a considerable amount of empirical support, largely from studies examining cardiac vagal control. However, recent advances in functional neuroanatomy, and in computational neuroscience, have yet to be incorporated into the NVI model. Here we present an updated/expanded version of the NVI model that incorporates these advances. Based on a review of studies of structural/functional anatomy, we first describe an eight-level hierarchy of nervous system structures, and the contribution that each level plausibly makes to vagal control. Second, we review recent work on a class of computational models of brain function known as “predictive coding” models. We illustrate how the computational dynamics of these models, when implemented within our proposed vagal control hierarchy, can increase understanding of the relationship between vagal control and both cognitive performance and emotional/physical health. We conclude by discussing novel implications of this updated NVI model for future research.}
}
@article{KOUTAMANIS199340,
title = {Computer vision in architectural design},
journal = {Design Studies},
volume = {14},
number = {1},
pages = {40-57},
year = {1993},
issn = {0142-694X},
doi = {https://doi.org/10.1016/S0142-694X(05)80004-3},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X05800043},
author = {Alexander Koutamanis and Vicky Mitossi},
keywords = {computer vision, visual design representations, computer-aided design and drafting},
abstract = {The visual representations traditionally used by architects form a primary source for the analysis and understanding of architectural design. Computerization and, in particular, computer vision offer a new opportunity for making explicit the general cognitive mechanisms and the domain knowledge involved in these representations, as well as for developing subsequently intelligent computer tools which support and facilitate the representation of design thinking and its products.}
}
@article{PESTI20132487,
title = {Symposium: Experimental design for poultry production and genomics research1 1Presented as part of the Experimental Design for Poultry Production and Genomics Research Symposium at the Poultry Science Association's annual meeting in Athens, Georgia, July 12, 2012.},
journal = {Poultry Science},
volume = {92},
number = {9},
pages = {2487-2489},
year = {2013},
issn = {0032-5791},
doi = {https://doi.org/10.3382/ps.2012-02733},
url = {https://www.sciencedirect.com/science/article/pii/S0032579119394623},
author = {Gene M. Pesti and Samuel E. Aggrey and Bryan I. Fancher},
keywords = {statistics, biometrics, experimental design, genomics},
abstract = {This symposium dealt with the theoretical and practical aspects of choosing and evaluating experimental designs, and how experimental results may be related to poultry production through modeling. Additionally, recent advances in techniques for generating high-throughput genomic sequencing data, genomic breeding values, genomics selection, and genome-wide association studies have provided unique computational challenges to the poultry industry. Such challenges were presented and discussed.}
}
@article{MARK2011354,
title = {Genital Herpes Testing Among Persons Living With HIV},
journal = {Journal of the Association of Nurses in AIDS Care},
volume = {22},
number = {5},
pages = {354-361},
year = {2011},
issn = {1055-3290},
doi = {https://doi.org/10.1016/j.jana.2011.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S1055329011000057},
author = {Hayley D. Mark and Marguerite Lucea and Joy P. Nanda and Jason E. Farley and Lisa Gilbert},
keywords = {genital herpes, herpes simplex virus, HIV, serological screening},
abstract = {This cross-sectional survey explored the frequency of genital herpes testing among 110 people living with HIV (PLWH) and reported barriers and facilitators related to testing. Forty-four percent of the respondents had not been tested for genital herpes since receiving an HIV diagnosis, 34% had been tested, and 22% preferred not to say. Respondents’ most frequently cited factors affecting a decision to not be tested were: (a) testing not being recommended by a provider, (b) not having herpes symptoms, and (c) not thinking they had herpes. Data from this study indicated that PLWH were not frequently tested for genital herpes; there was a limited understanding of the frequently subclinical nature of infection; and provider recommendations for testing, or lack thereof, affected testing decisions.}
}
@article{GEFEN2016828,
title = {Cytoskeleton and plasma-membrane damage resulting from exposure to sustained deformations: A review of the mechanobiology of chronic wounds},
journal = {Medical Engineering & Physics},
volume = {38},
number = {9},
pages = {828-833},
year = {2016},
issn = {1350-4533},
doi = {https://doi.org/10.1016/j.medengphy.2016.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S1350453316301114},
author = {Amit Gefen and Daphne Weihs},
keywords = {Chronic wounds, Mechanical loading, Sustained deformation, Cell damage},
abstract = {The purpose of this review paper is to summarize the current knowledge on cell-scale mechanically-inflicted deformation-damage, which is at the frontier of cell mechanobiology and biomechanics science, specifically in the context of chronic wounds. The dynamics of the mechanostructure of cells and particularly, the damage occurring to the cytoskeleton and plasma-membrane when cells are chronically deformed (as in a weight-bearing static posture) is correlated to formation of the most common chronic wounds and injuries, such as pressure ulcers (injuries). The first occurrence is microscopic injury which onsets as damage in individual cells and then progresses macroscopically to the tissue-scale. Here, we specifically focus on sub-catastrophic and catastrophic damage to cells that can result from mechanical loads that are delivered statically or at physiological rates; this results in apoptosis at prolonged times or necrosis, rapidly. We start by providing a basic background of cell mechanics and dynamics, focusing on the plasma-membrane and the cytoskeleton, and discuss approaches to apply and estimate deformations in cells. We then consider the effects of different levels of mechanical loads, i.e. low, high and intermediate, and describe the expected damage in terms of time-scales of application and in terms of cell response, providing experimental examples where available. Finally, we review different theoretical and computational modeling approaches that have been used to describe cell responses to sustained deformation. We highlight the insights that those models provide to explain, for example, experimentally observed variabilities in cell damage and death under loading.}
}
@article{GRAZZINI201726,
title = {Bayesian estimation of agent-based models},
journal = {Journal of Economic Dynamics and Control},
volume = {77},
pages = {26-47},
year = {2017},
issn = {0165-1889},
doi = {https://doi.org/10.1016/j.jedc.2017.01.014},
url = {https://www.sciencedirect.com/science/article/pii/S0165188917300222},
author = {Jakob Grazzini and Matteo G. Richiardi and Mike Tsionas},
keywords = {Agent-based, Estimation, Bayes, Approximate Bayesian computation, Likelihood},
abstract = {We consider Bayesian inference techniques for agent-based (AB) models, as an alternative to simulated minimum distance (SMD). Three computationally heavy steps are involved: (i) simulating the model, (ii) estimating the likelihood and (iii) sampling from the posterior distribution of the parameters. Computational complexity of AB models implies that efficient techniques have to be used with respect to points (ii) and (iii), possibly involving approximations. We first discuss non-parametric (kernel density) estimation of the likelihood, coupled with Markov chain Monte Carlo sampling schemes. We then turn to parametric approximations of the likelihood, which can be derived by observing the distribution of the simulation outcomes around the statistical equilibria, or by assuming a specific form for the distribution of external deviations in the data. Finally, we introduce Approximate Bayesian Computation techniques for likelihood-free estimation. These allow embedding SMD methods in a Bayesian framework, and are particularly suited when robust estimation is needed. These techniques are first tested in a simple price discovery model with one parameter, and then employed to estimate the behavioural macroeconomic model of De Grauwe (2012), with nine unknown parameters.}
}
@article{ENRIZ2005163,
title = {The legacy of the past, the reality of the present and the hopes of the future},
journal = {Journal of Molecular Structure: THEOCHEM},
volume = {731},
number = {1},
pages = {163-172},
year = {2005},
issn = {0166-1280},
doi = {https://doi.org/10.1016/j.theochem.2004.10.048},
url = {https://www.sciencedirect.com/science/article/pii/S0166128004008553},
author = {R.D. Enriz},
keywords = {Computational medicinal chemistry, Molecular modelling, Rational drug design},
abstract = {This view point paper has attempted to penetrate a field of research in which the efforts of a large number of the most varied modern computational techniques are converging, that of computational medicinal chemistry. We have never been so close to realising the dream of the pharmaceutical industry: that we may be able to design new drugs from first principles. However, there are still enough unknown areas where computational medicinal chemists can successfully use their sagacity and creativity. A personal point of view on the present approaches aimed for a rational drug design is given here. Perspectives on future developments in this field are also outlined in this assessment.}
}
@article{SOUSA2014569,
title = {Sociomaterial Enactment Drive of Business/IT Alignment: From Small Data to Big Impact},
journal = {Procedia Technology},
volume = {16},
pages = {569-582},
year = {2014},
note = {CENTERIS 2014 - Conference on ENTERprise Information Systems / ProjMAN 2014 - International Conference on Project MANagement / HCIST 2014 - International Conference on Health and Social Care Information Systems and Technologies},
issn = {2212-0173},
doi = {https://doi.org/10.1016/j.protcy.2014.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S2212017314002321},
author = {José L.R. Sousa and Ricardo J. Machado},
keywords = {Business / IT alignment, complex-networks, profiling framework, information systems;},
abstract = {Business/IT alignment is an information systems research field with a long existence and a high number of researchers and represents a central direction on the thinking about the relation between business and the information systems. It aims to achieve a paradigm, one in which there is a high degree of visibility and availability of information, about the information systems sociomateriality. Complex-networks constitute an approach to the study of emergent properties of complex systems that strongly focuses and relies on models and measures, through which build the system interdependence. Several contributions of complex-networks are: topology always affects the function; separated from the domain; quantification of element's relationships; visibility and capture of emergent properties. This work expects to contribute for the appropriate use of complex-networks models and measure in the drive of the information systems alignment. This work considers an exploratory case research strategy. It uses an exploratory case developed in the field of information systems that directed its deployment to sustain the business development and evolution. This paper illustrates a profiling framework that introduces a global and elementary use of the sociomaterial enactment. From the analysis of the exploratory case, the paper infers drivers of the information systems Business/IT alignment.}
}
@article{JU2017180,
title = {Single image haze removal based on the improved atmospheric scattering model},
journal = {Neurocomputing},
volume = {260},
pages = {180-191},
year = {2017},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2017.04.034},
url = {https://www.sciencedirect.com/science/article/pii/S0925231217307051},
author = {Mingye Ju and Zhenfei Gu and Dengyin Zhang},
keywords = {Improved atmospheric scattering model, Linear model, Gaussian–Laplacian pyramid, Image haze removal, Haze aware density feature},
abstract = {In this paper, we propose an improved atmospheric scattering model (IASM) to overcome the inherent limitation of the traditional atmospheric scattering model. Based on the IASM, a fast single image dehazing algorithm is also presented. In this algorithm, by constructing a linear model between the transmission and the haze aware density feature, the transmission map can be directly estimated through a linear operation on three components: luminance, saturation and gradient. Combining the sky-relevant feature and the proposed guided energy model (GEM), we can accurately estimate the atmospheric light and scene incident light, and can further restore the scene albedo via the IASM. Finally, an accelerating framework (AF) based on the Gaussian–Laplacian pyramid is proposed to increase the computational speed. Experimental results demonstrate that the proposed algorithm outperforms most of the prevalent algorithms in terms of visual effect and computational efficiency. Besides, it is also capable of processing various types of degraded images in addition to hazy images.}
}
@article{BELLAOUAR2021106654,
title = {Weighted automata sequence kernel: Unification and generalization},
journal = {Knowledge-Based Systems},
volume = {212},
pages = {106654},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2020.106654},
url = {https://www.sciencedirect.com/science/article/pii/S0950705120307838},
author = {Slimane Bellaouar and Hadda Cherroun and Attia Nehar and Djelloul Ziadi},
keywords = {Sequence kernel, Weighted automata, Formal power series, Sequence multiset kernel, Unification, Generalization},
abstract = {Sequence kernels have been widely used for learning from sequential data. In recent years, a significant effort has been devoted to sequence kernels focusing on individual problems, and so devising several approaches. As a contribution in developing a unified theory of machine learning, in this paper, we complement our previous general framework that deals with sequence kernels, termed weighted automata sequence kernel. We provide a more formal presentation of the framework fully supported with proofs. In fact, the mapping of a string s to a high dimensional feature space can be modeled by a formal power series that can be realized by a weighted automaton (WA) As representing all subsequences of the string s. The computation of the kernel K(s,t) between two strings is the behavior of the intersection weighted automaton As,t=As∩At. Regarding kernel computation efficiency, we propose a forward lookup automaton intersection technique to prevent unsuccessful ε-paths while evaluating the WA computations. The experiments use the Reuters-21578 collection. The results reveal that the kernel evaluation using our proposed technique is faster than the one that uses standard intersection. In order To highlight the unification aspect of our model, we study the relationship between our general framework and a variety of common sequence kernels. Finally, we prove that the proposed formalization can be generalized to sequence multiset kernels showing the robustness of our approach. In our case, the new defined sequence multiset kernel can also be seen as a tree kernel.}
}
@article{BALASUNDARAM2022764,
title = {Graph signatures: Identification and optimization},
journal = {European Journal of Operational Research},
volume = {296},
number = {3},
pages = {764-775},
year = {2022},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2021.03.051},
url = {https://www.sciencedirect.com/science/article/pii/S0377221721002770},
author = {Balabhaskar Balasundaram and Juan S. Borrero and Hao Pan},
keywords = {Networks, Relational, Temporal, Cross-graph mining, Frequent subgraph mining},
abstract = {We introduce a new graph-theoretic paradigm called a graph signature that describes persistent patterns in a sequence of graphs. This framework is motivated by the need to detect subgraphs of significance in temporal networks, e.g., social and biological networks that evolve over time. Because the subgraphs of interest may not all “look alike” in the snapshots of the temporal network, the framework deems a subgraph to be persistent if it satisfies one of several preselected properties in each snapshot of a consecutive subsequence. The persistency requirement is parameterized by the length of this subsequence. This discrete mathematical framework can be viewed more broadly as a way to generalize classical graph properties and invariants associated with a single graph to a sequence of graphs. In this introductory article, we formulate the graph signature identification problem as a mixed-integer program and propose an algorithmic framework based on dynamic programming. This methodology is applicable to any collection of mixed-integer representable graph properties. We also demonstrate how this framework can be tailored to exploit property-specific decomposition and scale reduction techniques through three different computational case-studies. Our experiments show that the dynamic programming algorithm solves this problem across most instances in our test bed to optimality. Moreover, for the instances in our test bed, the optimal signature sizes are comparable to those of their static counterparts, suggesting that our new framework can identify subgraphs of significance in complex dynamic networks.}
}
@article{BOLGOVA2025,
title = {Large Language Models in Biochemistry Education: Comparative Evaluation of Performance},
journal = {JMIR Medical Education},
volume = {11},
year = {2025},
issn = {2369-3762},
doi = {https://doi.org/10.2196/67244},
url = {https://www.sciencedirect.com/science/article/pii/S2369376225000534},
author = {Olena Bolgova and Inna Shypilova and Volodymyr Mavrych},
keywords = {ChatGPT, Claude, Gemini, Copilot, biochemistry, LLM, medical education, artificial intelligence, NLP, natural language processing, machine learning, large language model, AI, ML, comprehensive analysis, medical students, GPT-4, questionnaire, medical course, bioenergetics},
abstract = {Background
Recent advancements in artificial intelligence (AI), particularly in large language models (LLMs), have started a new era of innovation across various fields, with medicine at the forefront of this technological revolution. Many studies indicated that at the current level of development, LLMs can pass different board exams. However, the ability to answer specific subject-related questions requires validation.
Objective
The objective of this study was to conduct a comprehensive analysis comparing the performance of advanced LLM chatbots—Claude (Anthropic), GPT-4 (OpenAI), Gemini (Google), and Copilot (Microsoft)—against the academic results of medical students in the medical biochemistry course.
Methods
We used 200 USMLE (United States Medical Licensing Examination)–style multiple-choice questions (MCQs) selected from the course exam database. They encompassed various complexity levels and were distributed across 23 distinctive topics. The questions with tables and images were not included in the study. The results of 5 successive attempts by Claude 3.5 Sonnet, GPT-4‐1106, Gemini 1.5 Flash, and Copilot to answer this questionnaire set were evaluated based on accuracy in August 2024. Statistica 13.5.0.17 (TIBCO Software Inc) was used to analyze the data’s basic statistics. Considering the binary nature of the data, the chi-square test was used to compare results among the different chatbots, with a statistical significance level of P<.05.
Results
On average, the selected chatbots correctly answered 81.1% (SD 12.8%) of the questions, surpassing the students’ performance by 8.3% (P=.02). In this study, Claude showed the best performance in biochemistry MCQs, correctly answering 92.5% (185/200) of questions, followed by GPT-4 (170/200, 85%), Gemini (157/200, 78.5%), and Copilot (128/200, 64%). The chatbots demonstrated the best results in the following 4 topics: eicosanoids (mean 100%, SD 0%), bioenergetics and electron transport chain (mean 96.4%, SD 7.2%), hexose monophosphate pathway (mean 91.7%, SD 16.7%), and ketone bodies (mean 93.8%, SD 12.5%). The Pearson chi-square test indicated a statistically significant association between the answers of all 4 chatbots (P<.001 to P<.04).
Conclusions
Our study suggests that different AI models may have unique strengths in specific medical fields, which could be leveraged for targeted support in biochemistry courses. This performance highlights the potential of AI in medical education and assessment.}
}
@article{SUE20219,
title = {Generative design in factory layout planning},
journal = {Procedia CIRP},
volume = {99},
pages = {9-14},
year = {2021},
note = {14th CIRP Conference on Intelligent Computation in Manufacturing Engineering, 15-17 July 2020},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2021.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212827121002584},
author = {Marian Süße and Matthias Putz},
keywords = {Generative design, Computational design, Evolutionary design, Factory planning, Layout planning, Literature search},
abstract = {Planning and optimization of facility layouts have been investigated for decades and manifold approaches are applied for structuring and design of production layouts. However, results heavily depend on the experience and creativity of involved planning experts. Currently, complexity of planning processes constantly increases, e.g. due to further requirements of energy and media supply. Generative Design, hitherto mainly applied in component development, provides opportunities to cope with a much larger solution space and develop creative layout concepts. Thus, based on a structured overview on established planning methods, a concept and first results of factory layout planning with Generative Design are described.}
}
@article{WIDDICKS2024101092,
title = {A multi-dimensional approach to the future of digital research infrastructure for systemic environmental science},
journal = {Patterns},
volume = {5},
number = {11},
pages = {101092},
year = {2024},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2024.101092},
url = {https://www.sciencedirect.com/science/article/pii/S2666389924002642},
author = {Kelly Widdicks and Faiza Samreen and Gordon S. Blair and Susannah Rennie and John Watkins},
abstract = {Digital research infrastructure (DRI) for environmental science requires significant transformation to support the changing nature of science and utilize digital innovations. Numerous challenges prevent this change yet simultaneously pose exciting principles to drive the future of DRI. This opinion piece details a multi-dimensional approach toward these futures for the environmental community.}
}
@article{SESSIONS2022102549,
title = {Mapping geometric and electromagnetic feature spaces with machine learning for additively manufactured RF devices},
journal = {Additive Manufacturing},
volume = {50},
pages = {102549},
year = {2022},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2021.102549},
url = {https://www.sciencedirect.com/science/article/pii/S2214860421006965},
author = {Deanna Sessions and Venkatesh Meenakshisundaram and Andrew Gillman and Alexander Cook and Kazuko Fuchi and Philip R. Buskohl and Gregory H. Huff},
keywords = {Additive manufacturing, Direct-ink write, Electromagnetics, Machine learning, Radio frequency},
abstract = {Multi-material additive manufacturing enables transformative capabilities in customized, low-cost, and multi-functional electromagnetic devices. However, process-specific fabrication anomalies can result in non-intuitive effects on performance; we propose a framework for identifying defect mechanisms and their performance impact by mapping geometric variances to electromagnetic performance metrics. This method can accelerate additive fabrication feedback while avoiding the high computational cost of in-line electromagnetic simulation. We first used dimension reduction to explore the population of geometric manufacturing anomalies and electromagnetic performance. Convolutional neural networks are then trained to predict the electromagnetic performance of the printed geometries. In generating the networks, we explored two inputs: one image-derived geometric description and one using the same description with additional simulated electromagnetic information. Network latent space analysis shows the networks learned both geometric and electromagnetic values even without electromagnetic input. This result demonstrates it is possible to create accelerated additive feedback systems predicting electromagnetic performance without in-line simulation.}
}
@article{PRATAMA2023338,
title = {WizardOfMath: A top-down puzzle game with RPG elements to hone the player's arithmetic skills},
journal = {Procedia Computer Science},
volume = {216},
pages = {338-345},
year = {2023},
note = {7th International Conference on Computer Science and Computational Intelligence 2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.144},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922022220},
author = {Mulia Pratama and Yanfi Yanfi and Pualam Dipa Nusantara},
keywords = {Game, math, Game Development Life Cycle, Game Experience Questionnaire},
abstract = {As one of the important education subjects’ mathematics difficulties can lead to tension and be described as the most hated or feared subject. This study aims to create a puzzle game application with RPG elements called WizardOfMath to increase a user's interest in mathematics subject. The research method includes a method called Game Development Life Cycle (GDLC), which has a pre-production stage that is suitable for game development rather than the Waterfall method. A game application is built based on the prior requirement gathering. The evaluation was arranged using the Game Experience Questionnaire (GEQ) survey which is performed by providing an online form to the public. Reliability test of GEQ modules meets Cronbach's Alpha value above 0.7 and the validity test of the r table is greater than 0.05. The results calculation of the Game Experience Questionnaire (GEQ) from a total of 55 participants and 3 modular structures, which are the Core module, In-game Module, and Post-game Module obtained an average score of 4.06, 3.88, and 3.57 for positive aspects and 2,72, 2.67, and 2,61 for the negative aspect. The contribution of this study shows this puzzle game application with RPG elements decreased user tension and the negative effect of being involved with mathematics subjects.}
}
@article{SESTER2021301121,
title = {A comparative study of support vector machine and neural networks for file type identification using n-gram analysis},
journal = {Forensic Science International: Digital Investigation},
volume = {36},
pages = {301121},
year = {2021},
note = {DFRWS 2021 EU - Selected Papers and Extended Abstracts of the Eighth Annual DFRWS Europe Conference},
issn = {2666-2817},
doi = {https://doi.org/10.1016/j.fsidi.2021.301121},
url = {https://www.sciencedirect.com/science/article/pii/S2666281721000184},
author = {Joachim Sester and Darren Hayes and Mark Scanlon and Nhien-An Le-Khac},
keywords = {File type identification, n-grams analysis, Forensic analysis, Neural networks, Support vector machine},
abstract = {File type identification (FTI) has become a major discipline for anti-virus developers, firewall designers and for forensic cybercrime investigators. Over the past few years, research has seen the introduction of several classifiers and features. One of these advances is the so-called n-grams analysis, which is an interpretation of statistical counting in classified fragments. Recently, n-grams based approaches were already successfully combined with computational intelligence classifiers. However, the academic body of literature is scant when it comes to a comprehensive explanation of machine learning based approaches such as neural networks (NN) or support vector machines (SVM). For example, how the input parameters, including learning rate, different values of n for n-grams, etc. influence the results. In addition, very few studies have compared the scalability of NN vs. SVM approaches. Therefore, a systematic research in comparing different approaches is needed to address these questions. Hence, this paper investigates this type of comparison, by focusing on the n-gram analysis as a feature for the two different classifiers: SVMs and NNs. This paper details our experiments with two NNs and four SVMs, using linear kernels and RBF kernels on RealDC datasets. In general, we found that SVM-based approaches performed better than the NN, but their scalability is still a challenge.}
}
@article{LIM202256,
title = {XANDAR PHARMACEUTICAL: A model plant for process engineering education},
journal = {Education for Chemical Engineers},
volume = {40},
pages = {56-68},
year = {2022},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2022.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1749772822000173},
author = {Teng Shuen Lim and Zong Lin Chia and Song Yuan Seah and Shin Yee Wong},
keywords = {Pharmaceutical, 3D printed plant model, Industrial relevance, Visualization, Hands-on},
abstract = {This study explores the implementation of a detailed model pharmaceutical production facility in an undergraduate engineering class. Xandar Pharmaceuticals (XP), a fictitious manufacturer, was created and presented to undergraduate engineering students during a current good manufacturing practices (cGMP11current Good Manufacturing Practices.) course in two forms: (1) 3D virtual model and (2) 3D printed model. Data was collected from three separate cohorts over three years with a total of 197 participants. Surveys would gauge student’s sentiments and collect feedback, while quizzes assessed technical understanding. Statistical analysis and effect size calculations would evaluate the differences among the three cohorts. Survey results indicate the 3D printed model has small positive effects on study vs control (groups) regarding understanding of general industry related functions and practices. The 3D printed model also improved students’ interest in critical thinking and investigation. Qualitative feedback and sentiment analysis indicate the model was well received by students and received positive feedback related to visualization, industrial relevance, and student engagement. Use of the 3D printed model (but not the 3D virtual model) has had positive quantitative effects on student quiz scores and feedback. Qualitative improvements to student attitudes and interest are encouraging and suggest further use of 3D printed models in other courses may be beneficial.}
}
@article{PRABATHA2021116304,
title = {Community-level decentralized energy system planning under uncertainty: A comparison of mathematical models for strategy development},
journal = {Applied Energy},
volume = {283},
pages = {116304},
year = {2021},
issn = {0306-2619},
doi = {https://doi.org/10.1016/j.apenergy.2020.116304},
url = {https://www.sciencedirect.com/science/article/pii/S0306261920316895},
author = {Tharindu Prabatha and Hirushie Karunathilake and Amin {Mohammadpour Shotorbani} and Rehan Sadiq and Kasun Hewage},
keywords = {Community energy planning, Renewable energy, Uncertainty modelling, Linear programming, Robust multi-objective optimization, Monte Carlo simulation},
abstract = {Distributed energy systems renewable energy are one solution to the environmental and economic concerns of energy use. While energy planning and optimization have been conducted mainly as a mathematical exercise, practical approaches that incorporate the engineering realities and uncertainties are limited. Decision makers find challenges in community energy planning due to the lack of expertise, planning tools, and information. While a multitude of models and tools are currently available, there are no means of identifying the most appropriate or accurate methods, especially considering uncertainty. The main objective of this study is to compare and identify the strengths and limitations of various mathematical modelling techniques used in energy planning for grid connected renewable energy systems. As a case study demonstration, different multi-objective optimization techniques with and without uncertainty consideration (i.e. robust optimization, linear optimization, Taguchi Orthogonal Array method, and Monte Carlo simulation) were applied on a selected neighborhood in British Columbia. The optimization outcomes and the time and effort for evaluation were compared for the different methods. The findings indicate that robust optimization can be used to develop an uncertainty-based decision model. It significantly reduces evaluation time compared to the other methods. Although the presence of uncertainties can change the optimal configuration of a planned energy system, the assessment method itself does not significantly impact the outcomes. The findings of this study will enable the energy planners and researchers to compare different multi-objective optimization techniques, and to select the best for planning renewable energy projects, especially during the pre-project planning stage.}
}
@article{KS2024137917,
title = {Quantum computing basics, applications and future perspectives},
journal = {Journal of Molecular Structure},
volume = {1308},
pages = {137917},
year = {2024},
issn = {0022-2860},
doi = {https://doi.org/10.1016/j.molstruc.2024.137917},
url = {https://www.sciencedirect.com/science/article/pii/S002228602400440X},
author = {Balamurugan {K S} and Sivakami A and Mathankumar M and Yalla Jnan Devi {Satya prasad} and Irfan Ahmad},
keywords = {Qubits, Quantum computing, Quantum cryptography, Superposition},
abstract = {Quantum Computing observed a significant rise to public and technologies in past three decades, the reason behind for the development of quantum computing is to solve various problems which are so complex that traditional (classical) computers were not able to solve. New technologies, hardware components and software advancements are being discovered all around the world in order to use this powerful tool. But in addition to the development of technologies and the attempt to scale up the quantum computers, new challenges and problems too came in light which makes it tough for further progress in the quest to unlock the true development of quantum computers. Various methods has been identified for Quantum Information Processing (QIP), but the error rates were more than what we would expect often resulting in inappropriate computations which eventually gives inaccurate conclusions.In this work, we discuss about the prominent hardware and software methods to build the quantum computers with low error rates and better accuracy, we will look onto the topics related to qubits and its principles which are incorporated in Quantum Processing Units (QPUs) which govern the working of quantum computers, the topics of quantum algorithms and its methodology are also been discussed to provide a clear understanding of the manipulation of qubits according to the purpose needed. In addition to that we will talk about the applications like quantum teleportation and cryptography which utilizes the quantum computers, and discuss about the future enhancements which can be done using this technology.}
}
@article{MAC201613,
title = {Heuristic approaches in robot path planning: A survey},
journal = {Robotics and Autonomous Systems},
volume = {86},
pages = {13-28},
year = {2016},
issn = {0921-8890},
doi = {https://doi.org/10.1016/j.robot.2016.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0921889015300671},
author = {Thi Thoa Mac and Cosmin Copot and Duc Trung Tran and Robin {De Keyser}},
keywords = {Autonomous navigation, Robot path planning, Heuristic methods, Neural network, Fuzzy logic, Nature inspired algorithms, Potential field method},
abstract = {Autonomous navigation of a robot is a promising research domain due to its extensive applications. The navigation consists of four essential requirements known as perception, localization, cognition and path planning, and motion control in which path planning is the most important and interesting part. The proposed path planning techniques are classified into two main categories: classical methods and heuristic methods. The classical methods consist of cell decomposition, potential field method, subgoal network and road map. The approaches are simple; however, they commonly consume expensive computation and may possibly fail when the robot confronts with uncertainty. This survey concentrates on heuristic-based algorithms in robot path planning which are comprised of neural network, fuzzy logic, nature-inspired algorithms and hybrid algorithms. In addition, potential field method is also considered due to the good results. The strengths and drawbacks of each algorithm are discussed and future outline is provided.}
}
@article{XU2012816,
title = {Intelligent fault inference for rotating flexible rotors using Bayesian belief network},
journal = {Expert Systems with Applications},
volume = {39},
number = {1},
pages = {816-822},
year = {2012},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2011.07.079},
url = {https://www.sciencedirect.com/science/article/pii/S0957417411010414},
author = {Bin Gang Xu},
keywords = {Fault diagnosis, Bayesian belief network, Flexible rotor, Uncertainty inference},
abstract = {Flexible rotor is a crucial mechanical component of a diverse range of rotating machineries and its condition monitoring and fault diagnosis are of particular importance to the modern industry. In this paper, Bayesian belief network (BBN) is applied to the fault inference for rotating flexible rotors with attempt to enhance the reasoning capacity under conditions of uncertainty. A generalized three-layer configuration of BBN for the fault inference of rotating machinery is developed by fully incorporating human experts’ knowledge, machine faults and fault symptoms as well as machine running conditions. Compared with the Naive diagnosis network, the proposed topological structure of causalities takes account of more practical and complete diagnostic information in fault diagnosis. The network tallies well with the practical thinking of field experts in the whole processes of machine fault diagnosis. The applications of the proposed BBN network in the uncertainty inference of rotating flexible rotors show good agreements with our knowledge and practical experience of diagnosis.}
}
@article{DURKIN2024108584,
title = {Surrogate-based optimisation of process systems to recover resources from wastewater},
journal = {Computers & Chemical Engineering},
volume = {182},
pages = {108584},
year = {2024},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2024.108584},
url = {https://www.sciencedirect.com/science/article/pii/S0098135424000024},
author = {Alex Durkin and Lennart Otte and Miao Guo},
keywords = {Surrogate modelling, Derivative-free optimisation, Resource recovery from wastewater},
abstract = {Wastewater systems are transitioning towards integrative process systems to recover multiple resources whilst simultaneously satisfying regulations on final effluent quality. This work contributes to the literature by bringing a systems-thinking approach to resource recovery from wastewater, harnessing surrogate modelling and mathematical optimisation techniques to highlight holistic process systems. A surrogate-based process synthesis methodology was presented to harness high-fidelity data from black box process simulations, embedding first principles models, within a superstructure optimisation framework. Modelling tools were developed to facilitate tailored derivative-free optimisation solutions widely applicable to black box optimisation problems. The optimisation of a process system to recover energy and nutrients from a brewery wastewater reveals significant scope to reduce the environmental impacts of food and beverage production systems. Additionally, the application demonstrates the capabilities of the modelling methodology to highlight optimal processes to recover carbon, nitrogen, and phosphorous resources whilst also accounting for uncertainties inherent to wastewater systems.}
}
@article{LEAVY2011235,
title = {Elementary and middle grade students’ constructions of typicality},
journal = {The Journal of Mathematical Behavior},
volume = {30},
number = {3},
pages = {235-254},
year = {2011},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2011.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0732312311000174},
author = {Aisling M. Leavy and James A. Middleton},
keywords = {Statistical reasoning, Mathematical thinking, Elementary students, Middle grade students, Typicality, Data and statistics},
abstract = {This study addresses the measures chosen by students when selecting or constructing indices to properties of distributions of data. A series of individual teaching experiments were conducted to provide insight into the development of five 4th to 8th grade students’ conceptualizations of distribution over the course of 8 weeks of instruction. During the course of the teaching experiment (emergent) statistical tasks and analogous teacher activities were created and refined in an effort to support the development of understanding. In the process of development, attempts were made by students to coordinate center and variability when constructing measures to index properties of distributions. The results indicate that consideration of representativeness was a major factor that motivated modification of approaches to constructing indices of distributions, and subsequent coordination of indices of variation and center. In particular, the defining features of student's self-constructed “typical” values and notions of spread were examined, resulting in a model of development constituting eight “categories” ranging from the construction of values that did not reflect properties of the data (Category 1) to measures employing conceptual use of the mean in combination with other indices of center and spread (Category 8).}
}
@article{PILLAY2024113656,
title = {Performance of Softcup® menstrual cup and vulvovaginal swab samples for detection and quantification of genital cytokines},
journal = {Journal of Immunological Methods},
volume = {528},
pages = {113656},
year = {2024},
issn = {0022-1759},
doi = {https://doi.org/10.1016/j.jim.2024.113656},
url = {https://www.sciencedirect.com/science/article/pii/S0022175924000413},
author = {Nashlin Pillay and Gugulethu Favourate Mzobe and Marothi Letsoalo and Asavela Olona Kama and Andile Mtshali and Stanley Nzuzo Magini and Nikkishia Singh and Vani Govender and Natasha Samsunder and Megeshinee Naidoo and Dhayendre Moodley and Cheryl Baxter and Derseree Archary and Sinaye Ngcapu},
keywords = {Cytokines, Softcup® menstrual cup, Vulvovaginal swab, Detection, Genital inflammation},
abstract = {Cytokines are important mediators of immunity in the female genital tract, and their levels may be associated with various reproductive health outcomes. However, the measurement of cytokines and chemokines in vaginal fluid samples may be influenced by a variety of factors, each with the potential to affect the sensitivity and accuracy of the assay, including the interpretation and comparison of data. We measured and compared cytokine milieu in samples collected via Softcup® menstrual cup versus vulvovaginal swabs. One hundred and eighty vulvovaginal swabs from CAPRISA 088 and 42 Softcup supernatants from CAPRISA 016 cohorts of pregnant women were used to measure the concentrations of 28 cytokines through multiplexing. Cytokines measured in this study were detectable in each of the methods however, SoftCup supernatants showed consistently, higher detectability, expression ratios, and mean concentration of cytokines than vulvovaginal swabs. While mean concentrations differed, the majority of cytokines correlated between SoftCup supernatants and vulvovaginal swabs. Additionally, there were no significant differences in a number of participants between the two sampling methods for the classification of genital inflammation. Our findings suggest that SoftCup supernatants and vulvovaginal swab samples are suitable for the collection of genital specimens to study biological markers of genital inflammatory response. However, the Softcup menstrual cup performs better for the detection and quantification of soluble biomarkers that are found in low concentrations in cervicovaginal fluid.}
}
@article{ALBEROLA20161,
title = {An artificial intelligence tool for heterogeneous team formation in the classroom},
journal = {Knowledge-Based Systems},
volume = {101},
pages = {1-14},
year = {2016},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2016.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S0950705116000964},
author = {Juan M. Alberola and Elena {del Val} and Victor Sanchez-Anguix and Alberto Palomares and Maria {Dolores Teruel}},
keywords = {Team formation, Artificial intelligence, Belbin roles, Computational intelligence},
abstract = {Nowadays, there is increasing interest in the development of teamwork skills in the educational context. This growing interest is motivated by its pedagogical effectiveness and the fact that, in labour contexts, enterprises organise their employees in teams to carry out complex projects. Despite its crucial importance in the classroom and industry, there is a lack of support for the team formation process. Not only do many factors influence team performance, but the problem becomes exponentially costly if teams are to be optimised. In this article, we propose a tool whose aim it is to cover such a gap. It combines artificial intelligence techniques such as coalition structure generation, Bayesian learning, and Belbin’s role theory to facilitate the generation of working groups in an educational context. This tool improves current state of the art proposals in three ways: i) it takes into account the feedback of other teammates in order to establish the most predominant role of a student instead of self-perception questionnaires; ii) it handles uncertainty with regard to each student’s predominant team role; iii) it is iterative since it considers information from several interactions in order to improve the estimation of role assignments. We tested the performance of the proposed tool in an experiment involving students that took part in three different team activities. The experiments suggest that the proposed tool is able to improve different teamwork aspects such as team dynamics and student satisfaction.}
}
@article{DIAF2022102179,
title = {Sharks and minnows in a shoal of words: Measuring latent ideological positions based on text mining techniques},
journal = {European Journal of Political Economy},
volume = {75},
pages = {102179},
year = {2022},
issn = {0176-2680},
doi = {https://doi.org/10.1016/j.ejpoleco.2022.102179},
url = {https://www.sciencedirect.com/science/article/pii/S0176268022000015},
author = {Sami Diaf and Jörg Döpke and Ulrich Fritsche and Ida Rockenbach},
keywords = {Political economy, Ideology, Text scaling model, Wordfish, Wordshoal, Computational content analysis, Hierarchical factor model, Bayesian estimation, Polarization, Public policy, Monetary policy, Fiscal policy},
abstract = {We scale theoretical/ideological positions of economic research institutes over debates. Using only parts of German research institutes’ business cycle reports that deal with economic policy advice as an example, we extract sections from these reports dealing with monetary and fiscal policy issues from 1999 to 2020. To these corpora, we apply methods of unsupervised text scaling (Slapin and Proksch, 2008; Lauderdale and Herzog, 2016), namely Wordfish and Wordshoal. Roughly, results are in line with common sense in the public policy discourse. For monetary policy texts, we observe a strong, but short-lived consensus in debate-specific positions at the height of the financial crisis in 2008 and a larger polarization thereafter compared to the sample period before. For the fiscal policy text corpus, the polarization was similarly high before and after the crisis and decreases somewhat during the COVID-19 pandemic. For both policy areas, the German Institute of Economic Research (DIW), Berlin, and the Institute for World Economics (IfW), Kiel, tend to be the most diverse institutes within the spectrum of latent ideological positions. We argue that text-mining techniques might be useful to scale underlying ideological positions in policy-related publications.}
}
@article{LAKHLIFI2023104181,
title = {Heuristics and biases in medical decision-making under uncertainty: The case of neuropronostication for consciousness disorders},
journal = {La Presse Médicale},
volume = {52},
number = {2},
pages = {104181},
year = {2023},
note = {Disorders of Consciousness},
issn = {0755-4982},
doi = {https://doi.org/10.1016/j.lpm.2023.104181},
url = {https://www.sciencedirect.com/science/article/pii/S0755498223000180},
author = {Camille Lakhlifi and Benjamin Rohaut},
abstract = {Neuropronostication for consciousness disorders can be very complex and prone to high uncertainty. Despite notable advancements in the development of dedicated scales and physiological markers using innovative paradigms, these technical progressions are often overshadowed by factors intrinsic to the medical environment. Beyond the scarcity of objective data guiding medical decisions, factors like time pressure, fatigue, multitasking, and emotional load can drive clinicians to rely more on heuristic-based clinical reasoning. Such an approach, albeit beneficial under certain circumstances, may lead to systematic error judgments and impair medical decisions, especially in complex and uncertain environments. After a brief review of the main theoretical frameworks, this paper explores the influence of clinicians' cognitive biases on clinical reasoning and decision-making in the challenging context of neuroprognostication for consciousness disorders. The discussion further revolves around developing and implementing various strategies designed to mitigate these biases and their impact, aiming to enhance the quality of care and the patient safety.}
}
@article{CUI2024102334,
title = {Improved matrix model of sequence grid partition based on vector space sampling},
journal = {Physical Communication},
volume = {64},
pages = {102334},
year = {2024},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2024.102334},
url = {https://www.sciencedirect.com/science/article/pii/S1874490724000521},
author = {Lina Cui},
keywords = {Inference prediction, Data evaluation, Data matrix, Noise interference, Bayesian algorithm, Sparse grid},
abstract = {In information technology and data science predictive analysis work, the goal is to infer guess values as accurately as possible. A more densely spaced grid of points should be set as a tool for operating measurement models. It has a wide range of applications in psychology, education and other social science research fields. Although the grid seems to meet the requirements of further improving the accuracy of reasoning and guessing from a certain point of view, it also means complex calculations. When multiple false values fall between another grid point, the traditional Data Oriented Architecture fails. To guarantee the accuracy and efficiency of the calculation, it is necessary to solve the problem of excessive noise in the prediction of samples in meta-learning to solve the uncertainty caused by the noise database data or the assumption of the matrix model. This paper proposes a grid partition movable inference least squares method, that is, the sparse Bayesian least squares method based on the grid minimum matrix. The method is used for solving the problem that the prediction noise of the sample in the meta-learning is too large, to solve the uncertainty brought by the generated noise database data or the array matrix model assumption. According to the Symplectic Bayesian learning of the sequence matrix model, the basic parameters for processing the input sample database data are determined. The sparse Bayesian algorithm solution is used on the grid, combined with the off-grid inference guess of the sequence matrix model and the grid division. We set a coarse grid of points around the grid of false values. The rough point grid division is set, and the grid division around the false value is divided in detail. Then select more appropriate meta-learning parameters to clean up data. In the experiment analysis, we take the small sample study as the scene and carry on the concrete analysis to the experiment. The test proof in the fitting degree of the algorithm adopted in the paper surpasses the Support Vector Machine (SVM) and the k-Nearest Neighbor (KNN) algorithm by 3.5% and 6.4% respectively. The convergence effect surpasses the contrast plan above 10 and has a bigger superiority in the inference factor thrust. It proves that the optimization method in this paper has a strong promotion effect for the application of data forecasting systems in various industries, and has theoretical value as well as practical significance.}
}
@article{DAMBROT2020110,
title = {Theoretical and hypothetical pathways to real-time neuromorphic AGI/post-AGI ecosystems},
journal = {Procedia Computer Science},
volume = {169},
pages = {110-122},
year = {2020},
note = {Postproceedings of the 10th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2019 (Tenth Annual Meeting of the BICA Society), held August 15-19, 2019 in Seattle, Washington, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.02.122},
url = {https://www.sciencedirect.com/science/article/pii/S1877050920302453},
author = {S. Mason Dambrot},
keywords = {artificial general intelligence, counterfactual quantum entanglement, enplants, graphene, mediated artificial superintelligence, neural prosthetics, photonics, recurrent neural networks, spintronics, synthetic genomics, transdisciplinarity, transentity universal intelligence},
abstract = {While Homo sapiens is without doubt our planet’s most advanced species capable of imagining, creating and implementing tools, one of the many observable trends in evolution is the accelerating merger of biology and technology at increasing levels of scale. This is not surprising, given that our technology can be seen from a perspective in which the sensorimotor and, subsequently, prefrontal areas of our brain increasingly extending its motor (as did our evolutionary predecessors), perceptual, and—with computational advances, cognitive and memory capacities—into the exogenous environment. As such, this trajectory has taken us to a point in the above-mentioned merger at which the brain itself is beginning to meld with its physically expressed hardware and software counterparts—functionally at first, but increasingly structurally as well, initially by way of neural prostheses and brain-machine interfaces. Envisioning the extension of this trend, I propose theoretical technological pathways to a point at which humans and non-biological human counterparts may have the option to have identical neural substrates that—when integrated with Artificial General Intelligence (AGI), counterfactual quantum communications and computation, and AGI ecosystems—provide a global advance in shared knowledge and cognitive function while ameliorating current concerns associated with advanced AGI, as well as suggesting (and, if realized, accelerating) the far-future emergence of Transentity Universal Intelligence (TUI).}
}
@incollection{SUN2020139,
title = {7 - 3D printing technologies: current applications, future trends, and challenges},
editor = {Joanne Yip},
booktitle = {Latest Material and Technological Developments for Activewear},
publisher = {Woodhead Publishing},
pages = {139-151},
year = {2020},
series = {The Textile Institute Book Series},
isbn = {978-0-12-819492-8},
doi = {https://doi.org/10.1016/B978-0-12-819492-8.00007-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128194928000077},
author = {Lushan Sun},
keywords = {3D printing wearable, 3D CAD modeling, selective laser sintering, fused deposition modeling, material jetting, activewear, footwear},
abstract = {The increasing popularity of three-dimensional printing (3DP) technologies in recent decades has resulted in their exponential advancement so that they now provide products with the quality of those that are mass manufactured and mass customized. Today, they are considered to be a form of additive manufacturing and direct digital manufacturing (DDM), and the printing methods have since advanced to create higher quality output. Fashion designers and maker enthusiasts have since found that 3D printing technologies can be modified for apparel, thus allowing process and workflow disruption, or even hacking of the technologies, so that it is now important to rethink traditional skill sets and evolve them. Although 3D printing applications are still in their infancy, this chapter focuses on a few cases around activewear apparel and footwear with the use of 3D printing as they have been unique in their development approaches, direct and computational 3D modeling applications, material uses (e.g., nylon, thermoplastic polyurethane, or TPU), and applied technologies (e.g., fused deposition modeling, selective laser sintering, PolyJet printing). The intention is to shed light on the challenges and the potential future integration of 3D printing technologies in the activewear industry.}
}
@article{HYUN2018113,
title = {Balancing homogeneity and heterogeneity in design exploration by synthesizing novel design alternatives based on genetic algorithm and strategic styling decision},
journal = {Advanced Engineering Informatics},
volume = {38},
pages = {113-128},
year = {2018},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2018.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S1474034617305657},
author = {Kyung Hoon Hyun and Ji-Hyun Lee},
keywords = {Computational design, Design alternative, Design exploration, Styling strategy, Design synthesis, Family look},
abstract = {Designers constantly and consistently draft and develop both general concepts and directions to identify the solution that best fits the styling objectives of the lead designer. Designers often confront design fixations that cognitively clash to explore different design combinations. As design teams explore the range of possible design spaces of a certain design strategy, there is an opportunity for computational approaches to improve the styling process. By implementing product appearance similarity and styling strategy in computational design synthesis, it is possible to discover combinations that would otherwise remain unexplored by human designers. Numerous studies on design synthesis have been conducted. However, there has been no focus on the morphological synthesis of designs with strategic styling decisions. Considering this, the proposed study develops a method to synthesize car styling based on product appearance similarity for effective design exploration in the concept generation phase. The similarities of products across different generations, product portfolios, and competitors’ products are calculated to evaluate the strategic styling decision. The results of the strategic styling decision are used to formulate a fitness function. Car styling is then synthesized with a genetic algorithm based on this fitness function to generate car styling in accordance with the target strategic styling decision. In this respect, designers can computationally synthesize novel design alternatives that consider both homogeneity (family look in design) and heterogeneity (design trend in the market) by pinpointing the desired design exploration area. Ultimately, the style synthesis methodology proposed in this research can help designers to utilize the gradual visualization of styling strategies for more effective and efficient managerial design decisions. To do this, we conduct five major tasks: first, car design data are collected for design synthesis; second, the product appearance similarity is calculated to measure the strategic styling decision; third, synthesis validation is conducted to test whether the proposed methodology can create outside-the-box designs; fourth, a genetic algorithm is used to synthesize car designs in consideration of the strategic styling decision; finally, a series of in-depth interviews with experts and validation experiments are conducted with in-house automobile designers to examine the impact of the proposed methodology. The results showed that designers can quantitatively measure and compare the styling strategies of each car brand, then implement design upgrades, while still maintaining that specific style. Correspondingly, computationally generated design alternatives improve the satisfaction in ease, time, objective reflection and novelty of design outcomes when formulating design strategies in the concept generation phase.}
}
@incollection{SANFT20201,
title = {Unit 1 - Preliminaries: models, R, and lab techniques},
editor = {Rebecca Sanft and Anne Walter},
booktitle = {Exploring Mathematical Modeling in Biology Through Case Studies and Experimental Activities},
publisher = {Academic Press},
pages = {1-27},
year = {2020},
isbn = {978-0-12-819595-6},
doi = {https://doi.org/10.1016/B978-0-12-819595-6.00007-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128195956000074},
author = {Rebecca Sanft and Anne Walter},
keywords = {data structures, plotting, linear regression, for loops, micropipettes, absorbance spectroscopy, serial dilution},
abstract = {This section provides a brief introduction to mathematical modeling in biology and basic programming and lab skills. The purpose of this section is to build the reader's confidence and skill set in R and the lab before they engage in the modeling process. The R Basics section is for those who are new or relative novices in the use of R or other programming languages and introduces data structures, plotting, importing data, and linear regression. Exercises engage students with the material and reinforce the concepts presented. This section of the book may be extremely helpful on its own or to any course or team project that requires R. The Prelab Lab introduces pipettes, units, spectrophotometry, dilutions, and data analysis. Again, exercises are embedded to help users practice using the equipment and thinking about the methods they are using to collect data.}
}
@article{LAVALLE2025100889,
title = {Study of gender perspective in STEM degrees and its relationship with video games},
journal = {Entertainment Computing},
volume = {52},
pages = {100889},
year = {2025},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100889},
url = {https://www.sciencedirect.com/science/article/pii/S187595212400257X},
author = {Ana Lavalle and Miguel A. Teruel and Alejandro Maté and Juan Trujillo},
keywords = {Gender studies, Games, Cultural and social implications},
abstract = {At present, even though gender equality is an important matter of public interest, there are still areas in higher education where male presence is overwhelming. We refer to STEM (Science, Technology, Engineering, and Mathematics) studies in general and Computer Engineering in particular where there is only 16% of female presence in Spain. This fact made us think about the reason for this inequality. We attempted to answer such questions by means of a survey filled out by 138 students of STEM university degrees. Thanks to this survey, we have been able to understand the motivations and opinions of the students of STEM degrees regarding gender perspective. Our study highlights the possible influence of computer and video game use on enrollment in STEM degrees. Furthermore, it points out the differences between men and women in computer science skills before they start their studies. The answers provided by the surveyed women showed a correlation between women who play video games and those who get better grades. In addition, women who play video games feel more integrated into STEM degrees. Finally, differences were found in gender perspective between the male and female participants.}
}
@article{LIU2025102488,
title = {A generative and discriminative model for diversity-promoting recommendation},
journal = {Information Systems},
volume = {128},
pages = {102488},
year = {2025},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2024.102488},
url = {https://www.sciencedirect.com/science/article/pii/S0306437924001467},
author = {Yuli Liu},
keywords = {Determinantal point processes, Diversity-promoting recommendation, Discriminative model, Generative model, Joint modeling},
abstract = {Diversity-promoting recommender systems with the goal of recommending diverse and relevant results to users, have received significant attention. However, current studies often face a trade-off: they either recommend highly accurate but homogeneous items or boost diversity at the cost of relevance, making it challenging for users to find truly satisfying recommendations that meet both their obvious and potential needs. To overcome this competitive trade-off, we introduce a unified framework that simultaneously leverages a discriminative model and a generative model. This approach allows us to adjust the focus of learning dynamically. Specifically, our framework uses Variational Graph Auto-Encoders to enhance the diversity of recommendations, while Graph Convolution Networks are employed to ensure high accuracy in predicting user preferences. This dual focus enables our system to deliver recommendations that are both diverse and closely aligned with user interests. Inspired by the quality vs. diversity decomposition of Determinantal Point Process (DPP) kernel, we design the DPP likelihood-based loss function as the joint modeling loss. Extensive experiments on three real-world datasets, demonstrating that the unified framework goes beyond quality-diversity trade-off, i.e., instead of sacrificing accuracy for promoting diversity, the joint modeling actually boosts both metrics.}
}
@article{LI20112824,
title = {Human Action Recognition Based on Template Matching},
journal = {Procedia Engineering},
volume = {15},
pages = {2824-2830},
year = {2011},
note = {CEIS 2011},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2011.08.532},
url = {https://www.sciencedirect.com/science/article/pii/S1877705811020339},
author = {Chengyou Li and Tao Hua},
keywords = {computer vison ;key frame, ℜ transform, string matching},
abstract = {This paper presents a new method of human action recognition, which is based on ℜ transform and template matching after the key frame is extracted from a cycle. For a key binary human silhouette, ℜ transform is employed to represent low-level features. The advantage of the ℜ transform lies in its low computational complexity and geometric invariance. We utilize a novel string matching scheme based on edit distance is proposed to analyze different human actions. Compared with other methods, ours is superior because the descriptor is robust to frame loss in the video sequence, disjoint silhouettes and holes in the shape, and thus achieves better performance in similar activities recognition, simple representation, computational complexity and template generalization. Sufficient experiments have proved the efficiency.}
}
@article{MUDJAHIDIN2019968,
title = {Testing Methods on System Dynamics: A Model of Reliability, Average Reliability, and Demand of Service},
journal = {Procedia Computer Science},
volume = {161},
pages = {968-975},
year = {2019},
note = {The Fifth Information Systems International Conference, 23-24 July 2019, Surabaya, Indonesia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.11.206},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919319179},
author = { Mudjahidin and Rully Agus Hendrawan and Andre Parvian Aristio and Joko Lianto Buliali and Muhammad Nur Yuniarto},
keywords = {System dynamics, Testing method, structural testing, algorithms testing, behavioural testing},
abstract = {As a model used to simulate policies by creating scenarios, system dynamics must have similarities with real systems. Therefore, the system dynamics model should test so declare as the right model and representing the behaviour of a system. Thus, in this article, we propose three test methods to ensure the system dynamics model have appropriate structure, correct value according to the specified equation, and can use to establish the parameter of the model. We study articles to propose the testing methods (the structural testing, algorithms testing, and behavioural testing) and present the case study about reliability, average reliability, and its affected demands. In this article, we prove that the testing methods can be used to show the system dynamics model appropriates and represents the real system, all computation generated by the simulation output is proper to the specified equation and can use to choose the best parameter.}
}
@incollection{MURTY201981,
title = {5 - Alloy design in the 21st century: ICME, materials genome, and artificial intelligence strategies},
editor = {B.S. Murty and J.W. Yeh and S. Ranganathan and P.P. Bhattacharjee},
booktitle = {High-Entropy Alloys (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
pages = {81-101},
year = {2019},
isbn = {978-0-12-816067-1},
doi = {https://doi.org/10.1016/B978-0-12-816067-1.00005-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128160671000059},
author = {B.S. Murty and J.W. Yeh and S. Ranganathan and P.P. Bhattacharjee},
keywords = {Integrated Computational Materials Engineering (ICME), Calculation of phase diagrams (CALPHAD), Ab initio calculations, Molecular dynamic simulations, Monte Carlo simulations, Artificial intelligence},
abstract = {In recent years, there has been intense activity in the prediction of phases formed in high-entropy alloys (HEAs) through various means, and strategies based on Integrated Computational Materials Engineering (ICME) are taking prominent position in comparison with parametric approaches. While parametric approaches discussed in Chapter 3 are useful in rationalizing the phase obtained in HEAs and can also be useful in identifying the window of parameters that can lead to the formation of HEAs with a particular structure, the computational methods can be more predictive in nature. Among the computational approaches, the most prominent ones are CALPHAD method, ab initio calculations, molecular dynamics and Monte Carlo simulations, and phase-field modeling. The introduction of artificial intelligence is a disruptive addition and at the same time an extremely promising innovation. The current chapter gives an account of the status of these approaches in the HEA research.}
}
@article{LO2014358,
title = {Assembling the unexpected inspiration–from linking to jigsaw},
journal = {Frontiers of Architectural Research},
volume = {3},
number = {4},
pages = {358-367},
year = {2014},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2014.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S2095263514000247},
author = {Chia-Hui Nico Lo and Teng-Wen Chang and Ih-Cheng Lai},
keywords = {Idea linking, Puzzle solving, Design Jigsaw, Control strategy, Assembling, Early phrase of design},
abstract = {Linking pieces of design information for inspiration are an important part of the early phase of the design process. One key linking operation is assembling, wherein designers create new ideas by assembling partial or whole pieces of ideas together. How designers assemble the ideas reflect their design process. Hence, by developing a computational tool for assembling ideas, the underlying rules of design decision-making might be revealed. In this research, we employed a computational design method consisting of methodological mapping (jigsaw) and consequential analysis (Design Jigsaw system prototype) to create associations between varied types of information at different levels in the design information hierarchy. We then propose a system prototype called Design Jigsaw, based on the analysis of five representation schemes with network-like structures and sound delegation mechanisms. We also developed and explored the representation, components, and the control mechanisms involved in these components. The algorithm of the two main control strategies, grouping and matching/combining, is described in detail along with the procedural description of a jigsaw solving session. Furthermore, we conducted a design experiment to reify the process of the Design Jigsaw system prototype.}
}
@article{VANESSEN2021109218,
title = {Screening wave conditions for the occurrence of green water events on sailing ships},
journal = {Ocean Engineering},
volume = {234},
pages = {109218},
year = {2021},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2021.109218},
url = {https://www.sciencedirect.com/science/article/pii/S0029801821006478},
author = {Sanne M. {van Essen} and Charles Monroy and Zhirong Shen and Joop Helder and Dae-Hyun Kim and Sopheak Seng and Zhongfu Ge},
keywords = {Screening, Critical events, Extreme events, Waves, Green water, Design loads, Multi-fidelity approach, Potential flow, Coarse mesh CFD},
abstract = {Design loads for extreme wave events on ships, such as slamming and green water, are hard to define. These events depend on details in the incoming waves, ship motions and structure layout, which requires high-fidelity tools such as CFD or experiments to obtain the correct loads. These tools (presently) do not have the capability to fully resolve the long-term statistics of rare events in all metocean conditions over the ship’s lifetime. The idea of ‘screening’ is to use lower-fidelity numerical methods to identify the occurrence of extreme load events based on an indicator. A good indicator has a strong correlation to the design load, but is easier to calculate. A high-fidelity tool can then be used to find the loads in these events. The low-fidelity statistics and the high-fidelity loads can be combined to define a design load and its probability. The present study compares different numerical screening indicators for green water loads on a containership against experiments. The quality of the identification of the critical events and the required computational time served as comparison metrics. This showed that screening both with potential flow tools and with coarse mesh CFD tools is feasible, provided the indicator, grid, time step and wave input settings are well chosen. The results from coarse mesh CFD are slightly better than from potential flow, but the computational costs are much higher. The results also show that the peaks and steepness of the relative wave elevation around the bow are suitable green water load indicators, as well as the undisturbed wave crests at the bow. Fine mesh CFD calculations were done for the identified events based on an example indicator, which resulted in a green water load distribution very close to that of the experiments. This study shows that screening could potentially reduce the required high-fidelity modelling time with up to ∼90% compared to common practice.}
}
@article{DONG2012609,
title = {Effect of escape device for Submerged Floating Tunnel (SFT) on hydrodynamic loads applied to SFT},
journal = {Journal of Hydrodynamics, Ser. B},
volume = {24},
number = {4},
pages = {609-616},
year = {2012},
issn = {1001-6058},
doi = {https://doi.org/10.1016/S1001-6058(11)60284-9},
url = {https://www.sciencedirect.com/science/article/pii/S1001605811602849},
author = {Man-sheng DONG and Guo-ping MIAO and Long-chang YONG and Zhong-rong NIU and Huan-ping PANG and Chao-qun HOU},
keywords = {Submerged Floating Tunnel (SFT), conceptual design, flow, Airy wave, escape device, hydrodynamic load},
abstract = {This paper presents a potential approach to settle the problem of surviving major safety accidents in Submerged Floating Tunnel (SFT) that detachable emergency escape devices are set up outside SFT. The Computational Fluid Dynamics (CFD) technology is used to investigate the effect of emergency escape devices on the hydrodynamic load acting on SFT in uniform and oscillatory flows and water waves by numerical test. The governing equations, i.e., the Reynolds-Averaged Navier-Stokes (RANS) equations and k – ɛ standard turbulence equations, are solved by the Finite Volume Method (FVM). Analytic solutions for the Airy wave are applied to set boundary conditions to generate water wave. The VOF method is used to trace the free surface. In uniform flow, hydrodynamic loads, applied to SFT with emergency escape device, reduce obviously. But, in oscillatory flow, it has little influence on hydrodynamic loads acting on SFT. Horizontal and vertical wave loads of SFT magnify to some extend due to emergency escape devices so that the influence of emergency escape devices on hydrodynamic loads of SFT should be taken into consideration when designed.}
}
@article{LIU1995367,
title = {Some phenomena of seeing shapes in design},
journal = {Design Studies},
volume = {16},
number = {3},
pages = {367-385},
year = {1995},
issn = {0142-694X},
doi = {https://doi.org/10.1016/0142-694X(94)00001-T},
url = {https://www.sciencedirect.com/science/article/pii/0142694X9400001T},
author = {Yu-Tung Liu},
keywords = {restructuring shapes, emergent subshapes, shape recognition, design cognition, design computation},
abstract = {An empirical study was designed and conducted not only to reveal the importance of restructuring shapes in design but to determine some of the variables embedded in such visual behaviour. Four phenomena of seeing shapes in design have been found. Firstly, both experienced and inexperienced designers can recognize explicit subshapes, however, only the former group can recognize implicit subshapes. Secondly, when people look at a shape, explicit, close, namable subshapes are the first to emerge. Thirdly, the time needed to find emergent subshapes is in proportion to the complexity of the shape, namely the number of subshapes it subsumes. Finally, due to their prior experience and professional training, experienced designers have lowered their thresholds of recognizing activation so that they are able to discover implicit emergent subshapes. Implications for design cognition and design computation are drawn from this experiment.}
}
@article{BORGIANNI2015388,
title = {Integration of OTSM-TRIZ and Analytic Hierarchy Process for Choosing the Right Solution},
journal = {Procedia Engineering},
volume = {131},
pages = {388-400},
year = {2015},
note = {TRIZ and Knowledge-Based Innovation in Science and Industry},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2015.12.431},
url = {https://www.sciencedirect.com/science/article/pii/S1877705815043234},
author = {Yuri Borgianni and Francesco Saverio Frillici and Federico Rotini},
keywords = {Network of Problems, best solution selection, Analytic Hierarchy Process, hand steamer},
abstract = {A relevant part of TRIZ literature concerns the steps of the problem solving process, hence the analysis of the troublesome situation, the identification of the core problem and its resolution. Conversely, few efforts have been dedicated to support the last phase of the conceptual design process, which regards the selection of the most promising solutions to be further developed. The lack within TRIZ of an instrument capable to fulfill the abovementioned task led the authors to investigate the classical decision making methods and their applicability in the context of selecting the most valuable concepts downstream of problem solving phases characterized by divergent thinking. Several potential approaches have been surveyed and, among the others, the Weighted Sum Method and the Analytic Hierarchy Process seem to hold some of the characteristics requested by an ideal method to facilitate the decision making. In this paper, both of them have been tested through a real case study in order to verify their actual applicability and to reveal strengths and weaknesses with a particular focus on their capability to guide the decision process when a plurality of parties (e.g. policy makers, domain experts) are involved. The testing activity revealed that the Analytic Hierarchy Process resulted overall more appreciated by the experimenters, thanks to the systematic approach employed to select the best solution among a sample of alternatives developed through the Network of Problems.}
}
@article{FATTAHI2020107755,
title = {Stochastic optimization of disruption-driven supply chain network design with a new resilience metric},
journal = {International Journal of Production Economics},
volume = {230},
pages = {107755},
year = {2020},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2020.107755},
url = {https://www.sciencedirect.com/science/article/pii/S0925527320301407},
author = {Mohammad Fattahi and Kannan Govindan and Reza Maihami},
keywords = {Resilience metrics, Supply chain network design, Stochastic programming, Conic mixed-integer program},
abstract = {The supply chain (SC) ability to return quickly and effectively to its initial condition or even a more desirable state after a disruption is critically important, and is defined as SC resilience. Nevertheless, it has not been sufficiently quantified in the related literature. This study provides a new metric to quantify the SC resilience by using the stochastic programming. Our metric measures the expected value of the SC's cost increase due to a possible disruption event during its recovery period. Based on this measure, we propose a two-stage stochastic program for the supply chain network design under disruption events that optimizes location, allocation, inventory and order-size decisions. The stochastic program is formulated using quadratic conic optimization, and the sample average approximation (SAA) method is employed to handle the large number of disruption scenarios. A comprehensive computational study is carried out to highlight the applicability of the presented metric, the computational tractability of the stochastic program, and the performance of the SAA. Several key managerial and practical insights are gained based on the computational results. This new metric captures the time and cost of the SC's recovery after disruption events contrast to most of previous studies and main impacts of these two aspects on design decisions are highlighted. Further, it is shown computationally that the increase of SC's capacity is not a suitable strategy for designing resilient SCs in some business environments.}
}
@article{GUERRERO2022101155,
title = {Subnational sustainable development: The role of vertical intergovernmental transfers in reaching multidimensional goals},
journal = {Socio-Economic Planning Sciences},
volume = {83},
pages = {101155},
year = {2022},
issn = {0038-0121},
doi = {https://doi.org/10.1016/j.seps.2021.101155},
url = {https://www.sciencedirect.com/science/article/pii/S0038012121001476},
author = {Omar A. Guerrero and Gonzalo Castañeda and Georgina Trujillo and Lucy Hackett and Florian Chávez-Juárez},
keywords = {Subnational development, SDGs, State finances, Fiscal federalism, Sustainability, Policy priorities, Agent-based model},
abstract = {From a public finance point of view, achieving sustainable development hinges on two critical factors: the subnational implementation of public policies and the efficient allocation of resources across regions through vertical intergovernmental transfers. We introduce a framework that links these two mechanisms for analyzing the impact of reallocating federal transfers in the presence of regional heterogeneity from development indicators, budget sizes, expenditure returns, and long-term structural factors. Our study focuses on the case of Mexico and its 32 states. Using an agent-based computational model, we estimate the development gaps that will remain by the year 2030, and characterize their sensitivity to changes in the states’ budget sizes. Then, we estimate the optimal distribution of federal transfers to minimize these gaps. Crucially, these distributions depend on the specific development objectives set by the national government, and by various interdependencies between the heterogeneous qualities of the states. This work sheds new light on the complex problem of budgeting for the Sustainable Development Goals at the subnational level, and it is especially relevant for the study of fiscal decentralization from the expenditure point of view.}
}
@incollection{CARSTON2006559,
title = {Language of Thought},
editor = {Keith Brown},
booktitle = {Encyclopedia of Language & Linguistics (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {559-561},
year = {2006},
isbn = {978-0-08-044854-1},
doi = {https://doi.org/10.1016/B0-08-044854-2/04780-5},
url = {https://www.sciencedirect.com/science/article/pii/B0080448542047805},
author = {R. Carston},
keywords = {biosemantics, computational theory of mind, connectionism, intentional realism, intentionality, Mentalese, methodological solipsism, productivity (of thought), propositional attitude, psychosemantics, representational theory of mind, syntactic structure, systematicity (of thought)},
abstract = {Two key aspects of human public languages are syntax and semantics, where syntax concerns the combinatorial structure of linguistic expressions and semantics refers to their content or meaning. So, the claim that humans have a language of thought, defended in particular by Jerry Fodor, amounts to the view that thoughts are representational (semantic) and thought processes are computational, that is, they involve transformations of symbolic structures on the basis of their formal (syntactic) properties. The fact that thought, like language, exhibits ‘productivity’ and ‘systematicity’ argues for a system of mental representation that has language-like structure.}
}
@article{BOUDRY2013660,
title = {The mismeasure of machine: Synthetic biology and the trouble with engineering metaphors},
journal = {Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences},
volume = {44},
number = {4, Part B},
pages = {660-668},
year = {2013},
issn = {1369-8486},
doi = {https://doi.org/10.1016/j.shpsc.2013.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S1369848613000812},
author = {Maarten Boudry and Massimo Pigliucci},
keywords = {Synthetic biology, Adaptationism, Reverse engineering, Organism-machine metaphor, Analogical thinking},
abstract = {The scientific study of living organisms is permeated by machine and design metaphors. Genes are thought of as the “blueprint” of an organism, organisms are “reverse engineered” to discover their functionality, and living cells are compared to biochemical factories, complete with assembly lines, transport systems, messenger circuits, etc. Although the notion of design is indispensable to think about adaptations, and engineering analogies have considerable heuristic value (e.g., optimality assumptions), we argue they are limited in several important respects. In particular, the analogy with human-made machines falters when we move down to the level of molecular biology and genetics. Living organisms are far more messy and less transparent than human-made machines. Notoriously, evolution is an opportunistic tinkerer, blindly stumbling on “designs” that no sensible engineer would come up with. Despite impressive technological innovation, the prospect of artificially designing new life forms from scratch has proven more difficult than the superficial analogy with “programming” the right “software” would suggest. The idea of applying straightforward engineering approaches to living systems and their genomes—isolating functional components, designing new parts from scratch, recombining and assembling them into novel life forms—pushes the analogy with human artifacts beyond its limits. In the absence of a one-to-one correspondence between genotype and phenotype, there is no straightforward way to implement novel biological functions and design new life forms. Both the developmental complexity of gene expression and the multifarious interactions of genes and environments are serious obstacles for “engineering” a particular phenotype. The problem of reverse-engineering a desired phenotype to its genetic “instructions” is probably intractable for any but the most simple phenotypes. Recent developments in the field of bio-engineering and synthetic biology reflect these limitations. Instead of genetically engineering a desired trait from scratch, as the machine/engineering metaphor promises, researchers are making greater strides by co-opting natural selection to “search” for a suitable genotype, or by borrowing and recombining genetic material from extant life forms.}
}
@article{MAKKAR2019381,
title = {Cognitive spammer: A Framework for PageRank analysis with Split by Over-sampling and Train by Under-fitting},
journal = {Future Generation Computer Systems},
volume = {90},
pages = {381-404},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.07.046},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18305703},
author = {Aaisha Makkar and Neeraj Kumar},
keywords = {Internet of Things(IoT), Cognitive IoT, Web spam, PageRank},
abstract = {From the past few years, there is an exponential increase in one of the most popular technologies of the modern era called as Internet of Things (IoT). In IoT, various objects perform the tasks of sensing, communication, and computation for providing uninterrupted services (e.g., e-health, e-transportation, security access, etc.) to the end users. In this era, Cognitive Internet of Things (CIoT) is an another paradigm of IoT developed to enhance the capabilities of intelligence in IoT objects where these objects can take independent decisions in any environment. IoT follows the service oriented architecture (SOA), in which the application layer is the topmost layer. It enables the IoT objects to interact with the other objects located across the globe. The power of learning, thinking, and understanding by these objects, can make the information access more accurate and reliable but Web spam is one of the challenges while accessing information from the web. It has been observed from the literature review that search engines are preferred mostly by the people for accessing information. The efficient ranking by the search engines can reduce the computational cost of information exchange by IoT objects. Search engines should be able to prevent the spam from being injected into the web. But, the existing techniques for this problem target in finding the spam after its occurrence in search engine result pages. So, in this proposal, we present an intelligent cognitive spammer framework, Cognitive spammer, which eliminates the spam pages during the web page rank score calculation by search engines. The framework update the Google’s ranking algorithm, PageRank in such a way that it automatically prevents link spam by considering the link structure of web for rank score computation. The updated PageRank algorithm provided the better ranking of web pages. The proposed framework is validated with the WEBSPAM-UK2007 dataset. Before processing, the dataset is preprocessed with a new technique, called as ‘Split by Over-sampling and Train by Under-fitting’ to remove the trade off between imbalanced instances of target class. After data cleaning, we applied machine learning techniques (Bagged model, Boosted linear model, etc) with the web page features to make accurate predictions. The detection classifiers only consider the link features of the web page irrespective of the page content. Out of the fifteen classifiers, best three are ensemble, which results in better performance with overall accuracy improvement. Ten-fold cross validation has also been applied with the resulted ensemble model, which results in getting the accuracy of 99.6% in the proposed scheme.}
}
@article{SWOFFORD2022100220,
title = {Probabilistic reporting and algorithms in forensic science: Stakeholder perspectives within the American criminal justice system},
journal = {Forensic Science International: Synergy},
volume = {4},
pages = {100220},
year = {2022},
issn = {2589-871X},
doi = {https://doi.org/10.1016/j.fsisyn.2022.100220},
url = {https://www.sciencedirect.com/science/article/pii/S2589871X22000055},
author = {H. Swofford and C. Champod},
keywords = {Forensic science, Pattern evidence, Probabilities, Statistics, Algorithms},
abstract = {In recent years, there have been efforts to promote probabilistic reporting and the use of computational algorithms across several forensic science disciplines. Reactions to these efforts have been mixed—some stakeholders argue they promote greater scientific rigor whereas others argue that the opacity of algorithmic tools makes it challenging to meaningfully scrutinize the evidence presented against a defendant resulting from these systems. Consequently, the forensic community has been left with no clear path to navigate these concerns as each proposed approach has countervailing benefits and risks. To explore these issues further and provide a foundation for a path forward, this study draws on semi-structured interviews with fifteen participants to elicit the perspectives of key criminal justice stakeholders, including laboratory managers, prosecutors, defense attorneys, judges, and other academic scholars, on issues related to interpretation and reporting practices and the use of computational algorithms in forensic science within the American legal system.}
}
@article{GOODMAN2016818,
title = {Pragmatic Language Interpretation as Probabilistic Inference},
journal = {Trends in Cognitive Sciences},
volume = {20},
number = {11},
pages = {818-829},
year = {2016},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2016.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S136466131630122X},
author = {Noah D. Goodman and Michael C. Frank},
abstract = {Understanding language requires more than the use of fixed conventions and more than decoding combinatorial structure. Instead, comprehenders make exquisitely sensitive inferences about what utterances mean given their knowledge of the speaker, language, and context. Building on developments in game theory and probabilistic modeling, we describe the rational speech act (RSA) framework for pragmatic reasoning. RSA models provide a principled way to formalize inferences about meaning in context; they have been used to make successful quantitative predictions about human behavior in a variety of different tasks and situations, and they explain why complex phenomena, such as hyperbole and vagueness, occur. More generally, they provide a computational framework for integrating linguistic structure, world knowledge, and context in pragmatic language understanding.}
}
@article{SAZHIN2024110832,
title = {A behavioral dataset of predictive decisions given trends in information across adulthood},
journal = {Data in Brief},
volume = {56},
pages = {110832},
year = {2024},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2024.110832},
url = {https://www.sciencedirect.com/science/article/pii/S2352340924007960},
author = {Daniel Sazhin and Vishnu Murty and Chelsea Helion and David V. Smith},
keywords = {Strategic, Dynamic, Trends, Function learning, Cognition},
abstract = {Making early and good predictions is a critical feature of decision making in domains such as investing and predicting the spread of diseases. Past literature indicates that people use recent and longer-term trends to extrapolate future outcomes. Nonetheless, less is known about what differentiates the strategies people use to make better predictions than others. Furthermore, factors underlying predictive judgments could be an important behavioral component in psychosocial research investigating manic-depression, anxiety, and age effects. Additionally, predictive judgments may be moderated based on the experience of living in areas with greater income inequality. To address these issues, we used investment tasks where participants had to predict future outcomes of their investments based on a trend in information. In the task, participants predicted how many tokens a gold mine would produce on the twelfth turn. On each turn, participants could ask for more information at a cost, or make a prediction about whether the gold mine would produce more or less than 100 tokens by the 12th turn. The trend was determined by function type (exponential and inverse exponential functions), whether the function was more linear or curved (growth factors), and good or bad outcomes (final values). This paradigm could help disentangle to what degree people use recent or longer-term information to inform their predictive judgments. We used Qualtrics to conduct this study. We also collected questionnaire data quantifying anxiety, impulsivity, risk attitudes, manic-depressive symptoms, and other psychosocial characteristics. The study was administered to adults with age ranges across the lifespan (N = 360; 225 male, 132 female; 3 nonbinary; mean age: 44.3 years; SD: 15.4 years, min: 18 years, max: 78 years). Additionally, we sampled across areas with high- and low-income inequality, thereby allowing researchers to investigate if value-based decisions are associated with participants’ local communities. We outline potential ways to use and reuse this data, including exploring how individual differences are associated with predictive judgments.}
}
@article{XU2018309,
title = {A Platform for Fault Diagnosis of High-Speed Train based on Big Data⁎⁎Project supported by the National Natural Science Foundation, China(61490704, 61440015) and the National High-Tech. R&D Program, China (No. 2015AA043802).},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {18},
pages = {309-314},
year = {2018},
note = {10th IFAC Symposium on Advanced Control of Chemical Processes ADCHEM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.09.318},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318320007},
author = {Quan Xu and Peng Zhang and Wenqin Liu and Qiang Liu and Changxin Liu and Liangyong Wang and Anthony Toprac and S. {Joe Qin}},
keywords = {Fault Diagnosis, High-Speed Train, Big Data, Cloud Computing, Edge Computing},
abstract = {High-speed trains are very fast (e.g. 350km/h) and operate at high traffic density, so once a fault has occurred, the consequences are disastrous. In order to better control the train operational status by timely and rapid detection of faults, we need new methods to handle and analyze the huge volumes of high-speed railway data. In this paper, we propose a novel framework and platform for high-speed train fault diagnosis based on big data technologies. The framework aims to allow researchers to focus on fault detection algorithm development and on-line application, with all the complexities of big data import, storage, management, and realtime use handled transparently by the framework. The framework uses a combination of cloud computing and edge computing and a two-level architecture that handles the massive data of train operations. The platform uses Hadoop as its basic framework and combines HDFS, HBase, Redis and MySQL database as the data storage framework. A lossless data compression method is presented to reduce the data storage space and improve data storage efficiency. In order to support various types of data analysis tasks for fault diagnosis and prognosis, the framework integrates online computation, off-line computation, stream computation, real-time computation and so on. Moreover, the platform provides fault diagnosis and prognosis as services to users and a simple case study is given to further illustrate how the basic functions of the platform are implemented.}
}
@article{ALIANOFILHO2024122437,
title = {An effective approach for bi-objective multi-period touristic itinerary planning},
journal = {Expert Systems with Applications},
volume = {240},
pages = {122437},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122437},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423029391},
author = {Angelo {Aliano Filho} and Reinaldo Morabito},
keywords = {Touristic itinerary planning, Multi-objective optimization, Routing and scheduling problem, MIP-heuristic, Trade-off analysis},
abstract = {Planning effective itineraries for tourists is a major problem that has been gaining attention over the last years. This paper proposes a new bi-objective integer linear programming model for this problem. Decisions include the choice of the best itinerary to be performed considering multi-period routing, time windows for the visited attractions and the choice of restaurants and hotels. The conflicting objectives considered are: (i) maximizing the level of service offered by the itinerary, and (ii) minimizing the total distance traveled. The problem resolution, even for small instances by exact methods, is limited. This motivated the proposition of a new customized MIP-heuristic based on decomposition, fix-and-optimize and MIP-start, to produce good-quality solutions with moderate computational effort. Tchebycheff’s scalarization method was coupled to this heuristic and multiple compromise solutions were obtained. Extensive results with problem instances of different sizes and characteristics showed a good performance of this approach, capable of producing effective solutions within short runtimes. The analysis of the solutions indicated a strong conflict between the objectives, allowing the user to quantify the losses and gains when one criterion is prioritized over the other. A brief sensitivity analysis of some model parameters revealed interesting managerial insights. Some examples include quantifying the negative impacts in terms of the level of service offered by concentrating hotels and restaurants in the center of tourist attractions, increasing visit and transfer times between attractions and reducing the planning horizon for the entire itinerary. These aspects validate the potential of using this MIP model and applying this MIP-heuristic in real situations.}
}
@incollection{VOSKOGLOU202591,
title = {5 - Using gray numbers as tools for assessment and linear programming},
editor = {Michael Gr. Voskoglou},
booktitle = {Fuzzy Methods for Assessment and Decision Making},
publisher = {Morgan Kaufmann},
pages = {91-105},
year = {2025},
isbn = {978-0-443-23732-4},
doi = {https://doi.org/10.1016/B978-0-443-23732-4.00005-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044323732400005X},
author = {Michael Gr. Voskoglou},
keywords = {Generalized gray number, Gray assessment, Gray linear programming, Gray number, Gray set, Gray system, Problem solving, SIMPLEX algorithm, Whitenization},
abstract = {The gray system (GS) theory, introduced by Ju-Long Deng in 1982, is an alternative to the fuzzy systems' approach for managing the existing in real world uncertainties. Gray numbers (GNs), which are defined with the help of the closed real intervals, are the tools for performing all the necessary calculations in GS theory. This Chapter studies applications of GNs to assessment and to linear programming and presents examples illustrating their use in real world situations.}
}
@article{MAGALHAES20151157,
title = {Establishment ofAutomatization as a Requirement for Time Management Input Modules in Project Management Information Systems for Academic Activities – A Game Theory Approach},
journal = {Procedia Computer Science},
volume = {64},
pages = {1157-1162},
year = {2015},
note = {Conference on ENTERprise Information Systems/International Conference on Project MANagement/Conference on Health and Social Care Information Systems and Technologies, CENTERIS/ProjMAN / HCist 2015 October 7-9, 2015},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.08.596},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915027313},
author = {Sérgio Tenreiro de Magalhães and Maria José Magalhães and Vítor J. Sá},
keywords = {Academic Activities, Project Management, Project Management Information Systems, Game Theory.},
abstract = {Academics are expected to engage in several works in several different domains, namely research and development, general management and services to the community, while lecturing a set of courses. Academics might differ in their preference for some of these activities and also in their corresponding performance. Quality assurance in academic institutions implies monitoring performance, what is frequently done by measuring a set of quantitative results at the end of a certain period. Project Management best practices can change this frequent practice, introducing, for instance, the concept of cost efficiency, allowing for objective comparisons between different types of activities. For this to happen there is a need to monitor the time spent by each academic in each activities or, at least, in each set of activities of the same type. The challenge is to know how to do that. Game Theory has been studying decision making in competitive environment, which is increasingly the case in academic institutions. Therefore, there is a primary need to verify if a relevant percentage of the academics have a perception that there is an incentive to lie in their timesheets, due to competitive thinking. This paper presents a pilot study that allowed concluding that time management input modules in project management information systems for academic activities must be automated, eliminating the human factor in timesheet fillings.}
}
@article{WANG2023105931,
title = {Precision safety management (PSM): A novel and promising approach to safety management in the precision era},
journal = {Safety Science},
volume = {157},
pages = {105931},
year = {2023},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2022.105931},
url = {https://www.sciencedirect.com/science/article/pii/S0925753522002703},
author = {Bing Wang and Miaoting Yun and Qiong Liu and Yuanjie Wang},
keywords = {Precision, Safety management, precision safety management (PSM), Safety information, Safety risk},
abstract = {Precision is the ultimate goal of safety management. Over the past few decades, technological advances and applications (e.g., advances and applications of information technology) in safety management and its research have given safety managers the ability to pursue and realize relatively precise safety management based on sufficient and precise safety information. It can be said that safety management has entered the precision era. Therefore, in the present and future, developing novel approaches to precise prevention and control of safety risks is an inevitable safety management trend. This paper proposed a new and promising approach to safety management called precision safety management (PSM). The main objective of this paper was to answer the following five basic questions regarding PSM from a theoretical perspective: (i) What is PSM? (ii) Why is it necessary to develop PSM? (iii) What are the relationships between PSM and other concepts? (iv) What does PSM do? (v) How does an organization use PSM? Additionally, this paper presented the application of the PSM approach to the precise prevention and control of safety risks in a chemical industrial park as a case study. The main contributions of this paper are a theoretical framework for PSM, and a practical case for PSM. This study can help researchers and practitioners understand PSM and lay the foundation for its future research and practice.}
}
@article{SCHOLTE201894,
title = {Toward a systems theatre: Proposal for a program of non-trivial modeling},
journal = {Futures},
volume = {103},
pages = {94-105},
year = {2018},
note = {Futures of Society: The Interactions Revolution},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2018.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0016328717302033},
author = {Tom Scholte},
keywords = {Augusto Boal, Theatre of the Oppressed, Theatre for Living, Systems theory, Cybernetics, Konstantin Stanislavski, Soft Systems Methodology, System Dynamics, Critical systems heurstics, Enactive Management},
abstract = {This paper makes the case for, and calls for participants in, an interdisciplinary research program exploring the development of theatrical methods of social system modeling. It combines argumentation that synthesizes concepts from the theatre and the system sciences with results from a pilot application of some of the modeling methods discussed. Theatrical methods of modeling facilitate surprising insights regarding the impacts of emotion and other non-trivial factors on system behaviour that are difficult to address in purely computational and diagrammatic forms of modeling. While a theoretical relationship between systems approaches and the theatrical techniques discussed has been articulated elsewhere, this paper is the first to propose a more fulsome exploration of the potentialities of this relationship for systems praxis.}
}
@article{EISENHOFER201940,
title = {Steroid metabolomics: machine learning and multidimensional diagnostics for adrenal cortical tumors, hyperplasias, and related disorders},
journal = {Current Opinion in Endocrine and Metabolic Research},
volume = {8},
pages = {40-49},
year = {2019},
note = {Adrenal Cortex},
issn = {2451-9650},
doi = {https://doi.org/10.1016/j.coemr.2019.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S2451965019300341},
author = {Graeme Eisenhofer and Claudio Durán and Triantafyllos Chavakis and Carlo Vittorio Cannistraci},
keywords = {Steroids, Steroidomics, Machine learning, Mass spectrometry, LC-MS/MS, Adrenal, Primary aldosteronism, Cushing's syndrome, Adrenal cortical carcinoma},
abstract = {Steroid profiling applications have a long history primarily directed toward diagnosis of endocrine disorders of childhood. Technological advances in mass spectrometry enabling rapid, sensitive, and specific measurements of multiple steroids in biological fluids are now paving the way for numerous other applications, including diagnosis of adrenocortical carcinoma, primary aldosteronism, and different forms of hypercortisolism. Such analytical procedures that target combinations of steroids in a single biological sample have potential for efficient one-shot methods for diagnosis of multiple disorders of steroidogenesis. Moreover, within a specific disorder, such methods can facilitate subtyping for more rapid therapeutic stratification than allowed by current methods that rely on single measurements per sample at sequential time points in a diagnostic process. Combined with advances in computational mathematics, such as machine learning, it is now possible to move from traditional unidimensional approaches for interpreting diagnostic data to methods that can interpret patterns in data. Mass spectrometry–based steroidomics provides an ideal platform for advancing such multidimensional approaches for disease diagnosis and stratification. Bottlenecks that must be overcome to move forward include needs for laboratory harmonization and method certification combined with ingrained reliance on outmoded, but well-accepted, diagnostic methods and general inertia to take advantage of new technologies.}
}
@article{MONZON2025,
title = {Leveraging Generative Artificial Intelligence to Improve Motivation and Retrieval in Higher Education Learners},
journal = {JMIR Medical Education},
volume = {11},
year = {2025},
issn = {2369-3762},
doi = {https://doi.org/10.2196/59210},
url = {https://www.sciencedirect.com/science/article/pii/S2369376225000352},
author = {Noahlana Monzon and Franklin Alan Hays},
keywords = {educational technology, retrieval practice, flipped classroom, cognitive engagement, personalized learning, generative artificial intelligence, higher education, university education, learners, instructors, curriculum structure, learning, technologies, innovation, academic misconduct, gamification, self-directed, socio-economic disparities, interactive approach, medical education, chatGPT, machine learning, AI, large language models},
abstract = {Generative artificial intelligence (GenAI) presents novel approaches to enhance motivation, curriculum structure and development, and learning and retrieval processes for both learners and instructors. Though a focus for this emerging technology is academic misconduct, we sought to leverage GenAI in curriculum structure to facilitate educational outcomes. For instructors, GenAI offers new opportunities in course design and management while reducing time requirements to evaluate outcomes and personalizing learner feedback. These include innovative instructional designs such as flipped classrooms and gamification, enriching teaching methodologies with focused and interactive approaches, and team-based exercise development among others. For learners, GenAI offers unprecedented self-directed learning opportunities, improved cognitive engagement, and effective retrieval practices, leading to enhanced autonomy, motivation, and knowledge retention. Though empowering, this evolving landscape has integration challenges and ethical considerations, including accuracy, technological evolution, loss of learner’s voice, and socioeconomic disparities. Our experience demonstrates that the responsible application of GenAI’s in educational settings will revolutionize learning practices, making education more accessible and tailored, producing positive motivational outcomes for both learners and instructors. Thus, we argue that leveraging GenAI in educational settings will improve outcomes with implications extending from primary through higher and continuing education paradigms.}
}