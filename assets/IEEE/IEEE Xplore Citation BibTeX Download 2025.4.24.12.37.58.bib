@ARTICLE{9490241,
  author={Tedre, Matti and Toivonen, Tapani and Kahila, Juho and Vartiainen, Henriikka and Valtonen, Teemu and Jormanainen, Ilkka and Pears, Arnold},
  journal={IEEE Access}, 
  title={Teaching Machine Learning in K–12 Classroom: Pedagogical and Technological Trajectories for Artificial Intelligence Education}, 
  year={2021},
  volume={9},
  number={},
  pages={110558-110572},
  abstract={Over the past decades, numerous practical applications of machine learning techniques have shown the potential of AI-driven and data-driven approaches in a large number of computing fields. Machine learning is increasingly included in computing curricula in higher education, and a quickly growing number of initiatives are expanding it in K-12 computing education, too. As machine learning enters K-12 computing education, understanding how intuition and agency in the context of such systems is developed becomes a key research area. But as schools and teachers are already struggling with integrating traditional computational thinking and traditional artificial intelligence into school curricula, understanding the challenges behind teaching machine learning in K-12 is an even more daunting challenge for computing education research. Despite the central position of machine learning and AI in the field of modern computing, the computing education research body of literature contains remarkably few studies of how people learn to train, test, improve, and deploy machine learning systems. This is especially true of the K-12 curriculum space. This article charts the emerging trajectories in educational practice, theory, and technology related to teaching machine learning in K-12 education. The article situates the existing work in the context of computing education in general, and describes some differences that K-12 computing educators should take into account when facing this challenge. The article focuses on key aspects of the paradigm shift that will be required in order to successfully integrate machine learning into the broader K-12 computing curricula. A crucial step is abandoning the belief that rule-based “traditional” programming is a central aspect and building block in developing next generation computational thinking.},
  keywords={Education;Machine learning;Programming profession;Automation;Task analysis;Terminology;Technological innovation;Machine learning;artificial intelligence;K-12;school;computing education;computational thinking;pedagogy},
  doi={10.1109/ACCESS.2021.3097962},
  ISSN={2169-3536},
  month={},}@INPROCEEDINGS{10389248,
  author={Akinyemi, Lateef Adesola and Oshinuga, Olamide Peter and Eshilokun, Oluwafikunmi Adegbenga and Oladejo, Sunday Oladayo and Ekwe, Stephen Obono},
  booktitle={2023 International Conference on Electrical, Computer and Energy Technologies (ICECET)}, 
  title={Development of Phylum Chordata Sound Recognition System using Machine Learning and Deep Learning Techniques: A Case Study of Catfish}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Sound recognition refers to the technology or process of identifying and classifying different sounds or audio signals. This study aims to develop a sound recognition system using machine learning (ML) and deep learning (DL) techniques to classify and identify the gender of catfish based on their speech patterns. The study explores a range of supervised ML and DL algorithms, such as the Artificial Neural Network (ANN), Naïve Bayes (NB), and others, to predict the gender of catfish. The extracted features from the audio signals of the catfish serve as inputs to these algorithms, which produce binary predictions classifying the catfish samples as either male or female. This study further explores data analysis of the audio signals from the male and female catfish. The automated sound recognition system proves to be a more efficient and accurate approach compared to traditional methods, which rely on time-consuming visual observations. Evaluation metrics such as Accuracy, Sensitivity, and more, were employed to gauge the effectiveness of the algorithms in predicting the gender of catfish based on speech patterns. Among the employed algorithms, ANN and NB demonstrate the highest accuracy, achieving a score of 89.47%.},
  keywords={Deep learning;Machine learning algorithms;Artificial neural networks;Prediction algorithms;Feature extraction;Classification algorithms;Sound recognition;Sound Recognition;Deep Learning;Machine Learning Audio Data Analysis;Catfish;Gender Prediction},
  doi={10.1109/ICECET58911.2023.10389248},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10389513,
  author={Akinyemi, Lateef Adesola and Oshinuga, Olamide Peter and Ekwe, Stephen Obono and Oladejo, Sunday Oladayo},
  booktitle={2023 International Conference on Electrical, Computer and Energy Technologies (ICECET)}, 
  title={Enhancing Chronic Kidney Disease Prediction Through Data Preprocessing Optimization and Machine Learning Techniques}, 
  year={2023},
  volume={},
  number={},
  pages={1-6},
  abstract={Health care is the improvement of health through the avoidance and cure of diseases. Chronic kidney disease (CKD) poses a substantial health issue, ranking as the eighth leading cause of death and the tenth leading cause of disability-adjusted life years for both genders. This research aims to utilize machine learning (ML) algorithms to accurately predict the presence of CKD, which affects people worldwide. Timely identification of CKD is crucial to prevent it from progressing to its final and most critical phase. Machine learning algorithms, such as Decision Tree (DT), Naïve Bayes (NB), and others, are used in this study for the prediction of CKD. Various health parameters such as age, albumin level, and more are used as input into each ML algorithm to predict the presence of CKD. Data preprocessing is carried out to improve the quality of the data for more accurate prediction. The data is analyzed to uncover the connections between CKD health parameters and their impact on the occurrence of CKD. This research further examines the influence of different train and test data ratios on the accuracy of the employed algorithms. Evaluation metrics such as accuracy, sensitivity, and more are employed to gauge the effectiveness of each algorithm in making predictions. Random Forest stood out as the most accurate algorithm, with a score of 96.25%. Naive Bayes and XGBoost demonstrated the highest sensitivity, achieving a perfect score of 100%.},
  keywords={Machine learning algorithms;Sensitivity;Data preprocessing;Medical services;Prediction algorithms;Chronic kidney disease;Decision trees;Health Care;Machine Learning;Chronic Kidney Disease (CKD);Data Analysis;Data Preprocessing},
  doi={10.1109/ICECET58911.2023.10389513},
  ISSN={},
  month={Nov},}@INPROCEEDINGS{10630378,
  author={Akinyemi, Lateef Adesola and Oshinuga, Olamide Peter and Oladejo, Sunday Oladayo and Ekwe, Stephen Obono and Sumbwanyanbe, Mbuyu and Mnkandla, Ernest and Shoewu, Oluwagbemiga Omotayo},
  booktitle={2024 International Conference on Science, Engineering and Business for Driving Sustainable Development Goals (SEB4SDG)}, 
  title={Optimizing Network Management and Virtualization Using Machine Learning Approach: Network Slice Prediction}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  abstract={This study demonstrates the utilization of Machine Learning (ML) for network slice prediction, enabling the optimization of resources for diverse network slices. Traditional methods for network slice prediction often lack efficiency and result in inaccuracies. By leveraging ML algorithms such as Naive Bayes and Random Forest, an intelligent framework that automates network slice prediction is developed. This framework enhances network virtualization and management, facilitating resource allocation. The ML algorithms take real-time network conditions and usages, such as packet delay and smart city, as input and output for selecting the most suitable network slice. Data analysis is further conducted to reveal the connections between the input parameters and how these parameters influence the selection of the accurate network slice. Network slicing plays a crucial role as it enables the customization of services and facilitates efficient scaling to meet the specific needs of different applications and industries. The accuracy scores of the employed ML algorithms were generally perfect, except for the KNN and SVM classifiers, which achieved an accuracy of 94.30% and 92.16%, respectively, for the prediction of network slices based on incoming network connections and usages.},
  keywords={Support vector machines;Machine learning algorithms;Accuracy;Data analysis;Smart cities;Network slicing;Prediction algorithms;Network Slicing;Network Virtualization;Network Management;Machine Learning;Data Analysis},
  doi={10.1109/SEB4SDG60871.2024.10630378},
  ISSN={},
  month={April},}@INPROCEEDINGS{9105248,
  author={Boesl, Dominik},
  booktitle={2019 IEEE 19th International Symposium on Computational Intelligence and Informatics and 7th IEEE International Conference on Recent Achievements in Mechatronics, Automation, Computer Sciences and Robotics (CINTI-MACRo)}, 
  title={Disruption Cannot be Planned! On Robotics, Automation, Flying Cars and Smart Digital Solutions: PLENARY PAPER}, 
  year={2019},
  volume={},
  number={},
  pages={000111-000112},
  abstract={Is robotics, automation, digitalization, artificial intelligence or any of the current buzzwords really going to disrupt our world? Prof. Dominik Boesl thinks so – in fact, it can even been proven! In order to predict technological breakthroughs, the domains of foresight, future studies and technology assessment offer interesting methods. One of the most know might be the analysis of megatrends. Dominik Boesl is researching the future of robotics, automation and AI and their impact on society out of HDBW (Hochschule der Bayerischen Wirtschaft) in Munich. He is convinced, that our grandchildren will grow up as first Generation R of Robotic Natives – leaving us the last generation of Robotic Immigrants (still with analogue migration background, but …). These future generations will not be afraid of robotics and smart systems permeating their living realm. But technology is never good or bad out of itself – it depends on us how we apply it. The bigger the disruption, the larger the responsibility that comes with it. As chair of the global IEEE TechEthics initiative, Dominik fosters educated discourse about the application of technology and its impact on society and humankind. He is founder and chairman of the Robotic & AI Governance Foundation, pushing for governance structures to (self-)regulate the research, development and productization of automation technologies.},
  keywords={Robots;Automation;Artificial intelligence;Automobiles;Mechatronics;Informatics;Computational intelligence},
  doi={10.1109/CINTI-MACRo49179.2019.9105248},
  ISSN={2471-9269},
  month={Nov},}@INPROCEEDINGS{9178698,
  author={Zou, Sheng-rong and Shu, Yu-dan and Chen, Li and Shi, Xu-qing},
  booktitle={2020 5th International Conference on Computational Intelligence and Applications (ICCIA)}, 
  title={Refinement of the Cytokine Portion of the Immune System Based on Event-B}, 
  year={2020},
  volume={},
  number={},
  pages={145-149},
  abstract={The Event-B method is a kind of formal software development method, which is mainly used for the functional requirements of the system modeling and validation.The immune system is a large abstract model with high complexity.This paper adopts a new way of thinking,by studying the relationship between immune cytokines and immune cells,the interaction between cells and cytokines in the process of immunity was further explored.At the same time, based on Rodin platform, the formal method Event-B method was adopted, and the top-down strategy was used to refine and verify the immune system model layer by layer.The ideological method of Event-B specification verification was used to solve the problem of high error rate and low efficiency caused by non-formalization in the traditional software design process.},
  keywords={Immune system;Context modeling;Computational modeling;Cells (biology);Presses;Software;Biological system modeling;event-B;immune system;formal verification},
  doi={10.1109/ICCIA49625.2020.00035},
  ISSN={},
  month={June},}@INPROCEEDINGS{4630611,
  author={Grana, Manuel},
  booktitle={2008 IEEE International Conference on Fuzzy Systems (IEEE World Congress on Computational Intelligence)}, 
  title={A brief review of lattice computing}, 
  year={2008},
  volume={},
  number={},
  pages={1777-1781},
  abstract={Defining lattice computing as the class of algorithms that either apply lattice operators inf and sup or use lattice theory to produce generalizations or fusions of previous approaches, we find that a host of algorithms for data processing, classification, signal filtering, have been produced over the last decades. We give a fast and brief review, which by no means could be exhaustive; with the aim of showing that this area has been growing during the past decades and to highlight the ones that we think are broad avenues for future research. Although our emphasis is on Artificial Neural Networks and Fuzzy Systems in this review we include Mathematical Morphology as a notorious instance of Lattice Computing.},
  keywords={Lattices;Artificial neural networks;Conferences;Fuzzy systems;Associative memory;Joints;Classification algorithms},
  doi={10.1109/FUZZY.2008.4630611},
  ISSN={1098-7584},
  month={June},}@INPROCEEDINGS{7850033,
  author={Woźniak, Marcin and Połap, Dawid},
  booktitle={2016 IEEE Symposium Series on Computational Intelligence (SSCI)}, 
  title={On manipulation of initial population search space in heuristic algorithm through the use of parallel processing approach}, 
  year={2016},
  volume={},
  number={},
  pages={1-6},
  abstract={Increasing use of heuristic algorithms in various fields of science causes numerous modifications of the original algorithms in need for better performance and efficiency. The main problem of heuristics is time required to find optimal solution. For this purpose, we propose to use parallel processing in the initial phase of heuristic methods to decrease computing time. Implemented technique models migration of individuals by adjusting initial population to required conditions. Proposed approach is simulating parallel processes that take place in human brain while solving tasks. In this case, human intelligence is working parallel on various aspects of the problem to compare them in the end before final decision. Proposed approach is simulating this parallelization of thinking threads in the process of optimization. Presented experimental tests have been carried out and discussed in terms of advantages and disadvantages.},
  keywords={Whales;Sociology;Statistics;Optimization;Heuristic algorithms;Mathematical model;Parallel processing},
  doi={10.1109/SSCI.2016.7850033},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10361797,
  author={Geng, Yanjun},
  booktitle={2023 International Conference on Electronics and Devices, Computational Science (ICEDCS)}, 
  title={Construction of Intelligent Financial Management Platform Based on Hierarchical Inheritance Algorithm}, 
  year={2023},
  volume={},
  number={},
  pages={663-668},
  abstract={The Hierarchical Inheritance Algorithm (HGA) is an optimization technique inspired by biological principles of natural selection and heredity. This approach is versatile, finding applications in fields such as combinatorial optimization, machine learning, adaptive control, and more. It stands as a cornerstone in the realm of intelligent computing. This study bridges the gap between information technology and progressive management methodologies, aiming to adapt outdated financial management strategies to the demands of a digitalized environment, thereby pioneering a centralized financial management paradigm in the digital realm. The focus of this paper is to explore this digitally-centered financial management framework and propose a design tailored to the online ecosystem. Our findings indicate that the concept of smart finance goes beyond a mere blend of "AI and finance." Implementing its technologies involves a unique methodology that thrives on "closed-loop thinking." By gleaning insights from AI applications in diverse sectors, we've outlined the foundational pillars of smart finance application: digital transformation of specialized financial operations, algorithms rooted in financial logic that are fine-tuned through iterations, the fusion of data analytics within financial operations leading to product-centric solutions, and achieving a significant 90.00% platform integration for these smart financial solutions. As artificial intelligence technology continues to evolve, financial management will undoubtedly gravitate towards being instantaneous, user-centric, and intelligent. This will usher in heightened efficiency and benefits, heralding a new age in financial management.},
  keywords={Visualization;Machine learning algorithms;Scientific computing;Neural networks;Ecosystems;Finance;Machine learning;hierarchical inheritance algorithm;Intelligent financial management platform},
  doi={10.1109/ICEDCS60513.2023.00128},
  ISSN={},
  month={Sep.},}@INPROCEEDINGS{8324067,
  author={Quaiyum Ansari, Abdul},
  booktitle={2017 International Conference on Intelligent Communication and Computational Techniques (ICCT)}, 
  title={Keynote speakers: From fuzzy logic to neutrosophic logic: A paradigme shift and logics}, 
  year={2017},
  volume={},
  number={},
  pages={11-15},
  abstract={Prof. LotfiAskarZadeh had revolutionized the field of logics by proposing a novel logic, Fuzzy Logic, in 1965 where there is smooth transition between different classes due to overlapping of membership functions, and each element in fuzzy set has a degree of membership. Fuzzy Logic was considered appropriate to represent any vague or ambiguous situation of the real life problems. It also has the provision of allowing linguistic variables whose truth values may vary between 0 and 1; in contrast to two values of classical logic. Quite recently, Neutrosophic Logic was proposed by Florentine Smarandache, which is based on non-standard analysis that was given by Abraham Robinson in 1960s. Neutrosophic Logic was developed to represent mathematical model of uncertainty, vagueness, ambiguity, imprecision, undefined, unknown, incompleteness, inconsistency, redundancy, and contradiction that encompasses the shortcomings of every other logic studied in the past. All the factors stated are very integral to human thinking, as it is very rare that we tend to conclude/judge in definite environments, imprecision of human systems could be due to the imperfection of knowledge that human receives (observation) from the external world. Imperfection leads to a doubt about the value of a variable, a decision to be taken or a conclusion to be drawn for the actual system. Multiple factors could lead to uncertainty like incomplete knowledge (ignorance of the totality, limited view on a system because of its complexity), stochasticity (the case of intrinsic imperfection where a typical and single value does not exist), or the acquisition errors (intrinsically imperfect observations, the quantitative errors in measures). This talk would focus on the overlapping regions of the fuzzy systems in which a method has been proposed for using neutrosophic logic and assign a triplet of the form (t, i, f) for the points that are spanned by overlapping ranges, in such a way that every such output is assigned a true, indeterminate and false value.},
  keywords={Medical services;Conferences;Automotive engineering;Computer security;Fuzzy logic;Internet},
  doi={10.1109/INTELCCT.2017.8324067},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{8806880,
  author={Lin, Zin and Johnson, Steven G.},
  booktitle={2019 International Conference on Numerical Simulation of Optoelectronic Devices (NUSOD)}, 
  title={Freeform inverse design in photonics by re-thinking the question}, 
  year={2019},
  volume={},
  number={},
  pages={1-2},
  abstract={Recent developments in computational freeform inverse design have provided a fertile landscape of structures and topologies for nanophotonics. However, simply dumping millions of parameters into a simulation can easily lead to intractable computational problems. Fortunately, a given engineering problem often admits many different mathematical formulations, and by carefully matching the formulation to the available electromagnetic solvers and optimization algorithms one can set the stage for extraordinarily flexible automated design. In this talk, we will show that, with careful consideration and reformulation of the design problem, powerful inverse design techniques can be successfully applied to a multitude of interesting problems with rich physical behavior, ranging from light confinement in nonlinear multi-resonant cavities, robust bandgap maximization in 3D photonic crystals to beam-forming and manipulation through multi-layered metasurfaces.},
  keywords={Optimization;Frequency conversion;Photonics;Topology;Cavity resonators;Photonic band gap;Optics},
  doi={10.1109/NUSOD.2019.8806880},
  ISSN={2158-3242},
  month={July},}@ARTICLE{6633027,
  author={Sun, Changhao and Duan, Haibin and Shi, Yuhui},
  journal={IEEE Computational Intelligence Magazine}, 
  title={Optimal Satellite Formation Reconfiguration Based on Closed-Loop Brain Storm Optimization}, 
  year={2013},
  volume={8},
  number={4},
  pages={39-51},
  abstract={In recent years, satellite formation flying has become an increasingly hot topic for both the astronomy and earth science communities due to its potential merits compared with a single monolithic spacecraft system. This paper proposes a novel approach based on closed-loop brain storm optimization (CLBSO) algorithms to address the optimal formation reconfiguration of multiple satellites using two-impulse control. The optimal satellite formation reconfiguration is formulated as an optimization problem with the constraints of overall fuel cost minimization, final configuration, and collision avoidance. Three versions of CLBSOs are developed by replacing the creating operator in basic brain storm optimization (BSO) with closed-loop strategies, which facilitate search characteristic capture and enhance the optimization performance by taking advantage of feedback information in the search process. Numerical simulations are carried out using particle swarm optimization (PSO), basic BSO, and the three versions of CLBSOs. Comparison results show that all versions of CLBSOs outperform PSO and the original BSO in terms of final results and convergence speed. In addition, CLBSO reduces the computation burden and shortens CPU time to a certain extent in contrast with basic BSO. Furthermore, among the three CLBSO algorithms, the one using the strategy of difference with the best gains the best overall performance, which is inspired by the updating rule in PSO that each particle tends to move towards the individual with the best fitness.},
  keywords={Satellites;Optimization;Orbits;Space vehicles;Space missions;Closed loop systems},
  doi={10.1109/MCI.2013.2279560},
  ISSN={1556-6048},
  month={Nov},}@ARTICLE{4274794,
  author={Perlovsky, Leonid I.},
  journal={IEEE Computational Intelligence Magazine}, 
  title={Evolution of Languages, Consciousness and Cultures}, 
  year={2007},
  volume={2},
  number={3},
  pages={25-39},
  abstract={The knowledge instinct is a fundamental mechanism of the mind that drives evolution of higher cognitive functions. Neural modeling fields and dynamic logic describe it mathematically and relate to language, concepts, emotions, and behavior. Perception and cognition, consciousness and unconsciousness, are described, while overcoming past mathematical difficulties of modeling intelligence. The two main aspects of the knowledge instinct determining evolution are differentiation and synthesis. Differentiation proceeds from and unconscious states to more crisp and conscious, from less knowledge to more knowledge; it separates concepts from emotions, Its main mechanism is language. Synthesis strives to achieve unity and meaning of knowledge; it is necessary for resolving contradictions, concentrating will and for purposeful actions. Synthesis connects language and cognition. Its main mechanisms are emotionality of languages and the hierarchy of the mind. Differentiation and synthesis are in complex relationship of symbiosis and opposition. This Leads to complex dynamics of evolution of consciousness and languages. Its mathematical modeling predicts evolution of cultures. We discuss existing evidence and future research directions.},
  keywords={Cognition;Mathematical model;Cultural differences;Humans;Logic;Competitive intelligence;Symbiosis;Educational institutions;Earth;Military computing},
  doi={10.1109/MCI.2007.385364},
  ISSN={1556-6048},
  month={Aug},}@INPROCEEDINGS{10441858,
  author={Feng, Siqi and Shi, Jinfeng and Zhang, Rui and Wang, Yuyang and Chen, Fan and Wang, Xu},
  booktitle={2023 IEEE International Conference on Electrical, Automation and Computer Engineering (ICEACE)}, 
  title={Optimization of Credit Score Card Portfolio Based on QUBO Model with Quantum Annealing Algorithm}, 
  year={2023},
  volume={},
  number={},
  pages={1187-1195},
  abstract={With the progress of science and technology, quantum computing plays an increasingly important role in various fields, solving the traditional arithmetic bottleneck. In the financial field, credit scoring is the core of the lending industry and one of the most common risk management tools. Researchers are trying to find the best threshold-setting scheme to maximize revenue through quantum computing. This approach not only solves the optimization problem in the field of credit score cards, but also provides a new way of thinking about optimization problems in other fields. In this paper, three traditional computational models, one-fold credit card strategy, two-fold credit card combination strategy, and three-fold credit card combination strategy, are proposed, and the credit score card scheme that maximizes the final revenue is solved for each of the three models. The results show that under the one-fold credit card strategy, the final income is maximized at $54,087.2; under the two-fold credit card combination strategy, the final income is maximized at $41,106.3; and under the three-fold credit card combination strategy, the final income is maximized at $27,914.8. Secondly, this paper converts these three strategies to the QUBO model and obtains the maximum final income that is consistent with the maximum under the traditional model, with an accuracy of 96.3%, 92.7%, and 89.9% for the three combination strategies, respectively.},
  keywords={Analytical models;Computational modeling;Credit cards;Quantum annealing;Risk management;Optimization;Portfolios;Quantum computing;Credit scoring;Computational model;QUBO model},
  doi={10.1109/ICEACE60673.2023.10441858},
  ISSN={},
  month={Dec},}@INPROCEEDINGS{10184658,
  author={Guo, Shengnan and Lin, Youfang and Gong, Letian and Wang, Chenyu and Zhou, Zeyu and Shen, Zekai and Huang, Yiheng and Wan, Huaiyu},
  booktitle={2023 IEEE 39th International Conference on Data Engineering (ICDE)}, 
  title={Self-Supervised Spatial-Temporal Bottleneck Attentive Network for Efficient Long-term Traffic Forecasting}, 
  year={2023},
  volume={},
  number={},
  pages={1585-1596},
  abstract={In intelligent transportation systems, accurate long-term traffic forecasting is informative for administrators and travelers to make wise decisions in advance. Recently proposed spatial-temporal forecasting models perform well for short-term traffic forecasting, but two challenges hinder their applications for long-term forecasting in practice. Firstly, existing traffic forecasting models do not have satisfactory scalability on effectiveness and efficiency, i.e., as the prediction time spans extend, existing models either cannot capture the long-term spatial-temporal dynamics of traffic data or equip global receptive fields at the cost of quadratic computational complexity. Secondly, the dilemma between the models’ strong appetite for high-quality training data and their generalization ability is also a challenge we have to face. Thus how to improve data utilization efficiency deserves thoughtful thinking. Aiming at solving the long-term traffic forecasting problem and facilitating the deployment of traffic forecasting models in practice, this paper proposes an efficient and effective Self-supervised Spatial-Temporal Bottleneck Attentive Network (SSTBAN). Specifically, SSTBAN follows a multi-task framework by incorporating a self-supervised learner to produce robust latent representations for historical traffic data, so as to improve its generalization performance and robustness for forecasting. Besides, we design a spatial-temporal bottleneck attention mechanism, reducing the computational complexity meanwhile encoding global spatial-temporal dynamics. Extensive experiments on real-world long-term traffic forecasting tasks, including traffic speed forecasting and traffic flow forecasting under nine scenarios, demonstrate that SSTBAN not only achieves the overall best performance but also has good computation efficiency and data utilization efficiency.},
  keywords={Computational modeling;Training data;Predictive models;Data models;Robustness;Computational efficiency;Forecasting;traffic forecasting;spatial-temporal graph data;self-supervised learning},
  doi={10.1109/ICDE55515.2023.00125},
  ISSN={2375-026X},
  month={April},}@ARTICLE{10571575,
  author={Bari, Salman and Wang, Xiagong and Schoha Haidari, Ahmad and Wollherr, Dirk},
  journal={IEEE Open Journal of Intelligent Transportation Systems}, 
  title={Factor Graph-Based Planning as Inference for Autonomous Vehicle Racing}, 
  year={2024},
  volume={5},
  number={},
  pages={380-392},
  abstract={Factor graph, as a bipartite graphical model, offers a structured representation by revealing local connections among graph nodes. This study explores the utilization of factor graphs in modeling the autonomous racecar planning problem, presenting an alternate perspective to the traditional optimizationbased formulation. We model the planning problem as a probabilistic inference over a factor graph, with factor nodes capturing the joint distribution of motion objectives. By leveraging the duality between optimization and inference, a fast solution to the maximum a posteriori estimation of the factor graph is obtained via least-squares optimization. The localized design thinking inherent in this formulation ensures that motion objectives depend on a small subset of variables. We exploit the locality feature of the factor graph structure to integrate the minimum curvature path and local planning computations into a unified algorithm. This diverges from the conventional separation of global and local planning modules, where curvature minimization occurs at the global level. The evaluation of the proposed framework demonstrated superior performance for cumulative curvature and average speed across the racetrack. Furthermore, the results highlight the computational efficiency of our approach. While acknowledging the structural design advantages and computational efficiency of the proposed methodology, we also address its limitations and outline potential directions for future research.},
  keywords={Planning;Optimization;Computational efficiency;Computational modeling;Probabilistic logic;Minimization;Collision avoidance;Motion planning;Autonomous vehicles;Predictive control;Motion planning;autonomous racing vehicles;probabilistic inference;factor graph;model predictive control},
  doi={10.1109/OJITS.2024.3418956},
  ISSN={2687-7813},
  month={},}@INPROCEEDINGS{6381073,
  author={Sowah, Robert A. and Mills, Godfrey A. and Bremang, Appah and Armoo, Stephen K.},
  booktitle={2012 IEEE 4th International Conference on Adaptive Science & Technology (ICAST)}, 
  title={Integrating MATLAB/Simulink technical computing environment into engineering education pedagogy in Ghanaian Universities: — A case study for University of Ghana, Computer Engineering Department}, 
  year={2012},
  volume={},
  number={},
  pages={92-97},
  abstract={In this paper we proposed a joint model of simultaneous instruction with laboratory sessions to reinforce and enrich the learning experience so as to train better engineers able to compete globally. In accomplishing this, we integrated MATLAB/Simulink, the de facto standard for technical computing into the university curriculum for teaching four Computer Engineering courses namely Numerical Methods, Signals and Systems, Digital Control Systems and Digital Signal Processing during the 2010/2011 academic year at the University of Ghana. The course modules were developed in consultation with others and taught over the period of 13 weeks. The semester examinations consisted of laboratory and written exams sessions of three hours duration. After grading and collating of examinations results, it was clearly demonstrated by students that they have learnt the key requirements in engineering education. The statistics was staggering and interesting; for example 90 students registered the Numerical Methods, one withdrew due to ill-health and 89 students passed the seemingly difficult course with good grades.},
  keywords={Decision support systems;Conferences;Nonlinear dynamical systems;brain research;computational science;lecture labs;educational pedagogy},
  doi={10.1109/ICASTech.2012.6381073},
  ISSN={2326-9448},
  month={Oct},}@INPROCEEDINGS{9284871,
  author={Malik, Pravir and Nallamothula, Lalitha},
  booktitle={2020 11th IEEE Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)}, 
  title={Fourfold Properties of Light and its Relevance to Quantum Computation}, 
  year={2020},
  volume={},
  number={},
  pages={0707-0711},
  abstract={The four quantum phenomena of superposition, entanglement, tunneling, and annealing are envisioned to give quantum computing the ability to solve complex problems in very low-resolution times as compared with classical computers. These four phenomena are closely related to the light-based properties of presence, knowledge, power, and harmony, central to a multilayered light-based model of reality that also offers alternative foundation for thinking about quanta and quantum computation. In this model, it is not a random process from infinite superposed possibilities that exist at the quantum-level as supposed by the Copenhagen Interpretation, and as assumed as the foundation of the infinite processing capability of quantum objects by contemporary pioneers of quantum computing. Rather, the infinite processing capability is due to a more ordered display of superposition, entanglement, tunneling and annealing: superposition, as an ordered concord of light's property of presence stacked in logical arrangement by layer of light; entanglement, as a display of light's property of knowledge that occurs at a speed faster than the known speed of light; tunneling, as a display of light's property of power that allows toggling between different “realities” in layers of light; and annealing, expressing light's property of harmony, to find an ideal minimum state among a number of realities. These four quantum phenomena derivative from an implicit and natural unity within light, engineered to maintain that unity externally, will confer quantum computation with the possibility of extraordinary processing power.},
  keywords={Quantum computing;Annealing;Quantum entanglement;Computational modeling;Tunneling;Mobile communication;Random processes;Quantum Computation;Symmetries in Light;Superposition;Entanglement;Quantum Tunneling;Quantum Annealing;Computational Power},
  doi={10.1109/IEMCON51383.2020.9284871},
  ISSN={2644-3163},
  month={Nov},}@ARTICLE{6673982,
  author={Hinsen, Konrad},
  journal={Computing in Science & Engineering}, 
  title={Daydreaming about Scientific Programming}, 
  year={2013},
  volume={15},
  number={5},
  pages={77-79},
  abstract={Did you ever think about what your ideal work environment for scientific programming would be like? This article describes how to do things we don't do (yet) using tools that don't exist (yet). It's about a possible future world of scientific programming. More precisely, it's about the kind of work environment I hope to be able to use one day. The example code uses made-up and possibly incoherent syntax, and has never been tested.},
  keywords={Scientific computing;Programming;Data models;Algorithm design and analysis;Computational modeling;scientific programming;computational design;algorithm design;data structures;scientific computing},
  doi={10.1109/MCSE.2013.104},
  ISSN={1558-366X},
  month={Sep.},}@INPROCEEDINGS{8599558,
  author={Landauer, Christopher},
  booktitle={2018 IEEE 3rd International Workshops on Foundations and Applications of Self* Systems (FAS*W)}, 
  title={Outriggers and Training Wheels for Cooperating Systems}, 
  year={2018},
  volume={},
  number={},
  pages={226-230},
  abstract={When we build systems to operate in hazardous or remote environments, and especially when we expect them to cooperate with others in support of a goal, we rely on them to operate as correctly as possible in the (unpredicted, frequently unpredictable) situations they encounter. But the environment does whatever it does; we have essentially no control and only limited knowledge of what it does, and only the most meager notion of what it will do. In this paper, we describe an architecture for these component systems that we think will be better suited to the vagaries of environmental behavior than others. We advocate a collection of subsidiary systems to operate in parallel with the main system, to act as “outriggers” for unexpected environmental behaviors or system failures, or as “training wheels” during development.. We describe our initial notions of how they relate to the original system and how to implement them.},
  keywords={Training;Wheels;Wrapping;Space vehicles;Computational modeling;Adaptation models;Monitoring;Protective Modeling, Behavior Merging, Multi Resolution Modeling, Run-Time Verification and Validation, Computational Reflection, Wrapping Integration Infrastructure, Self-Modeling Systems},
  doi={10.1109/FAS-W.2018.00051},
  ISSN={},
  month={Sep.},}@ARTICLE{4580541,
  author={Resnyansky, Lucy},
  journal={IEEE Intelligent Systems}, 
  title={Social Modeling as an Interdisciplinary Research Practice}, 
  year={2008},
  volume={23},
  number={4},
  pages={20-27},
  abstract={Social modeling applies computational methods and techniques to the analysis of social processes and human behavior. It's expected to provide conceptual and technological tools for supporting analysis and decision making in areas related to national and public security, political stability, law and order, and sociocultural changes. Modeling social and cultural processes must draw on the knowledge obtained within social sciences, including conceptual models, cultural insights, and empirical data. However, how to best integrate social scientific knowledge into modeling remains an open research problem. The author presents the perspective of a social scientist to describe why modeling can be useful for social research on political violence, social conflicts, and cultural changes. She develops an interactionist approach to interdisciplinary research practice and discusses how this approach can help identify the problems related to the integration of social scientific knowledge in modeling. The discussion focuses upon research on political violence and related sociocultural processes.},
  keywords={Ontologies;Cultural differences;Lenses;Equations;Context modeling;Computational modeling;Humans;Decision making;Data security;National security;simulation;sociology},
  doi={10.1109/MIS.2008.72},
  ISSN={1941-1294},
  month={July},}@INPROCEEDINGS{49272,
  author={Fischer, G. and Mastaglio, T.},
  booktitle={[1989] Proceedings of the Twenty-Second Annual Hawaii International Conference on System Sciences. Volume III: Decision Support and Knowledge Based Systems Track}, 
  title={Computer-based critics}, 
  year={1989},
  volume={3},
  number={},
  pages={427-436 vol.3},
  abstract={The authors describe computer-based critics and articulate some of the general principles learned from their system-building experience. They propose a general framework for critics, present specific requirements, and describe two prototypical critic systems: LISP-CRITIC, which criticizes Lisp programs, and CRACK, a system that assists the user in designing a kitchen. The authors illustrate the generalized main components of the critic systems and discuss their evaluation. It is concluded that computer-based critics incorporate many powerful ideas from human-computer communications and artificial intelligence into a system that makes use of the best aspects of human and computational cognition. They have the potential to provide a symbiotic relationship between a user and a knowledge-based system. The results should be applicable to the entire class of cooperative problem-solving systems.<>},
  keywords={Humans;Artificial intelligence;Buildings;Intelligent systems;Computational intelligence;Application software;Intelligent structures;Machine intelligence;Symbiosis;Communication system control},
  doi={10.1109/HICSS.1989.49272},
  ISSN={},
  month={Jan},}@INPROCEEDINGS{8659041,
  author={Reimer, Yolanda J. and Coe, Michael and Blank, Lisa M. and Braun, Jeffrey},
  booktitle={2018 IEEE Frontiers in Education Conference (FIE)}, 
  title={Effects of Professional Development on Programming Knowledge and Self-Efficacy}, 
  year={2018},
  volume={},
  number={},
  pages={1-8},
  abstract={This Research Full Paper presents the effects of our weeklong Professional Development class on the programming skills of nineteen high school teachers, their confidence in programming, and their confidence for teaching programming. A primary objective of the CS10K and CS For All initiatives is the education of K-12 teachers in aspects of computer science and computational thinking so they can teach CS courses in their schools. Many of these educators have degrees in disciplines other than computer science, such as math, science, and business, so preparing them to teach CS is a challenge, particularly since most K-12 teachers have limited time to devote to learning new curriculum. This study describes how we managed a short course in computational thinking and programming to a group of high school teachers. We illustrate through survey data assessment and evaluation that significant gains in skill level and self-efficacy can be realized within a short but intensive week of face-to-face training. Five months later we follow-up with the same cohort to see if earlier achievements remain evident over time. Discussion throughout the paper identifies strengths and weaknesses of the training week, which is useful to others planning to undertake similar PD offerings.},
  keywords={Programming profession;Python;Education;Schedules;Business;Professional development;CS10K;CS For All;computational thinking;programming skill;K-12 instruction},
  doi={10.1109/FIE.2018.8659041},
  ISSN={2377-634X},
  month={Oct},}@INPROCEEDINGS{9637170,
  author={Kastner-Hauler, Oliver and Tengler, Karin and Demarle-Meusel, Heike and Sabitzer, Barbara},
  booktitle={2021 IEEE Frontiers in Education Conference (FIE)}, 
  title={Adapting an OER Textbook for the Inverted Classroom Model — How To Flip the Classroom with BBC micro:bit Example Tasks}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  abstract={Full Paper Research-to-Practice The current COVID-19 crisis has created significant challenges for schools. The growing importance of “flipping the classroom” and the needful emphasizing of online-learning were owed to the situation. To meet these requirements, materials and tasks must be adapted. The Open Educational Resource (OER) textbook “Computational Thinking with the BBC micro:bit” was developed for the introduction of Computational Thinking (CT) for 10-14-year-old pupils in Austria's secondary schools. Example tasks in the textbook are designed with an open end and present extensions with ideas for further development instead of ending abruptly. This article provides a guideline for a clear distinction in redesigning existing lessons following the Inverted Classroom Model (ICM) using videos for pre-class work and live task extensions for in-class work. Which parts in the learning design must remain as live lessons and which parts can be adapted for video lessons? The respective research shows that examples that have a makerspace activity as an extension are especially helpful for an efficient determination of the appropriate part in the learning design and particularly suitable for an adaptation with ICM. The central advantage of the ICM is that it responds flexibly to the individual learning needs of each student. It allows students to take their time reviewing the material at their own pace without getting left behind. The textbook used here encourages pupils to find their own solutions by explorative learning using the block-based programming environment MakeCode. Additional information to be uncovered by the learner is provided for every single step in the accompanying online wiki website. Results from observations showed that this uncover-function, being a central element of the online material, encouraged the learners to explore their own way in finding a solution with playful elements and increased motivation. The many haptic elements of a makerspace activity are in particular useful for consolidation of the learned and are predisposed for in-class work and deepening the understanding following the constructionism theory. A Design-Based Research (DBR) approach is used to create and evaluate the redesign of a proven example task in a pilot project. Teachers, who are already familiar with the BBC micro:bit and the OER textbook, were trained on how to use the “flip-version” of an example task in their lessons and asked to develop a lesson plan for implementation. The didactic approach to redesigning the material and teacher training was evaluated during the first cycle of DBR. Results from expert interviews showed that the redesigned material and training deliver a solid ground for rework and further research on a larger scale.},
  keywords={Training;Adaptation models;Streaming media;Time measurement;Online services;Distributed Bragg reflectors;Task analysis;microbit;inverted classroom;flipped classroom;makerspace;block-based programming;computational thinking;computer science},
  doi={10.1109/FIE49875.2021.9637170},
  ISSN={2377-634X},
  month={Oct},}@INPROCEEDINGS{8783592,
  author={Kaminski, Márcia Regina and Boscarioli, Clodis},
  booktitle={2018 XIII Latin American Conference on Learning Technologies (LACLO)}, 
  title={Production of Scratch Learning Objects by Elementary School Students}, 
  year={2018},
  volume={},
  number={},
  pages={299-306},
  abstract={The development of Computational Thinking through visual programming using resources such as Scratch has been used frequently in teaching and learning contexts due to the important contributions to these processes. In order to integrate the work of Development of Computational Thinking into the curricular pedagogical content and the protagonism of the students, this paper presents practical suggestions for using Scratch as a tool for Learning Objects production by students for students. The suggestions are the result of the experience developed in a school of the public school of Cascavel/PR whose results show that this integration promotes interest in the content and commitment of the students, making learning meaningful.},
  keywords={Visualization;Software;Production;Programming;Education;Tools;Tornadoes;Computational Thinking;Scratch;Learning Objects Production},
  doi={10.1109/LACLO.2018.00060},
  ISSN={},
  month={Oct},}
